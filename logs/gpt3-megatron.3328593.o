a16n03
10.134.1.120
[2024-03-03 23:52:11,311] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-03 23:52:11,311] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-03 23:52:11,313] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-03 23:52:11,313] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-03 23:52:11,313] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-03 23:52:11,314] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-03 23:52:11,315] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-03 23:52:11,315] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-03 23:52:11,315] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-03 23:52:11,315] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-03 23:52:11,315] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-03 23:52:11,315] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-03 23:52:11,360] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-03 23:52:11,360] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-03 23:52:11,360] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-03 23:52:11,360] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-03 23:52:11,360] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-03 23:52:11,360] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-03 23:52:13,476] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-03 23:52:13,476] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-03 23:52:13,476] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-03 23:52:13,476] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-03 23:52:13,476] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-03 23:52:13,476] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-03 23:52:13,484] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-03 23:52:13,484] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-03 23:52:13,484] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-03 23:52:13,484] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-03 23:52:13,484] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-03 23:52:13,484] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-03 23:52:13,486] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-03 23:52:13,486] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-03 23:52:13,486] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-03 23:52:13,486] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-03 23:52:13,486] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-03 23:52:13,486] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-03 23:52:13,487] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-03 23:52:13,487] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-03 23:52:13,488] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-03 23:52:13,488] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-03 23:52:13,488] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-03 23:52:13,488] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-03 23:52:13,643] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-03 23:52:13,643] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-03 23:52:13,643] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-03 23:52:13,643] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-03 23:52:13,643] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-03 23:52:13,643] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 0
MA = 10.134.1.120
World view:  0 48 10.134.1.120
using world size: 48, data-parallel-size: 1, sequence-parallel size: 1, tensor-model-parallel size: 6, pipeline-model-parallel size: 8 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  add_bias_linear ................................. True
  add_position_embedding .......................... True
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  aml_data_download_path .......................... None
  apply_layernorm_1p .............................. False
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  async_tensor_model_parallel_allreduce ........... False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  barrier_with_L1_time ............................ True
  bert_binary_head ................................ True
  bert_embedder_type .............................. megatron
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... False
  checkpoint_in_cpu ............................... False
  checkpoint_num_layers ........................... 1
  classes_fraction ................................ 1.0
  clip_grad ....................................... 1.0
  compression_training ............................ False
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  create_moe_param_group .......................... False
  curriculum_learning_legacy ...................... False
  data_cache_path ................................. None
  data_efficiency_curriculum_learning ............. False
  data_impl ....................................... infer
  data_parallel_random_init ....................... False
  data_parallel_size .............................. 1
  data_path ....................................... ['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document']
  data_per_class_fraction ......................... 1.0
  data_sharding ................................... True
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_num_layers .............................. None
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... True
  deepspeed_activation_checkpointing .............. False
  deepspeed_config ................................ ds_config.json
  deepspeed_mpi ................................... False
  dino_bottleneck_size ............................ 256
  dino_freeze_last_layer .......................... 1
  dino_head_hidden_size ........................... 2048
  dino_local_crops_number ......................... 10
  dino_local_img_size ............................. 96
  dino_norm_last_layer ............................ False
  dino_teacher_temp ............................... 0.07
  dino_warmup_teacher_temp ........................ 0.04
  dino_warmup_teacher_temp_epochs ................. 30
  distribute_checkpointed_activations ............. False
  distribute_saved_activations .................... False
  distributed_backend ............................. nccl
  distributed_timeout_minutes ..................... 10
  ds_fused_adam ................................... False
  ds_inference .................................... False
  ds_pipeline_enabled ............................. True
  ds_sequence_parallel_size ....................... 1
  embedding_path .................................. None
  embedding_weights_in_fp32 ....................... False
  empty_unused_memory_level ....................... 0
  enable_expert_tensor_parallelism ................ False
  encoder_num_layers .............................. 24
  encoder_seq_length .............................. 2048
  end_weight_decay ................................ 0.1
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... 40
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  exit_on_missing_checkpoint ...................... False
  exit_signal_handler ............................. False
  expert_interval ................................. 2
  ffn_hidden_size ................................. 8256
  finetune ........................................ False
  force_ds_sequence_parallel ...................... False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  fp8_amax_compute_algo ........................... most_recent
  fp8_amax_history_len ............................ 1
  fp8_e4m3 ........................................ False
  fp8_hybrid ...................................... False
  fp8_interval .................................... 1
  fp8_margin ...................................... 0
  fp8_wgrad ....................................... True
  global_batch_size ............................... 1536
  gradient_accumulation_fusion .................... True
  head_lr_mult .................................... 1.0
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 2064
  hidden_size_teacher ............................. None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_h ........................................... 224
  img_w ........................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference ....................................... False
  inference_batch_times_seqlen_threshold .......... 512
  init_method_std ................................. 0.006
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  iter_per_epoch .................................. 1250
  kd .............................................. False
  kd_alpha_ce ..................................... 1
  kd_beta_ce ...................................... 1
  kd_temp ......................................... 1.0
  kv_channels ..................................... 86
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ None
  load_tag ........................................ None
  load_teacher .................................... None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 10
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... False
  log_optimizer_states_to_tensorboard ............. False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  log_world_size_to_tensorboard ................... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 6e-05
  lr_decay_iters .................................. None
  lr_decay_samples ................................ 126953125
  lr_decay_style .................................. cosine
  lr_decay_tokens ................................. None
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 183105
  lr_warmup_tokens ................................ None
  make_vocab_size_divisible_by .................... 128
  mask_factor ..................................... 1.0
  mask_prob ....................................... 0.15
  mask_type ....................................... random
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 2048
  max_tokens_to_oom ............................... 12000
  mem_efficient_ln ................................ True
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... gpt2-merges.txt
  micro_batch_size ................................ 1
  min_loss_scale .................................. 1.0
  min_lr .......................................... 6e-06
  mlp_type ........................................ standard
  mmap_warmup ..................................... False
  moe_eval_capacity_factor ........................ 1.0
  moe_expert_parallel_size ........................ 1
  moe_loss_coeff .................................. 0.1
  moe_min_capacity ................................ 4
  moe_token_dropping .............................. True
  moe_train_capacity_factor ....................... 1.0
  mos ............................................. False
  no_load_lr_state ................................ False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_persist_layer_norm ........................... False
  no_pipeline_parallel ............................ False
  no_save_optim ................................... None
  no_save_rng ..................................... None
  normalization ................................... layernorm
  num_attention_heads ............................. 24
  num_attention_heads_teacher ..................... None
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... [1]
  num_experts_switch .............................. None
  num_experts_teacher ............................. [1]
  num_key_value_heads ............................. 24
  num_layers ...................................... 24
  num_layers_per_virtual_pipeline_stage ........... None
  num_layers_teacher .............................. None
  num_workers ..................................... 2
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  output_bert_embeddings .......................... False
  overlap_p2p_comm ................................ False
  override_opt_param_scheduler .................... False
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  perform_initialization .......................... True
  pipeline_model_parallel_size .................... 8
  pipeline_model_parallel_split_rank .............. None
  profile_backward ................................ False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... ['12', '12', '5859375']
  random_ltd ...................................... False
  rank ............................................ 0
  recompute_granularity ........................... full
  recompute_method ................................ uniform
  recompute_num_layers ............................ 1
  remote_device ................................... none
  repeated_dataloader ............................. False
  reset_attention_mask ............................ False
  reset_iteration ................................. False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  retro_add_retriever ............................. False
  retro_cyclic_train_iters ........................ None
  retro_encoder_attention_dropout ................. 0.1
  retro_encoder_hidden_dropout .................... 0.1
  retro_encoder_layers ............................ 2
  retro_num_neighbors ............................. 2
  retro_num_retrieved_chunks ...................... 2
  retro_return_doc_ids ............................ False
  retro_workdir ................................... None
  return_data_index ............................... False
  rope_theta ...................................... 10000
  rotary_percent .................................. 1.0
  sample_rate ..................................... 1.0
  save ............................................ checkpoints/gpt2_345m
  save_interval ................................... 1000
  scatter_gather_tensors_in_pipeline .............. True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 2048
  sequence_parallel ............................... False
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  skip_train ...................................... False
  split ........................................... 98,2,0
  split_transformers .............................. False
  squared_relu .................................... False
  standalone_embedding_stage ...................... False
  start_weight_decay .............................. 0.1
  swiglu .......................................... False
  swin_backbone_type .............................. tiny
  synchronize_each_layer .......................... False
  tensor_model_parallel_size ...................... 6
  tensorboard_dir ................................. logs/gpt3_175B
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  test_data_path .................................. None
  tile_factor ..................................... 1
  timing_log_level ................................ 0
  timing_log_option ............................... minmax
  titles_data_path ................................ None
  tokenizer_model ................................. None
  tokenizer_type .................................. GPT2BPETokenizer
  topk ............................................ 1
  train_data_exact_num_epochs ..................... None
  train_data_path ................................. None
  train_desc_path ................................. None
  train_doc_idx_path .............................. None
  train_idx_path .................................. None
  train_iters ..................................... None
  train_sample_idx_path ........................... None
  train_samples ................................... 146484375
  train_shuffle_idx_path .......................... None
  train_tokens .................................... None
  transformer_impl ................................ local
  transformer_pipeline_model_parallel_size ........ 8
  universal_checkpoint ............................ False
  untie_embeddings_and_output_weights ............. False
  use_checkpoint_args ............................. False
  use_checkpoint_opt_param_scheduler .............. False
  use_contiguous_buffers_in_local_ddp ............. True
  use_cpu_initialization .......................... None
  use_dataset_only ................................ False
  use_distributed_optimizer ....................... False
  use_flash_attn .................................. False
  use_flash_attn_triton ........................... False
  use_flash_attn_v1 ............................... False
  use_flash_attn_v2 ............................... False
  use_one_sent_docs ............................... False
  use_pin_memory .................................. False
  use_ring_exchange_p2p ........................... False
  use_rotary_position_embeddings .................. False
  use_tutel ....................................... False
  valid_data_path ................................. None
  variable_seq_lengths ............................ False
  virtual_pipeline_model_parallel_size ............ None
  vision_backbone_type ............................ vit
  vision_pretraining .............................. False
  vision_pretraining_type ......................... classify
  vocab_extra_ids ................................. 0
  vocab_file ...................................... gpt2-vocab.json
  vocab_size ...................................... None
  weight_decay .................................... 0.1
  weight_decay_incr_style ......................... constant
  world_size ...................................... 48
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 1.0
-------------------- end of arguments ---------------------
will use batch size rampup starting from global batch size 12 to global batch size 1536 with batch size increments 12 over 5859375 samples.
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 431 dummy tokens (new size: 50688)
> initializing torch distributed ...
> initialized tensor model parallel with size 6
> initialized pipeline model parallel with size 8
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
make: Entering directory '/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/data'
make: Nothing to be done for 'default'.
make: Leaving directory '/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/data'
> compiling dataset index builder ...
>>> done with dataset index builder. Compilation time: 0.073 seconds
> compiling and loading fused kernels ...
ninja: no work to do.
ninja: no work to do.
ninja: no work to do.
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 4
MA = 10.134.1.120
World view:  4 48 10.134.1.120
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 5
MA = 10.134.1.120
World view:  5 48 10.134.1.120
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 3
MA = 10.134.1.120
World view:  3 48 10.134.1.120
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 12
MA = 10.134.1.120
World view:  12 48 10.134.1.120
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 15
MA = 10.134.1.120
World view:  15 48 10.134.1.120
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 7
MA = 10.134.1.120
World view:  7 48 10.134.1.120
>>> done with compiling and loading fused kernels. Compilation time: 2.013 seconds
time to initialize megatron (seconds): 14.632
[after megatron is initialized] datetime: 2024-03-03 23:52:29 
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=False, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=0, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': False, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
building GPT model ...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 9
MA = 10.134.1.120
World view:  9 48 10.134.1.120
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=False, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=125Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=False, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 22
MA = 10.134.1.120
World view:  22 48 10.134.1.120
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=FArgs= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=False, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 8
MA = 10.134.1.120
World view:  8 48 10.134.1.120
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=Fal--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 1
MA = 10.134.1.120
World view:  1 48 10.134.1.120
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=Fal--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 23
MA = 10.134.1.120
World view:  23 48 10.134.1.120
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=F--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 42
MA = 10.134.1.120
World view:  42 48 10.134.1.120
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=F0, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=9, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': False, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
[2024-03-03 23:52:29,088] [INFO] [utils.py:785:see_memory_usage] Before Building Model
se, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=125--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 18
MA = 10.134.1.120
World view:  18 48 10.134.1.120
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=False, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=7, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': FalsArgs= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=False, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=alse, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 46
MA = 10.134.1.120
World view:  46 48 10.134.1.120
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=Fse, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=125None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=4, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': Falsalse, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=42, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': False, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
[2024-03-03 23:52:29,088] [INFO] [utils.py:785:see_memory_usage] Before Building Model
e, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
[2024-03-03 23:52:29,088] [INFO] [utils.py:785:see_memory_usage] Before Building Model
0, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=1, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': False, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
[2024-03-03 23:52:29,088] [INFO] [utils.py:785:see_memory_usage] Before Building Model
alse, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1alse, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=10, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=8, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': False, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
[2024-03-03 23:52:29,088] [INFO] [utils.py:785:see_memory_usage] Before Building Model
None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=5, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': Fals250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=22, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': False, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
[2024-03-03 23:52:29,088] [INFO] [utils.py:785:see_memory_usage] Before Building Model
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 45
MA = 10.134.1.120
World view:  45 48 10.134.1.120
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=F--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 32
MA = 10.134.1.120
World view:  32 48 10.134.1.120
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=F250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=23, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': False, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
[2024-03-03 23:52:29,088] [INFO] [utils.py:785:see_memory_usage] Before Building Model
alse, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 34
MA = 10.134.1.120
World view:  34 48 10.134.1.120
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=F--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 13
MA = 10.134.1.120
World view:  13 48 10.134.1.120
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=Fe, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
[2024-03-03 23:52:29,088] [INFO] [utils.py:785:see_memory_usage] Before Building Model
250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=18, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': False, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
[2024-03-03 23:52:29,088] [INFO] [utils.py:785:see_memory_usage] Before Building Model
250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=46, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': False, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
[2024-03-03 23:52:29,089] [INFO] [utils.py:785:see_memory_usage] Before Building Model
[2024-03-03 23:52:29,089] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
e, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
[2024-03-03 23:52:29,088] [INFO] [utils.py:785:see_memory_usage] Before Building Model
[2024-03-03 23:52:29,089] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=45, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': False, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
[2024-03-03 23:52:29,089] [INFO] [utils.py:785:see_memory_usage] Before Building Model
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 31
MA = 10.134.1.120
World view:  31 48 10.134.1.120
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=F[2024-03-03 23:52:29,089] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
[2024-03-03 23:52:29,089] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
alse, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 39
MA = 10.134.1.120
World view:  39 48 10.134.1.120
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=F--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 16
MA = 10.134.1.120
World view:  16 48 10.134.1.120
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=F[2024-03-03 23:52:29,089] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
[2024-03-03 23:52:29,089] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 50.53 GB, percent = 8.5%
alse, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1[2024-03-03 23:52:29,089] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 50.71 GB, percent = 8.5%
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 44
MA = 10.134.1.120
World view:  44 48 10.134.1.120
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=F--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 24
MA = 10.134.1.120
World view:  24 48 10.134.1.120
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=F[2024-03-03 23:52:29,089] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 17
MA = 10.134.1.120
World view:  17 48 10.134.1.120
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=F[2024-03-03 23:52:29,089] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 50.71 GB, percent = 8.5%
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 43
MA = 10.134.1.120
World view:  43 48 10.134.1.120
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=False, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1alse, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1[2024-03-03 23:52:29,089] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 50.71 GB, percent = 8.5%
alse, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1alse, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1[2024-03-03 23:52:29,089] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 19
MA = 10.134.1.120
World view:  19 48 10.134.1.120
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=False, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=31, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': False, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
[2024-03-03 23:52:29,088] [INFO] [utils.py:785:see_memory_usage] Before Building Model
[2024-03-03 23:52:29,089] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
alse, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=44, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': False, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
[2024-03-03 23:52:29,089] [INFO] [utils.py:785:see_memory_usage] Before Building Model
[2024-03-03 23:52:29,089] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 51.05 GB, percent = 8.5%
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 21
MA = 10.134.1.120
World view:  21 48 10.134.1.120
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=F[2024-03-03 23:52:29,090] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=34, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': False, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
[2024-03-03 23:52:29,088] [INFO] [utils.py:785:see_memory_usage] Before Building Model
alse, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=19, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': False, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
[2024-03-03 23:52:29,089] [INFO] [utils.py:785:see_memory_usage] Before Building Model
250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=43, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': False, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
[2024-03-03 23:52:29,090] [INFO] [utils.py:785:see_memory_usage] Before Building Model
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 28
MA = 10.134.1.120
World view:  28 48 10.134.1.120
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=F[2024-03-03 23:52:29,089] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 51.05 GB, percent = 8.5%
[2024-03-03 23:52:29,089] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
alse, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1[2024-03-03 23:52:29,090] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=32, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': False, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
[2024-03-03 23:52:29,088] [INFO] [utils.py:785:see_memory_usage] Before Building Model
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 20
MA = 10.134.1.120
World view:  20 48 10.134.1.120
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=F[2024-03-03 23:52:29,090] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 50.53 GB, percent = 8.5%
250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=39, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': False, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
[2024-03-03 23:52:29,088] [INFO] [utils.py:785:see_memory_usage] Before Building Model
[2024-03-03 23:52:29,089] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 51.05 GB, percent = 8.5%
alse, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1alse, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1[2024-03-03 23:52:29,090] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 50.53 GB, percent = 8.5%
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 33
MA = 10.134.1.120
World view:  33 48 10.134.1.120
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=F[2024-03-03 23:52:29,089] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=21, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': False, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
[2024-03-03 23:52:29,090] [INFO] [utils.py:785:see_memory_usage] Before Building Model
[2024-03-03 23:52:29,090] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 11
MA = 10.134.1.120
World view:  11 48 10.134.1.120
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=F250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=20, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': False, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
[2024-03-03 23:52:29,090] [INFO] [utils.py:785:see_memory_usage] Before Building Model
[2024-03-03 23:52:29,090] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
[2024-03-03 23:52:29,089] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
alse, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1alse, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1[2024-03-03 23:52:29,090] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
[2024-03-03 23:52:29,091] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 50.53 GB, percent = 8.5%
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 2
MA = 10.134.1.120
World view:  2 48 10.134.1.120
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=Fal--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 10
MA = 10.134.1.120
World view:  10 48 10.134.1.120
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=F[2024-03-03 23:52:29,090] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
[2024-03-03 23:52:29,091] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 50.53 GB, percent = 8.5%
[2024-03-03 23:52:29,089] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
[2024-03-03 23:52:29,091] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
alse, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=24, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': False, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
[2024-03-03 23:52:29,088] [INFO] [utils.py:785:see_memory_usage] Before Building Model
[2024-03-03 23:52:29,091] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 50.71 GB, percent = 8.5%
alse, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1[2024-03-03 23:52:29,089] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 51.22 GB, percent = 8.6%
250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=13, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': False, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
[2024-03-03 23:52:29,088] [INFO] [utils.py:785:see_memory_usage] Before Building Model
[2024-03-03 23:52:29,091] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 50.71 GB, percent = 8.5%
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 6
MA = 10.134.1.120
World view:  6 48 10.134.1.120
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=Fal[2024-03-03 23:52:29,091] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 50.71 GB, percent = 8.5%
[2024-03-03 23:52:29,089] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=28, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': False, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
[2024-03-03 23:52:29,088] [INFO] [utils.py:785:see_memory_usage] Before Building Model
[2024-03-03 23:52:29,089] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 51.22 GB, percent = 8.6%
alse, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=16, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': False, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
[2024-03-03 23:52:29,088] [INFO] [utils.py:785:see_memory_usage] Before Building Model
250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=33, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': False, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
[2024-03-03 23:52:29,089] [INFO] [utils.py:785:see_memory_usage] Before Building Model
250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=11, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': False, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
[2024-03-03 23:52:29,089] [INFO] [utils.py:785:see_memory_usage] Before Building Model
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 29
MA = 10.134.1.120
World view:  29 48 10.134.1.120
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=F--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 30
MA = 10.134.1.120
World view:  30 48 10.134.1.120
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=Fse, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=125--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 36
MA = 10.134.1.120
World view:  36 48 10.134.1.120
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=Fse, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=125[2024-03-03 23:52:29,089] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 50.47 GB, percent = 8.4%
250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=17, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': False, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
[2024-03-03 23:52:29,088] [INFO] [utils.py:785:see_memory_usage] Before Building Model
alse, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1[2024-03-03 23:52:29,089] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 51.22 GB, percent = 8.6%
250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=10, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': False, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
[2024-03-03 23:52:29,089] [INFO] [utils.py:785:see_memory_usage] Before Building Model
[2024-03-03 23:52:29,089] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 50.47 GB, percent = 8.4%
0, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=6, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': False, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
[2024-03-03 23:52:29,089] [INFO] [utils.py:785:see_memory_usage] Before Building Model
250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=29, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': False, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
[2024-03-03 23:52:29,089] [INFO] [utils.py:785:see_memory_usage] Before Building Model
alse, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1[2024-03-03 23:52:29,089] [INFO] [utils.py:785:see_memory_usage] Before Building Model
[2024-03-03 23:52:29,089] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
[2024-03-03 23:52:29,090] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
[2024-03-03 23:52:29,089] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 50.47 GB, percent = 8.4%
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 26
MA = 10.134.1.120
World view:  26 48 10.134.1.120
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=F0, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=2, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': False, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
[2024-03-03 23:52:29,089] [INFO] [utils.py:785:see_memory_usage] Before Building Model
[2024-03-03 23:52:29,090] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
alse, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 35
MA = 10.134.1.120
World view:  35 48 10.134.1.120
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=F[2024-03-03 23:52:29,089] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
[2024-03-03 23:52:29,090] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 51.05 GB, percent = 8.5%
[2024-03-03 23:52:29,089] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=30, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': False, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
[2024-03-03 23:52:29,089] [INFO] [utils.py:785:see_memory_usage] Before Building Model
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=False, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=[2024-03-03 23:52:29,090] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
alse, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1alse, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=3, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': Fals[2024-03-03 23:52:29,090] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 51.05 GB, percent = 8.5%
250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=36, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': False, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
[2024-03-03 23:52:29,089] [INFO] [utils.py:785:see_memory_usage] Before Building Model
250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=35, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': False, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
[2024-03-03 23:52:29,090] [INFO] [utils.py:785:see_memory_usage] Before Building Model
[2024-03-03 23:52:29,089] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
[2024-03-03 23:52:29,091] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 51.05 GB, percent = 8.5%
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 27
MA = 10.134.1.120
World view:  27 48 10.134.1.120
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=F[2024-03-03 23:52:29,090] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
e, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
[2024-03-03 23:52:29,089] [INFO] [utils.py:785:see_memory_usage] Before Building Model
[2024-03-03 23:52:29,090] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 50.47 GB, percent = 8.5%
[2024-03-03 23:52:29,090] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
[2024-03-03 23:52:29,089] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
[2024-03-03 23:52:29,090] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
[2024-03-03 23:52:29,090] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
[2024-03-03 23:52:29,089] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
[2024-03-03 23:52:29,091] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 50.47 GB, percent = 8.5%
[2024-03-03 23:52:29,091] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 50.47 GB, percent = 8.5%
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 14
MA = 10.134.1.120
World view:  14 48 10.134.1.120
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=F--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 25
MA = 10.134.1.120
World view:  25 48 10.134.1.120
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=F[2024-03-03 23:52:29,090] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 51.22 GB, percent = 8.6%
alse, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1[2024-03-03 23:52:29,090] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 41
MA = 10.134.1.120
World view:  41 48 10.134.1.120
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=False, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=26, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': False, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
[2024-03-03 23:52:29,089] [INFO] [utils.py:785:see_memory_usage] Before Building Model
[2024-03-03 23:52:29,090] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
alse, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1[2024-03-03 23:52:29,091] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 51.22 GB, percent = 8.6%
alse, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1[2024-03-03 23:52:29,089] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 51.53 GB, percent = 8.6%
[2024-03-03 23:52:29,090] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 50.09 GB, percent = 8.4%
[2024-03-03 23:52:29,091] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 51.22 GB, percent = 8.6%
250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=27, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': False, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
[2024-03-03 23:52:29,089] [INFO] [utils.py:785:see_memory_usage] Before Building Model
[2024-03-03 23:52:29,089] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 50.84 GB, percent = 8.5%
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=False, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=25, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': False, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
[2024-03-03 23:52:29,089] [INFO] [utils.py:785:see_memory_usage] Before Building Model
[2024-03-03 23:52:29,090] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 50.09 GB, percent = 8.4%
250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=41, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': False, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
[2024-03-03 23:52:29,089] [INFO] [utils.py:785:see_memory_usage] Before Building Model
[2024-03-03 23:52:29,089] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 51.53 GB, percent = 8.6%
[2024-03-03 23:52:29,090] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
[2024-03-03 23:52:29,090] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 50.1 GB, percent = 8.4%
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 38
MA = 10.134.1.120
World view:  38 48 10.134.1.120
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=FArgs= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=False, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=[2024-03-03 23:52:29,090] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
[2024-03-03 23:52:29,090] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
[2024-03-03 23:52:29,090] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=14, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': False, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
[2024-03-03 23:52:29,089] [INFO] [utils.py:785:see_memory_usage] Before Building Model
[2024-03-03 23:52:29,090] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
[2024-03-03 23:52:29,090] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 50.1 GB, percent = 8.4%
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 40
MA = 10.134.1.120
World view:  40 48 10.134.1.120
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=F[2024-03-03 23:52:29,089] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 51.53 GB, percent = 8.6%
[2024-03-03 23:52:29,091] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 50.1 GB, percent = 8.4%
[2024-03-03 23:52:29,091] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 50.1 GB, percent = 8.4%
alse, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=12, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': Falalse, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=15, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': Fal--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 37
MA = 10.134.1.120
World view:  37 48 10.134.1.120
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=Fse, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
[2024-03-03 23:52:29,089] [INFO] [utils.py:785:see_memory_usage] Before Building Model
[2024-03-03 23:52:29,090] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 50.84 GB, percent = 8.5%
se, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
[2024-03-03 23:52:29,089] [INFO] [utils.py:785:see_memory_usage] Before Building Model
250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=40, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': False, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
[2024-03-03 23:52:29,090] [INFO] [utils.py:785:see_memory_usage] Before Building Model
[2024-03-03 23:52:29,090] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=38, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': False, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
[2024-03-03 23:52:29,090] [INFO] [utils.py:785:see_memory_usage] Before Building Model
[2024-03-03 23:52:29,090] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 51.53 GB, percent = 8.6%
alse, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1[2024-03-03 23:52:29,090] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=37, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': False, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
[2024-03-03 23:52:29,090] [INFO] [utils.py:785:see_memory_usage] Before Building Model
[2024-03-03 23:52:29,090] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
[2024-03-03 23:52:29,090] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
[2024-03-03 23:52:29,091] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 51.53 GB, percent = 8.6%
[2024-03-03 23:52:29,090] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 50.84 GB, percent = 8.5%
[2024-03-03 23:52:29,091] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 51.53 GB, percent = 8.6%
[2024-03-03 23:52:29,091] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
[2024-03-03 23:52:29,091] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
[2024-03-03 23:52:29,091] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
[2024-03-03 23:52:29,091] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 50.84 GB, percent = 8.5%
[2024-03-03 23:52:29,091] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 50.84 GB, percent = 8.5%
[2024-03-03 23:52:29,091] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 50.84 GB, percent = 8.5%
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 47
MA = 10.134.1.120
World view:  47 48 10.134.1.120
> setting tensorboard ...
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[1], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2064, ffn_hidden_size=8256, num_attention_heads=24, num_key_value_heads=24, kv_channels=86, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=1536, rampup_batch_size=['12', '12', '5859375'], recompute_granularity='full', distribute_saved_activations=False, recompute_method='uniform', recompute_num_layers=1, checkpoint_activations=False, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=None, train_samples=146484375, train_tokens=None, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=False, tensorboard_dir='logs/gpt3_175B', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.1, create_moe_param_group=False, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=False, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.006, init_method_xavier_uniform=False, lr=6e-05, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=126953125, lr_decay_tokens=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=183105, lr_warmup_tokens=None, min_lr=6e-06, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, save='checkpoints/gpt2_345m', save_interval=1000, no_save_optim=None, no_save_rng=None, load=None, load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=6, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=8, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=1, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=40, eval_interval=1000, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=2, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='infer', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1000, log_timers_to_tensorboard=False, log_batch_size_to_tensorboard=False, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=False, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=False, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=True, rank=47, world_size=48, transformer_pipeline_model_parallel_size=8, data_parallel_size=1, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'fp16': {'enabled': True}, 'train_micro_batch_size_per_gpu': 1, 'train_batch_size': 640, 'optimizer': {'type': 'Adam', 'params': {'torch_adam': False, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}}, 'zero_optimization': {'stage': 1}})
[2024-03-03 23:52:29,128] [INFO] [utils.py:785:see_memory_usage] Before Building Model
[2024-03-03 23:52:29,129] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.09 GB         CA 0.0 GB         Max_CA 0 GB 
[2024-03-03 23:52:29,130] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 50.53 GB, percent = 8.5%
SEED_LAYERS=False BASE_SEED=1234 SEED_FN=None
Using topology: {ProcessCoord(pipe=0, data=0, model=0): 0, ProcessCoord(pipe=0, data=0, model=1): 1, ProcessCoord(pipe=0, data=0, model=2): 2, ProcessCoord(pipe=0, data=0, model=3): 3, ProcessCoord(pipe=0, data=0, model=4): 4, ProcessCoord(pipe=0, data=0, model=5): 5, ProcessCoord(pipe=1, data=0, model=0): 6, ProcessCoord(pipe=1, data=0, model=1): 7, ProcessCoord(pipe=1, data=0, model=2): 8, ProcessCoord(pipe=1, data=0, model=3): 9, ProcessCoord(pipe=1, data=0, model=4): 10, ProcessCoord(pipe=1, data=0, model=5): 11, ProcessCoord(pipe=2, data=0, model=0): 12, ProcessCoord(pipe=2, data=0, model=1): 13, ProcessCoord(pipe=2, data=0, model=2): 14, ProcessCoord(pipe=2, data=0, model=3): 15, ProcessCoord(pipe=2, data=0, model=4): 16, ProcessCoord(pipe=2, data=0, model=5): 17, ProcessCoord(pipe=3, data=0, model=0): 18, ProcessCoord(pipe=3, data=0, model=1): 19, ProcessCoord(pipe=3, data=0, model=2): 20, ProcessCoord(pipe=3, data=0, model=3): 21, ProcessCoord(pipe=3, data=0, model=4): 22, ProcessCoord(pipe=3, data=0, model=5): 23, ProcessCoord(pipe=4, data=0, model=0): 24, ProcessCoord(pipe=4, data=0, model=1): 25, ProcessCoord(pipe=4, data=0, model=2): 26, ProcessCoord(pipe=4, data=0, model=3): 27, ProcessCoord(pipe=4, data=0, model=4): 28, ProcessCoord(pipe=4, data=0, model=5): 29, ProcessCoord(pipe=5, data=0, model=0): 30, ProcessCoord(pipe=5, data=0, model=1): 31, ProcessCoord(pipe=5, data=0, model=2): 32, ProcessCoord(pipe=5, data=0, model=3): 33, ProcessCoord(pipe=5, data=0, model=4): 34, ProcessCoord(pipe=5, data=0, model=5): 35, ProcessCoord(pipe=6, data=0, model=0): 36, ProcessCoord(pipe=6, data=0, model=1): 37, ProcessCoord(pipe=6, data=0, model=2): 38, ProcessCoord(pipe=6, data=0, model=3): 39, ProcessCoord(pipe=6, data=0, model=4): 40, ProcessCoord(pipe=6, data=0, model=5): 41, ProcessCoord(pipe=7, data=0, model=0): 42, ProcessCoord(pipe=7, data=0, model=1): 43, ProcessCoord(pipe=7, data=0, model=2): 44, ProcessCoord(pipe=7, data=0, model=3): 45, ProcessCoord(pipe=7, data=0, model=4): 46, ProcessCoord(pipe=7, data=0, model=5): 47}
[2024-03-03 23:52:29,773] [INFO] [module.py:358:_partition_layers] Partitioning pipeline stages with method type:transformer
 > number of parameters on (tensor, pipeline) model parallel rank (1, 5): 25604952
 > number of parameters on (tensor, pipeline) model parallel rank (2, 1): 25604952
 > number of parameters on (tensor, pipeline) model parallel rank (4, 5): 25604952
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 25604952
 > number of parameters on (tensor, pipeline) model parallel rank (3, 5): 25604952
 > number of parameters on (tensor, pipeline) model parallel rank (5, 6): 25604952
 > number of parameters on (tensor, pipeline) model parallel rank (2, 3): 25604952
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 25604952
 > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 25604952
 > number of parameters on (tensor, pipeline) model parallel rank (5, 5): 25604952
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 25604952
 > number of parameters on (tensor, pipeline) model parallel rank (0, 6): 25604952
 > number of parameters on (tensor, pipeline) model parallel rank (1, 3): 25604952
 > number of parameters on (tensor, pipeline) model parallel rank (5, 1): 25604952
 > number of parameters on (tensor, pipeline) model parallel rank (2, 2): 25604952
 > number of parameters on (tensor, pipeline) model parallel rank (2, 5): 25604952
 > number of parameters on (tensor, pipeline) model parallel rank (4, 3): 25604952
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 25604952
 > number of parameters on (tensor, pipeline) model parallel rank (5, 3): 25604952
 > number of parameters on (tensor, pipeline) model parallel rank (3, 1): 25604952
 > number of parameters on (tensor, pipeline) model parallel rank (3, 7): 47272824
 > number of parameters on (tensor, pipeline) model parallel rank (0, 5): 25604952
 > number of parameters on (tensor, pipeline) model parallel rank (3, 3): 25604952
 > number of parameters on (tensor, pipeline) model parallel rank (1, 6): 25604952
 > number of parameters on (tensor, pipeline) model parallel rank (4, 1): 25604952
 > number of parameters on (tensor, pipeline) model parallel rank (5, 2): 25604952
 > number of parameters on (tensor, pipeline) model parallel rank (4, 4): 25604952
 > number of parameters on (tensor, pipeline) model parallel rank (2, 6): 25604952
 > number of parameters on (tensor, pipeline) model parallel rank (4, 2): 25604952
 > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 25604952
 > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 47268696
 > number of parameters on (tensor, pipeline) model parallel rank (5, 4): 25604952
 > number of parameters on (tensor, pipeline) model parallel rank (4, 6): 25604952
 > number of parameters on (tensor, pipeline) model parallel rank (3, 2): 25604952
 > number of parameters on (tensor, pipeline) model parallel rank (3, 4): 25604952
 > number of parameters on (tensor, pipeline) model parallel rank (3, 6): 25604952
 > number of parameters on (tensor, pipeline) model parallel rank (1, 4): 25604952
 > number of parameters on (tensor, pipeline) model parallel rank (2, 4): 25604952
 > number of parameters on (tensor, pipeline) model parallel rank (0, 7): 47272824
 > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 47268696
 > number of parameters on (tensor, pipeline) model parallel rank (2, 7): 47272824
 > number of parameters on (tensor, pipeline) model parallel rank (5, 7): 47272824
 > number of parameters on (tensor, pipeline) model parallel rank (5, 0): 47268696
 > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 47268696
 > number of parameters on (tensor, pipeline) model parallel rank (4, 7): 47272824
 > number of parameters on (tensor, pipeline) model parallel rank (4, 0): 47268696
 > number of parameters on (tensor, pipeline) model parallel rank (1, 7): 47272824
stage=0 layers=5
     0: _to_float16
     1: EmbeddingPipe
     2: ParallelTransformerLayerPipe
     3: ParallelTransformerLayerPipe
     4: ParallelTransformerLayerPipe
stage=1 layers=3
     5: ParallelTransformerLayerPipe
     6: ParallelTransformerLayerPipe
     7: ParallelTransformerLayerPipe
stage=2 layers=3
     8: ParallelTransformerLayerPipe
     9: ParallelTransformerLayerPipe
    10: ParallelTransformerLayerPipe
stage=3 layers=3
    11: ParallelTransformerLayerPipe
    12: ParallelTransformerLayerPipe
    13: ParallelTransformerLayerPipe
stage=4 layers=3
    14: ParallelTransformerLayerPipe
    15: ParallelTransformerLayerPipe
    16: ParallelTransformerLayerPipe
stage=5 layers=3
    17: ParallelTransformerLayerPipe
    18: ParallelTransformerLayerPipe
    19: ParallelTransformerLayerPipe
stage=6 layers=3
    20: ParallelTransformerLayerPipe
    21: ParallelTransformerLayerPipe
    22: ParallelTransformerLayerPipe
stage=7 layers=6
    23: ParallelTransformerLayerPipe
    24: ParallelTransformerLayerPipe
    25: ParallelTransformerLayerPipe
    26: MixedFusedLayerNorm
    27: EmbeddingPipe
    28: float16_to_fp32
  loss: CrossEntropy
[2024-03-03 23:52:30,229] [INFO] [utils.py:785:see_memory_usage] After Building Model
[2024-03-03 23:52:30,230] [INFO] [utils.py:786:see_memory_usage] MA 0.11 GB         Max_MA 0.13 GB         CA 0.14 GB         Max_CA 0 GB 
[2024-03-03 23:52:30,231] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 52.77 GB, percent = 8.8%
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 47268696
setting training iterations to 112413
> learning rate decay style: cosine
DeepSpeed is enabled.
[2024-03-03 23:52:30,264] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0+f5c834a6e, git-hash=f5c834a6e, git-branch=HEAD
[2024-03-03 23:52:30,311] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-03-03 23:52:30,311] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2024-03-03 23:52:30,312] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-03-03 23:52:30,312] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2024-03-03 23:52:30,312] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'apex.optimizers.fused_adam.FusedAdam'>
[2024-03-03 23:52:30,312] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 1 optimizer
[2024-03-03 23:52:30,312] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500,000,000
[2024-03-03 23:52:30,312] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500,000,000
[2024-03-03 23:52:30,312] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2024-03-03 23:52:30,312] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [1, 1] and sizes[(47224320, False), (44376, False)] 
[2024-03-03 23:52:30,472] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2024-03-03 23:52:30,473] [INFO] [utils.py:786:see_memory_usage] MA 0.27 GB         Max_MA 0.27 GB         CA 0.29 GB         Max_CA 0 GB 
[2024-03-03 23:52:30,474] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 57.02 GB, percent = 9.5%
[2024-03-03 23:52:30,562] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2024-03-03 23:52:30,563] [INFO] [utils.py:786:see_memory_usage] MA 0.62 GB         Max_MA 0.8 GB         CA 0.82 GB         Max_CA 1 GB 
[2024-03-03 23:52:30,563] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 57.57 GB, percent = 9.6%
[2024-03-03 23:52:30,563] [INFO] [stage_1_and_2.py:493:__init__] optimizer state initialized
[2024-03-03 23:52:30,638] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2024-03-03 23:52:30,639] [INFO] [utils.py:786:see_memory_usage] MA 0.62 GB         Max_MA 0.62 GB         CA 0.82 GB         Max_CA 1 GB 
[2024-03-03 23:52:30,639] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 57.57 GB, percent = 9.6%
[2024-03-03 23:52:30,642] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
[2024-03-03 23:52:30,642] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-03-03 23:52:30,642] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.optimizer_param_scheduler.OptimizerParamScheduler object at 0x200127e9b990>
[2024-03-03 23:52:30,642] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]
[2024-03-03 23:52:30,642] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2024-03-03 23:52:30,643] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-03-03 23:52:30,643] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-03-03 23:52:30,643] [INFO] [config.py:964:print]   amp_enabled .................. False
[2024-03-03 23:52:30,643] [INFO] [config.py:964:print]   amp_params ................... False
[2024-03-03 23:52:30,643] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-03-03 23:52:30,643] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2024-03-03 23:52:30,643] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2024-03-03 23:52:30,643] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2024-03-03 23:52:30,643] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2024-03-03 23:52:30,643] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x200127e9be10>
[2024-03-03 23:52:30,643] [INFO] [config.py:964:print]   communication_data_type ...... None
[2024-03-03 23:52:30,643] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-03-03 23:52:30,643] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2024-03-03 23:52:30,644] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2024-03-03 23:52:30,644] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-03-03 23:52:30,644] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2024-03-03 23:52:30,644] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2024-03-03 23:52:30,644] [INFO] [config.py:964:print]   disable_allgather ............ False
[2024-03-03 23:52:30,644] [INFO] [config.py:964:print]   dump_state ................... False
[2024-03-03 23:52:30,644] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2024-03-03 23:52:30,644] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2024-03-03 23:52:30,644] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2024-03-03 23:52:30,644] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-03-03 23:52:30,644] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2024-03-03 23:52:30,644] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2024-03-03 23:52:30,644] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2024-03-03 23:52:30,644] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2024-03-03 23:52:30,644] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2024-03-03 23:52:30,644] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2024-03-03 23:52:30,644] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-03-03 23:52:30,644] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2024-03-03 23:52:30,644] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2024-03-03 23:52:30,644] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2024-03-03 23:52:30,644] [INFO] [config.py:964:print]   global_rank .................. 0
[2024-03-03 23:52:30,644] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2024-03-03 23:52:30,644] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 640
[2024-03-03 23:52:30,644] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2024-03-03 23:52:30,644] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2024-03-03 23:52:30,645] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-03-03 23:52:30,645] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 65536
[2024-03-03 23:52:30,645] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2024-03-03 23:52:30,645] [INFO] [config.py:964:print]   loss_scale ................... 0
[2024-03-03 23:52:30,645] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2024-03-03 23:52:30,645] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2024-03-03 23:52:30,645] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2024-03-03 23:52:30,645] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-03-03 23:52:30,645] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-03-03 23:52:30,645] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2024-03-03 23:52:30,645] [INFO] [config.py:964:print]   optimizer_name ............... adam
[2024-03-03 23:52:30,645] [INFO] [config.py:964:print]   optimizer_params ............. {'torch_adam': False, 'adam_w_mode': False, 'lr': 'auto', 'betas': 'auto', 'eps': 'auto', 'weight_decay': 'auto'}
[2024-03-03 23:52:30,645] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2024-03-03 23:52:30,645] [INFO] [config.py:964:print]   pld_enabled .................. False
[2024-03-03 23:52:30,645] [INFO] [config.py:964:print]   pld_params ................... False
[2024-03-03 23:52:30,645] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2024-03-03 23:52:30,645] [INFO] [config.py:964:print]   scheduler_name ............... None
[2024-03-03 23:52:30,645] [INFO] [config.py:964:print]   scheduler_params ............. None
[2024-03-03 23:52:30,645] [INFO] [config.py:964:print]   sparse_attention ............. None
[2024-03-03 23:52:30,645] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2024-03-03 23:52:30,645] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2024-03-03 23:52:30,645] [INFO] [config.py:964:print]   train_batch_size ............. 640
[2024-03-03 23:52:30,645] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2024-03-03 23:52:30,646] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2024-03-03 23:52:30,646] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2024-03-03 23:52:30,646] [INFO] [config.py:964:print]   world_size ................... 1
[2024-03-03 23:52:30,646] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  False
[2024-03-03 23:52:30,646] [INFO] [config.py:964:print]   zero_config .................. stage=1 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2024-03-03 23:52:30,646] [INFO] [config.py:964:print]   zero_enabled ................. True
[2024-03-03 23:52:30,646] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2024-03-03 23:52:30,646] [INFO] [config.py:964:print]   zero_optimization_stage ...... 1
[2024-03-03 23:52:30,646] [INFO] [config.py:950:print_user_config]   json = {
    "fp16": {
        "enabled": true
    }, 
    "train_micro_batch_size_per_gpu": 1, 
    "train_batch_size": 640, 
    "optimizer": {
        "type": "Adam", 
        "params": {
            "torch_adam": false, 
            "adam_w_mode": false, 
            "lr": "auto", 
            "betas": "auto", 
            "eps": "auto", 
            "weight_decay": "auto"
        }
    }, 
    "zero_optimization": {
        "stage": 1
    }
}
[2024-03-03 23:52:30,646] [INFO] [engine.py:83:__init__] CONFIG: micro_batches=640 micro_batch_size=1
Rank: 7 partition count [1, 1] and sizes[(25560576, False), (44376, False)] 
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=7 STAGE=1 LAYERS=3 [5, 8) STAGE_PARAMS=25604952 (25.605M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
Rank: 2 partition count [1, 1] and sizes[(47224320, False), (44376, False)] 
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=2 STAGE=0 LAYERS=5 [0, 5) STAGE_PARAMS=47268696 (47.269M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
Rank: 42 partition count [1, 1] and sizes[(47224320, False), (48504, False)] 
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=42 STAGE=7 LAYERS=6 [23, 29) STAGE_PARAMS=47272824 (47.273M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
Rank: 18 partition count [1, 1] and sizes[(25560576, False), (44376, False)] 
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=18 STAGE=3 LAYERS=3 [11, 14) STAGE_PARAMS=25604952 (25.605M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
Rank: 11 partition count [1, 1] and sizes[(25560576, False), (44376, False)] 
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=11 STAGE=1 LAYERS=3 [5, 8) STAGE_PARAMS=25604952 (25.605M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
Rank: 5 partition count [1, 1] and sizes[(47224320, False), (44376, False)] 
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=5 STAGE=0 LAYERS=5 [0, 5) STAGE_PARAMS=47268696 (47.269M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
Rank: 47 partition count [1, 1] and sizes[(47224320, False), (48504, False)] 
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=47 STAGE=7 LAYERS=6 [23, 29) STAGE_PARAMS=47272824 (47.273M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
Rank: 20 partition count [1, 1] and sizes[(25560576, False), (44376, False)] 
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=20 STAGE=3 LAYERS=3 [11, 14) STAGE_PARAMS=25604952 (25.605M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
Rank: 8 partition count [1, 1] and sizes[(25560576, False), (44376, False)] 
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=8 STAGE=1 LAYERS=3 [5, 8) STAGE_PARAMS=25604952 (25.605M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
Rank: 43 partition count [1, 1] and sizes[(47224320, False), (48504, False)] 
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=43 STAGE=7 LAYERS=6 [23, 29) STAGE_PARAMS=47272824 (47.273M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
Rank: 1 partition count [1, 1] and sizes[(47224320, False), (44376, False)] 
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=1 STAGE=0 LAYERS=5 [0, 5) STAGE_PARAMS=47268696 (47.269M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
Rank: 19 partition count [1, 1] and sizes[(25560576, False), (44376, False)] 
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=19 STAGE=3 LAYERS=3 [11, 14) STAGE_PARAMS=25604952 (25.605M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
Rank: 10 partition count [1, 1] and sizes[(25560576, False), (44376, False)] 
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=10 STAGE=1 LAYERS=3 [5, 8) STAGE_PARAMS=25604952 (25.605M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
Rank: 44 partition count [1, 1] and sizes[(47224320, False), (48504, False)] 
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=44 STAGE=7 LAYERS=6 [23, 29) STAGE_PARAMS=47272824 (47.273M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
Rank: 21 partition count [1, 1] and sizes[(25560576, False), (44376, False)] 
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=21 STAGE=3 LAYERS=3 [11, 14) STAGE_PARAMS=25604952 (25.605M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
Rank: 3 partition count [1, 1] and sizes[(47224320, False), (44376, False)] 
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=3 STAGE=0 LAYERS=5 [0, 5) STAGE_PARAMS=47268696 (47.269M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
Rank: 45 partition count [1, 1] and sizes[(47224320, False), (48504, False)] 
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=45 STAGE=7 LAYERS=6 [23, 29) STAGE_PARAMS=47272824 (47.273M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
Rank: 6 partition count [1, 1] and sizes[(25560576, False), (44376, False)] 
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=6 STAGE=1 LAYERS=3 [5, 8) STAGE_PARAMS=25604952 (25.605M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
Rank: 22 partition count [1, 1] and sizes[(25560576, False), (44376, False)] 
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=22 STAGE=3 LAYERS=3 [11, 14) STAGE_PARAMS=25604952 (25.605M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
Rank: 4 partition count [1, 1] and sizes[(47224320, False), (44376, False)] 
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=4 STAGE=0 LAYERS=5 [0, 5) STAGE_PARAMS=47268696 (47.269M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
Rank: 46 partition count [1, 1] and sizes[(47224320, False), (48504, False)] 
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=46 STAGE=7 LAYERS=6 [23, 29) STAGE_PARAMS=47272824 (47.273M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
Rank: 9 partition count [1, 1] and sizes[(25560576, False), (44376, False)] 
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=9 STAGE=1 LAYERS=3 [5, 8) STAGE_PARAMS=25604952 (25.605M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
Rank: 13 partition count [1, 1] and sizes[(25560576, False), (44376, False)] 
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=13 STAGE=2 LAYERS=3 [8, 11) STAGE_PARAMS=25604952 (25.605M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
Rank: 23 partition count [1, 1] and sizes[(25560576, False), (44376, False)] 
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=23 STAGE=3 LAYERS=3 [11, 14) STAGE_PARAMS=25604952 (25.605M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
Rank: 31 partition count [1, 1] and sizes[(25560576, False), (44376, False)] 
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=31 STAGE=5 LAYERS=3 [17, 20) STAGE_PARAMS=25604952 (25.605M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
Rank: 30 partition count [1, 1] and sizes[(25560576, False), (44376, False)] 
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=30 STAGE=5 LAYERS=3 [17, 20) STAGE_PARAMS=25604952 (25.605M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=0 STAGE=0 LAYERS=5 [0, 5) STAGE_PARAMS=47268696 (47.269M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
Rank: 16 partition count [1, 1] and sizes[(25560576, False), (44376, False)] 
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=16 STAGE=2 LAYERS=3 [8, 11) STAGE_PARAMS=25604952 (25.605M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
Rank: 32 partition count [1, 1] and sizes[(25560576, False), (44376, False)] 
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=32 STAGE=5 LAYERS=3 [17, 20) STAGE_PARAMS=25604952 (25.605M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
Rank: 25 partition count [1, 1] and sizes[(25560576, False), (44376, False)] 
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=25 STAGE=4 LAYERS=3 [14, 17) STAGE_PARAMS=25604952 (25.605M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
Rank: 33 partition count [1, 1] and sizes[(25560576, False), (44376, False)] 
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=33 STAGE=5 LAYERS=3 [17, 20) STAGE_PARAMS=25604952 (25.605M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
Rank: 34 partition count [1, 1] and sizes[(25560576, False), (44376, False)] 
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=34 STAGE=5 LAYERS=3 [17, 20) STAGE_PARAMS=25604952 (25.605M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
Rank: 26 partition count [1, 1] and sizes[(25560576, False), (44376, False)] 
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=26 STAGE=4 LAYERS=3 [14, 17) STAGE_PARAMS=25604952 (25.605M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
Rank: 14 partition count [1, 1] and sizes[(25560576, False), (44376, False)] 
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=14 STAGE=2 LAYERS=3 [8, 11) STAGE_PARAMS=25604952 (25.605M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
Rank: 35 partition count [1, 1] and sizes[(25560576, False), (44376, False)] 
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=35 STAGE=5 LAYERS=3 [17, 20) STAGE_PARAMS=25604952 (25.605M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
Rank: 27 partition count [1, 1] and sizes[(25560576, False), (44376, False)] 
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=27 STAGE=4 LAYERS=3 [14, 17) STAGE_PARAMS=25604952 (25.605M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
Rank: 12 partition count [1, 1] and sizes[(25560576, False), (44376, False)] 
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=12 STAGE=2 LAYERS=3 [8, 11) STAGE_PARAMS=25604952 (25.605M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
Rank: 29 partition count [1, 1] and sizes[(25560576, False), (44376, False)] 
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=29 STAGE=4 LAYERS=3 [14, 17) STAGE_PARAMS=25604952 (25.605M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
Rank: 17 partition count [1, 1] and sizes[(25560576, False), (44376, False)] 
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=17 STAGE=2 LAYERS=3 [8, 11) STAGE_PARAMS=25604952 (25.605M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
Rank: 24 partition count [1, 1] and sizes[(25560576, False), (44376, False)] 
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=24 STAGE=4 LAYERS=3 [14, 17) STAGE_PARAMS=25604952 (25.605M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
Rank: 15 partition count [1, 1] and sizes[(25560576, False), (44376, False)] 
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=15 STAGE=2 LAYERS=3 [8, 11) STAGE_PARAMS=25604952 (25.605M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
Rank: 38 partition count [1, 1] and sizes[(25560576, False), (44376, False)] 
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=38 STAGE=6 LAYERS=3 [20, 23) STAGE_PARAMS=25604952 (25.605M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
Rank: 28 partition count [1, 1] and sizes[(25560576, False), (44376, False)] 
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=28 STAGE=4 LAYERS=3 [14, 17) STAGE_PARAMS=25604952 (25.605M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
Rank: 39 partition count [1, 1] and sizes[(25560576, False), (44376, False)] 
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=39 STAGE=6 LAYERS=3 [20, 23) STAGE_PARAMS=25604952 (25.605M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
Rank: 36 partition count [1, 1] and sizes[(25560576, False), (44376, False)] 
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=36 STAGE=6 LAYERS=3 [20, 23) STAGE_PARAMS=25604952 (25.605M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
Rank: 40 partition count [1, 1] and sizes[(25560576, False), (44376, False)] 
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=40 STAGE=6 LAYERS=3 [20, 23) STAGE_PARAMS=25604952 (25.605M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
Rank: 37 partition count [1, 1] and sizes[(25560576, False), (44376, False)] 
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=37 STAGE=6 LAYERS=3 [20, 23) STAGE_PARAMS=25604952 (25.605M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
Rank: 41 partition count [1, 1] and sizes[(25560576, False), (44376, False)] 
[2024-03-03 23:52:31,456] [INFO] [engine.py:138:__init__] RANK=41 STAGE=6 LAYERS=3 [20, 23) STAGE_PARAMS=25604952 (25.605M) TOTAL_PARAMS=1489027392 (1489.027M) UNIQUE_PARAMS=1359044928 (1359.045M)
[after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-03 23:52:31 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      146484375
    validation: 6942720
    test:       61440
> building train, validation, and test datasets for GPT ...
Single data path provided for train, valid & test
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.019520 seconds
    number of documents: 27933
 > dataset split:
    train:
     document indices in [0, 27374) total of 27374 documents
    validation:
     document indices in [27374, 27933) total of 559 documents
    test:
     document indices in [27933, 27933) total of 0 documents
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > last epoch number of samples (874) is smaller than 80% of number of samples per epoch (4714), setting separate_last_epoch to True
 > elasped time to build and save doc-idx mapping (seconds): 77.408596
    using:
     number of documents:       27374
     number of epochs:          31072
     sequence length:           2048
     total number of samples:   146488215
 > elasped time to build and save sample-idx mapping (seconds): 3.604007
 > building shuffle index with split [0, 146483501) and [146483501, 146488215) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 11.018240
 > loading doc-idx mapping from /gpfs/alpine2/stf218/world-shared/sajal/gptdata/index-cache/aeb891501082bd2f872cf9299ed493fd_doc_idx.npy
 > loading sample-idx mapping from /gpfs/alpine2/stf218/world-shared/sajal/gptdata/index-cache/aeb891501082bd2f872cf9299ed493fd_sample_idx.npy
 > loading shuffle-idx mapping from /gpfs/alpine2/stf218/world-shared/sajal/gptdata/index-cache/aeb891501082bd2f872cf9299ed493fd_shuffle_idx.npy
    loaded indexed file in 0.042 seconds
    total number of samples: 146488216
    total number of epochs: 31072
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > last epoch number of samples (41) is smaller than 80% of number of samples per epoch (82), setting separate_last_epoch to True
 > elasped time to build and save doc-idx mapping (seconds): 3.567324
    using:
     number of documents:       559
     number of epochs:          84421
     sequence length:           2048
     total number of samples:   6942761
 > elasped time to build and save sample-idx mapping (seconds): 0.179006
 > building shuffle index with split [0, 6942679) and [6942679, 6942761) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.321582
 > loading doc-idx mapping from /gpfs/alpine2/stf218/world-shared/sajal/gptdata/index-cache/cbc9cf027dacb2b040eb64abc9d93a0a_doc_idx.npy
 > loading sample-idx mapping from /gpfs/alpine2/stf218/world-shared/sajal/gptdata/index-cache/cbc9cf027dacb2b040eb64abc9d93a0a_sample_idx.npy
 > loading shuffle-idx mapping from /gpfs/alpine2/stf218/world-shared/sajal/gptdata/index-cache/cbc9cf027dacb2b040eb64abc9d93a0a_shuffle_idx.npy
    loaded indexed file in 0.007 seconds
    total number of samples: 6942762
    total number of epochs: 84421
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2024-03-03 23:54:08 
done with setup ...
training ...
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (2534.30, 2945.34)
    train/valid/test-data-iterators-setup ..........: (96818.33, 96962.05)
[before the start of training step] datetime: 2024-03-03 23:54:09 
[2024-03-03 23:54:09,293] [INFO] [checkpointing.py:529:forward] Activation Checkpointing Information
[2024-03-03 23:54:09,293] [INFO] [checkpointing.py:530:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2024-03-03 23:54:09,293] [INFO] [checkpointing.py:531:forward] ----contiguous Memory Checkpointing False with None total layers
[2024-03-03 23:54:09,293] [INFO] [checkpointing.py:533:forward] ----Synchronization False
[2024-03-03 23:54:09,293] [INFO] [checkpointing.py:534:forward] ----Profiling time in checkpointing False
[2024-03-03 23:54:25,327] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648
[2024-03-03 23:54:26,321] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2147483648, reducing to 1073741824
[2024-03-03 23:54:27,181] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1073741824, reducing to 536870912
[2024-03-03 23:54:28,043] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 536870912, reducing to 268435456
[2024-03-03 23:54:28,899] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 268435456, reducing to 134217728
[2024-03-03 23:54:29,769] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 134217728, reducing to 67108864
[2024-03-03 23:54:30,622] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 67108864, reducing to 33554432
[2024-03-03 23:54:31,480] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 33554432, reducing to 16777216
[2024-03-03 23:54:32,339] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16777216, reducing to 8388608
[2024-03-03 23:54:33,190] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8388608, reducing to 4194304
[2024-03-03 23:54:33,191] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=10, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration       10/  112413 | consumed samples:          120 | consumed tokens:       245760 | elapsed time per iteration (ms): 2404.0 | learning rate: 0.000E+00 | global batch size:    12 | lm loss: 1.087298E+01 | loss scale: 4194304.0 | grad norm: 0.000 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 4.992 | tokens per gpu per second (tgs): 212.977 | TFLOPs: 2.57 |
steps: 10 loss: 10.8750 iter time (s): 2.403 samples/sec: 4.993
[2024-03-03 23:54:34,052] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4194304, reducing to 2097152
[2024-03-03 23:54:34,920] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2097152, reducing to 1048576
[2024-03-03 23:54:35,779] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1048576, reducing to 524288
[2024-03-03 23:54:36,644] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 524288, reducing to 262144
[2024-03-03 23:54:37,503] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, reducing to 131072
[2024-03-03 23:54:41,932] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=15, lr=[1.9660850331776848e-08, 1.9660850331776848e-08], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration       20/  112413 | consumed samples:          240 | consumed tokens:       491520 | elapsed time per iteration (ms): 874.3 | learning rate: 1.966E-08 | global batch size:    12 | lm loss: 1.087314E+01 | loss scale: 131072.0 | grad norm: 31.663 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.725 | tokens per gpu per second (tgs): 585.618 | TFLOPs: 7.07 |
[Rank 3] (after 20 iterations) memory (MB) | allocated: 808.44921875 | max allocated: 1328.02880859375 | reserved: 1830.0 | max reserved: 1830.0
[Rank 22] (after 20 iterations) memory (MB) | allocated: 447.13818359375 | max allocated: 812.568359375 | reserved: 1030.0 | max reserved: 1030.0
[Rank 43] (after 20 iterations) memory (MB) | allocated: 941.1767578125 | max allocated: 1304.54248046875 | reserved: 1798.0 | max reserved: 1798.0
[Rank 1] (after 20 iterations) memory (MB) | allocated: 807.9599609375 | max allocated: 1326.2373046875 | reserved: 1866.0 | max reserved: 1866.0
[Rank 7] (after 20 iterations) memory (MB) | allocated: 447.13818359375 | max allocated: 862.9443359375 | reserved: 1060.0 | max reserved: 1060.0
[Rank 21] (after 20 iterations) memory (MB) | allocated: 447.13818359375 | max allocated: 812.568359375 | reserved: 1030.0 | max reserved: 1030.0
[Rank 44] (after 20 iterations) memory (MB) | allocated: 941.1767578125 | max allocated: 1303.09375 | reserved: 1730.0 | max reserved: 1730.0
[Rank 2] (after 20 iterations) memory (MB) | allocated: 807.9287109375 | max allocated: 1325.8515625 | reserved: 1830.0 | max reserved: 1830.0
[Rank 9] (after 20 iterations) memory (MB) | allocated: 447.13818359375 | max allocated: 862.9443359375 | reserved: 1060.0 | max reserved: 1060.0
[Rank 23] (after 20 iterations) memory (MB) | allocated: 447.13818359375 | max allocated: 812.568359375 | reserved: 1030.0 | max reserved: 1030.0
[Rank 45] (after 20 iterations) memory (MB) | allocated: 941.1767578125 | max allocated: 1303.34375 | reserved: 1730.0 | max reserved: 1730.0
[Rank 8] (after 20 iterations) memory (MB) | allocated: 447.13818359375 | max allocated: 862.9443359375 | reserved: 1060.0 | max reserved: 1060.0
[Rank 20] (after 20 iterations) memory (MB) | allocated: 447.13818359375 | max allocated: 812.568359375 | reserved: 1030.0 | max reserved: 1030.0
[Rank 46] (after 20 iterations) memory (MB) | allocated: 941.1767578125 | max allocated: 1303.4677734375 | reserved: 1796.0 | max reserved: 1796.0
[Rank 5] (after 20 iterations) memory (MB) | allocated: 807.9287109375 | max allocated: 1326.8515625 | reserved: 1864.0 | max reserved: 1864.0
[Rank 32] (after 20 iterations) memory (MB) | allocated: 447.13818359375 | max allocated: 763.41845703125 | reserved: 966.0 | max reserved: 966.0
[Rank 18] (after 20 iterations) memory (MB) | allocated: 447.31494140625 | max allocated: 811.0166015625 | reserved: 1014.0 | max reserved: 1014.0
[Rank 47] (after 20 iterations) memory (MB) | allocated: 941.1767578125 | max allocated: 1304.45849609375 | reserved: 1730.0 | max reserved: 1730.0
[Rank 10] (after 20 iterations) memory (MB) | allocated: 447.13818359375 | max allocated: 862.9443359375 | reserved: 1060.0 | max reserved: 1060.0
steps: 20 loss: 10.8751 iter time (s): 0.873 samples/sec: 13.742
[Rank 0] (after 20 iterations) memory (MB) | allocated: 808.04296875 | max allocated: 1326.21533203125 | reserved: 1830.0 | max reserved: 1830.0
[Rank 27] (after 20 iterations) memory (MB) | allocated: 447.13818359375 | max allocated: 787.70263671875 | reserved: 998.0 | max reserved: 998.0
[Rank 19] (after 20 iterations) memory (MB) | allocated: 447.13818359375 | max allocated: 813.5146484375 | reserved: 1030.0 | max reserved: 1030.0
[Rank 42] (after 20 iterations) memory (MB) | allocated: 941.1767578125 | max allocated: 1303.76513671875 | reserved: 1730.0 | max reserved: 1730.0
[Rank 33] (after 20 iterations) memory (MB) | allocated: 447.13818359375 | max allocated: 763.41845703125 | reserved: 966.0 | max reserved: 966.0
[Rank 13] (after 20 iterations) memory (MB) | allocated: 447.13818359375 | max allocated: 837.47509765625 | reserved: 1062.0 | max reserved: 1062.0
[Rank 11] (after 20 iterations) memory (MB) | allocated: 447.13818359375 | max allocated: 862.9443359375 | reserved: 1060.0 | max reserved: 1060.0
[Rank 4] (after 20 iterations) memory (MB) | allocated: 807.9599609375 | max allocated: 1326.23486328125 | reserved: 1830.0 | max reserved: 1830.0
[Rank 30] (after 20 iterations) memory (MB) | allocated: 447.13818359375 | max allocated: 761.1083984375 | reserved: 984.0 | max reserved: 984.0
[Rank 25] (after 20 iterations) memory (MB) | allocated: 447.13818359375 | max allocated: 787.70263671875 | reserved: 998.0 | max reserved: 998.0
[Rank 6] (after 20 iterations) memory (MB) | allocated: 447.13818359375 | max allocated: 862.97509765625 | reserved: 1078.0 | max reserved: 1078.0
[Rank 16] (after 20 iterations) memory (MB) | allocated: 447.13818359375 | max allocated: 837.47509765625 | reserved: 1062.0 | max reserved: 1062.0
[Rank 34] (after 20 iterations) memory (MB) | allocated: 447.13818359375 | max allocated: 763.41845703125 | reserved: 966.0 | max reserved: 966.0
[Rank 29] (after 20 iterations) memory (MB) | allocated: 447.13818359375 | max allocated: 788.69189453125 | reserved: 998.0 | max reserved: 998.0
[Rank 35] (after 20 iterations) memory (MB) | allocated: 447.13818359375 | max allocated: 763.41845703125 | reserved: 966.0 | max reserved: 966.0
[Rank 31] (after 20 iterations) memory (MB) | allocated: 447.13818359375 | max allocated: 763.41845703125 | reserved: 966.0 | max reserved: 966.0
[Rank 15] (after 20 iterations) memory (MB) | allocated: 447.13818359375 | max allocated: 837.47509765625 | reserved: 1062.0 | max reserved: 1062.0
[Rank 26] (after 20 iterations) memory (MB) | allocated: 447.13818359375 | max allocated: 788.69189453125 | reserved: 998.0 | max reserved: 998.0
[Rank 12] (after 20 iterations) memory (MB) | allocated: 447.13818359375 | max allocated: 837.43359375 | reserved: 1012.0 | max reserved: 1012.0
[Rank 28] (after 20 iterations) memory (MB) | allocated: 447.13818359375 | max allocated: 788.69189453125 | reserved: 998.0 | max reserved: 998.0
[Rank 37] (after 20 iterations) memory (MB) | allocated: 447.78369140625 | max allocated: 734.91064453125 | reserved: 922.0 | max reserved: 922.0
[Rank 24] (after 20 iterations) memory (MB) | allocated: 447.31494140625 | max allocated: 786.22412109375 | reserved: 982.0 | max reserved: 982.0
[Rank 14] (after 20 iterations) memory (MB) | allocated: 447.13818359375 | max allocated: 837.47509765625 | reserved: 1062.0 | max reserved: 1062.0
[Rank 17] (after 20 iterations) memory (MB) | allocated: 447.13818359375 | max allocated: 837.47509765625 | reserved: 1062.0 | max reserved: 1062.0
[Rank 38] (after 20 iterations) memory (MB) | allocated: 447.78369140625 | max allocated: 734.91064453125 | reserved: 922.0 | max reserved: 922.0
[Rank 39] (after 20 iterations) memory (MB) | allocated: 447.78369140625 | max allocated: 734.91064453125 | reserved: 922.0 | max reserved: 922.0
[Rank 41] (after 20 iterations) memory (MB) | allocated: 447.78369140625 | max allocated: 734.91064453125 | reserved: 922.0 | max reserved: 922.0
[Rank 40] (after 20 iterations) memory (MB) | allocated: 447.78369140625 | max allocated: 734.91064453125 | reserved: 922.0 | max reserved: 922.0
[Rank 36] (after 20 iterations) memory (MB) | allocated: 447.78369140625 | max allocated: 735.99365234375 | reserved: 952.0 | max reserved: 952.0
[2024-03-03 23:54:50,630] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=15, lr=[5.898255099533055e-08, 5.898255099533055e-08], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration       30/  112413 | consumed samples:          360 | consumed tokens:       737280 | elapsed time per iteration (ms): 869.9 | learning rate: 5.898E-08 | global batch size:    12 | lm loss: 1.083413E+01 | loss scale: 131072.0 | grad norm: 30.303 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.795 | tokens per gpu per second (tgs): 588.599 | TFLOPs: 7.10 |
steps: 30 loss: 10.7696 iter time (s): 0.869 samples/sec: 13.813
[2024-03-03 23:54:59,246] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=15, lr=[9.830425165888426e-08, 9.830425165888426e-08], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration       40/  112413 | consumed samples:          480 | consumed tokens:       983040 | elapsed time per iteration (ms): 861.5 | learning rate: 9.830E-08 | global batch size:    12 | lm loss: 1.061626E+01 | loss scale: 131072.0 | grad norm: 22.712 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.929 | tokens per gpu per second (tgs): 594.296 | TFLOPs: 7.17 |
steps: 40 loss: 10.5096 iter time (s): 0.860 samples/sec: 13.946
[2024-03-03 23:55:07,870] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=15, lr=[1.3762595232243794e-07, 1.3762595232243794e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration       50/  112413 | consumed samples:          600 | consumed tokens:      1228800 | elapsed time per iteration (ms): 862.4 | learning rate: 1.376E-07 | global batch size:    12 | lm loss: 1.036064E+01 | loss scale: 131072.0 | grad norm: 15.603 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.914 | tokens per gpu per second (tgs): 593.683 | TFLOPs: 7.16 |
steps: 50 loss: 10.2534 iter time (s): 0.861 samples/sec: 13.931
[2024-03-03 23:55:16,519] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=15, lr=[1.7694765298599163e-07, 1.7694765298599163e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration       60/  112413 | consumed samples:          720 | consumed tokens:      1474560 | elapsed time per iteration (ms): 864.9 | learning rate: 1.769E-07 | global batch size:    12 | lm loss: 1.011059E+01 | loss scale: 131072.0 | grad norm: 10.655 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.875 | tokens per gpu per second (tgs): 591.989 | TFLOPs: 7.14 |
steps: 60 loss: 10.0267 iter time (s): 0.864 samples/sec: 13.892
[2024-03-03 23:55:25,200] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=15, lr=[2.1626935364954536e-07, 2.1626935364954536e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration       70/  112413 | consumed samples:          840 | consumed tokens:      1720320 | elapsed time per iteration (ms): 868.1 | learning rate: 2.163E-07 | global batch size:    12 | lm loss: 9.949129E+00 | loss scale: 131072.0 | grad norm: 7.260 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.823 | tokens per gpu per second (tgs): 589.764 | TFLOPs: 7.12 |
steps: 70 loss: 9.8802 iter time (s): 0.867 samples/sec: 13.839
[2024-03-03 23:55:33,829] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=15, lr=[2.5559105431309904e-07, 2.5559105431309904e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration       80/  112413 | consumed samples:          960 | consumed tokens:      1966080 | elapsed time per iteration (ms): 862.9 | learning rate: 2.556E-07 | global batch size:    12 | lm loss: 9.826001E+00 | loss scale: 131072.0 | grad norm: 5.512 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.907 | tokens per gpu per second (tgs): 593.376 | TFLOPs: 7.16 |
steps: 80 loss: 9.7753 iter time (s): 0.862 samples/sec: 13.924
[2024-03-03 23:55:42,477] [INFO] [logging.py:96:log_dist] [Rank 0] step=90, skipped=15, lr=[2.9491275497665273e-07, 2.9491275497665273e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration       90/  112413 | consumed samples:         1080 | consumed tokens:      2211840 | elapsed time per iteration (ms): 864.8 | learning rate: 2.949E-07 | global batch size:    12 | lm loss: 9.750729E+00 | loss scale: 131072.0 | grad norm: 7.874 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.876 | tokens per gpu per second (tgs): 592.031 | TFLOPs: 7.14 |
steps: 90 loss: 9.6498 iter time (s): 0.864 samples/sec: 13.893
[2024-03-03 23:55:51,121] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=15, lr=[3.3423445564020646e-07, 3.3423445564020646e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      100/  112413 | consumed samples:         1200 | consumed tokens:      2457600 | elapsed time per iteration (ms): 864.3 | learning rate: 3.342E-07 | global batch size:    12 | lm loss: 9.704304E+00 | loss scale: 131072.0 | grad norm: 4.714 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.884 | tokens per gpu per second (tgs): 592.384 | TFLOPs: 7.15 |
steps: 100 loss: 9.7252 iter time (s): 0.863 samples/sec: 13.901
[2024-03-03 23:55:59,770] [INFO] [logging.py:96:log_dist] [Rank 0] step=110, skipped=15, lr=[3.7355615630376015e-07, 3.7355615630376015e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      110/  112413 | consumed samples:         1320 | consumed tokens:      2703360 | elapsed time per iteration (ms): 864.9 | learning rate: 3.736E-07 | global batch size:    12 | lm loss: 9.676561E+00 | loss scale: 131072.0 | grad norm: 4.508 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.874 | tokens per gpu per second (tgs): 591.946 | TFLOPs: 7.14 |
steps: 110 loss: 9.6658 iter time (s): 0.864 samples/sec: 13.891
[2024-03-03 23:56:08,393] [INFO] [logging.py:96:log_dist] [Rank 0] step=120, skipped=15, lr=[4.1287785696731383e-07, 4.1287785696731383e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      120/  112413 | consumed samples:         1440 | consumed tokens:      2949120 | elapsed time per iteration (ms): 862.2 | learning rate: 4.129E-07 | global batch size:    12 | lm loss: 9.632957E+00 | loss scale: 131072.0 | grad norm: 4.612 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.918 | tokens per gpu per second (tgs): 593.818 | TFLOPs: 7.17 |
steps: 120 loss: 9.6112 iter time (s): 0.861 samples/sec: 13.934
[2024-03-03 23:56:17,022] [INFO] [logging.py:96:log_dist] [Rank 0] step=130, skipped=15, lr=[4.521995576308675e-07, 4.521995576308675e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      130/  112413 | consumed samples:         1560 | consumed tokens:      3194880 | elapsed time per iteration (ms): 863.0 | learning rate: 4.522E-07 | global batch size:    12 | lm loss: 9.585786E+00 | loss scale: 131072.0 | grad norm: 4.364 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.906 | tokens per gpu per second (tgs): 593.302 | TFLOPs: 7.16 |
steps: 130 loss: 9.5570 iter time (s): 0.862 samples/sec: 13.923
[2024-03-03 23:56:25,651] [INFO] [logging.py:96:log_dist] [Rank 0] step=140, skipped=15, lr=[4.915212582944212e-07, 4.915212582944212e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      140/  112413 | consumed samples:         1680 | consumed tokens:      3440640 | elapsed time per iteration (ms): 862.9 | learning rate: 4.915E-07 | global batch size:    12 | lm loss: 9.521369E+00 | loss scale: 131072.0 | grad norm: 4.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.907 | tokens per gpu per second (tgs): 593.371 | TFLOPs: 7.16 |
steps: 140 loss: 9.5346 iter time (s): 0.862 samples/sec: 13.924
[2024-03-03 23:56:34,284] [INFO] [logging.py:96:log_dist] [Rank 0] step=150, skipped=15, lr=[5.308429589579749e-07, 5.308429589579749e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      150/  112413 | consumed samples:         1800 | consumed tokens:      3686400 | elapsed time per iteration (ms): 863.3 | learning rate: 5.308E-07 | global batch size:    12 | lm loss: 9.479429E+00 | loss scale: 131072.0 | grad norm: 4.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.900 | tokens per gpu per second (tgs): 593.063 | TFLOPs: 7.16 |
steps: 150 loss: 9.4570 iter time (s): 0.862 samples/sec: 13.917
[2024-03-03 23:56:42,978] [INFO] [logging.py:96:log_dist] [Rank 0] step=160, skipped=15, lr=[5.701646596215287e-07, 5.701646596215287e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      160/  112413 | consumed samples:         1920 | consumed tokens:      3932160 | elapsed time per iteration (ms): 869.4 | learning rate: 5.702E-07 | global batch size:    12 | lm loss: 9.405735E+00 | loss scale: 131072.0 | grad norm: 5.697 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.803 | tokens per gpu per second (tgs): 588.935 | TFLOPs: 7.11 |
steps: 160 loss: 9.3618 iter time (s): 0.868 samples/sec: 13.820
[2024-03-03 23:56:51,651] [INFO] [logging.py:96:log_dist] [Rank 0] step=170, skipped=15, lr=[6.094863602850824e-07, 6.094863602850824e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      170/  112413 | consumed samples:         2040 | consumed tokens:      4177920 | elapsed time per iteration (ms): 867.3 | learning rate: 6.095E-07 | global batch size:    12 | lm loss: 9.386684E+00 | loss scale: 131072.0 | grad norm: 4.253 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.836 | tokens per gpu per second (tgs): 590.341 | TFLOPs: 7.12 |
steps: 170 loss: 9.3318 iter time (s): 0.866 samples/sec: 13.853
[2024-03-03 23:57:00,315] [INFO] [logging.py:96:log_dist] [Rank 0] step=180, skipped=15, lr=[6.488080609486361e-07, 6.488080609486361e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      180/  112413 | consumed samples:         2160 | consumed tokens:      4423680 | elapsed time per iteration (ms): 866.4 | learning rate: 6.488E-07 | global batch size:    12 | lm loss: 9.325630E+00 | loss scale: 131072.0 | grad norm: 4.591 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.851 | tokens per gpu per second (tgs): 590.961 | TFLOPs: 7.13 |
steps: 180 loss: 9.2888 iter time (s): 0.865 samples/sec: 13.867
[2024-03-03 23:57:08,971] [INFO] [logging.py:96:log_dist] [Rank 0] step=190, skipped=15, lr=[6.881297616121898e-07, 6.881297616121898e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      190/  112413 | consumed samples:         2280 | consumed tokens:      4669440 | elapsed time per iteration (ms): 865.5 | learning rate: 6.881E-07 | global batch size:    12 | lm loss: 9.275037E+00 | loss scale: 131072.0 | grad norm: 4.288 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.864 | tokens per gpu per second (tgs): 591.547 | TFLOPs: 7.14 |
steps: 190 loss: 9.2956 iter time (s): 0.864 samples/sec: 13.881
[2024-03-03 23:57:17,655] [INFO] [logging.py:96:log_dist] [Rank 0] step=200, skipped=15, lr=[7.274514622757435e-07, 7.274514622757435e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      200/  112413 | consumed samples:         2400 | consumed tokens:      4915200 | elapsed time per iteration (ms): 868.4 | learning rate: 7.275E-07 | global batch size:    12 | lm loss: 9.231490E+00 | loss scale: 131072.0 | grad norm: 4.617 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.818 | tokens per gpu per second (tgs): 589.564 | TFLOPs: 7.11 |
steps: 200 loss: 9.2404 iter time (s): 0.867 samples/sec: 13.835
[2024-03-03 23:57:26,293] [INFO] [logging.py:96:log_dist] [Rank 0] step=210, skipped=15, lr=[7.667731629392971e-07, 7.667731629392971e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      210/  112413 | consumed samples:         2520 | consumed tokens:      5160960 | elapsed time per iteration (ms): 863.7 | learning rate: 7.668E-07 | global batch size:    12 | lm loss: 9.206681E+00 | loss scale: 131072.0 | grad norm: 4.378 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.893 | tokens per gpu per second (tgs): 592.781 | TFLOPs: 7.15 |
steps: 210 loss: 9.1865 iter time (s): 0.863 samples/sec: 13.910
[2024-03-03 23:57:34,926] [INFO] [logging.py:96:log_dist] [Rank 0] step=220, skipped=15, lr=[8.060948636028509e-07, 8.060948636028509e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      220/  112413 | consumed samples:         2640 | consumed tokens:      5406720 | elapsed time per iteration (ms): 863.3 | learning rate: 8.061E-07 | global batch size:    12 | lm loss: 9.148300E+00 | loss scale: 131072.0 | grad norm: 4.384 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.900 | tokens per gpu per second (tgs): 593.059 | TFLOPs: 7.16 |
steps: 220 loss: 9.1306 iter time (s): 0.862 samples/sec: 13.916
[2024-03-03 23:57:43,578] [INFO] [logging.py:96:log_dist] [Rank 0] step=230, skipped=15, lr=[8.454165642664045e-07, 8.454165642664045e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      230/  112413 | consumed samples:         2760 | consumed tokens:      5652480 | elapsed time per iteration (ms): 865.2 | learning rate: 8.454E-07 | global batch size:    12 | lm loss: 9.093247E+00 | loss scale: 131072.0 | grad norm: 5.565 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.869 | tokens per gpu per second (tgs): 591.760 | TFLOPs: 7.14 |
steps: 230 loss: 8.9742 iter time (s): 0.864 samples/sec: 13.886
[2024-03-03 23:57:52,251] [INFO] [logging.py:96:log_dist] [Rank 0] step=240, skipped=15, lr=[8.847382649299583e-07, 8.847382649299583e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      240/  112413 | consumed samples:         2880 | consumed tokens:      5898240 | elapsed time per iteration (ms): 867.3 | learning rate: 8.847E-07 | global batch size:    12 | lm loss: 9.061250E+00 | loss scale: 131072.0 | grad norm: 4.574 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.837 | tokens per gpu per second (tgs): 590.361 | TFLOPs: 7.12 |
steps: 240 loss: 9.1013 iter time (s): 0.866 samples/sec: 13.853
[2024-03-03 23:58:00,862] [INFO] [logging.py:96:log_dist] [Rank 0] step=250, skipped=15, lr=[9.24059965593512e-07, 9.24059965593512e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      250/  112413 | consumed samples:         3000 | consumed tokens:      6144000 | elapsed time per iteration (ms): 861.0 | learning rate: 9.241E-07 | global batch size:    12 | lm loss: 9.010829E+00 | loss scale: 131072.0 | grad norm: 4.550 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.936 | tokens per gpu per second (tgs): 594.624 | TFLOPs: 7.17 |
steps: 250 loss: 9.0111 iter time (s): 0.860 samples/sec: 13.953
[2024-03-03 23:58:09,501] [INFO] [logging.py:96:log_dist] [Rank 0] step=260, skipped=15, lr=[9.633816662570655e-07, 9.633816662570655e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      260/  112413 | consumed samples:         3120 | consumed tokens:      6389760 | elapsed time per iteration (ms): 864.0 | learning rate: 9.634E-07 | global batch size:    12 | lm loss: 8.974315E+00 | loss scale: 131072.0 | grad norm: 4.374 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.889 | tokens per gpu per second (tgs): 592.608 | TFLOPs: 7.15 |
steps: 260 loss: 8.9758 iter time (s): 0.863 samples/sec: 13.906
[2024-03-03 23:58:18,157] [INFO] [logging.py:96:log_dist] [Rank 0] step=270, skipped=15, lr=[1.0027033669206194e-06, 1.0027033669206194e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      270/  112413 | consumed samples:         3240 | consumed tokens:      6635520 | elapsed time per iteration (ms): 865.6 | learning rate: 1.003E-06 | global batch size:    12 | lm loss: 8.917566E+00 | loss scale: 131072.0 | grad norm: 5.413 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.864 | tokens per gpu per second (tgs): 591.522 | TFLOPs: 7.14 |
steps: 270 loss: 8.7965 iter time (s): 0.865 samples/sec: 13.881
[2024-03-03 23:58:26,795] [INFO] [logging.py:96:log_dist] [Rank 0] step=280, skipped=15, lr=[1.042025067584173e-06, 1.042025067584173e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      280/  112413 | consumed samples:         3360 | consumed tokens:      6881280 | elapsed time per iteration (ms): 863.8 | learning rate: 1.042E-06 | global batch size:    12 | lm loss: 8.879971E+00 | loss scale: 131072.0 | grad norm: 5.065 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.893 | tokens per gpu per second (tgs): 592.759 | TFLOPs: 7.15 |
steps: 280 loss: 8.8959 iter time (s): 0.863 samples/sec: 13.910
[2024-03-03 23:58:35,448] [INFO] [logging.py:96:log_dist] [Rank 0] step=290, skipped=15, lr=[1.0813467682477268e-06, 1.0813467682477268e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      290/  112413 | consumed samples:         3480 | consumed tokens:      7127040 | elapsed time per iteration (ms): 865.3 | learning rate: 1.081E-06 | global batch size:    12 | lm loss: 8.863800E+00 | loss scale: 131072.0 | grad norm: 4.743 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.868 | tokens per gpu per second (tgs): 591.705 | TFLOPs: 7.14 |
steps: 290 loss: 8.8535 iter time (s): 0.864 samples/sec: 13.885
[2024-03-03 23:58:44,080] [INFO] [logging.py:96:log_dist] [Rank 0] step=300, skipped=15, lr=[1.1206684689112803e-06, 1.1206684689112803e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      300/  112413 | consumed samples:         3600 | consumed tokens:      7372800 | elapsed time per iteration (ms): 863.1 | learning rate: 1.121E-06 | global batch size:    12 | lm loss: 8.838798E+00 | loss scale: 131072.0 | grad norm: 4.793 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.903 | tokens per gpu per second (tgs): 593.191 | TFLOPs: 7.16 |
steps: 300 loss: 8.8446 iter time (s): 0.862 samples/sec: 13.920
[2024-03-03 23:58:52,700] [INFO] [logging.py:96:log_dist] [Rank 0] step=310, skipped=15, lr=[1.1599901695748342e-06, 1.1599901695748342e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      310/  112413 | consumed samples:         3720 | consumed tokens:      7618560 | elapsed time per iteration (ms): 862.0 | learning rate: 1.160E-06 | global batch size:    12 | lm loss: 8.812289E+00 | loss scale: 131072.0 | grad norm: 5.501 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.921 | tokens per gpu per second (tgs): 593.964 | TFLOPs: 7.17 |
steps: 310 loss: 8.7836 iter time (s): 0.861 samples/sec: 13.938
[2024-03-03 23:59:01,311] [INFO] [logging.py:96:log_dist] [Rank 0] step=320, skipped=15, lr=[1.1993118702383879e-06, 1.1993118702383879e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      320/  112413 | consumed samples:         3840 | consumed tokens:      7864320 | elapsed time per iteration (ms): 861.1 | learning rate: 1.199E-06 | global batch size:    12 | lm loss: 8.752196E+00 | loss scale: 131072.0 | grad norm: 5.598 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.935 | tokens per gpu per second (tgs): 594.577 | TFLOPs: 7.17 |
steps: 320 loss: 8.7876 iter time (s): 0.860 samples/sec: 13.952
[2024-03-03 23:59:09,938] [INFO] [logging.py:96:log_dist] [Rank 0] step=330, skipped=15, lr=[1.2386335709019415e-06, 1.2386335709019415e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      330/  112413 | consumed samples:         3960 | consumed tokens:      8110080 | elapsed time per iteration (ms): 862.6 | learning rate: 1.239E-06 | global batch size:    12 | lm loss: 8.759213E+00 | loss scale: 131072.0 | grad norm: 5.309 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.911 | tokens per gpu per second (tgs): 593.524 | TFLOPs: 7.16 |
steps: 330 loss: 8.7256 iter time (s): 0.862 samples/sec: 13.928
[2024-03-03 23:59:18,543] [INFO] [logging.py:96:log_dist] [Rank 0] step=340, skipped=15, lr=[1.2779552715654952e-06, 1.2779552715654952e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      340/  112413 | consumed samples:         4080 | consumed tokens:      8355840 | elapsed time per iteration (ms): 860.5 | learning rate: 1.278E-06 | global batch size:    12 | lm loss: 8.709985E+00 | loss scale: 131072.0 | grad norm: 4.605 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.946 | tokens per gpu per second (tgs): 595.009 | TFLOPs: 7.18 |
steps: 340 loss: 8.7037 iter time (s): 0.859 samples/sec: 13.962
[2024-03-03 23:59:27,157] [INFO] [logging.py:96:log_dist] [Rank 0] step=350, skipped=15, lr=[1.317276972229049e-06, 1.317276972229049e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      350/  112413 | consumed samples:         4200 | consumed tokens:      8601600 | elapsed time per iteration (ms): 861.4 | learning rate: 1.317E-06 | global batch size:    12 | lm loss: 8.683159E+00 | loss scale: 131072.0 | grad norm: 4.779 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.931 | tokens per gpu per second (tgs): 594.376 | TFLOPs: 7.17 |
steps: 350 loss: 8.7259 iter time (s): 0.860 samples/sec: 13.948
[2024-03-03 23:59:35,827] [INFO] [logging.py:96:log_dist] [Rank 0] step=360, skipped=15, lr=[1.3565986728926026e-06, 1.3565986728926026e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      360/  112413 | consumed samples:         4320 | consumed tokens:      8847360 | elapsed time per iteration (ms): 867.0 | learning rate: 1.357E-06 | global batch size:    12 | lm loss: 8.635684E+00 | loss scale: 131072.0 | grad norm: 4.583 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.841 | tokens per gpu per second (tgs): 590.559 | TFLOPs: 7.13 |
steps: 360 loss: 8.5653 iter time (s): 0.866 samples/sec: 13.858
[2024-03-03 23:59:44,452] [INFO] [logging.py:96:log_dist] [Rank 0] step=370, skipped=15, lr=[1.3959203735561563e-06, 1.3959203735561563e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      370/  112413 | consumed samples:         4440 | consumed tokens:      9093120 | elapsed time per iteration (ms): 862.5 | learning rate: 1.396E-06 | global batch size:    12 | lm loss: 8.597478E+00 | loss scale: 131072.0 | grad norm: 4.749 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.914 | tokens per gpu per second (tgs): 593.653 | TFLOPs: 7.16 |
steps: 370 loss: 8.5258 iter time (s): 0.861 samples/sec: 13.931
[2024-03-03 23:59:53,056] [INFO] [logging.py:96:log_dist] [Rank 0] step=380, skipped=15, lr=[1.4352420742197102e-06, 1.4352420742197102e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      380/  112413 | consumed samples:         4560 | consumed tokens:      9338880 | elapsed time per iteration (ms): 860.4 | learning rate: 1.435E-06 | global batch size:    12 | lm loss: 8.560956E+00 | loss scale: 131072.0 | grad norm: 4.589 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.946 | tokens per gpu per second (tgs): 595.040 | TFLOPs: 7.18 |
steps: 380 loss: 8.5731 iter time (s): 0.859 samples/sec: 13.963
[2024-03-04 00:00:01,663] [INFO] [logging.py:96:log_dist] [Rank 0] step=390, skipped=15, lr=[1.4745637748832639e-06, 1.4745637748832639e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      390/  112413 | consumed samples:         4680 | consumed tokens:      9584640 | elapsed time per iteration (ms): 860.6 | learning rate: 1.475E-06 | global batch size:    12 | lm loss: 8.544709E+00 | loss scale: 131072.0 | grad norm: 5.298 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.943 | tokens per gpu per second (tgs): 594.915 | TFLOPs: 7.18 |
steps: 390 loss: 8.5040 iter time (s): 0.860 samples/sec: 13.960
[2024-03-04 00:00:10,297] [INFO] [logging.py:96:log_dist] [Rank 0] step=400, skipped=15, lr=[1.5138854755468173e-06, 1.5138854755468173e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      400/  112413 | consumed samples:         4800 | consumed tokens:      9830400 | elapsed time per iteration (ms): 863.4 | learning rate: 1.514E-06 | global batch size:    12 | lm loss: 8.522852E+00 | loss scale: 131072.0 | grad norm: 4.407 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.899 | tokens per gpu per second (tgs): 593.005 | TFLOPs: 7.16 |
steps: 400 loss: 8.5295 iter time (s): 0.862 samples/sec: 13.915
[2024-03-04 00:00:18,920] [INFO] [logging.py:96:log_dist] [Rank 0] step=410, skipped=15, lr=[1.553207176210371e-06, 1.553207176210371e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      410/  112413 | consumed samples:         4920 | consumed tokens:     10076160 | elapsed time per iteration (ms): 862.3 | learning rate: 1.553E-06 | global batch size:    12 | lm loss: 8.491195E+00 | loss scale: 131072.0 | grad norm: 4.830 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.916 | tokens per gpu per second (tgs): 593.738 | TFLOPs: 7.16 |
steps: 410 loss: 8.4814 iter time (s): 0.861 samples/sec: 13.932
[2024-03-04 00:00:27,525] [INFO] [logging.py:96:log_dist] [Rank 0] step=420, skipped=15, lr=[1.592528876873925e-06, 1.592528876873925e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      420/  112413 | consumed samples:         5040 | consumed tokens:     10321920 | elapsed time per iteration (ms): 860.5 | learning rate: 1.593E-06 | global batch size:    12 | lm loss: 8.428340E+00 | loss scale: 131072.0 | grad norm: 4.729 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.945 | tokens per gpu per second (tgs): 594.998 | TFLOPs: 7.18 |
steps: 420 loss: 8.4257 iter time (s): 0.859 samples/sec: 13.962
[2024-03-04 00:00:36,166] [INFO] [logging.py:96:log_dist] [Rank 0] step=430, skipped=15, lr=[1.6318505775374786e-06, 1.6318505775374786e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      430/  112413 | consumed samples:         5160 | consumed tokens:     10567680 | elapsed time per iteration (ms): 864.0 | learning rate: 1.632E-06 | global batch size:    12 | lm loss: 8.412271E+00 | loss scale: 131072.0 | grad norm: 4.491 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.888 | tokens per gpu per second (tgs): 592.566 | TFLOPs: 7.15 |
steps: 430 loss: 8.3718 iter time (s): 0.863 samples/sec: 13.905
[2024-03-04 00:00:44,827] [INFO] [logging.py:96:log_dist] [Rank 0] step=440, skipped=15, lr=[1.6711722782010323e-06, 1.6711722782010323e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      440/  112413 | consumed samples:         5280 | consumed tokens:     10813440 | elapsed time per iteration (ms): 866.1 | learning rate: 1.671E-06 | global batch size:    12 | lm loss: 8.387105E+00 | loss scale: 131072.0 | grad norm: 4.633 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.855 | tokens per gpu per second (tgs): 591.133 | TFLOPs: 7.13 |
steps: 440 loss: 8.3472 iter time (s): 0.865 samples/sec: 13.871
[2024-03-04 00:00:53,435] [INFO] [logging.py:96:log_dist] [Rank 0] step=450, skipped=15, lr=[1.7104939788645862e-06, 1.7104939788645862e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      450/  112413 | consumed samples:         5400 | consumed tokens:     11059200 | elapsed time per iteration (ms): 860.8 | learning rate: 1.710E-06 | global batch size:    12 | lm loss: 8.328463E+00 | loss scale: 131072.0 | grad norm: 4.606 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.941 | tokens per gpu per second (tgs): 594.800 | TFLOPs: 7.18 |
steps: 450 loss: 8.2870 iter time (s): 0.860 samples/sec: 13.958
[2024-03-04 00:01:02,090] [INFO] [logging.py:96:log_dist] [Rank 0] step=460, skipped=15, lr=[1.7498156795281396e-06, 1.7498156795281396e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      460/  112413 | consumed samples:         5520 | consumed tokens:     11304960 | elapsed time per iteration (ms): 865.5 | learning rate: 1.750E-06 | global batch size:    12 | lm loss: 8.299699E+00 | loss scale: 131072.0 | grad norm: 5.149 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.865 | tokens per gpu per second (tgs): 591.577 | TFLOPs: 7.14 |
steps: 460 loss: 8.3347 iter time (s): 0.864 samples/sec: 13.882
[2024-03-04 00:01:10,696] [INFO] [logging.py:96:log_dist] [Rank 0] step=470, skipped=15, lr=[1.7891373801916933e-06, 1.7891373801916933e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      470/  112413 | consumed samples:         5640 | consumed tokens:     11550720 | elapsed time per iteration (ms): 860.5 | learning rate: 1.789E-06 | global batch size:    12 | lm loss: 8.270976E+00 | loss scale: 131072.0 | grad norm: 5.143 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.945 | tokens per gpu per second (tgs): 594.993 | TFLOPs: 7.18 |
steps: 470 loss: 8.2346 iter time (s): 0.859 samples/sec: 13.962
[2024-03-04 00:01:19,312] [INFO] [logging.py:96:log_dist] [Rank 0] step=480, skipped=15, lr=[1.828459080855247e-06, 1.828459080855247e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      480/  112413 | consumed samples:         5760 | consumed tokens:     11796480 | elapsed time per iteration (ms): 861.6 | learning rate: 1.828E-06 | global batch size:    12 | lm loss: 8.229276E+00 | loss scale: 131072.0 | grad norm: 4.491 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.928 | tokens per gpu per second (tgs): 594.244 | TFLOPs: 7.17 |
steps: 480 loss: 8.1635 iter time (s): 0.861 samples/sec: 13.945
[2024-03-04 00:01:27,960] [INFO] [logging.py:96:log_dist] [Rank 0] step=490, skipped=15, lr=[1.8677807815188009e-06, 1.8677807815188009e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      490/  112413 | consumed samples:         5880 | consumed tokens:     12042240 | elapsed time per iteration (ms): 864.8 | learning rate: 1.868E-06 | global batch size:    12 | lm loss: 8.170773E+00 | loss scale: 131072.0 | grad norm: 4.474 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.876 | tokens per gpu per second (tgs): 592.048 | TFLOPs: 7.14 |
steps: 490 loss: 8.1465 iter time (s): 0.864 samples/sec: 13.893
[2024-03-04 00:01:36,562] [INFO] [logging.py:96:log_dist] [Rank 0] step=500, skipped=15, lr=[1.9071024821823544e-06, 1.9071024821823544e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      500/  112413 | consumed samples:         6000 | consumed tokens:     12288000 | elapsed time per iteration (ms): 860.2 | learning rate: 1.907E-06 | global batch size:    12 | lm loss: 8.150906E+00 | loss scale: 131072.0 | grad norm: 4.448 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.951 | tokens per gpu per second (tgs): 595.238 | TFLOPs: 7.18 |
steps: 500 loss: 8.1521 iter time (s): 0.859 samples/sec: 13.968
[2024-03-04 00:01:45,182] [INFO] [logging.py:96:log_dist] [Rank 0] step=510, skipped=15, lr=[1.946424182845908e-06, 1.946424182845908e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      510/  112413 | consumed samples:         6120 | consumed tokens:     12533760 | elapsed time per iteration (ms): 862.1 | learning rate: 1.946E-06 | global batch size:    12 | lm loss: 8.117468E+00 | loss scale: 131072.0 | grad norm: 4.384 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.920 | tokens per gpu per second (tgs): 593.929 | TFLOPs: 7.17 |
steps: 510 loss: 8.1115 iter time (s): 0.861 samples/sec: 13.937
[2024-03-04 00:01:53,802] [INFO] [logging.py:96:log_dist] [Rank 0] step=520, skipped=15, lr=[1.985745883509462e-06, 1.985745883509462e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      520/  112413 | consumed samples:         6240 | consumed tokens:     12779520 | elapsed time per iteration (ms): 862.0 | learning rate: 1.986E-06 | global batch size:    12 | lm loss: 8.103872E+00 | loss scale: 131072.0 | grad norm: 4.304 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.921 | tokens per gpu per second (tgs): 593.974 | TFLOPs: 7.17 |
steps: 520 loss: 8.1185 iter time (s): 0.861 samples/sec: 13.938
[2024-03-04 00:02:02,452] [INFO] [logging.py:96:log_dist] [Rank 0] step=530, skipped=15, lr=[2.0250675841730154e-06, 2.0250675841730154e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      530/  112413 | consumed samples:         6360 | consumed tokens:     13025280 | elapsed time per iteration (ms): 865.0 | learning rate: 2.025E-06 | global batch size:    12 | lm loss: 8.055282E+00 | loss scale: 131072.0 | grad norm: 3.843 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.873 | tokens per gpu per second (tgs): 591.928 | TFLOPs: 7.14 |
steps: 530 loss: 8.0403 iter time (s): 0.864 samples/sec: 13.890
[2024-03-04 00:02:11,102] [INFO] [logging.py:96:log_dist] [Rank 0] step=540, skipped=15, lr=[2.0643892848365693e-06, 2.0643892848365693e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      540/  112413 | consumed samples:         6480 | consumed tokens:     13271040 | elapsed time per iteration (ms): 865.0 | learning rate: 2.064E-06 | global batch size:    12 | lm loss: 7.952781E+00 | loss scale: 131072.0 | grad norm: 4.495 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.873 | tokens per gpu per second (tgs): 591.930 | TFLOPs: 7.14 |
steps: 540 loss: 7.9689 iter time (s): 0.864 samples/sec: 13.890
[2024-03-04 00:02:19,742] [INFO] [logging.py:96:log_dist] [Rank 0] step=550, skipped=15, lr=[2.1037109855001228e-06, 2.1037109855001228e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      550/  112413 | consumed samples:         6600 | consumed tokens:     13516800 | elapsed time per iteration (ms): 864.0 | learning rate: 2.104E-06 | global batch size:    12 | lm loss: 7.950013E+00 | loss scale: 131072.0 | grad norm: 4.260 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.889 | tokens per gpu per second (tgs): 592.584 | TFLOPs: 7.15 |
steps: 550 loss: 7.9634 iter time (s): 0.863 samples/sec: 13.906
[2024-03-04 00:02:28,340] [INFO] [logging.py:96:log_dist] [Rank 0] step=560, skipped=15, lr=[2.1430326861636767e-06, 2.1430326861636767e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      560/  112413 | consumed samples:         6720 | consumed tokens:     13762560 | elapsed time per iteration (ms): 859.8 | learning rate: 2.143E-06 | global batch size:    12 | lm loss: 7.960959E+00 | loss scale: 131072.0 | grad norm: 3.939 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.956 | tokens per gpu per second (tgs): 595.476 | TFLOPs: 7.19 |
steps: 560 loss: 7.9304 iter time (s): 0.859 samples/sec: 13.974
[2024-03-04 00:02:36,961] [INFO] [logging.py:96:log_dist] [Rank 0] step=570, skipped=15, lr=[2.18235438682723e-06, 2.18235438682723e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      570/  112413 | consumed samples:         6840 | consumed tokens:     14008320 | elapsed time per iteration (ms): 862.1 | learning rate: 2.182E-06 | global batch size:    12 | lm loss: 7.899176E+00 | loss scale: 131072.0 | grad norm: 3.872 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.920 | tokens per gpu per second (tgs): 593.914 | TFLOPs: 7.17 |
steps: 570 loss: 7.8797 iter time (s): 0.861 samples/sec: 13.937
[2024-03-04 00:02:45,599] [INFO] [logging.py:96:log_dist] [Rank 0] step=580, skipped=15, lr=[2.221676087490784e-06, 2.221676087490784e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      580/  112413 | consumed samples:         6960 | consumed tokens:     14254080 | elapsed time per iteration (ms): 863.7 | learning rate: 2.222E-06 | global batch size:    12 | lm loss: 7.836518E+00 | loss scale: 131072.0 | grad norm: 3.988 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.893 | tokens per gpu per second (tgs): 592.773 | TFLOPs: 7.15 |
steps: 580 loss: 7.7525 iter time (s): 0.863 samples/sec: 13.910
[2024-03-04 00:02:54,215] [INFO] [logging.py:96:log_dist] [Rank 0] step=590, skipped=15, lr=[2.260997788154338e-06, 2.260997788154338e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      590/  112413 | consumed samples:         7080 | consumed tokens:     14499840 | elapsed time per iteration (ms): 861.6 | learning rate: 2.261E-06 | global batch size:    12 | lm loss: 7.807430E+00 | loss scale: 131072.0 | grad norm: 3.931 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.928 | tokens per gpu per second (tgs): 594.251 | TFLOPs: 7.17 |
steps: 590 loss: 7.7733 iter time (s): 0.861 samples/sec: 13.945
[2024-03-04 00:03:02,832] [INFO] [logging.py:96:log_dist] [Rank 0] step=600, skipped=15, lr=[2.3003194888178914e-06, 2.3003194888178914e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      600/  112413 | consumed samples:         7200 | consumed tokens:     14745600 | elapsed time per iteration (ms): 861.7 | learning rate: 2.300E-06 | global batch size:    12 | lm loss: 7.763835E+00 | loss scale: 131072.0 | grad norm: 3.778 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.926 | tokens per gpu per second (tgs): 594.180 | TFLOPs: 7.17 |
steps: 600 loss: 7.8032 iter time (s): 0.861 samples/sec: 13.943
[2024-03-04 00:03:11,492] [INFO] [logging.py:96:log_dist] [Rank 0] step=610, skipped=15, lr=[2.3396411894814453e-06, 2.3396411894814453e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      610/  112413 | consumed samples:         7320 | consumed tokens:     14991360 | elapsed time per iteration (ms): 866.0 | learning rate: 2.340E-06 | global batch size:    12 | lm loss: 7.716013E+00 | loss scale: 131072.0 | grad norm: 3.927 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.857 | tokens per gpu per second (tgs): 591.253 | TFLOPs: 7.13 |
steps: 610 loss: 7.6946 iter time (s): 0.865 samples/sec: 13.874
[2024-03-04 00:03:20,112] [INFO] [logging.py:96:log_dist] [Rank 0] step=620, skipped=15, lr=[2.3789628901449988e-06, 2.3789628901449988e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      620/  112413 | consumed samples:         7440 | consumed tokens:     15237120 | elapsed time per iteration (ms): 862.0 | learning rate: 2.379E-06 | global batch size:    12 | lm loss: 7.697057E+00 | loss scale: 131072.0 | grad norm: 3.725 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.921 | tokens per gpu per second (tgs): 593.973 | TFLOPs: 7.17 |
steps: 620 loss: 7.6831 iter time (s): 0.861 samples/sec: 13.938
[2024-03-04 00:03:28,711] [INFO] [logging.py:96:log_dist] [Rank 0] step=630, skipped=15, lr=[2.4182845908085527e-06, 2.4182845908085527e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
 iteration      630/  112413 | consumed samples:         7560 | consumed tokens:     15482880 | elapsed time per iteration (ms): 859.9 | learning rate: 2.418E-06 | global batch size:    12 | lm loss: 7.649090E+00 | loss scale: 131072.0 | grad norm: 3.546 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.956 | tokens per gpu per second (tgs): 595.435 | TFLOPs: 7.18 |

------------------------------------------------------------
Sender: LSF System <lsfadmin@batch1>
Subject: Job 3328593: <gpt3-megatron> in cluster <summit> Exited

Job <gpt3-megatron> was submitted from host <login3> by user <sajaldash> in cluster <summit> at Sun Mar  3 23:51:49 2024
Job was executed on host(s) <1*batch1>, in queue <debug>, as user <sajaldash> in cluster <summit> at Sun Mar  3 23:51:50 2024
                            <42*a16n03>
                            <42*a16n05>
                            <42*a16n07>
                            <42*a27n08>
                            <42*a27n09>
                            <42*a27n10>
                            <42*a28n13>
                            <42*a28n14>
</ccs/home/sajaldash> was used as the home directory.
</gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed> was used as the working directory.
Started at Sun Mar  3 23:51:50 2024
Terminated at Mon Mar  4 00:03:38 2024
Results reported at Mon Mar  4 00:03:38 2024

The output (if any) is above this job summary.



PS:

Read file <logs/gpt3-megatron.3328593.e> for stderr output of this job.

