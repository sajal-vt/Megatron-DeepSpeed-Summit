a15n05
10.134.1.104
[2024-03-25 16:04:08,712] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-25 16:04:08,712] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-25 16:04:08,712] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-25 16:04:08,712] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-25 16:04:08,713] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-25 16:04:08,712] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-25 16:04:08,713] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-25 16:04:08,712] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-25 16:04:08,713] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-25 16:04:08,713] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-25 16:04:08,713] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-25 16:04:08,713] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-25 16:04:08,714] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-25 16:04:08,714] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-25 16:04:08,714] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-25 16:04:08,715] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-25 16:04:08,715] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-25 16:04:08,715] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-25 16:04:08,721] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-25 16:04:08,721] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-25 16:04:08,721] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-25 16:04:08,721] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-25 16:04:08,721] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-25 16:04:08,721] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-25 16:04:08,722] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-25 16:04:08,723] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-25 16:04:08,723] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-25 16:04:08,723] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-25 16:04:08,723] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-25 16:04:08,723] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-25 16:04:08,727] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-25 16:04:08,727] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-25 16:04:08,727] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-25 16:04:08,727] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-25 16:04:08,727] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-25 16:04:08,727] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-25 16:04:08,728] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-25 16:04:08,728] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-25 16:04:08,728] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-25 16:04:08,728] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-25 16:04:08,728] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-25 16:04:08,728] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-25 16:04:08,817] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-25 16:04:08,817] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-25 16:04:08,817] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-25 16:04:08,817] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-25 16:04:08,817] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-25 16:04:08,817] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 0
MA = 10.134.1.104
World view:  0 48 10.134.1.104
using world size: 48, data-parallel-size: 48, sequence-parallel size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  add_bias_linear ................................. True
  add_position_embedding .......................... True
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  aml_data_download_path .......................... None
  apply_layernorm_1p .............................. False
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  async_tensor_model_parallel_allreduce ........... False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  barrier_with_L1_time ............................ True
  bert_binary_head ................................ True
  bert_embedder_type .............................. megatron
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... True
  checkpoint_in_cpu ............................... False
  checkpoint_num_layers ........................... 1
  classes_fraction ................................ 1.0
  clip_grad ....................................... 1.0
  compression_training ............................ False
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  create_moe_param_group .......................... True
  curriculum_learning_legacy ...................... False
  data_cache_path ................................. None
  data_efficiency_curriculum_learning ............. False
  data_impl ....................................... mmap
  data_parallel_random_init ....................... False
  data_parallel_size .............................. 48
  data_path ....................................... ['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document']
  data_per_class_fraction ......................... 1.0
  data_sharding ................................... True
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_num_layers .............................. None
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... True
  deepspeed_activation_checkpointing .............. True
  deepspeed_config ................................ ds_config_gpt_gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test.json
  deepspeed_mpi ................................... False
  dino_bottleneck_size ............................ 256
  dino_freeze_last_layer .......................... 1
  dino_head_hidden_size ........................... 2048
  dino_local_crops_number ......................... 10
  dino_local_img_size ............................. 96
  dino_norm_last_layer ............................ False
  dino_teacher_temp ............................... 0.07
  dino_warmup_teacher_temp ........................ 0.04
  dino_warmup_teacher_temp_epochs ................. 30
  distribute_checkpointed_activations ............. False
  distribute_saved_activations .................... False
  distributed_backend ............................. nccl
  distributed_timeout_minutes ..................... 10
  ds_fused_adam ................................... False
  ds_inference .................................... False
  ds_pipeline_enabled ............................. False
  ds_sequence_parallel_size ....................... 1
  embedding_path .................................. None
  embedding_weights_in_fp32 ....................... False
  empty_unused_memory_level ....................... 0
  enable_expert_tensor_parallelism ................ False
  encoder_num_layers .............................. 24
  encoder_seq_length .............................. 2048
  end_weight_decay ................................ 0.1
  eod_mask_loss ................................... False
  eval_interval ................................... 100
  eval_iters ...................................... 10
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... 30000000
  exit_interval ................................... None
  exit_on_missing_checkpoint ...................... False
  exit_signal_handler ............................. False
  expert_interval ................................. 2
  ffn_hidden_size ................................. 8192
  finetune ........................................ False
  force_ds_sequence_parallel ...................... False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  fp8_amax_compute_algo ........................... most_recent
  fp8_amax_history_len ............................ 1
  fp8_e4m3 ........................................ False
  fp8_hybrid ...................................... False
  fp8_interval .................................... 1
  fp8_margin ...................................... 0
  fp8_wgrad ....................................... True
  global_batch_size ............................... 96
  gradient_accumulation_fusion .................... True
  head_lr_mult .................................... 1.0
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 2048
  hidden_size_teacher ............................. None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_h ........................................... 224
  img_w ........................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference ....................................... False
  inference_batch_times_seqlen_threshold .......... 512
  init_method_std ................................. 0.014
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  iter_per_epoch .................................. 1250
  kd .............................................. False
  kd_alpha_ce ..................................... 1
  kd_beta_ce ...................................... 1
  kd_temp ......................................... 1.0
  kv_channels ..................................... 128
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ /gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test
  load_tag ........................................ None
  load_teacher .................................... None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... True
  log_interval .................................... 10
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... False
  log_optimizer_states_to_tensorboard ............. False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... True
  log_validation_ppl_to_tensorboard ............... True
  log_world_size_to_tensorboard ................... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.00012
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_decay_tokens ................................. 300000000000
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_warmup_tokens ................................ 375000000
  make_vocab_size_divisible_by .................... 128
  mask_factor ..................................... 1.0
  mask_prob ....................................... 0.15
  mask_type ....................................... random
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 2048
  max_tokens_to_oom ............................... 12000
  mem_efficient_ln ................................ True
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... gpt2-merges.txt
  micro_batch_size ................................ 1
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-06
  mlp_type ........................................ standard
  mmap_warmup ..................................... False
  moe_eval_capacity_factor ........................ 1.0
  moe_expert_parallel_size ........................ 8
  moe_loss_coeff .................................. 0.01
  moe_min_capacity ................................ 4
  moe_token_dropping .............................. True
  moe_train_capacity_factor ....................... 1.0
  mos ............................................. False
  no_load_lr_state ................................ False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_persist_layer_norm ........................... False
  no_pipeline_parallel ............................ True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  normalization ................................... layernorm
  num_attention_heads ............................. 16
  num_attention_heads_teacher ..................... None
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... [8]
  num_experts_switch .............................. None
  num_experts_teacher ............................. [1]
  num_key_value_heads ............................. 16
  num_layers ...................................... 24
  num_layers_per_virtual_pipeline_stage ........... None
  num_layers_teacher .............................. None
  num_workers ..................................... 0
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  output_bert_embeddings .......................... False
  overlap_p2p_comm ................................ False
  override_opt_param_scheduler .................... True
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  perform_initialization .......................... True
  pipeline_model_parallel_size .................... 1
  pipeline_model_parallel_split_rank .............. None
  profile_backward ................................ False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  random_ltd ...................................... False
  rank ............................................ 0
  recompute_granularity ........................... None
  recompute_method ................................ None
  recompute_num_layers ............................ 1
  remote_device ................................... none
  repeated_dataloader ............................. False
  reset_attention_mask ............................ False
  reset_iteration ................................. False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  retro_add_retriever ............................. False
  retro_cyclic_train_iters ........................ None
  retro_encoder_attention_dropout ................. 0.1
  retro_encoder_hidden_dropout .................... 0.1
  retro_encoder_layers ............................ 2
  retro_num_neighbors ............................. 2
  retro_num_retrieved_chunks ...................... 2
  retro_return_doc_ids ............................ False
  retro_workdir ................................... None
  return_data_index ............................... False
  rope_theta ...................................... 10000
  rotary_percent .................................. 1.0
  sample_rate ..................................... 1.0
  save ............................................ /gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test
  save_interval ................................... 10
  scatter_gather_tensors_in_pipeline .............. True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 2048
  sequence_parallel ............................... False
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  skip_train ...................................... False
  split ........................................... 98,2,0
  split_transformers .............................. False
  squared_relu .................................... False
  standalone_embedding_stage ...................... False
  start_weight_decay .............................. 0.1
  swiglu .......................................... False
  swin_backbone_type .............................. tiny
  synchronize_each_layer .......................... False
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. /gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1
  test_data_path .................................. None
  tile_factor ..................................... 1
  timing_log_level ................................ 0
  timing_log_option ............................... minmax
  titles_data_path ................................ None
  tokenizer_model ................................. None
  tokenizer_type .................................. GPT2BPETokenizer
  topk ............................................ 1
  train_data_exact_num_epochs ..................... None
  train_data_path ................................. None
  train_desc_path ................................. None
  train_doc_idx_path .............................. None
  train_idx_path .................................. None
  train_iters ..................................... 4577636
  train_sample_idx_path ........................... None
  train_samples ................................... None
  train_shuffle_idx_path .......................... None
  train_tokens .................................... 300000000000
  transformer_impl ................................ local
  transformer_pipeline_model_parallel_size ........ 1
  universal_checkpoint ............................ False
  untie_embeddings_and_output_weights ............. False
  use_checkpoint_args ............................. False
  use_checkpoint_opt_param_scheduler .............. False
  use_contiguous_buffers_in_local_ddp ............. True
  use_cpu_initialization .......................... None
  use_dataset_only ................................ False
  use_distributed_optimizer ....................... False
  use_flash_attn .................................. False
  use_flash_attn_triton ........................... False
  use_flash_attn_v1 ............................... False
  use_flash_attn_v2 ............................... False
  use_one_sent_docs ............................... False
  use_pin_memory .................................. False
  use_ring_exchange_p2p ........................... False
  use_rotary_position_embeddings .................. False
  use_tutel ....................................... False
  valid_data_path ................................. None
  variable_seq_lengths ............................ False
  virtual_pipeline_model_parallel_size ............ None
  vision_backbone_type ............................ vit
  vision_pretraining .............................. False
  vision_pretraining_type ......................... classify
  vocab_extra_ids ................................. 0
  vocab_file ...................................... gpt2-vocab.json
  vocab_size ...................................... None
  weight_decay .................................... 0.1
  weight_decay_incr_style ......................... constant
  world_size ...................................... 48
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 1.0
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 2
> building GPT2BPETokenizer tokenizer ...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 21
MA = 10.134.1.104
World view:  21 48 10.134.1.104
[2024-03-25 16:04:16,711] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-25 16:04:16,711] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 28
MA = 10.134.1.104
World view:  28 48 10.134.1.104
[2024-03-25 16:04:16,711] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 6
MA = 10.134.1.104
World view:  6 48 10.134.1.104
[2024-03-25 16:04:16,711] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 23
MA = 10.134.1.104
World view:  23 48 10.134.1.104
[2024-03-25 16:04:16,711] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 20
MA = 10.134.1.104
World view:  20 48 10.134.1.104
[2024-03-25 16:04:16,711] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 8
MA = 10.134.1.104
World view:  8 48 10.134.1.104
[2024-03-25 16:04:16,711] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-25 16:04:16,711] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 25
MA = 10.134.1.104
World view:  25 48 10.134.1.104
[2024-03-25 16:04:16,711] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 33
MA = 10.134.1.104
World view:  33 48 10.134.1.104
[2024-03-25 16:04:16,711] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 18
MA = 10.134.1.104
World view:  18 48 10.134.1.104
[2024-03-25 16:04:16,711] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
 > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 45
MA = 10.134.1.104
World view:  45 48 10.134.1.104
[2024-03-25 16:04:16,712] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 7
MA = 10.134.1.104
World view:  7 48 10.134.1.104
[2024-03-25 16:04:16,711] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-25 16:04:16,711] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-25 16:04:16,712] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-25 16:04:16,711] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-25 16:04:16,711] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 29
MA = 10.134.1.104
World view:  29 48 10.134.1.104
[2024-03-25 16:04:16,711] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 44
MA = 10.134.1.104
World view:  44 48 10.134.1.104
[2024-03-25 16:04:16,712] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 40
MA = 10.134.1.104
World view:  40 48 10.134.1.104
[2024-03-25 16:04:16,711] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-25 16:04:16,711] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-25 16:04:16,711] [INFO] [comm.py:616:init_distributed] cdb=None
> initializing torch distributed ...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 43
MA = 10.134.1.104
World view:  43 48 10.134.1.104
[2024-03-25 16:04:16,712] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-25 16:04:16,712] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 11
MA = 10.134.1.104
World view:  11 48 10.134.1.104
[2024-03-25 16:04:16,711] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-25 16:04:16,711] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 19
MA = 10.134.1.104
World view:  19 48 10.134.1.104
[2024-03-25 16:04:16,711] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 15
MA = 10.134.1.104
World view:  15 48 10.134.1.104
[2024-03-25 16:04:16,711] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 42
MA = 10.134.1.104
World view:  42 48 10.134.1.104
[2024-03-25 16:04:16,712] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-25 16:04:16,712] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 34
MA = 10.134.1.104
World view:  34 48 10.134.1.104
[2024-03-25 16:04:16,711] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-25 16:04:16,712] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 26
MA = 10.134.1.104
World view:  26 48 10.134.1.104
[2024-03-25 16:04:16,711] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-25 16:04:16,712] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-25 16:04:16,711] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 22
MA = 10.134.1.104
World view:  22 48 10.134.1.104
[2024-03-25 16:04:16,712] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 4
MA = 10.134.1.104
World view:  4 48 10.134.1.104
[2024-03-25 16:04:16,712] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 46
MA = 10.134.1.104
World view:  46 48 10.134.1.104
[2024-03-25 16:04:16,713] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-25 16:04:16,712] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 13
MA = 10.134.1.104
World view:  13 48 10.134.1.104
[2024-03-25 16:04:16,711] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 10
MA = 10.134.1.104
World view:  10 48 10.134.1.104
[2024-03-25 16:04:16,711] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-25 16:04:16,713] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-25 16:04:16,711] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-25 16:04:16,711] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-25 16:04:16,712] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 30
MA = 10.134.1.104
World view:  30 48 10.134.1.104
[2024-03-25 16:04:16,711] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 9
MA = 10.134.1.104
World view:  9 48 10.134.1.104
[2024-03-25 16:04:16,712] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-25 16:04:16,712] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 27
MA = 10.134.1.104
World view:  27 48 10.134.1.104
[2024-03-25 16:04:16,711] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-25 16:04:16,711] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-25 16:04:16,711] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-25 16:04:16,711] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-25 16:04:16,712] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 5
MA = 10.134.1.104
World view:  5 48 10.134.1.104
[2024-03-25 16:04:16,712] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 31
MA = 10.134.1.104
World view:  31 48 10.134.1.104
[2024-03-25 16:04:16,711] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-25 16:04:16,711] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 32
MA = 10.134.1.104
World view:  32 48 10.134.1.104
[2024-03-25 16:04:16,711] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-25 16:04:16,712] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-25 16:04:16,711] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-25 16:04:16,711] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 1
MA = 10.134.1.104
World view:  1 48 10.134.1.104
[2024-03-25 16:04:16,712] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-25 16:04:16,712] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 35
MA = 10.134.1.104
World view:  35 48 10.134.1.104
[2024-03-25 16:04:16,712] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 12
MA = 10.134.1.104
World view:  12 48 10.134.1.104
[2024-03-25 16:04:16,711] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-25 16:04:16,711] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-25 16:04:16,712] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-25 16:04:16,711] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-25 16:04:16,712] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-25 16:04:16,711] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-25 16:04:16,711] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 3
MA = 10.134.1.104
World view:  3 48 10.134.1.104
[2024-03-25 16:04:16,712] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-25 16:04:16,712] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 39
MA = 10.134.1.104
World view:  39 48 10.134.1.104
[2024-03-25 16:04:16,711] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 24
MA = 10.134.1.104
World view:  24 48 10.134.1.104
[2024-03-25 16:04:16,711] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-25 16:04:16,712] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-25 16:04:16,712] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-25 16:04:16,712] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-25 16:04:16,712] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 17
MA = 10.134.1.104
World view:  17 48 10.134.1.104
[2024-03-25 16:04:16,712] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-25 16:04:16,712] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 14
MA = 10.134.1.104
World view:  14 48 10.134.1.104
[2024-03-25 16:04:16,712] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 2
MA = 10.134.1.104
World view:  2 48 10.134.1.104
[2024-03-25 16:04:16,712] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-25 16:04:16,712] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-25 16:04:16,712] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-25 16:04:16,712] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 16
MA = 10.134.1.104
World view:  16 48 10.134.1.104
[2024-03-25 16:04:16,712] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 41
MA = 10.134.1.104
World view:  41 48 10.134.1.104
[2024-03-25 16:04:16,712] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-25 16:04:16,712] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-25 16:04:16,712] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 38
MA = 10.134.1.104
World view:  38 48 10.134.1.104
[2024-03-25 16:04:16,712] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-25 16:04:16,712] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 36
MA = 10.134.1.104
World view:  36 48 10.134.1.104
[2024-03-25 16:04:16,712] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-25 16:04:16,713] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 37
MA = 10.134.1.104
World view:  37 48 10.134.1.104
[2024-03-25 16:04:16,713] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-25 16:04:16,713] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 48 47
MA = 10.134.1.104
World view:  47 48 10.134.1.104
> setting tensorboard ...
[2024-03-25 16:04:27,439] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-25 16:04:27,439] [INFO] [comm.py:616:init_distributed] cdb=None
> initialized tensor model parallel with size 1
> initialized pipeline model parallel with size 1
> setting random seeds to 1234 ...
[2024-03-25 16:04:30,386] [INFO] [checkpointing.py:226:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
make: Entering directory '/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/data'
make: Nothing to be done for 'default'.
make: Leaving directory '/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/data'
> compiling dataset index builder ...
>>> done with dataset index builder. Compilation time: 0.089 seconds
> compiling and loading fused kernels ...
ninja: no work to do.
ninja: no work to do.
ninja: no work to do.
>>> done with compiling and loading fused kernels. Compilation time: 2.793 seconds
time to initialize megatron (seconds): 19.912
[after megatron is initialized] datetime: 2024-03-25 16:04:35 
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=0, world_size=48, transformer_pipeline_model_parallel_size=1, data_parallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
building GPT model ...
[2024-03-25 16:04:35,465] [INFO] [utils.py:785:see_memory_usage] Before Building Model
[2024-03-25 16:04:35,466] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.53 GB         CA 0.0 GB         Max_CA 1 GB 
[2024-03-25 16:04:35,466] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 51.18 GB, percent = 8.6%
[2024-03-25 16:04:35,738] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 8 | num_local_experts: 1 | expert_parallel_size: 8
[2024-03-25 16:04:35,771] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 8 | num_local_experts: 1 | expert_parallel_size: 8
[2024-03-25 16:04:35,801] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 8 | num_local_experts: 1 | expert_parallel_size: 8
[2024-03-25 16:04:35,833] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 8 | num_local_experts: 1 | expert_parallel_size: 8
[2024-03-25 16:04:35,862] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 8 | num_local_experts: 1 | expert_parallel_size: 8
[2024-03-25 16:04:35,894] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 8 | num_local_experts: 1 | expert_parallel_size: 8
[2024-03-25 16:04:35,935] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 8 | num_local_experts: 1 | expert_parallel_size: 8
[2024-03-25 16:04:35,968] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 8 | num_local_experts: 1 | expert_parallel_size: 8
[2024-03-25 16:04:35,998] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 8 | num_local_experts: 1 | expert_parallel_size: 8
[2024-03-25 16:04:36,046] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 8 | num_local_experts: 1 | expert_parallel_size: 8
[2024-03-25 16:04:36,076] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 8 | num_local_experts: 1 | expert_parallel_size: 8
[2024-03-25 16:04:36,107] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 8 | num_local_experts: 1 | expert_parallel_size: 8
[2024-03-25 16:04:36,198] [INFO] [utils.py:785:see_memory_usage] After Building Model
[2024-03-25 16:04:36,199] [INFO] [utils.py:786:see_memory_usage] MA 2.44 GB         Max_MA 2.51 GB         CA 2.54 GB         Max_CA 3 GB 
[2024-03-25 16:04:36,200] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 65.54 GB, percent = 11.0%
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1316016128
> learning rate decay style: cosine
DeepSpeed is enabled.
[2024-03-25 16:04:36,207] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0+f5c834a6e, git-hash=f5c834a6e, git-branch=HEAD
No existing process group found, creating a new group named: ep_size_8
[2024-03-25 16:04:36,236] [INFO] [logging.py:96:log_dist] [Rank 0] Creating expert and data parallel groups with size 8
[2024-03-25 16:04:36,308] [INFO] [logging.py:96:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_8 with ranks: [0, 8, 16, 24, 32, 40]
[2024-03-25 16:04:36,319] [INFO] [logging.py:96:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_8 with ranks: [1, 9, 17, 25, 33, 41]
[2024-03-25 16:04:36,329] [INFO] [logging.py:96:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_8 with ranks: [2, 10, 18, 26, 34, 42]
[2024-03-25 16:04:36,340] [INFO] [logging.py:96:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_8 with ranks: [3, 11, 19, 27, 35, 43]
[2024-03-25 16:04:36,350] [INFO] [logging.py:96:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_8 with ranks: [4, 12, 20, 28, 36, 44]
[2024-03-25 16:04:36,361] [INFO] [logging.py:96:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_8 with ranks: [5, 13, 21, 29, 37, 45]
[2024-03-25 16:04:36,371] [INFO] [logging.py:96:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_8 with ranks: [6, 14, 22, 30, 38, 46]
[2024-03-25 16:04:36,382] [INFO] [logging.py:96:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_8 with ranks: [7, 15, 23, 31, 39, 47]
[2024-03-25 16:04:36,393] [INFO] [logging.py:96:log_dist] [Rank 0] creating expert parallel process group named ep_size_8 with ranks: [0, 1, 2, 3, 4, 5, 6, 7]
[2024-03-25 16:04:36,403] [INFO] [logging.py:96:log_dist] [Rank 0] creating expert parallel process group named ep_size_8 with ranks: [8, 9, 10, 11, 12, 13, 14, 15]
[2024-03-25 16:04:36,414] [INFO] [logging.py:96:log_dist] [Rank 0] creating expert parallel process group named ep_size_8 with ranks: [16, 17, 18, 19, 20, 21, 22, 23]
[2024-03-25 16:04:36,424] [INFO] [logging.py:96:log_dist] [Rank 0] creating expert parallel process group named ep_size_8 with ranks: [24, 25, 26, 27, 28, 29, 30, 31]
[2024-03-25 16:04:36,435] [INFO] [logging.py:96:log_dist] [Rank 0] creating expert parallel process group named ep_size_8 with ranks: [32, 33, 34, 35, 36, 37, 38, 39]
[2024-03-25 16:04:36,445] [INFO] [logging.py:96:log_dist] [Rank 0] creating expert parallel process group named ep_size_8 with ranks: [40, 41, 42, 43, 44, 45, 46, 47]
[2024-03-25 16:04:37,462] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-03-25 16:04:37,469] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2024-03-25 16:04:37,469] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-03-25 16:04:37,535] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2024-03-25 16:04:37,535] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=46, world_size=48, transformer_pipeline_model_parallel_size=1, datatensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=20, world_size=48, transformer_pipeline_model_parallel_size=1, data_parallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
_parallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=34, world_size=48, transformer_pipeline_model_parallel_size=1, data_parallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=41, world_size=48, transformer_pipeline_model_parallel_size=1, data_parallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=19, world_size=48, transformer_pipeline_model_parallel_size=1, data_parallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=10, world_size=48, transformer_pipeline_model_parallel_size=1, data_parallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=37, world_size=48, transformer_pipeline_model_parallel_size=1, data_parallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=4, world_size=48, transformer_pipeline_model_parallel_size=1, data_parallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=32, world_size=48, transformer_pipeline_model_parallel_size=1, data_parallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=43, world_size=48, transformer_pipeline_model_parallel_size=1, data_parallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=5, world_size=48, transformer_pipeline_model_parallel_size=1, data_parallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=33, world_size=48, transformer_pipeline_model_parallel_size=1, data_parallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=42, world_size=48, transformer_pipeline_model_parallel_size=1, data_parallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=3, world_size=48, transformer_pipeline_model_parallel_size=1, data_parallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=6, world_size=48, transformer_pipeline_model_parallel_size=1, data_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=8, world_size=48, transformer_pipeline_model_parallel_size=1, data_Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_parallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=11, world_size=48, transformer_pipeline_model_parallel_size=1, dataparallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
_parallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=21, world_size=48, transformer_pipeline_model_parallel_size=1, data_parallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=47, world_size=48, transformer_pipeline_model_parallel_size=1, data_parallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=16, world_size=48, transformer_pipeline_model_parallel_size=1, datatensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=17, world_size=48, transformer_pipeline_model_parallel_size=1, data_parallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
_parallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=40, world_size=48, transformer_pipeline_model_parallel_size=1, data_parallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=14, world_size=48, transformer_pipeline_model_parallel_size=1, datatensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=12, world_size=48, transformer_pipeline_model_parallel_size=1, data_parallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
_parallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=15, world_size=48, transformer_pipeline_model_parallel_size=1, data_parallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=36, world_size=48, transformer_pipeline_model_parallel_size=1, data_parallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=44, world_size=48, transformer_pipeline_model_parallel_size=1, dataArgs= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather__parallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=39, world_size=48, transformer_pipeline_model_parallel_size=1, data_parallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=2, world_size=48, transformer_pipeline_model_parallel_size=1, data_parallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=30, world_size=48, transformer_pipeline_model_parallel_size=1, datatensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=31, world_size=48, transformer_pipeline_model_parallel_size=1, data_parallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
_parallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=1, world_size=48, transformer_pipeline_model_parallel_size=1, data_parallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=13, world_size=48, transformer_pipeline_model_parallel_size=1, data_parallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=7, world_size=48, transformer_pipeline_model_parallel_size=1, data_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=9, world_size=48, transformer_pipeline_model_parallel_size=1, data_parallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
parallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=22, world_size=48, transformer_pipeline_model_parallel_size=1, datatensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=23, world_size=48, transformer_pipeline_model_parallel_size=1, datatensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=18, world_size=48, transformer_pipeline_model_parallel_size=1, data_parallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
_parallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
_parallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=45, world_size=48, transformer_pipeline_model_parallel_size=1, data_parallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=35, world_size=48, transformer_pipeline_model_parallel_size=1, data_parallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=24, world_size=48, transformer_pipeline_model_parallel_size=1, datatensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=25, world_size=48, transformer_pipeline_model_parallel_size=1, datatensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=26, world_size=48, transformer_pipeline_model_parallel_size=1, datatensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=27, world_size=48, transformer_pipeline_model_parallel_size=1, datatensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=29, world_size=48, transformer_pipeline_model_parallel_size=1, dataArgs= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather__parallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
_parallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
_parallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
_parallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=28, world_size=48, transformer_pipeline_model_parallel_size=1, data_parallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
_parallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Args= Namespace(num_layers=24, encoder_num_layers=24, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=2048, ffn_hidden_size=8192, num_attention_heads=16, num_key_value_heads=16, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=96, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=4577636, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test_batch4_2024.03.25-16.04.00', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', save_interval=10, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-1.3B-lr-1.2e-4-minlr-1.0e-6-bs-96-gpus-96-mp-1-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true-test', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=1, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=38, world_size=48, transformer_pipeline_model_parallel_size=1, data_parallel_size=48, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50304, deepspeed_config_dict={'train_batch_size': 96, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 587406, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8

------------------------------------------------------------
Sender: LSF System <lsfadmin@batch4>
Subject: Job 3363452: <gpt3-megatron> in cluster <summit> Done

Job <gpt3-megatron> was submitted from host <login2> by user <sajaldash> in cluster <summit> at Mon Mar 25 16:03:11 2024
Job was executed on host(s) <1*batch4>, in queue <debug>, as user <sajaldash> in cluster <summit> at Mon Mar 25 16:03:40 2024
                            <42*a15n05>
                            <42*a18n17>
                            <42*a18n18>
                            <42*a19n01>
                            <42*a19n02>
                            <42*a19n03>
                            <42*a19n04>
                            <42*a19n05>
                            <42*a19n06>
                            <42*a19n07>
                            <42*a19n08>
                            <42*a19n09>
                            <42*a19n10>
                            <42*a19n11>
                            <42*a19n12>
                            <42*a19n13>
</ccs/home/sajaldash> was used as the home directory.
</gpfs/alpine2/world-shared/stf218/sajal/Megatron-DeepSpeed> was used as the working directory.
Started at Mon Mar 25 16:03:40 2024
Terminated at Mon Mar 25 16:04:43 2024
Results reported at Mon Mar 25 16:04:43 2024

The output (if any) is above this job summary.



PS:

Read file <logs/gpt3-megatron.3363452.e> for stderr output of this job.

