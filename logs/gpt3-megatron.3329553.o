g25n18
10.134.17.137
[2024-03-04 12:18:36,876] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:36,877] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:36,877] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:36,877] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:36,876] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:36,876] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:36,877] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:36,877] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:36,877] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:36,877] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:36,878] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:36,878] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:36,881] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:36,881] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:36,881] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:36,881] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:36,881] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:36,881] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:36,885] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:36,885] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:36,885] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:36,885] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:36,885] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:36,885] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:36,885] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:36,885] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:36,885] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:36,885] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:36,885] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:36,885] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:36,885] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:36,887] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:36,889] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:36,890] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:36,890] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:36,890] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,112] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,112] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,112] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,112] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,112] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,113] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,112] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,113] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,113] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,113] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,113] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,113] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,113] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,113] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,113] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,113] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,113] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,113] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,113] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,113] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,113] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,113] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,113] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,113] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,113] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,114] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,113] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,113] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,113] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,114] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,114] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,113] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,113] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,114] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,114] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,113] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,113] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,114] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,115] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,113] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,113] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,115] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,113] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,115] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,113] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,115] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,114] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,115] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,114] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,115] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,114] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,114] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,114] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,114] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,114] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,114] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,114] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,114] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,114] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 12:18:40,114] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 32
MA = 10.134.17.137
World view:  32 96 10.134.17.137
[2024-03-04 12:18:40,345] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:40,345] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 31
MA = 10.134.17.137
World view:  31 96 10.134.17.137
[2024-03-04 12:18:40,345] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:40,345] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 34
MA = 10.134.17.137
World view:  34 96 10.134.17.137
[2024-03-04 12:18:40,345] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:40,345] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 35
MA = 10.134.17.137
World view:  35 96 10.134.17.137
[2024-03-04 12:18:40,345] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:40,346] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 30
MA = 10.134.17.137
World view:  30 96 10.134.17.137
[2024-03-04 12:18:40,346] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 33
MA = 10.134.17.137
World view:  33 96 10.134.17.137
[2024-03-04 12:18:40,346] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:40,346] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 12:18:40,346] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 42
MA = 10.134.17.137
World view:  42 96 10.134.17.137
[2024-03-04 12:18:40,376] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 44
MA = 10.134.17.137
World view:  44 96 10.134.17.137
[2024-03-04 12:18:40,376] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:40,376] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 12:18:40,376] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 43
MA = 10.134.17.137
World view:  43 96 10.134.17.137
[2024-03-04 12:18:40,376] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:40,376] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 47
MA = 10.134.17.137
World view:  47 96 10.134.17.137
[2024-03-04 12:18:40,377] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:40,377] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 46
MA = 10.134.17.137
World view:  46 96 10.134.17.137
[2024-03-04 12:18:40,378] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:40,378] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 45
MA = 10.134.17.137
World view:  45 96 10.134.17.137
[2024-03-04 12:18:40,379] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:40,379] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 26
MA = 10.134.17.137
World view:  26 96 10.134.17.137
[2024-03-04 12:18:40,380] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:40,380] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 24
MA = 10.134.17.137
World view:  24 96 10.134.17.137
[2024-03-04 12:18:40,381] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:40,381] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 25
MA = 10.134.17.137
World view:  25 96 10.134.17.137
[2024-03-04 12:18:40,381] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:40,381] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 27
MA = 10.134.17.137
World view:  27 96 10.134.17.137
[2024-03-04 12:18:40,381] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 29
MA = 10.134.17.137
World view:  29 96 10.134.17.137
[2024-03-04 12:18:40,381] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:40,381] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 12:18:40,381] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 28
MA = 10.134.17.137
World view:  28 96 10.134.17.137
[2024-03-04 12:18:40,381] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:40,381] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 37
MA = 10.134.17.137
World view:  37 96 10.134.17.137
[2024-03-04 12:18:40,382] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:40,382] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 38
MA = 10.134.17.137
World view:  38 96 10.134.17.137
[2024-03-04 12:18:40,382] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 40
MA = 10.134.17.137
World view:  40 96 10.134.17.137
[2024-03-04 12:18:40,382] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:40,382] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 12:18:40,382] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 36
MA = 10.134.17.137
World view:  36 96 10.134.17.137
[2024-03-04 12:18:40,382] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:40,383] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 41
MA = 10.134.17.137
World view:  41 96 10.134.17.137
[2024-03-04 12:18:40,383] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:40,383] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 39
MA = 10.134.17.137
World view:  39 96 10.134.17.137
[2024-03-04 12:18:40,383] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:40,383] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 20
MA = 10.134.17.137
World view:  20 96 10.134.17.137
[2024-03-04 12:18:40,386] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 19
MA = 10.134.17.137
World view:  19 96 10.134.17.137
[2024-03-04 12:18:40,386] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:40,386] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 12:18:40,386] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 21
MA = 10.134.17.137
World view:  21 96 10.134.17.137
[2024-03-04 12:18:40,386] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 23
MA = 10.134.17.137
World view:  23 96 10.134.17.137
[2024-03-04 12:18:40,386] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:40,386] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 18
MA = 10.134.17.137
World view:  18 96 10.134.17.137
[2024-03-04 12:18:40,386] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:40,386] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 12:18:40,386] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 22
MA = 10.134.17.137
World view:  22 96 10.134.17.137
[2024-03-04 12:18:40,386] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:40,387] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 48
MA = 10.134.17.137
World view:  48 96 10.134.17.137
[2024-03-04 12:18:40,394] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 50
MA = 10.134.17.137
World view:  50 96 10.134.17.137
[2024-03-04 12:18:40,394] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:40,394] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 12:18:40,394] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 51
MA = 10.134.17.137
World view:  51 96 10.134.17.137
[2024-03-04 12:18:40,395] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:40,395] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 52
MA = 10.134.17.137
World view:  52 96 10.134.17.137
[2024-03-04 12:18:40,395] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:40,395] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 49
MA = 10.134.17.137
World view:  49 96 10.134.17.137
[2024-03-04 12:18:40,395] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:40,396] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 53
MA = 10.134.17.137
World view:  53 96 10.134.17.137
[2024-03-04 12:18:40,396] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:40,396] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 0
MA = 10.134.17.137
World view:  0 96 10.134.17.137
using world size: 96, data-parallel-size: 24, sequence-parallel size: 1, tensor-model-parallel size: 4, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  add_bias_linear ................................. True
  add_position_embedding .......................... True
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  aml_data_download_path .......................... None
  apply_layernorm_1p .............................. False
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  async_tensor_model_parallel_allreduce ........... False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  barrier_with_L1_time ............................ True
  bert_binary_head ................................ True
  bert_embedder_type .............................. megatron
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... True
  checkpoint_in_cpu ............................... False
  checkpoint_num_layers ........................... 1
  classes_fraction ................................ 1.0
  clip_grad ....................................... 1.0
  compression_training ............................ False
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  create_moe_param_group .......................... True
  curriculum_learning_legacy ...................... False
  data_cache_path ................................. None
  data_efficiency_curriculum_learning ............. False
  data_impl ....................................... mmap
  data_parallel_random_init ....................... False
  data_parallel_size .............................. 24
  data_path ....................................... ['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document']
  data_per_class_fraction ......................... 1.0
  data_sharding ................................... True
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_num_layers .............................. None
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... True
  deepspeed_activation_checkpointing .............. True
  deepspeed_config ................................ ds_config_gpt_gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true.json
  deepspeed_mpi ................................... False
  dino_bottleneck_size ............................ 256
  dino_freeze_last_layer .......................... 1
  dino_head_hidden_size ........................... 2048
  dino_local_crops_number ......................... 10
  dino_local_img_size ............................. 96
  dino_norm_last_layer ............................ False
  dino_teacher_temp ............................... 0.07
  dino_warmup_teacher_temp ........................ 0.04
  dino_warmup_teacher_temp_epochs ................. 30
  distribute_checkpointed_activations ............. False
  distribute_saved_activations .................... False
  distributed_backend ............................. nccl
  distributed_timeout_minutes ..................... 10
  ds_fused_adam ................................... False
  ds_inference .................................... False
  ds_pipeline_enabled ............................. False
  ds_sequence_parallel_size ....................... 1
  embedding_path .................................. None
  embedding_weights_in_fp32 ....................... False
  empty_unused_memory_level ....................... 0
  enable_expert_tensor_parallelism ................ False
  encoder_num_layers .............................. 32
  encoder_seq_length .............................. 2048
  end_weight_decay ................................ 0.1
  eod_mask_loss ................................... False
  eval_interval ................................... 100
  eval_iters ...................................... 10
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... 30000000
  exit_interval ................................... None
  exit_on_missing_checkpoint ...................... False
  exit_signal_handler ............................. False
  expert_interval ................................. 2
  ffn_hidden_size ................................. 16320
  finetune ........................................ False
  force_ds_sequence_parallel ...................... False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  fp8_amax_compute_algo ........................... most_recent
  fp8_amax_history_len ............................ 1
  fp8_e4m3 ........................................ False
  fp8_hybrid ...................................... False
  fp8_interval .................................... 1
  fp8_margin ...................................... 0
  fp8_wgrad ....................................... True
  global_batch_size ............................... 24
  gradient_accumulation_fusion .................... True
  head_lr_mult .................................... 1.0
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 4080
  hidden_size_teacher ............................. None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_h ........................................... 224
  img_w ........................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference ....................................... False
  inference_batch_times_seqlen_threshold .......... 512
  init_method_std ................................. 0.014
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  iter_per_epoch .................................. 1250
  kd .............................................. False
  kd_alpha_ce ..................................... 1
  kd_beta_ce ...................................... 1
  kd_temp ......................................... 1.0
  kv_channels ..................................... 170
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true
  load_tag ........................................ None
  load_teacher .................................... None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... True
  log_interval .................................... 10
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... False
  log_optimizer_states_to_tensorboard ............. False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... True
  log_validation_ppl_to_tensorboard ............... True
  log_world_size_to_tensorboard ................... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.00012
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_decay_tokens ................................. 300000000000
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_warmup_tokens ................................ 375000000
  make_vocab_size_divisible_by .................... 128
  mask_factor ..................................... 1.0
  mask_prob ....................................... 0.15
  mask_type ....................................... random
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 2048
  max_tokens_to_oom ............................... 12000
  mem_efficient_ln ................................ True
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... gpt2-merges.txt
  micro_batch_size ................................ 1
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-06
  mlp_type ........................................ standard
  mmap_warmup ..................................... False
  moe_eval_capacity_factor ........................ 1.0
  moe_expert_parallel_size ........................ 8
  moe_loss_coeff .................................. 0.01
  moe_min_capacity ................................ 4
  moe_token_dropping .............................. True
  moe_train_capacity_factor ....................... 1.0
  mos ............................................. False
  no_load_lr_state ................................ False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_persist_layer_norm ........................... False
  no_pipeline_parallel ............................ True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  normalization ................................... layernorm
  num_attention_heads ............................. 24
  num_attention_heads_teacher ..................... None
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... [8]
  num_experts_switch .............................. None
  num_experts_teacher ............................. [1]
  num_key_value_heads ............................. 24
  num_layers ...................................... 32
  num_layers_per_virtual_pipeline_stage ........... None
  num_layers_teacher .............................. None
  num_workers ..................................... 0
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  output_bert_embeddings .......................... False
  overlap_p2p_comm ................................ False
  override_opt_param_scheduler .................... True
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  perform_initialization .......................... True
  pipeline_model_parallel_size .................... 1
  pipeline_model_parallel_split_rank .............. None
  profile_backward ................................ False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  random_ltd ...................................... False
  rank ............................................ 0
  recompute_granularity ........................... None
  recompute_method ................................ None
  recompute_num_layers ............................ 1
  remote_device ................................... none
  repeated_dataloader ............................. False
  reset_attention_mask ............................ False
  reset_iteration ................................. False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  retro_add_retriever ............................. False
  retro_cyclic_train_iters ........................ None
  retro_encoder_attention_dropout ................. 0.1
  retro_encoder_hidden_dropout .................... 0.1
  retro_encoder_layers ............................ 2
  retro_num_neighbors ............................. 2
  retro_num_retrieved_chunks ...................... 2
  retro_return_doc_ids ............................ False
  retro_workdir ................................... None
  return_data_index ............................... False
  rope_theta ...................................... 10000
  rotary_percent .................................. 1.0
  sample_rate ..................................... 1.0
  save ............................................ /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true
  save_interval ................................... 10000
  scatter_gather_tensors_in_pipeline .............. True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 2048
  sequence_parallel ............................... False
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  skip_train ...................................... False
  split ........................................... 98,2,0
  split_transformers .............................. False
  squared_relu .................................... False
  standalone_embedding_stage ...................... False
  start_weight_decay .............................. 0.1
  swiglu .......................................... False
  swin_backbone_type .............................. tiny
  synchronize_each_layer .......................... False
  tensor_model_parallel_size ...................... 4
  tensorboard_dir ................................. /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1
  test_data_path .................................. None
  tile_factor ..................................... 1
  timing_log_level ................................ 0
  timing_log_option ............................... minmax
  titles_data_path ................................ None
  tokenizer_model ................................. None
  tokenizer_type .................................. GPT2BPETokenizer
  topk ............................................ 1
  train_data_exact_num_epochs ..................... None
  train_data_path ................................. None
  train_desc_path ................................. None
  train_doc_idx_path .............................. None
  train_idx_path .................................. None
  train_iters ..................................... 18310546
  train_sample_idx_path ........................... None
  train_samples ................................... None
  train_shuffle_idx_path .......................... None
  train_tokens .................................... 300000000000
  transformer_impl ................................ local
  transformer_pipeline_model_parallel_size ........ 1
  universal_checkpoint ............................ False
  untie_embeddings_and_output_weights ............. False
  use_checkpoint_args ............................. False
  use_checkpoint_opt_param_scheduler .............. False
  use_contiguous_buffers_in_local_ddp ............. True
  use_cpu_initialization .......................... None
  use_dataset_only ................................ False
  use_distributed_optimizer ....................... False
  use_flash_attn .................................. False
  use_flash_attn_triton ........................... False
  use_flash_attn_v1 ............................... False
  use_flash_attn_v2 ............................... False
  use_one_sent_docs ............................... False
  use_pin_memory .................................. False
  use_ring_exchange_p2p ........................... False
  use_rotary_position_embeddings .................. False
  use_tutel ....................................... False
  valid_data_path ................................. None
  variable_seq_lengths ............................ False
  virtual_pipeline_model_parallel_size ............ None
  vision_backbone_type ............................ vit
  vision_pretraining .............................. False
  vision_pretraining_type ......................... classify
  vocab_extra_ids ................................. 0
  vocab_file ...................................... gpt2-vocab.json
  vocab_size ...................................... None
  weight_decay .................................... 0.1
  weight_decay_incr_style ......................... constant
  world_size ...................................... 96
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 1.0
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 1
> building GPT2BPETokenizer tokenizer ...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 3
MA = 10.134.17.137
World view:  3 96 10.134.17.137
[2024-03-04 12:18:46,627] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 5
MA = 10.134.17.137
World view:  5 96 10.134.17.137
[2024-03-04 12:18:46,627] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:46,627] [INFO] [comm.py:616:init_distributed] cdb=None
 > padded vocab (size: 50257) with 431 dummy tokens (new size: 50688)
> initializing torch distributed ...
[2024-03-04 12:18:46,627] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 12:18:46,627] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:46,627] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 1
MA = 10.134.17.137
World view:  1 96 10.134.17.137
[2024-03-04 12:18:46,627] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:46,627] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 12:18:46,627] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 2
MA = 10.134.17.137
World view:  2 96 10.134.17.137
[2024-03-04 12:18:46,627] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:46,627] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 4
MA = 10.134.17.137
World view:  4 96 10.134.17.137
[2024-03-04 12:18:46,627] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:46,627] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 69
MA = 10.134.17.137
World view:  69 96 10.134.17.137
[2024-03-04 12:18:46,629] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:46,629] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 70
MA = 10.134.17.137
World view:  70 96 10.134.17.137
[2024-03-04 12:18:46,629] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 62
MA = 10.134.17.137
World view:  62 96 10.134.17.137
[2024-03-04 12:18:46,629] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:46,630] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 92
MA = 10.134.17.137
World view:  92 96 10.134.17.137
[2024-03-04 12:18:46,630] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:46,630] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 71
MA = 10.134.17.137
World view:  71 96 10.134.17.137
[2024-03-04 12:18:46,630] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 94
MA = 10.134.17.137
World view:  94 96 10.134.17.137
[2024-03-04 12:18:46,630] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:46,630] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 93
MA = 10.134.17.137
World view:  93 96 10.134.17.137
[2024-03-04 12:18:46,630] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 61
MA = 10.134.17.137
World view:  61 96 10.134.17.137
[2024-03-04 12:18:46,630] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:46,630] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 66
MA = 10.134.17.137
World view:  66 96 10.134.17.137
[2024-03-04 12:18:46,630] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 91
MA = 10.134.17.137
World view:  91 96 10.134.17.137
[2024-03-04 12:18:46,630] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 64
MA = 10.134.17.137
World view:  64 96 10.134.17.137
[2024-03-04 12:18:46,630] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:46,630] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 12:18:46,630] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 12:18:46,630] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 67
MA = 10.134.17.137
World view:  67 96 10.134.17.137
[2024-03-04 12:18:46,631] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:46,630] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 82
MA = 10.134.17.137
World view:  82 96 10.134.17.137
[2024-03-04 12:18:46,631] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 63
MA = 10.134.17.137
World view:  63 96 10.134.17.137
[2024-03-04 12:18:46,631] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 68
MA = 10.134.17.137
World view:  68 96 10.134.17.137
[2024-03-04 12:18:46,631] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:46,630] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 12:18:46,631] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 12:18:46,631] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 12:18:46,631] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 12:18:46,630] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 80
MA = 10.134.17.137
World view:  80 96 10.134.17.137
[2024-03-04 12:18:46,631] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 60
MA = 10.134.17.137
World view:  60 96 10.134.17.137
[2024-03-04 12:18:46,631] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:46,631] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 90
MA = 10.134.17.137
World view:  90 96 10.134.17.137
[2024-03-04 12:18:46,631] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:46,631] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 65
MA = 10.134.17.137
World view:  65 96 10.134.17.137
[2024-03-04 12:18:46,631] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:46,631] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 83
MA = 10.134.17.137
World view:  83 96 10.134.17.137
[2024-03-04 12:18:46,631] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:46,632] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 12:18:46,632] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 12:18:46,632] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 81
MA = 10.134.17.137
World view:  81 96 10.134.17.137
[2024-03-04 12:18:46,632] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:46,632] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 79
MA = 10.134.17.137
World view:  79 96 10.134.17.137
[2024-03-04 12:18:46,632] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 74
MA = 10.134.17.137
World view:  74 96 10.134.17.137
[2024-03-04 12:18:46,633] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:46,632] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 78
MA = 10.134.17.137
World view:  78 96 10.134.17.137
[2024-03-04 12:18:46,633] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 6
MA = 10.134.17.137
World view:  6 96 10.134.17.137
[2024-03-04 12:18:46,633] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:46,633] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 72
MA = 10.134.17.137
World view:  72 96 10.134.17.137
[2024-03-04 12:18:46,633] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:46,633] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 12:18:46,633] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 7
MA = 10.134.17.137
World view:  7 96 10.134.17.137
[2024-03-04 12:18:46,633] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:46,633] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 12:18:46,633] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 76
MA = 10.134.17.137
World view:  76 96 10.134.17.137
[2024-03-04 12:18:46,634] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 8
MA = 10.134.17.137
World view:  8 96 10.134.17.137
[2024-03-04 12:18:46,633] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:46,634] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 12:18:46,633] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 73
MA = 10.134.17.137
World view:  73 96 10.134.17.137
[2024-03-04 12:18:46,634] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 9
MA = 10.134.17.137
World view:  9 96 10.134.17.137
[2024-03-04 12:18:46,633] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:46,634] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 12:18:46,634] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 77
MA = 10.134.17.137
World view:  77 96 10.134.17.137
[2024-03-04 12:18:46,634] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 11
MA = 10.134.17.137
World view:  11 96 10.134.17.137
[2024-03-04 12:18:46,634] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:46,634] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 12:18:46,634] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 75
MA = 10.134.17.137
World view:  75 96 10.134.17.137
[2024-03-04 12:18:46,635] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 10
MA = 10.134.17.137
World view:  10 96 10.134.17.137
[2024-03-04 12:18:46,634] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:46,635] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 12:18:46,635] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 88
MA = 10.134.17.137
World view:  88 96 10.134.17.137
[2024-03-04 12:18:46,635] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:46,635] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 87
MA = 10.134.17.137
World view:  87 96 10.134.17.137
[2024-03-04 12:18:46,635] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:46,635] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 86
MA = 10.134.17.137
World view:  86 96 10.134.17.137
[2024-03-04 12:18:46,635] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:46,635] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 84
MA = 10.134.17.137
World view:  84 96 10.134.17.137
[2024-03-04 12:18:46,635] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:46,635] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 85
MA = 10.134.17.137
World view:  85 96 10.134.17.137
[2024-03-04 12:18:46,635] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:46,636] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 89
MA = 10.134.17.137
World view:  89 96 10.134.17.137
[2024-03-04 12:18:46,636] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 57
MA = 10.134.17.137
World view:  57 96 10.134.17.137
[2024-03-04 12:18:46,637] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:46,637] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 12:18:46,636] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 59
MA = 10.134.17.137
World view:  59 96 10.134.17.137
[2024-03-04 12:18:46,637] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:46,638] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 56
MA = 10.134.17.137
World view:  56 96 10.134.17.137
[2024-03-04 12:18:46,638] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:46,638] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 54
MA = 10.134.17.137
World view:  54 96 10.134.17.137
[2024-03-04 12:18:46,638] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 58
MA = 10.134.17.137
World view:  58 96 10.134.17.137
[2024-03-04 12:18:46,638] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:46,638] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 12:18:46,638] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 55
MA = 10.134.17.137
World view:  55 96 10.134.17.137
[2024-03-04 12:18:46,638] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:46,638] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 14
MA = 10.134.17.137
World view:  14 96 10.134.17.137
[2024-03-04 12:18:46,982] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:46,982] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 16
MA = 10.134.17.137
World view:  16 96 10.134.17.137
[2024-03-04 12:18:46,982] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 13
MA = 10.134.17.137
World view:  13 96 10.134.17.137
[2024-03-04 12:18:46,982] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:46,982] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 12:18:46,982] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 15
MA = 10.134.17.137
World view:  15 96 10.134.17.137
[2024-03-04 12:18:46,982] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 12
MA = 10.134.17.137
World view:  12 96 10.134.17.137
[2024-03-04 12:18:46,982] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:46,982] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 12:18:46,982] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 17
MA = 10.134.17.137
World view:  17 96 10.134.17.137
[2024-03-04 12:18:46,982] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:46,982] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 95
MA = 10.134.17.137
World view:  95 96 10.134.17.137
> setting tensorboard ...
[2024-03-04 12:18:52,132] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 12:18:52,132] [INFO] [comm.py:616:init_distributed] cdb=None
> initialized tensor model parallel with size 4
> initialized pipeline model parallel with size 1
> setting random seeds to 1234 ...
[2024-03-04 12:18:56,715] [INFO] [checkpointing.py:226:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
make: Entering directory '/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/data'
make: Nothing to be done for 'default'.
make: Leaving directory '/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/data'
> compiling dataset index builder ...
>>> done with dataset index builder. Compilation time: 0.103 seconds
WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
> compiling and loading fused kernels ...
ninja: no work to do.
ninja: no work to do.
ninja: no work to do.
g25n18:898852:898852 [0] NCCL INFO Bootstrap : Using ib0:10.41.17.137<0>
g25n18:898852:898852 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g25n18:898852:898852 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g25n18:898852:898852 [0] NCCL INFO cudaDriverVersion 12020
NCCL version 2.18.3+cuda11.8
>>> done with compiling and loading fused kernels. Compilation time: 2.531 seconds
time to initialize megatron (seconds): 21.275
[after megatron is initialized] datetime: 2024-03-04 12:19:01 
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=0, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
building GPT model ...
[2024-03-04 12:19:01,269] [INFO] [utils.py:785:see_memory_usage] Before Building Model
[2024-03-04 12:19:01,270] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.27 GB         CA 0.0 GB         Max_CA 0 GB 
[2024-03-04 12:19:01,270] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 48.44 GB, percent = 8.1%
[2024-03-04 12:19:01,723] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 8 | num_local_experts: 1 | expert_parallel_size: 8
[2024-03-04 12:19:01,766] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 8 | num_local_experts: 1 | expert_parallel_size: 8
[2024-03-04 12:19:01,797] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 8 | num_local_experts: 1 | expert_parallel_size: 8
[2024-03-04 12:19:01,829] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 8 | num_local_experts: 1 | expert_parallel_size: 8
[2024-03-04 12:19:01,876] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 8 | num_local_experts: 1 | expert_parallel_size: 8
[2024-03-04 12:19:01,919] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 8 | num_local_experts: 1 | expert_parallel_size: 8
[2024-03-04 12:19:01,966] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 8 | num_local_experts: 1 | expert_parallel_size: 8
[2024-03-04 12:19:02,012] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 8 | num_local_experts: 1 | expert_parallel_size: 8
[2024-03-04 12:19:02,085] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 8 | num_local_experts: 1 | expert_parallel_size: 8
[2024-03-04 12:19:02,144] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 8 | num_local_experts: 1 | expert_parallel_size: 8
[2024-03-04 12:19:02,193] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 8 | num_local_experts: 1 | expert_parallel_size: 8
[2024-03-04 12:19:02,210] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 8 | num_local_experts: 1 | expert_parallel_size: 8
[2024-03-04 12:19:02,223] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 8 | num_local_experts: 1 | expert_parallel_size: 8
[2024-03-04 12:19:02,236] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 8 | num_local_experts: 1 | expert_parallel_size: 8
[2024-03-04 12:19:02,250] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 8 | num_local_experts: 1 | expert_parallel_size: 8
[2024-03-04 12:19:02,262] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 8 | num_local_experts: 1 | expert_parallel_size: 8
[2024-03-04 12:19:02,351] [INFO] [utils.py:785:see_memory_usage] After Building Model
[2024-03-04 12:19:02,352] [INFO] [utils.py:786:see_memory_usage] MA 6.1 GB         Max_MA 6.35 GB         CA 6.35 GB         Max_CA 6 GB 
[2024-03-04 12:19:02,352] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 82.17 GB, percent = 13.8%
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 3257904480
> learning rate decay style: cosine
DeepSpeed is enabled.
[2024-03-04 12:19:02,363] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0+f5c834a6e, git-hash=f5c834a6e, git-branch=HEAD
No existing process group found, creating a new group named: ep_size_8
[2024-03-04 12:19:02,408] [INFO] [logging.py:96:log_dist] [Rank 0] Creating expert and data parallel groups with size 8
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=2, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
 > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 3257904480
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=3, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
 > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 3257904480
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=1, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
 > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 3257904480
[2024-03-04 12:19:02,603] [INFO] [logging.py:96:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_8 with ranks: [0, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88]
[2024-03-04 12:19:02,613] [INFO] [logging.py:96:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_8 with ranks: [1, 9, 17, 25, 33, 41, 49, 57, 65, 73, 81, 89]
[2024-03-04 12:19:02,624] [INFO] [logging.py:96:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_8 with ranks: [2, 10, 18, 26, 34, 42, 50, 58, 66, 74, 82, 90]
[2024-03-04 12:19:02,635] [INFO] [logging.py:96:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_8 with ranks: [3, 11, 19, 27, 35, 43, 51, 59, 67, 75, 83, 91]
[2024-03-04 12:19:02,645] [INFO] [logging.py:96:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_8 with ranks: [4, 12, 20, 28, 36, 44, 52, 60, 68, 76, 84, 92]
[2024-03-04 12:19:02,656] [INFO] [logging.py:96:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_8 with ranks: [5, 13, 21, 29, 37, 45, 53, 61, 69, 77, 85, 93]
[2024-03-04 12:19:02,667] [INFO] [logging.py:96:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_8 with ranks: [6, 14, 22, 30, 38, 46, 54, 62, 70, 78, 86, 94]
[2024-03-04 12:19:02,678] [INFO] [logging.py:96:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_8 with ranks: [7, 15, 23, 31, 39, 47, 55, 63, 71, 79, 87, 95]
[2024-03-04 12:19:02,689] [INFO] [logging.py:96:log_dist] [Rank 0] creating expert parallel process group named ep_size_8 with ranks: [0, 1, 2, 3, 4, 5, 6, 7]
[2024-03-04 12:19:02,700] [INFO] [logging.py:96:log_dist] [Rank 0] creating expert parallel process group named ep_size_8 with ranks: [8, 9, 10, 11, 12, 13, 14, 15]
[2024-03-04 12:19:02,710] [INFO] [logging.py:96:log_dist] [Rank 0] creating expert parallel process group named ep_size_8 with ranks: [16, 17, 18, 19, 20, 21, 22, 23]
[2024-03-04 12:19:02,721] [INFO] [logging.py:96:log_dist] [Rank 0] creating expert parallel process group named ep_size_8 with ranks: [24, 25, 26, 27, 28, 29, 30, 31]
[2024-03-04 12:19:02,732] [INFO] [logging.py:96:log_dist] [Rank 0] creating expert parallel process group named ep_size_8 with ranks: [32, 33, 34, 35, 36, 37, 38, 39]
[2024-03-04 12:19:02,742] [INFO] [logging.py:96:log_dist] [Rank 0] creating expert parallel process group named ep_size_8 with ranks: [40, 41, 42, 43, 44, 45, 46, 47]
[2024-03-04 12:19:02,753] [INFO] [logging.py:96:log_dist] [Rank 0] creating expert parallel process group named ep_size_8 with ranks: [48, 49, 50, 51, 52, 53, 54, 55]
[2024-03-04 12:19:02,764] [INFO] [logging.py:96:log_dist] [Rank 0] creating expert parallel process group named ep_size_8 with ranks: [56, 57, 58, 59, 60, 61, 62, 63]
[2024-03-04 12:19:02,775] [INFO] [logging.py:96:log_dist] [Rank 0] creating expert parallel process group named ep_size_8 with ranks: [64, 65, 66, 67, 68, 69, 70, 71]
[2024-03-04 12:19:02,785] [INFO] [logging.py:96:log_dist] [Rank 0] creating expert parallel process group named ep_size_8 with ranks: [72, 73, 74, 75, 76, 77, 78, 79]
[2024-03-04 12:19:02,796] [INFO] [logging.py:96:log_dist] [Rank 0] creating expert parallel process group named ep_size_8 with ranks: [80, 81, 82, 83, 84, 85, 86, 87]
[2024-03-04 12:19:02,807] [INFO] [logging.py:96:log_dist] [Rank 0] creating expert parallel process group named ep_size_8 with ranks: [88, 89, 90, 91, 92, 93, 94, 95]
g25n18:898854:898854 [2] NCCL INFO cudaDriverVersion 12020
g25n18:898854:898854 [2] NCCL INFO Bootstrap : Using ib0:10.41.17.137<0>
g25n18:898854:898854 [2] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g25n18:898854:898854 [2] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g25n18:898854:899150 [2] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.137<0>
g25n18:898854:899150 [2] NCCL INFO Using network IB
g25n18:898854:899150 [2] NCCL INFO comm 0x14ddf3bd0 rank 2 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x28395441218594c5 - Init START
g25n18:898854:899150 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g25n18:898854:899150 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] -1/-1/-1->2->0 [2] 3/-1/-1->2->1 [3] -1/-1/-1->2->0
g25n18:898854:899150 [2] NCCL INFO P2P Chunksize set to 131072
g25n18:898854:899150 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/IPC
g25n18:898854:899150 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/IPC
g25n18:898854:899150 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/IPC
g25n18:898854:899150 [2] NCCL INFO Channel 03/0 : 2[2] -> 1[1] via P2P/IPC
g25n18:898854:899150 [2] NCCL INFO Connected all rings
g25n18:898854:899150 [2] NCCL INFO Channel 01/0 : 2[2] -> 0[0] via P2P/IPC
g25n18:898854:899150 [2] NCCL INFO Channel 03/0 : 2[2] -> 0[0] via P2P/IPC
g25n18:898854:899150 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/IPC
g25n18:898854:899150 [2] NCCL INFO Channel 02/0 : 2[2] -> 1[1] via P2P/IPC
g25n18:898854:899150 [2] NCCL INFO Connected all trees
g25n18:898854:899150 [2] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g25n18:898854:899150 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g25n18:898854:899150 [2] NCCL INFO comm 0x14ddf3bd0 rank 2 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x28395441218594c5 - Init COMPLETE
NCCL version 2.18.3+cuda11.8
g25n18:898853:898853 [1] NCCL INFO cudaDriverVersion 12020
g25n18:898853:898853 [1] NCCL INFO Bootstrap : Using ib0:10.41.17.137<0>
g25n18:898853:898853 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g25n18:898853:898853 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g25n18:898853:899153 [1] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.137<0>
g25n18:898853:899153 [1] NCCL INFO Using network IB
g25n18:898853:899153 [1] NCCL INFO comm 0x141103ca0 rank 1 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x28395441218594c5 - Init START
g25n18:898853:899153 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g25n18:898853:899153 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 0/-1/-1->1->5 [2] 2/-1/-1->1->0 [3] 0/-1/-1->1->5
g25n18:898853:899153 [1] NCCL INFO P2P Chunksize set to 131072
g25n18:898853:899153 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/IPC
g25n18:898853:899153 [1] NCCL INFO Channel 02/0 : 1[1] -> 2[2] via P2P/IPC
g25n18:898853:899153 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/IPC
g25n18:898853:899153 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/IPC
g25n18:898853:899153 [1] NCCL INFO Connected all rings
g25n18:898853:899153 [1] NCCL INFO Channel 01/0 : 1[1] -> 5[5] via P2P/IPC
g25n18:898853:899153 [1] NCCL INFO Channel 03/0 : 1[1] -> 5[5] via P2P/IPC
g25n18:898853:899153 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/IPC
g25n18:898853:899153 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/IPC
g25n18:898853:899153 [1] NCCL INFO Connected all trees
g25n18:898853:899153 [1] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g25n18:898853:899153 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g25n18:898853:899153 [1] NCCL INFO comm 0x141103ca0 rank 1 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x28395441218594c5 - Init COMPLETE
NCCL version 2.18.3+cuda11.8
g25n18:898855:898855 [3] NCCL INFO cudaDriverVersion 12020
g25n18:898855:898855 [3] NCCL INFO Bootstrap : Using ib0:10.41.17.137<0>
g25n18:898855:898855 [3] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g25n18:898855:898855 [3] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g25n18:898855:899149 [3] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.137<0>
g25n18:898855:899149 [3] NCCL INFO Using network IB
g25n18:898855:899149 [3] NCCL INFO comm 0x150794500 rank 3 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x28395441218594c5 - Init START
g25n18:898855:899149 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g25n18:898855:899149 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 5/-1/-1->3->4 [2] 4/-1/-1->3->2 [3] 5/-1/-1->3->4
g25n18:898855:899149 [3] NCCL INFO P2P Chunksize set to 131072
g25n18:898855:899149 [3] NCCL INFO Channel 00/0 : 3[3] -> 4[4] via P2P/IPC
g25n18:898855:899149 [3] NCCL INFO Channel 01/0 : 3[3] -> 4[4] via P2P/IPC
g25n18:898855:899149 [3] NCCL INFO Channel 02/0 : 3[3] -> 4[4] via P2P/IPC
g25n18:898855:899149 [3] NCCL INFO Channel 03/0 : 3[3] -> 4[4] via P2P/IPC
g25n18:898855:899149 [3] NCCL INFO Channel 01/0 : 90[0] -> 3[3] [receive] via NET/IB/3
g25n18:898855:899149 [3] NCCL INFO Channel 03/0 : 90[0] -> 3[3] [receive] via NET/IB/3
g25n18:898855:899149 [3] NCCL INFO Connected all rings
g25n18:898855:899149 [3] NCCL INFO Channel 01/0 : 3[3] -> 5[5] via P2P/IPC
g25n18:898855:899149 [3] NCCL INFO Channel 03/0 : 3[3] -> 5[5] via P2P/IPC
g25n18:898855:899149 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/IPC
g25n18:898855:899149 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/IPC
g25n18:898855:899149 [3] NCCL INFO Connected all trees
g25n18:898855:899149 [3] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g25n18:898855:899149 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g25n18:898855:899149 [3] NCCL INFO comm 0x150794500 rank 3 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x28395441218594c5 - Init COMPLETE
NCCL version 2.18.3+cuda11.8
g25n18:898856:898856 [4] NCCL INFO cudaDriverVersion 12020
g25n18:898856:898856 [4] NCCL INFO Bootstrap : Using ib0:10.41.17.137<0>
g25n18:898856:898856 [4] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g25n18:898856:898856 [4] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g25n18:898856:899152 [4] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.137<0>
g25n18:898856:899152 [4] NCCL INFO Using network IB
g25n18:898856:899152 [4] NCCL INFO comm 0x1495c4110 rank 4 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x28395441218594c5 - Init START
g25n18:898856:899152 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g25n18:898856:899152 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 3/52/-1->4->-1 [2] 5/-1/-1->4->3 [3] 3/-1/-1->4->10
g25n18:898856:899152 [4] NCCL INFO P2P Chunksize set to 131072
g25n18:898856:899152 [4] NCCL INFO Channel 00/0 : 4[4] -> 5[5] via P2P/IPC
g25n18:898856:899152 [4] NCCL INFO Channel 01/0 : 4[4] -> 5[5] via P2P/IPC
g25n18:898856:899152 [4] NCCL INFO Channel 02/0 : 4[4] -> 5[5] via P2P/IPC
g25n18:898856:899152 [4] NCCL INFO Channel 03/0 : 4[4] -> 5[5] via P2P/IPC
g25n18:898856:899152 [4] NCCL INFO Connected all rings
g25n18:898856:899152 [4] NCCL INFO Channel 03/0 : 4[4] -> 10[4] [send] via NET/IB/3
g25n18:898856:899152 [4] NCCL INFO Channel 01/0 : 52[4] -> 4[4] [receive] via NET/IB/3
g25n18:898856:899152 [4] NCCL INFO Channel 01/0 : 4[4] -> 52[4] [send] via NET/IB/3
g25n18:898856:899152 [4] NCCL INFO Channel 03/0 : 10[4] -> 4[4] [receive] via NET/IB/3
g25n18:898856:899152 [4] NCCL INFO Channel 00/0 : 4[4] -> 3[3] via P2P/IPC
g25n18:898856:899152 [4] NCCL INFO Channel 01/0 : 4[4] -> 3[3] via P2P/IPC
g25n18:898856:899152 [4] NCCL INFO Channel 02/0 : 4[4] -> 3[3] via P2P/IPC
g25n18:898856:899152 [4] NCCL INFO Channel 03/0 : 4[4] -> 3[3] via P2P/IPC
g25n18:898856:899152 [4] NCCL INFO Connected all trees
g25n18:898856:899152 [4] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g25n18:898856:899152 [4] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g25n18:898856:899152 [4] NCCL INFO comm 0x1495c4110 rank 4 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x28395441218594c5 - Init COMPLETE
g25n18:898856:899267 [4] NCCL INFO Using network IB
g25n18:898856:899267 [4] NCCL INFO comm 0x14c27c3c0 rank 1 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x864be1a757d9f276 - Init START
g25n18:898856:899267 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g25n18:898856:899267 [4] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
g25n18:898856:899267 [4] NCCL INFO P2P Chunksize set to 131072
g25n18:898856:899267 [4] NCCL INFO Channel 00/0 : 1[4] -> 2[2] [send] via NET/IB/1
g25n18:898856:899267 [4] NCCL INFO Channel 01/0 : 1[4] -> 2[2] [send] via NET/IB/1
g25n18:898856:899267 [4] NCCL INFO Connected all rings
g25n18:898856:899267 [4] NCCL INFO Channel 00/0 : 1[4] -> 0[0] via P2P/IPC
g25n18:898856:899267 [4] NCCL INFO Channel 01/0 : 1[4] -> 0[0] via P2P/IPC
g25n18:898856:899267 [4] NCCL INFO Connected all trees
g25n18:898856:899267 [4] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g25n18:898856:899267 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g25n18:898856:899267 [4] NCCL INFO comm 0x14c27c3c0 rank 1 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x864be1a757d9f276 - Init COMPLETE
NCCL version 2.18.3+cuda11.8
g25n18:898857:898857 [5] NCCL INFO cudaDriverVersion 12020
g25n18:898857:898857 [5] NCCL INFO Bootstrap : Using ib0:10.41.17.137<0>
g25n18:898857:898857 [5] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g25n18:898857:898857 [5] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g25n18:898857:899151 [5] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.137<0>
g25n18:898857:899151 [5] NCCL INFO Using network IB
g25n18:898857:899151 [5] NCCL INFO comm 0x135ea3890 rank 5 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x28395441218594c5 - Init START
g25n18:898857:899151 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g25n18:898857:899151 [5] NCCL INFO Trees [0] -1/-1/-1->5->4 [1] 1/-1/-1->5->3 [2] -1/-1/-1->5->4 [3] 1/-1/-1->5->3
g25n18:898857:899151 [5] NCCL INFO P2P Chunksize set to 131072
g25n18:898857:899151 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[0] [send] via NET/IB/1
g25n18:898857:899151 [5] NCCL INFO Channel 02/0 : 5[5] -> 6[0] [send] via NET/IB/1
g25n18:898857:899151 [5] NCCL INFO Channel 01/0 : 5[5] -> 2[2] via P2P/IPC
g25n18:898857:899151 [5] NCCL INFO Channel 03/0 : 5[5] -> 2[2] via P2P/IPC
g25n18:898857:899151 [5] NCCL INFO Connected all rings
g25n18:898857:899151 [5] NCCL INFO Channel 01/0 : 5[5] -> 1[1] via P2P/IPC
g25n18:898857:899151 [5] NCCL INFO Channel 03/0 : 5[5] -> 1[1] via P2P/IPC
g25n18:898857:899151 [5] NCCL INFO Channel 01/0 : 5[5] -> 3[3] via P2P/IPC
g25n18:898857:899151 [5] NCCL INFO Channel 03/0 : 5[5] -> 3[3] via P2P/IPC
g25n18:898857:899151 [5] NCCL INFO Channel 00/0 : 5[5] -> 4[4] via P2P/IPC
g25n18:898857:899151 [5] NCCL INFO Channel 02/0 : 5[5] -> 4[4] via P2P/IPC
g25n18:898857:899151 [5] NCCL INFO Connected all trees
g25n18:898857:899151 [5] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g25n18:898857:899151 [5] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g25n18:898857:899151 [5] NCCL INFO comm 0x135ea3890 rank 5 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x28395441218594c5 - Init COMPLETE
g25n18:898857:899262 [5] NCCL INFO Using network IB
g25n18:898857:899262 [5] NCCL INFO comm 0x139c34be0 rank 1 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xeee4c143812ce6e4 - Init START
g25n18:898857:899262 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g25n18:898857:899262 [5] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
g25n18:898857:899262 [5] NCCL INFO P2P Chunksize set to 131072
g25n18:898857:899262 [5] NCCL INFO Channel 00/0 : 1[5] -> 2[3] [send] via NET/IB/3
g25n18:898857:899262 [5] NCCL INFO Channel 01/0 : 1[5] -> 2[3] [send] via NET/IB/3
g25n18:898857:899262 [5] NCCL INFO Connected all rings
g25n18:898857:899262 [5] NCCL INFO Channel 00/0 : 1[5] -> 0[1] via P2P/IPC
g25n18:898857:899262 [5] NCCL INFO Channel 01/0 : 1[5] -> 0[1] via P2P/IPC
g25n18:898857:899262 [5] NCCL INFO Connected all trees
g25n18:898857:899262 [5] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g25n18:898857:899262 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g25n18:898857:899262 [5] NCCL INFO comm 0x139c34be0 rank 1 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xeee4c143812ce6e4 - Init COMPLETE
NCCL version 2.18.3+cuda11.8
g26n01:811332:811332 [1] NCCL INFO cudaDriverVersion 12020
g26n01:811332:811332 [1] NCCL INFO Bootstrap : Using ib0:10.41.17.138<0>
g26n01:811332:811332 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g26n01:811332:811332 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g26n01:811332:811562 [1] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.138<0>
g26n01:811332:811562 [1] NCCL INFO Using network IB
g26n01:811332:811562 [1] NCCL INFO comm 0x13db43c90 rank 7 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x28395441218594c5 - Init START
g26n01:811332:811562 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g26n01:811332:811562 [1] NCCL INFO Trees [0] 8/-1/-1->7->6 [1] 6/-1/-1->7->11 [2] 8/12/-1->7->6 [3] 6/-1/-1->7->11
g26n01:811332:811562 [1] NCCL INFO P2P Chunksize set to 131072
g26n01:811332:811562 [1] NCCL INFO Channel 00/0 : 7[1] -> 8[2] via P2P/IPC
g26n01:811332:811562 [1] NCCL INFO Channel 02/0 : 7[1] -> 8[2] via P2P/IPC
g26n01:811332:811562 [1] NCCL INFO Channel 01/0 : 7[1] -> 6[0] via P2P/IPC
g26n01:811332:811562 [1] NCCL INFO Channel 03/0 : 7[1] -> 6[0] via P2P/IPC
g26n01:811332:811562 [1] NCCL INFO Connected all rings
g26n01:811332:811562 [1] NCCL INFO Channel 01/0 : 7[1] -> 11[5] via P2P/IPC
g26n01:811332:811562 [1] NCCL INFO Channel 03/0 : 7[1] -> 11[5] via P2P/IPC
g26n01:811332:811562 [1] NCCL INFO Channel 02/0 : 7[1] -> 12[0] [send] via NET/IB/0
g26n01:811332:811562 [1] NCCL INFO Channel 02/0 : 12[0] -> 7[1] [receive] via NET/IB/0
g26n01:811332:811562 [1] NCCL INFO Channel 00/0 : 7[1] -> 6[0] via P2P/IPC
g26n01:811332:811562 [1] NCCL INFO Channel 02/0 : 7[1] -> 6[0] via P2P/IPC
g26n01:811332:811562 [1] NCCL INFO Connected all trees
g26n01:811332:811562 [1] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g26n01:811332:811562 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g26n01:811332:811562 [1] NCCL INFO comm 0x13db43c90 rank 7 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x28395441218594c5 - Init COMPLETE
g26n01:811332:811645 [1] NCCL INFO Using network IB
g26n01:811332:811645 [1] NCCL INFO comm 0x1404f31a0 rank 1 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0x15c93a2ee1bf52db - Init START
g26n01:811332:811645 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g26n01:811332:811645 [1] NCCL INFO Trees [0] 2/-1/-1->1->3 [1] 2/0/-1->1->4
g26n01:811332:811645 [1] NCCL INFO P2P Chunksize set to 131072
g26n01:811332:811645 [1] NCCL INFO Channel 00/0 : 0[3] -> 1[1] [receive] via NET/IB/2
g26n01:811332:811645 [1] NCCL INFO Channel 01/0 : 0[3] -> 1[1] [receive] via NET/IB/2
g26n01:811332:811645 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[5] via P2P/IPC
g26n01:811332:811645 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[5] via P2P/IPC
g26n01:811332:811645 [1] NCCL INFO Connected all rings
g26n01:811332:811645 [1] NCCL INFO Channel 00/0 : 1[1] -> 3[3] [send] via NET/IB/2
g26n01:811332:811645 [1] NCCL INFO Channel 01/0 : 1[1] -> 4[1] [send] via NET/IB/2
g26n01:811332:811645 [1] NCCL INFO Channel 01/0 : 4[1] -> 1[1] [receive] via NET/IB/2
g26n01:811332:811645 [1] NCCL INFO Channel 00/0 : 3[3] -> 1[1] [receive] via NET/IB/2
g26n01:811332:811645 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[3] [send] via NET/IB/2
g26n01:811332:811645 [1] NCCL INFO Connected all trees
g26n01:811332:811645 [1] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g26n01:811332:811645 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g26n01:811332:811645 [1] NCCL INFO comm 0x1404f31a0 rank 1 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0x15c93a2ee1bf52db - Init COMPLETE
NCCL version 2.18.3+cuda11.8
g26n01:811331:811331 [0] NCCL INFO cudaDriverVersion 12020
g26n01:811331:811331 [0] NCCL INFO Bootstrap : Using ib0:10.41.17.138<0>
g26n01:811331:811331 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g26n01:811331:811331 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g26n01:811331:811564 [0] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.138<0>
g26n01:811331:811564 [0] NCCL INFO Using network IB
g26n01:811331:811564 [0] NCCL INFO comm 0x149bb3f90 rank 6 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x28395441218594c5 - Init START
g26n01:811331:811564 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g26n01:811331:811564 [0] NCCL INFO Trees [0] 7/-1/-1->6->13 [1] 8/-1/-1->6->7 [2] 7/0/-1->6->18 [3] 8/-1/-1->6->7
g26n01:811331:811564 [0] NCCL INFO P2P Chunksize set to 131072
g26n01:811331:811564 [0] NCCL INFO Channel 00/0 : 5[5] -> 6[0] [receive] via NET/IB/0
g26n01:811331:811564 [0] NCCL INFO Channel 02/0 : 5[5] -> 6[0] [receive] via NET/IB/0
g26n01:811331:811564 [0] NCCL INFO Channel 00/0 : 6[0] -> 7[1] via P2P/IPC
g26n01:811331:811564 [0] NCCL INFO Channel 02/0 : 6[0] -> 7[1] via P2P/IPC
g26n01:811331:811564 [0] NCCL INFO Channel 01/0 : 6[0] -> 15[3] [send] via NET/IB/2
g26n01:811331:811564 [0] NCCL INFO Channel 03/0 : 6[0] -> 15[3] [send] via NET/IB/2
g26n01:811331:811564 [0] NCCL INFO Connected all rings
g26n01:811331:811564 [0] NCCL INFO Channel 01/0 : 6[0] -> 7[1] via P2P/IPC
g26n01:811331:811564 [0] NCCL INFO Channel 03/0 : 6[0] -> 7[1] via P2P/IPC
g26n01:811331:811564 [0] NCCL INFO Channel 01/0 : 6[0] -> 8[2] via P2P/IPC
g26n01:811331:811564 [0] NCCL INFO Channel 03/0 : 6[0] -> 8[2] via P2P/IPC
g26n01:811331:811564 [0] NCCL INFO Channel 02/0 : 0[0] -> 6[0] [receive] via NET/IB/0
g26n01:811331:811564 [0] NCCL INFO Channel 00/0 : 6[0] -> 13[1] [send] via NET/IB/0
g26n01:811331:811564 [0] NCCL INFO Channel 02/0 : 6[0] -> 18[0] [send] via NET/IB/0
g26n01:811331:811564 [0] NCCL INFO Channel 02/0 : 18[0] -> 6[0] [receive] via NET/IB/0
g26n01:811331:811564 [0] NCCL INFO Channel 00/0 : 13[1] -> 6[0] [receive] via NET/IB/0
g26n01:811331:811564 [0] NCCL INFO Channel 02/0 : 6[0] -> 0[0] [send] via NET/IB/0
g26n01:811331:811564 [0] NCCL INFO Connected all trees
g26n01:811331:811564 [0] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g26n01:811331:811564 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g26n01:811331:811564 [0] NCCL INFO comm 0x149bb3f90 rank 6 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x28395441218594c5 - Init COMPLETE
g26n01:811331:811644 [0] NCCL INFO Using network IB
g26n01:811331:811644 [0] NCCL INFO comm 0x14c8f94f0 rank 1 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0xe45164e09488020a - Init START
g26n01:811331:811644 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g26n01:811331:811644 [0] NCCL INFO Trees [0] 2/-1/-1->1->3 [1] 2/0/-1->1->4
g26n01:811331:811644 [0] NCCL INFO P2P Chunksize set to 131072
g26n01:811331:811644 [0] NCCL INFO Channel 00/0 : 0[2] -> 1[0] [receive] via NET/IB/0
g26n01:811331:811644 [0] NCCL INFO Channel 01/0 : 0[2] -> 1[0] [receive] via NET/IB/0
g26n01:811331:811644 [0] NCCL INFO Channel 00/0 : 1[0] -> 2[4] via P2P/IPC
g26n01:811331:811644 [0] NCCL INFO Channel 01/0 : 1[0] -> 2[4] via P2P/IPC
g26n01:811331:811644 [0] NCCL INFO Connected all rings
g26n01:811331:811644 [0] NCCL INFO Channel 00/0 : 1[0] -> 3[2] [send] via NET/IB/0
g26n01:811331:811644 [0] NCCL INFO Channel 01/0 : 1[0] -> 4[0] [send] via NET/IB/0
g26n01:811331:811644 [0] NCCL INFO Channel 01/0 : 4[0] -> 1[0] [receive] via NET/IB/0
g26n01:811331:811644 [0] NCCL INFO Channel 00/0 : 3[2] -> 1[0] [receive] via NET/IB/0
g26n01:811331:811644 [0] NCCL INFO Channel 01/0 : 1[0] -> 0[2] [send] via NET/IB/0
g26n01:811331:811644 [0] NCCL INFO Connected all trees
g26n01:811331:811644 [0] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g26n01:811331:811644 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g26n01:811331:811644 [0] NCCL INFO comm 0x14c8f94f0 rank 1 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0xe45164e09488020a - Init COMPLETE
NCCL version 2.18.3+cuda11.8
[2024-03-04 12:19:03,574] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-03-04 12:19:03,579] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2024-03-04 12:19:03,579] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-03-04 12:19:03,640] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2024-03-04 12:19:03,640] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'apex.optimizers.fused_adam.FusedAdam'>
[2024-03-04 12:19:03,641] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 1 optimizer
[2024-03-04 12:19:03,641] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500,000,000
[2024-03-04 12:19:03,641] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500,000,000
[2024-03-04 12:19:03,641] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2024-03-04 12:19:03,641] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
g32n03:753099:753099 [3] NCCL INFO cudaDriverVersion 12020
g32n03:753099:753099 [3] NCCL INFO Bootstrap : Using ib0:10.41.17.248<0>
g32n03:753099:753099 [3] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g32n03:753099:753099 [3] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g32n03:753099:753332 [3] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.248<0>
g32n03:753099:753332 [3] NCCL INFO Using network IB
g32n03:753099:753332 [3] NCCL INFO comm 0x15a684670 rank 93 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x28395441218594c5 - Init START
g32n03:753099:753332 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g32n03:753099:753332 [3] NCCL INFO Trees [0] 94/-1/-1->93->92 [1] 95/-1/-1->93->94 [2] 94/-1/-1->93->92 [3] 95/-1/-1->93->94
g32n03:753099:753332 [3] NCCL INFO P2P Chunksize set to 131072
g32n03:753099:753332 [3] NCCL INFO Channel 00/0 : 93[3] -> 94[4] via P2P/IPC
g32n03:753099:753332 [3] NCCL INFO Channel 01/0 : 93[3] -> 94[4] via P2P/IPC
g32n03:753099:753332 [3] NCCL INFO Channel 02/0 : 93[3] -> 94[4] via P2P/IPC
g32n03:753099:753332 [3] NCCL INFO Channel 03/0 : 93[3] -> 94[4] via P2P/IPC
g32n03:753099:753332 [3] NCCL INFO Channel 01/0 : 84[0] -> 93[3] [receive] via NET/IB/3
g32n03:753099:753332 [3] NCCL INFO Channel 03/0 : 84[0] -> 93[3] [receive] via NET/IB/3
g32n03:753099:753332 [3] NCCL INFO Connected all rings
g32n03:753099:753332 [3] NCCL INFO Channel 01/0 : 93[3] -> 95[5] via P2P/IPC
g32n03:753099:753332 [3] NCCL INFO Channel 03/0 : 93[3] -> 95[5] via P2P/IPC
g32n03:753099:753332 [3] NCCL INFO Channel 00/0 : 93[3] -> 92[2] via P2P/IPC
g32n03:753099:753332 [3] NCCL INFO Channel 02/0 : 93[3] -> 92[2] via P2P/IPC
g32n03:753099:753332 [3] NCCL INFO Connected all trees
g32n03:753099:753332 [3] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g32n03:753099:753332 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g32n03:753099:753332 [3] NCCL INFO comm 0x15a684670 rank 93 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x28395441218594c5 - Init COMPLETE
g32n03:753099:753409 [3] NCCL INFO Using network IB
g32n03:753099:753409 [3] NCCL INFO comm 0x15c34da50 rank 23 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xeee4c143812ce6e4 - Init START
g32n03:753099:753409 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g32n03:753099:753409 [3] NCCL INFO Trees [0] -1/-1/-1->23->21 [1] 11/-1/-1->23->-1
g32n03:753099:753409 [3] NCCL INFO P2P Chunksize set to 131072
g32n03:753099:753409 [3] NCCL INFO Channel 00/0 : 22[5] -> 23[3] [receive] via NET/IB/3
g32n03:753099:753409 [3] NCCL INFO Channel 01/0 : 22[5] -> 23[3] [receive] via NET/IB/3
g32n03:753099:753409 [3] NCCL INFO Channel 00/0 : 23[3] -> 0[1] [send] via NET/IB/3
g32n03:753099:753409 [3] NCCL INFO Channel 01/0 : 23[3] -> 0[1] [send] via NET/IB/3
g32n03:753099:753409 [3] NCCL INFO Connected all rings
g32n03:753099:753409 [3] NCCL INFO Channel 00/0 : 21[1] -> 23[3] [receive] via NET/IB/3
g32n03:753099:753409 [3] NCCL INFO Channel 01/0 : 11[3] -> 23[3] [receive] via NET/IB/3
g32n03:753099:753409 [3] NCCL INFO Channel 01/0 : 23[3] -> 11[3] [send] via NET/IB/3
g32n03:753099:753409 [3] NCCL INFO Channel 00/0 : 23[3] -> 21[1] [send] via NET/IB/3
g32n03:753099:753409 [3] NCCL INFO Connected all trees
g32n03:753099:753409 [3] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g32n03:753099:753409 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g32n03:753099:753409 [3] NCCL INFO comm 0x15c34da50 rank 23 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xeee4c143812ce6e4 - Init COMPLETE
g32n03:753099:753430 [3] NCCL INFO Using network IB
g32n03:753099:753430 [3] NCCL INFO comm 0x15c4e1f30 rank 11 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xad5995bf820e7340 - Init START
g32n03:753099:753430 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,0000000g32n03:753101:753101 [5] NCCL INFO cudaDriverVersion 12020
g32n03:753101:753101 [5] NCCL INFO Bootstrap : Using ib0:10.41.17.248<0>
g32n03:753101:753101 [5] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g32n03:753101:753101 [5] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g32n03:753101:753330 [5] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.248<0>
g32n03:753101:753330 [5] NCCL INFO Using network IB
g32n03:753101:753330 [5] NCCL INFO comm 0x16f5dcee0 rank 95 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x28395441218594c5 - Init START
g32n03:753101:753330 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g32n03:753101:753330 [5] NCCL INFO Trees [0] -1/-1/-1->95->94 [1] 91/-1/-1->95->93 [2] -1/-1/-1->95->94 [3] 91/-1/-1->95->93
g32n03:753101:753330 [5] NCCL INFO P2P Chunksize set to 131072
g32n03:753101:753330 [5] NCCL INFO Channel 00/0 : 95[5] -> 0[0] [send] via NET/IB/1
g32n03:753101:753330 [5] NCCL INFO Channel 02/0 : 95[5] -> 0[0] [send] via NET/IB/1
g32n03:753101:753330 [5] NCCL INFO Channel 01/0 : 95[5] -> 92[2] via P2P/IPC
g32n03:753101:753330 [5] NCCL INFO Channel 03/0 : 95[5] -> 92[2] via P2P/IPC
g32n03:753101:753330 [5] NCCL INFO Connected all rings
g32n03:753101:753330 [5] NCCL INFO Channel 01/0 : 95[5] -> 91[1] via P2P/IPC
g32n03:753101:753330 [5] NCCL INFO Channel 03/0 : 95[5] -> 91[1] via P2P/IPC
g32n03:753101:753330 [5] NCCL INFO Channel 01/0 : 95[5] -> 93[3] via P2P/IPC
g32n03:753101:753330 [5] NCCL INFO Channel 03/0 : 95[5] -> 93[3] via P2P/IPC
g32n03:753101:753330 [5] NCCL INFO Channel 00/0 : 95[5] -> 94[4] via P2P/IPC
g32n03:753101:753330 [5] NCCL INFO Channel 02/0 : 95[5] -> 94[4] via P2P/IPC
g32n03:753101:753330 [5] NCCL INFO Connected all trees
g32n03:753101:753330 [5] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g32n03:753101:753330 [5] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g32n03:753101:753330 [5] NCCL INFO comm 0x16f5dcee0 rank 95 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x28395441218594c5 - Init COMPLETE
g32n03:753101:753410 [5] NCCL INFO Using network IB
g32n03:753101:753410 [5] NCCL INFO comm 0x172244420 rank 23 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x15c93a2ee1bf52db - Init START
g32n03:753101:753410 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g32n03:753101:753410 [5] NCCL INFO Trees [0] -1/-1/-1->23->22 [1] -1/-1/-1->23->22
g32n03:753101:753410 [5] NCCL INFO P2P Chunksize set to 131072
g32n03:753101:753410 [5] NCCL INFO Channel 00/0 : 23[5] -> 0[3] [send] via NET/IB/3
g32n03:753101:753410 [5] NCCL INFO Channel 01/0 : 23[5] -> 0[3] [send] via NET/IB/3
g32n03:753101:753410 [5] NCCL INFO Connected all rings
g32n03:753101:753410 [5] NCCL INFO Channel 00/0 : 23[5] -> 22[1] via P2P/IPC
g32n03:753101:753410 [5] NCCL INFO Channel 01/0 : 23[5] -> 22[1] via P2P/IPC
g32n03:753101:753410 [5] NCCL INFO Connected all trees
g32n03:753101:753410 [5] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g32n03:753101:753410 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g32n03:753101:753410 [5] NCCL INFO comm 0x172244420 rank 23 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x15c93a2ee1bf52db - Init COMPLETE
g32n03:753101:753432 [5] NCCL INFO Using network IB
g32n03:753101:753432 [5] NCCL INFO comm 0x172260e40 rank 11 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x6876d4a326688ade - Init START
g32n03:753101:753432 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g32n03:753101:753432 [5] NCCL INFO Trees [0] -1/-1/-1->11->10 [1] 3/-1/-1->11->-1
g32n03:753101:753432 [5] NCCL INFO P2P Chunksize set to 131072
g32n03:753101:753432 [5] NCCL INFO Channel 00/0 : 10[3] -> 11[5] [receive] via NET/IB/3
g32n03:753101:753432 [5] NCCL INFO Channel 01/0 : 10[3] -> 11[5] [receive] via NET/IB/3
g32n00
g32n03:753099:753430 [3] NCCL INFO Trees [0] -1/-1/-1->11->10 [1] 3/-1/-1->11->-1
g32n03:753099:753430 [3] NCCL INFO P2P Chunksize set to 131072
g32n03:753099:753430 [3] NCCL INFO Channel 00/0 : 10[1] -> 11[3] [receive] via NET/IB/3
g32n03:753099:753430 [3] NCCL INFO Channel 01/0 : 10[1] -> 11[3] [receive] via NET/IB/3
g32n03:753099:753430 [3] NCCL INFO Channel 00/0 : 11[3] -> 0[5] [send] via NET/IB/3
g32n03:753099:753430 [3] NCCL INFO Channel 01/0 : 11[3] -> 0[5] [send] via NET/IB/3
g32n03:753099:753430 [3] NCCL INFO Connected all rings
g32n03:753099:753430 [3] NCCL INFO Channel 01/0 : 11[3] -> 3[5] [send] via NET/IB/3
g32n03:753099:753430 [3] NCCL INFO Channel 01/0 : 3[5] -> 11[3] [receive] via NET/IB/3
g32n03:753099:753430 [3] NCCL INFO Channel 00/0 : 11[3] -> 10[1] [send] via NET/IB/3
g32n03:753099:753430 [3] NCCL INFO Connected all trees
g32n03:753099:753430 [3] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g32n03:753099:753430 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g32n03:753099:753430 [3] NCCL INFO comm 0x15c4e1f30 rank 11 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xad5995bf820e7340 - Init COMPLETE
3:753101:753432 [5] NCCL INFO Channel 00/0 : 11[5] -> 0[1] [send] via NET/IB/3
g32n03:753101:753432 [5] NCCL INFO Channel 01/0 : 11[5] -> 0[1] [send] via NET/IB/3
g32n03:753101:753432 [5] NCCL INFO Connected all rings
g32n03:753101:753432 [5] NCCL INFO Channel 01/0 : 11[5] -> 3[1] [send] via NET/IB/3
g32n03:753101:753432 [5] NCCL INFO Channel 01/0 : 3[1] -> 11[5] [receive] via NET/IB/3
g32n03:753101:753432 [5] NCCL INFO Channel 00/0 : 11[5] -> 10[3] [send] via NET/IB/3
g32n03:753101:753432 [5] NCCL INFO Connected all trees
g32n03:753101:753432 [5] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g32n03:753101:753432 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g32n03:753101:753432 [5] NCCL INFO comm 0x172260e40 rank 11 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x6876d4a326688ade - Init COMPLETE
g25n18:898857:899288 [5] NCCL INFO Using network IB
g25n18:898857:899288 [5] NCCL INFO comm 0x13875b520 rank 0 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xad5995bf820e7340 - Init START
g25n18:898857:899288 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g25n18:898857:899288 [5] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7   8   9  10  11
g25n18:898857:899288 [5] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7   8   9  10  11
g25n18:898857:899288 [5] NCCL INFO Trees [0] 8/-1/-1->0->-1 [1] -1/-1/-1->0->1
g25n18:898857:899288 [5] NCCL INFO P2P Chunksize set to 131072
g25n18:898857:899288 [5] NCCL INFO Channel 00/0 : 11[3] -> 0[5] [receive] via NET/IB/3
g25n18:898857:899288 [5] NCCL INFO Channel 01/0 : 11[3] -> 0[5] [receive] via NET/IB/3
g25n18:898857:899288 [5] NCCL INFO Channel 00/0 : 0[5] -> 1[1] [send] via NET/IB/3
g25n18:898857:899288 [5] NCCL INFO Channel 01/0 : 0[5] -> 1[1] [send] via NET/IB/3
g25n18:898857:899288 [5] NCCL INFO Connected all rings
g25n18:898857:899288 [5] NCCL INFO Channel 00/0 : 8[3] -> 0[5] [receive] via NET/IB/3
g25n18:898857:899288 [5] NCCL INFO Channel 00/0 : 0[5] -> 8[3] [send] via NET/IB/3
g25n18:898857:899288 [5] NCCL INFO Channel 01/0 : 1[1] -> 0[5] [receive] via NET/IB/3
g25n18:898857:899288 [5] NCCL INFO Connected all trees
g25n18:898857:899288 [5] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g25n18:898857:899288 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g25n18:898857:899288 [5] NCCL INFO comm 0x13875b520 rank 0 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xad5995bf820e7340 - Init COMPLETE
g25n18:898855:899264 [3] NCCL INFO Using network IB
g25n18:898855:899264 [3] NCCL INFO comm 0x1544c5760 rank 0 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x15c93a2ee1bf52db - Init START
g25n18:898855:899264 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g25n18:898855:899264 [3] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
g25n18:898855:899264 [3] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
g25n18:898855:899264 [3] NCCL INFO Trees [0] 12/-1/-1->0->-1 [1] -1/-1/-1->0->1
g25n18:898855:899264 [3] NCCL INFO P2P Chunksize set to 131072
g25n18:898855:899264 [3] NCCL INFO Channel 00/0 : 23[5] -> 0[3] [receive] via NET/IB/3
g25n18:898855:899264 [3] NCCL INFO Channel 01/0 : 23[5] -> 0[3] [receive] via NET/IB/3
g25n18:898855:899264 [3] NCCL INFO Channel 00/0 : 0[3] -> 1[1] [send] via NET/IB/3
g25n18:898855:899264 [3] NCCL INFO Channel 01/0 : 0[3] -> 1[1] [send] via NET/IB/3
g25n18:898855:899264 [3] NCCL INFO Connected all rings
g25n18:898855:899264 [3] NCCL INFO Channel 00/0 : 12[3] -> 0[3] [receive] via NET/IB/3
g25n18:898855:899264 [3] NCCL INFO Channel 00/0 : 0[3] -> 12[3] [send] via NET/IB/3
g25n18:898855:899264 [3] NCCL INFO Channel 01/0 : 1[1] -> 0[3] [receive] via NET/IB/3
g25n18:898855:899264 [3] NCCL INFO Connected all trees
g25n18:898855:899264 [3] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g25n18:898855:899264 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g25n18:898855:899264 [3] NCCL INFO comm 0x1544c5760 rank 0 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x15c93a2ee1bf52db - Init COMPLETE
g25n18:898855:899292 [3] NCCL INFO Using network IB
g25n18:898855:899292 [3] NCCL INFO comm 0x1524b77b0 rank 0 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x1542a3b966c3490b - Init START
g25n18:898855:899292 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g25n18:898855:899292 [3] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7   8   9  10  11
g25n18:898855:899292 [3] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7   8   9  10  11
g25n18:898855:899292 [3] NCCL INFO Trees [0] 8/-1/-1->0->-1 [1] -1/-1/-1->0->1
g25n18:898855:899292 [3] NCCL INFO P2P Chunksize set to 131072
g25n18:898855:899292 [3] NCCL INFO Channel 00/0 : 11[1] -> 0[3] [receive] via NET/IB/3
g25n18:898855:899292 [3] NCCL INFO Channel 01/0 : 11[1] -> 0[3] [receive] via NET/IB/3
g25n18:898855:899292 [3] NCCL INFO Channel 00/0 : 0[3] -> 1[5] [send] via NET/IB/3
g25n18:898855:899292 [3] NCCL INFO Channel 01/0 : 0[3] -> 1[5] [send] via NET/IB/3
g25n18:898855:899292 [3] NCCL INFO Connected all rings
g25n18:898855:899292 [3] NCCL INFO Channel 00/0 : 8[1] -> 0[3] [receive] via NET/IB/3
g25n18:898855:899292 [3] NCCL INFO Channel 00/0 : 0[3] -> 8[1] [send] via NET/IB/3
g25n18:898855:899292 [3] NCCL INFO Channel 01/0 : 1[5] -> 0[3] [receive] via NET/IB/3
g25n18:898855:899292 [3] NCCL INFO Connected all trees
g25n18:898855:899292 [3] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g25n18:898855:899292 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g25n18:898855:899292 [3] NCCL INFO comm 0x1524b77b0 rank 0 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x1542a3b966c3490b - Init COMPLETE
g25n18:898856:899284 [4] NCCL INFO Using network IB
g25n18:898856:899284 [4] NCCL INFO comm 0x14c3d5b40 rank 0 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x51bdf7b6b8c3aecb - Init START
g25n18:898856:899284 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g25n18:898856:899284 [4] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7   8   9  10  11
g25n18:898856:899284 [4] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7   8   9  10  11
g25n18:898856:899284 [4] NCCL INFO Trees [0] 8/-1/-1->0->-1 [1] -1/-1/-1->0->1
g25n18:898856:899284 [4] NCCL INFO P2P Chunksize set to 131072
g25n18:898856:899284 [4] NCCL INFO Channel 00/0 : 11[2] -> 0[4] [receive] via NET/IB/1
g25n18:898856:899284 [4] NCCL INFO Channel 01/0 : 11[2] -> 0[4] [receive] via NET/IB/1
g25n18:898856:899284 [4] NCCL INFO Channel 00/0 : 0[4] -> 1[0] [send] via NET/IB/1
g25n18:898856:899284 [4] NCCL INFO Channel 01/0 : 0[4] -> 1[0] [send] via NET/IB/1
g25n18:898856:899284 [4] NCCL INFO Connected all rings
g25n18:898856:899284 [4] NCCL INFO Channel 00/0 : 8[2] -> 0[4] [receive] via NET/IB/1
g25n18:898856:899284 [4] NCCL INFO Channel 00/0 : 0[4] -> 8[2] [send] via NET/IB/1
g25n18:898856:899284 [4] NCCL INFO Channel 01/0 : 1[0] -> 0[4] [receive] via NET/IB/1
g25n18:898856:899284 [4] NCCL INFO Connected all trees
g25n18:898856:899284 [4] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g25n18:898856:899284 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g25n18:898856:899284 [4] NCCL INFO comm 0x14c3d5b40 rank 0 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x51bdf7b6b8c3aecb - Init COMPLETE
g26n01:811334:811334 [3] NCCL INFO cudaDriverVersion 12020
g26n01:811334:811334 [3] NCCL INFO Bootstrap : Using ib0:10.41.17.138<0>
g26n01:811334:811334 [3] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g26n01:811334:811334 [3] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g26n01:811334:811566 [3] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.138<0>
g26n01:811334:811566 [3] NCCL INFO Using network IB
g26n01:811334:811566 [3] NCCL INFO comm 0x13eae4360 rank 9 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x28395441218594c5 - Init START
g26n01:811334:811566 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g26n01:811334:811566 [3] NCCL INFO Trees [0] 10/-1/-1->9->8 [1] 11/-1/-1->9->10 [2] 10/-1/-1->9->8 [3] 11/16/-1->9->10
g26n01:811334:811566 [3] NCCL INFO P2P Chunksize set to 131072
g26n01:811334:811566 [3] NCCL INFO Channel 00/0 : 9[3] -> 10[4] via P2P/IPC
g26n01:811334:811566 [3] NCCL INFO Channel 01/0 : 9[3] -> 10[4] via P2P/IPC
g26n01:811334:811566 [3] NCCL INFO Channel 02/0 : 9[3] -> 10[4] via P2P/IPC
g26n01:811334:811566 [3] NCCL INFO Channel 03/0 : 9[3] -> 10[4] via P2P/IPC
g26n01:811334:811566 [3] NCCL INFO Channel 01/0 : 0[0] -> 9[3] [receive] via NET/IB/3
g26n01:811334:811566 [3] NCCL INFO Channel 03/0 : 0[0] -> 9[3] [receive] via NET/IB/3
g26n01:811334:811566 [3] NCCL INFO Connected all rings
g26n01:811334:811566 [3] NCCL INFO Channel 01/0 : 9[3] -> 11[5] via P2P/IPC
g26n01:811334:811566 [3] NCCL INFO Channel 03/0 : 9[3] -> 11[5] via P2P/IPC
g26n01:811334:811566 [3] NCCL INFO Channel 03/0 : 9[3] -> 16[4] [send] via NET/IB/3
g26n01:811334:811566 [3] NCCL INFO Channel 03/0 : 16[4] -> 9[3] [receive] via NET/IB/3
g26n01:811334:811566 [3] NCCL INFO Channel 00/0 : 9[3] -> 8[2] via P2P/IPC
g26n01:811334:811566 [3] NCCL INFO Channel 02/0 : 9[3] -> 8[2] via P2P/IPC
g26n01:811334:811566 [3] NCCL INFO Connected all trees
g26n01:811334:811566 [3] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g26n01:811334:811566 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g26n01:811334:811566 [3] NCCL INFO comm 0x13eae4360 rank 9 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x28395441218594c5 - Init COMPLETE
g26n01:811334:811647 [3] NCCL INFO Using network IB
g26n01:811334:811647 [3] NCCL INFO comm 0x14076cbe0 rank 2 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xeee4c143812ce6e4 - Init START
g26n01:811334:811647 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g26n01:811334:811647 [3] NCCL INFO Trees [0] -1/-1/-1->2->4 [1] 3/0/-1->2->5
g26n01:811334:811647 [3] NCCL INFO P2P Chunksize set to 131072
g26n01:811334:811647 [3] NCCL INFO Channel 00/0 : 1[5] -> 2[3] [receive] via NET/IB/3
g26n01:811334:811647 [3] NCCL INFO Channel 01/0 : 1[5] -> 2[3] [receive] via NET/IB/3
g26n01:811334:811647 [3] NCCL INFO Channel 00/0 : 2[3] -> 3[1] [send] via NET/IB/3
g26n01:811334:811647 [3] NCCL INFO Channel 01/0 : 2[3] -> 3[1] [send] via NET/IB/3
g26n01:811334:811647 [3] NCCL INFO Connected all rings
g26n01:811334:811647 [3] NCCL INFO Channel 01/0 : 0[1] -> 2[3] [receive] via NET/IB/3
g26n01:811334:811647 [3] NCCL INFO Channel 00/0 : 2[3] -> 4[5] [send] via NET/IB/3
g26n01:811334:811647 [3] NCCL INFO Channel 01/0 : 2[3] -> 5[3] [send] via NET/IB/3
g26n01:811334:811647 [3] NCCL INFO Channel 01/0 : 5[3] -> 2[3] [receive] via NET/IB/3
g26n01:811334:811647 [3] NCCL INFO Channel 00/0 : 4[5] -> 2[3] [receive] via NET/IB/3
g26n01:811334:811647 [3] NCCL INFO Channel 01/0 : 2[3] -> 0[1] [send] via NET/IB/3
g26n01:811334:811647 [3] NCCL INFO Channel 01/0 : 3[1] -> 2[3] [receive] via NET/IB/3
g26n01:811334:811647 [3] NCCL INFO Connected all trees
g26n01:811334:811647 [3] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g26n01:811334:811647 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g26n01:811334:811647 [3] NCCL INFO comm 0x14076cg26n01:811336:811336 [5] NCCL INFO cudaDriverVersion 12020
g26n01:811336:811336 [5] NCCL INFO Bootstrap : Using ib0:10.41.17.138<0>
g26n01:811336:811336 [5] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g26n01:811336:811336 [5] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g26n01:811336:811563 [5] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.138<0>
g26n01:811336:811563 [5] NCCL INFO Using network IB
g26n01:811336:811563 [5] NCCL INFO comm 0x14d9b3e50 rank 11 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x28395441218594c5 - Init START
g26n01:811336:811563 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g26n01:811336:811563 [5] NCCL INFO Trees [0] -1/-1/-1->11->10 [1] 7/-1/-1->11->9 [2] -1/-1/-1->11->10 [3] 7/-1/-1->11->9
g26n01:811336:811563 [5] NCCL INFO P2P Chunksize set to 131072
g26n01:811336:811563 [5] NCCL INFO Channel 00/0 : 11[5] -> 12[0] [send] via NET/IB/1
g26n01:811336:811563 [5] NCCL INFO Channel 02/0 : 11[5] -> 12[0] [send] via NET/IB/1
g26n01:811336:811563 [5] NCCL INFO Channel 01/0 : 11[5] -> 8[2] via P2P/IPC
g26n01:811336:811563 [5] NCCL INFO Channel 03/0 : 11[5] -> 8[2] via P2P/IPC
g26n01:811336:811563 [5] NCCL INFO Connected all rings
g26n01:811336:811563 [5] NCCL INFO Channel 01/0 : 11[5] -> 7[1] via P2P/IPC
g26n01:811336:811563 [5] NCCL INFO Channel 03/0 : 11[5] -> 7[1] via P2P/IPC
g26n01:811336:811563 [5] NCCL INFO Channel 01/0 : 11[5] -> 9[3] via P2P/IPC
g26n01:811336:811563 [5] NCCL INFO Channel 03/0 : 11[5] -> 9[3] via P2P/IPC
g26n01:811336:811563 [5] NCCL INFO Channel 00/0 : 11[5] -> 10[4] via P2P/IPC
g26n01:811336:811563 [5] NCCL INFO Channel 02/0 : 11[5] -> 10[4] via P2P/IPC
g26n01:811336:811563 [5] NCCL INFO Connected all trees
g26n01:811336:811563 [5] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g26n01:811336:811563 [5] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g26n01:811336:811563 [5] NCCL INFO comm 0x14d9b3e50 rank 11 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x28395441218594c5 - Init COMPLETE
g26n01:811336:811646 [5] NCCL INFO Using network IB
g26n01:811336:811646 [5] NCCL INFO comm 0x150738cb0 rank 2 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x15c93a2ee1bf52db - Init START
g26n01:811336:811646 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g26n01:811336:811646 [5] NCCL INFO Trees [0] -1/-1/-1->2->1 [1] 3/-1/-1->2->1
g26n01:811336:811646 [5] NCCL INFO P2P Chunksize set to 131072
g26n01:811336:811646 [5] NCCL INFO Channel 00/0 : 2[5] -> 3[3] [send] via NET/IB/3
g26n01:811336:811646 [5] NCCL INFO Channel 01/0 : 2[5] -> 3[3] [send] via NET/IB/3
g26n01:811336:811646 [5] NCCL INFO Connected all rings
g26n01:811336:811646 [5] NCCL INFO Channel 01/0 : 3[3] -> 2[5] [receive] via NET/IB/2
g26n01:811336:811646 [5] NCCL INFO Channel 00/0 : 2[5] -> 1[1] via P2P/IPC
g26n01:811336:811646 [5] NCCL INFO Channel 01/0 : 2[5] -> 1[1] via P2P/IPC
g26n01:811336:811646 [5] NCCL INFO Connected all trees
g26n01:811336:811646 [5] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g26n01:811336:811646 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g26n01:811336:811646 [5] NCCL INFO comm 0x150738cb0 rank 2 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x15c93a2ee1bf52db - Init COMPLETE
g26n01:811336:811666 [5] NCCL INFO Using network IB
g26n01:811336:811666 [5] NCCL INFO comm 0x1508b4360 rank 1 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x1542a3b966c3490b - Init START
g26n01:811336:811666 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g26n01:811336:811666 [5] NCCL INFO Trees [0] -1/-1/-1->1->2 [1] 2/0/-1->1->3
g26n01:811336:811666 [5] NCCL INFO P2P Chunksize set to 131072
g26n01:811336:811666 [5] NCCL INFO Channel 00/0 : 0[3] -> 1[5] [receive] via NET/IB/3
g26n01:811336:811666 [5] NCCL INFO Cbe0 rank 2 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xeee4c143812ce6e4 - Init COMPLETE
g26n01:811334:811667 [3] NCCL INFO Using network IB
g26n01:811334:811667 [3] NCCL INFO comm 0x141993870 rank 1 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x17fc85f125de53ec - Init START
g26n01:811334:811667 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g26n01:811334:811667 [3] NCCL INFO Trees [0] -1/-1/-1->1->2 [1] 2/0/-1->1->3
g26n01:811334:811667 [3] NCCL INFO P2P Chunksize set to 131072
g26n01:811334:811667 [3] NCCL INFO Channel 00/0 : 0[1] -> 1[3] [receive] via NET/IB/3
g26n01:811334:811667 [3] NCCL INFO Channel 01/0 : 0[1] -> 1[3] [receive] via NET/IB/3
g26n01:811334:811667 [3] NCCL INFO Channel 00/0 : 1[3] -> 2[5] [send] via NET/IB/3
g26n01:811334:811667 [3] NCCL INFO Channel 01/0 : 1[3] -> 2[5] [send] via NET/IB/3
g26n01:811334:811667 [3] NCCL INFO Connected all rings
g26n01:811334:811667 [3] NCCL INFO Channel 01/0 : 1[3] -> 3[1] [send] via NET/IB/3
g26n01:811334:811667 [3] NCCL INFO Channel 01/0 : 3[1] -> 1[3] [receive] via NET/IB/3
g26n01:811334:811667 [3] NCCL INFO Channel 00/0 : 2[5] -> 1[3] [receive] via NET/IB/3
g26n01:811334:811667 [3] NCCL INFO Channel 01/0 : 2[5] -> 1[3] [receive] via NET/IB/3
g26n01:811334:811667 [3] NCCL INFO Channel 01/0 : 1[3] -> 0[1] [send] via NET/IB/3
g26n01:811334:811667 [3] NCCL INFO Connected all trees
g26n01:811334:811667 [3] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g26n01:811334:811667 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g26n01:811334:811667 [3] NCCL INFO comm 0x141993870 rank 1 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x17fc85f125de53ec - Init COMPLETE
hannel 01/0 : 0[3] -> 1[5] [receive] via NET/IB/3
g26n01:811336:811666 [5] NCCL INFO Channel 00/0 : 1[5] -> 2[1] [send] via NET/IB/3
g26n01:811336:811666 [5] NCCL INFO Channel 01/0 : 1[5] -> 2[1] [send] via NET/IB/3
g26n01:811336:811666 [5] NCCL INFO Connected all rings
g26n01:811336:811666 [5] NCCL INFO Channel 01/0 : 1[5] -> 3[3] [send] via NET/IB/3
g26n01:811336:811666 [5] NCCL INFO Channel 01/0 : 3[3] -> 1[5] [receive] via NET/IB/3
g26n01:811336:811666 [5] NCCL INFO Channel 00/0 : 2[1] -> 1[5] [receive] via NET/IB/3
g26n01:811336:811666 [5] NCCL INFO Channel 01/0 : 2[1] -> 1[5] [receive] via NET/IB/3
g26n01:811336:811666 [5] NCCL INFO Channel 01/0 : 1[5] -> 0[3] [send] via NET/IB/3
g26n01:811336:811666 [5] NCCL INFO Connected all trees
g26n01:811336:811666 [5] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g26n01:811336:811666 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g26n01:811336:811666 [5] NCCL INFO comm 0x1508b4360 rank 1 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x1542a3b966c3490b - Init COMPLETE
g28n01:770096:770096 [3] NCCL INFO cudaDriverVersion 12020
g28n01:770096:770096 [3] NCCL INFO Bootstrap : Using ib0:10.41.17.174<0>
g28n01:770096:770096 [3] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g28n01:770096:770096 [3] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g28n01:770096:770328 [3] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.174<0>
g28n01:770096:770328 [3] NCCL INFO Using network IB
g28n01:770096:770328 [3] NCCL INFO comm 0x16a0245c0 rank 33 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x28395441218594c5 - Init START
g28n01:770096:770328 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g28n01:770096:770328 [3] NCCL INFO Trees [0] 34/-1/-1->33->32 [1] 35/-1/-1->33->34 [2] 34/-1/-1->33->32 [3] 35/40/-1->33->34
g28n01:770096:770328 [3] NCCL INFO P2P Chunksize set to 131072
g28n01:770096:770328 [3] NCCL INFO Channel 00/0 : 33[3] -> 34[4] via P2P/IPC
g28n01:770096:770328 [3] NCCL INFO Channel 01/0 : 33[3] -> 34[4] via P2P/IPC
g28n01:770096:770328 [3] NCCL INFO Channel 02/0 : 33[3] -> 34[4] via P2P/IPC
g28n01:770096:770328 [3] NCCL INFO Channel 03/0 : 33[3] -> 34[4] via P2P/IPC
g28n01:770096:770328 [3] NCCL INFO Channel 01/0 : 24[0] -> 33[3] [receive] via NET/IB/3
g28n01:770096:770328 [3] NCCL INFO Channel 03/0 : 24[0] -> 33[3] [receive] via NET/IB/3
g28n01:770096:770328 [3] NCCL INFO Connected all rings
g28n01:770096:770328 [3] NCCL INFO Channel 01/0 : 33[3] -> 35[5] via P2P/IPC
g28n01:770096:770328 [3] NCCL INFO Channel 03/0 : 33[3] -> 35[5] via P2P/IPC
g28n01:770096:770328 [3] NCCL INFO Channel 03/0 : 33[3] -> 40[4] [send] via NET/IB/3
g28n01:770096:770328 [3] NCCL INFO Channel 03/0 : 40[4] -> 33[3] [receive] via NET/IB/3
g28n01:770096:770328 [3] NCCL INFO Channel 00/0 : 33[3] -> 32[2] via P2P/IPC
g28n01:770096:770328 [3] NCCL INFO Channel 02/0 : 33[3] -> 32[2] via P2P/IPC
g28n01:770096:770328 [3] NCCL INFO Connected all trees
g28n01:770096:770328 [3] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g28n01:770096:770328 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g28n01:770096:770328 [3] NCCL INFO comm 0x16a0245c0 rank 33 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x28395441218594c5 - Init COMPLETE
g28n01:770096:770415 [3] NCCL INFO Using network IB
g28n01:770096:770415 [3] NCCL INFO comm 0x16c910d30 rank 8 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xeee4c143812ce6e4 - Init START
g28n01:770096:770415 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g28n01:770096:770415 [3] NCCL INFO Trees [0] -1/-1/-1->8->10 [1] 9/6/-1->8->5
g28n01:770096:770415 [3] NCCL INFO P2P Chunksize set to 131072
g28n01:770096:770415 [3] NCCL INFO Channel 00/0 : 7[5] -> 8[3] [receive] via NET/IB/3
g28n01:770096:770415 [3] NCCL INFO Channel 01/0 : 7[5] -> 8[3] [receive] via NET/IB/3
g28n01:770096:770415 [3] NCCL INFO Channel 00/0 : 8[3] -> 9[1] [send] via NET/IB/3
g28n01:770096:770415 [3] NCCL INFO Channel 01/0 : 8[3] -> 9[1] [send] via NET/IB/3
g28n01:770096:770415 [3] NCCL INFO Connected all rings
g28n01:770096:770415 [3] NCCL INFO Channel 01/0 : 6[1] -> 8[3] [receive] via NET/IB/3
g28n01:770096:770415 [3] NCCL INFO Channel 00/0 : 8[3] -> 10[5] [send] via NET/IB/3
g28n01:770096:770415 [3] NCCL INFO Channel 01/0 : 5[3] -> 8[3] [receive] via NET/IB/3
g28n01:770096:770415 [3] NCCL INFO Channel 01/0 : 8[3] -> 5[3] [send] via NET/IB/3
g28n01:770096:770415 [3] NCCL INFO Channel 00/0 : 10[5] -> 8[3] [receive] via NET/IB/3
g28n01:770096:770415 [3] NCCL INFO Channel 01/0 : 8[3] -> 6[1] [send] via NET/IB/3
g28n01:770096:770415 [3] NCCL INFO Channel 01/0 : 9[1] -> 8[3] [receive] via NET/IB/3
g28n01:770096:770415 [3] NCCL INFO Connected all trees
g28n01:770096:770415 [3] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g28n01:770096:770415 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n01:770096:770415 [3] NCCL INFO comm 0x16c910d30 rank 8 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xeee4c143812ce6e4 - Init COMPLETE
g28n01:770096:770430 [3] NCCL INFO Using network IB
g28n01:770096:770430 [3] NCCL INFO comm 0x16cd06820 rank 4 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x17fc85f125de53ec - Init START
g28n01:770096:770430 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g28n01:770096:770430 [3] NCCL INFO Trees [0] 2/6/-1->4->8 [1] -1/-1/-1->4->5
g28n01:770096:770430 [3] NCCL INFO P2P Chunksize set to 131072
g28n01:770096:770430 [3] NCCL INFO Channel 00/0 : 3[1] -> 4[3] [receive] via NET/IB/3
g28n01:770096:770430 [3] NCCL INFO Channel 01/0 : 3[1] -> 4[3] [receive] via NET/IB/3
g28n01:770096:770430 [3] NCCL INFO Channel 00/0 : 4[3] -> 5[5] [send] via NET/IB/3
g28n01:770096:770430 [3] NCCL INFO Channel 01/0 : 4[3] -> 5[5] [send] via NET/IB/3
g28n01:770096:770430 [3] NCCL INFO Connected all rings
g28n01:770096:770430 [3] NCCL INFO Channel 00/0 : 2[5] -> 4[3] [receive] via NET/IB/3
g28n01:770096:770430 [3] NCCL INFO Channel 00/0 : 4[3] -> 6[1] [send] via NET/IB/3
g28n01:770096:770430 [3] NCCL INFO Channel 00/0 : 4[3] -> 8[5] [send] via NET/IB/3
g28n01:770096:770430 [3] NCCL INFO Channel 00/0 : 8[5] -> 4[3] [receive] via NET/IB/3
g28n01:770096:770430 [3] NCCL INFO Channel 00/0 : 6[1] -> 4[3] [receive] via NET/IB/3
g28n01:770096:770430 [3] NCCL INFO Channel 00/0 : 4[3] -> 2[5] [send] via NET/IB/3
g28n01:770096:770430 [3] NCCL INFO Channel 01/0 : 5[5] -> 4[3] [receive] via NET/IB/3
g28n01:770096:770430 [3] NCCL INFO Connected all trees
g28n01:770096:770430 [3] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g28n01:770096:770430 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n01:770096:770430 [3] NCCL INFO comm 0x16cd06820 rank 4 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x17fc85f125de53ec - Init COMPLETE
g28n01:770100:770100 [5] NCCL INFO cudaDriverVersion 12020
g28n01:770100:770100 [5] NCCL INFO Bootstrap : Using ib0:10.41.17.174<0>
g28n01:770100:770100 [5] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g28n01:770100:770100 [5] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g28n01:770100:770327 [5] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.174<0>
g28n01:770100:770327 [5] NCCL INFO Using network IB
g28n01:770100:770327 [5] NCCL INFO comm 0x1394f44a0 rank 35 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x28395441218594c5 - Init START
g28n01:770100:770327 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g28n01:770100:770327 [5] NCCL INFO Trees [0] -1/-1/-1->35->34 [1] 31/-1/-1->35->33 [2] -1/-1/-1->35->34 [3] 31/-1/-1->35->33
g28n01:770100:770327 [5] NCCL INFO P2P Chunksize set to 131072
g28n01:770100:770327 [5] NCCL INFO Channel 00/0 : 35[5] -> 36[0] [send] via NET/IB/1
g28n01:770100:770327 [5] NCCL INFO Channel 02/0 : 35[5] -> 36[0] [send] via NET/IB/1
g28n01:770100:770327 [5] NCCL INFO Channel 01/0 : 35[5] -> 32[2] via P2P/IPC
g28n01:770100:770327 [5] NCCL INFO Channel 03/0 : 35[5] -> 32[2] via P2P/IPC
g28n01:770100:770327 [5] NCCL INFO Connected all rings
g28n01:770100:770327 [5] NCCL INFO Channel 01/0 : 35[5] -> 31[1] via P2P/IPC
g28n01:770100:770327 [5] NCCL INFO Channel 03/0 : 35[5] -> 31[1] via P2P/IPC
g28n01:770100:770327 [5] NCCL INFO Channel 01/0 : 35[5] -> 33[3] via P2P/IPC
g28n01:770100:770327 [5] NCCL INFO Channel 03/0 : 35[5] -> 33[3] via P2P/IPC
g28n01:770100:770327 [5] NCCL INFO Channel 00/0 : 35[5] -> 34[4] via P2P/IPC
g28n01:770100:770327 [5] NCCL INFO Channel 02/0 : 35[5] -> 34[4] via P2P/IPC
g28n01:770100:770327 [5] NCCL INFO Connected all trees
g28n01:770100:770327 [5] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g28n01:770100:770327 [5] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g28n01:770100:770327 [5] NCCL INFO comm 0x1394f44a0 rank 35 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x28395441218594c5 - Init COMPLETE
g28n01:770100:770414 [5] NCCL INFO Using network IB
g28n01:770100:770414 [5] NCCL INFO comm 0x13be59bc0 rank 8 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x15c93a2ee1bf52db - Init START
g28n01:770100:770414 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g28n01:770100:770414 [5] NCCL INFO Trees [0] -1/-1/-1->8->7 [1] 9/-1/-1->8->7
g28n01:770100:770414 [5] NCCL INFO P2P Chunksize set to 131072
g28n01:770100:770414 [5] NCCL INFO Channel 00/0 : 8[5] -> 9[3] [send] via NET/IB/3
g28n01:770100:770414 [5] NCCL INFO Channel 01/0 : 8[5] -> 9[3] [send] via NET/IB/3
g28n01:770100:770414 [5] NCCL INFO Connected all rings
g28n01:770100:770414 [5] NCCL INFO Channel 01/0 : 9[3] -> 8[5] [receive] via NET/IB/2
g28n01:770100:770414 [5] NCCL INFO Channel 00/0 : 8[5] -> 7[1] via P2P/IPC
g28n01:770100:770414 [5] NCCL INFO Channel 01/0 : 8[5] -> 7[1] via P2P/IPC
g28n01:770100:770414 [5] NCCL INFO Connected all trees
g28n01:770100:770414 [5] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g28n01:770100:770414 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n01:770100:770414 [5] NCCL INFO comm 0x13be59bc0 rank 8 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x15c93a2ee1bf52db - Init COMPLETE
g28n01:770100:770433 [5] NCCL INFO Using network IB
g28n01:770100:770433 [5] NCCL INFO comm 0x13c1d6940 rank 4 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x1542a3b966c3490b - Init START
g28n01:770100:770433 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g28n01:770100:770433 [5] NCCL INFO Trees [0] 2/6/-1->4->8 [1] -1/-1/-1->4->5
g28n01:770100:770433 [5] NCCL INFO P2P Chunksize set to 131072
g28n01:770100:770433 [5] NCCL INFO Channel 00/0 : 3[3] -> 4[5] [receive] via NET/IB/3
g28n01:770100:770433 [5] NCCL INFO Channel 01/0 : 3[3] -> 4[5] [receive] via NET/IB/3
g28n01:770100:770433 [5] NCCL INFO Channel 00/0 : 4[5] -> 5[1] [send] via NET/IB/3
g28n01:770100:770433 [5] NCCL INFO Channel 01/0 : 4[5] -> 5[1] [send] via NET/IB/3
g28n01:770100:770433 [5] NCCL INFO Connected all rings
g28n01:770100:770433 [5] NCCL INFO Channel 00/0 : 2[1] -> 4[5] [receive] via NET/IB/3
g28n01:770100:770433 [5] NCCL INFO Channel 00/0 : 4[5] -> 6[3] [send] via NET/IB/3
g28n01:770100:770433 [5] NCCL INFO Channel 00/0 : 4[5] -> 8[1] [send] via NET/IB/3
g28n01:770100:770433 [5] NCCL INFO Channel 00/0 : 8[1] -> 4[5] [receive] via NET/IB/3
g28n01:770100:770433 [5] NCCL INFO Channel 00/0 : 6[3] -> 4[5] [receive] via NET/IB/3
g28n01:770100:770433 [5] NCCL INFO Channel 00/0 : 4[5] -> 2[1] [send] via NET/IB/3
g28n01:770100:770433 [5] NCCL INFO Channel 01/0 : 5[1] -> 4[5] [receive] via NET/IB/3
g28n01:770100:770433 [5] NCCL INFO Connected all trees
g28n01:770100:770433 [5] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g28n01:770100:770433 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n01:770100:770433 [5] NCCL INFO comm 0x13c1d6940 rank 4 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x1542a3b966c3490b - Init COMPLETE
g31n05:704349:704349 [5] NCCL INFO cudaDriverVersion 12020
g31n05:704349:704349 [5] NCCL INFO Bootstrap : Using ib0:10.41.17.232<0>
g31n05:704349:704349 [5] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g31n05:704349:704349 [5] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g31n05:704349:704582 [5] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.232<0>
g31n05:704349:704582 [5] NCCL INFO Using network IB
g31n05:704349:704582 [5] NCCL INFO comm 0x1401541c0 rank 83 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x28395441218594c5 - Init START
g31n05:704349:704582 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g31n05:704349:704582 [5] NCCL INFO Trees [0] -1/-1/-1->83->82 [1] 79/-1/-1->83->81 [2] -1/-1/-1->83->82 [3] 79/-1/-1->83->81
g31n05:704349:704582 [5] NCCL INFO P2P Chunksize set to 131072
g31n05:704349:704582 [5] NCCL INFO Channel 00/0 : 83[5] -> 84[0] [send] via NET/IB/1
g31n05:704349:704582 [5] NCCL INFO Channel 02/0 : 83[5] -> 84[0] [send] via NET/IB/1
g31n05:704349:704582 [5] NCCL INFO Channel 01/0 : 83[5] -> 80[2] via P2P/IPC
g31n05:704349:704582 [5] NCCL INFO Channel 03/0 : 83[5] -> 80[2] via P2P/IPC
g31n05:704349:704582 [5] NCCL INFO Connected all rings
g31n05:704349:704582 [5] NCCL INFO Channel 01/0 : 83[5] -> 79[1] via P2P/IPC
g31n05:704349:704582 [5] NCCL INFO Channel 03/0 : 83[5] -> 79[1] via P2P/IPC
g31n05:704349:704582 [5] NCCL INFO Channel 01/0 : 83[5] -> 81[3] via P2P/IPC
g31n05:704349:704582 [5] NCCL INFO Channel 03/0 : 83[5] -> 81[3] via P2P/IPC
g31n05:704349:704582 [5] NCCL INFO Channel 00/0 : 83[5] -> 82[4] via P2P/IPC
g31n05:704349:704582 [5] NCCL INFO Channel 02/0 : 83[5] -> 82[4] via P2P/IPC
g31n05:704349:704582 [5] NCCL INFO Connected all trees
g31n05:704349:704582 [5] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g31n05:704349:704582 [5] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g31n05:704349:704582 [5] NCCL INFO comm 0x1401541c0 rank 83 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x28395441218594c5 - Init COMPLETE
g31n05:704349:704661 [5] NCCL INFO Using network IB
g31n05:704349:704661 [5] NCCL INFO comm 0x142d88640 rank 20 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x15c93a2ee1bf52db - Init START
g31n05:704349:704661 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g31n05:704349:704661 [5] NCCL INFO Trees [0] -1/-1/-1->20->19 [1] 21/-1/-1->20->19
g31n05:704349:704661 [5] NCCL INFO P2P Chunksize set to 131072
g31n05:704349:704661 [5] NCCL INFO Channel 00/0 : 20[5] -> 21[3] [send] via NET/IB/3
g31n05:704349:704661 [5] NCCL INFO Channel 01/0 : 20[5] -> 21[3] [send] via NET/IB/3
g31n05:704349:704661 [5] NCCL INFO Connected all rings
g31n05:704349:704661 [5] NCCL INFO Channel 01/0 : 21[3] -> 20[5] [receive] via NET/IB/2
g31n05:704349:704661 [5] NCCL INFO Channel 00/0 : 20[5] -> 19[1] via P2P/IPC
g31n05:704349:704661 [5] NCCL INFO Channel 01/0 : 20[5] -> 19[1] via P2P/IPC
g31n05:704349:704661 [5] NCCL INFO Connected all trees
g31n05:704349:704661 [5] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g31n05:704349:704661 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n05:704349:704661 [5] NCCL INFO comm 0x142d88640 rank 20 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x15c93a2ee1bf52db - Init COMPLETE
g31n05:704349:704681 [5] NCCL INFO Using network IB
g31n05:704349:704681 [5] NCCL INFO comm 0x142a42100 rank 10 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x1542a3b966c3490b - Init START
g31n05:704349:704681 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g31n05:704349:704681 [5] NCCL INFO Trees [0] 9/11/-1->10->8 [1] -1/-1/-1->10->9
g31n05:704349:704681 [5] NCCL INFO P2P Chunksize set to 131072
g31n05:704349:704681 [5] NCCL INFO Channel 00/0 : 9[3] -> 10[5] [receive] via NET/IB/3
g31ng31n05:704347:704347 [3] NCCL INFO cudaDriverVersion 12020
g31n05:704347:704347 [3] NCCL INFO Bootstrap : Using ib0:10.41.17.232<0>
g31n05:704347:704347 [3] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g31n05:704347:704347 [3] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g31n05:704347:704583 [3] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.232<0>
g31n05:704347:704583 [3] NCCL INFO Using network IB
g31n05:704347:704583 [3] NCCL INFO comm 0x16cbf4270 rank 81 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x28395441218594c5 - Init START
g31n05:704347:704583 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g31n05:704347:704583 [3] NCCL INFO Trees [0] 82/-1/-1->81->80 [1] 83/-1/-1->81->82 [2] 82/-1/-1->81->80 [3] 83/88/-1->81->82
g31n05:704347:704583 [3] NCCL INFO P2P Chunksize set to 131072
g31n05:704347:704583 [3] NCCL INFO Channel 00/0 : 81[3] -> 82[4] via P2P/IPC
g31n05:704347:704583 [3] NCCL INFO Channel 01/0 : 81[3] -> 82[4] via P2P/IPC
g31n05:704347:704583 [3] NCCL INFO Channel 02/0 : 81[3] -> 82[4] via P2P/IPC
g31n05:704347:704583 [3] NCCL INFO Channel 03/0 : 81[3] -> 82[4] via P2P/IPC
g31n05:704347:704583 [3] NCCL INFO Channel 01/0 : 72[0] -> 81[3] [receive] via NET/IB/3
g31n05:704347:704583 [3] NCCL INFO Channel 03/0 : 72[0] -> 81[3] [receive] via NET/IB/3
g31n05:704347:704583 [3] NCCL INFO Connected all rings
g31n05:704347:704583 [3] NCCL INFO Channel 01/0 : 81[3] -> 83[5] via P2P/IPC
g31n05:704347:704583 [3] NCCL INFO Channel 03/0 : 81[3] -> 83[5] via P2P/IPC
g31n05:704347:704583 [3] NCCL INFO Channel 03/0 : 81[3] -> 88[4] [send] via NET/IB/3
g31n05:704347:704583 [3] NCCL INFO Channel 03/0 : 88[4] -> 81[3] [receive] via NET/IB/3
g31n05:704347:704583 [3] NCCL INFO Channel 00/0 : 81[3] -> 80[2] via P2P/IPC
g31n05:704347:704583 [3] NCCL INFO Channel 02/0 : 81[3] -> 80[2] via P2P/IPC
g31n05:704347:704583 [3] NCCL INFO Connected all trees
g31n05:704347:704583 [3] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g31n05:704347:704583 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g31n05:704347:704583 [3] NCCL INFO comm 0x16cbf4270 rank 81 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x28395441218594c5 - Init COMPLETE
g31n05:704347:704660 [3] NCCL INFO Using network IB
g31n05:704347:704660 [3] NCCL INFO comm 0x16f9150f0 rank 20 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xeee4c143812ce6e4 - Init START
g31n05:704347:704660 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g31n05:704347:704660 [3] NCCL INFO Trees [0] -1/-1/-1->20->22 [1] 21/18/-1->20->17
g31n05:704347:704660 [3] NCCL INFO P2P Chunksize set to 131072
g31n05:704347:704660 [3] NCCL INFO Channel 00/0 : 19[5] -> 20[3] [receive] via NET/IB/3
g31n05:704347:704660 [3] NCCL INFO Channel 01/0 : 19[5] -> 20[3] [receive] via NET/IB/3
g31n05:704347:704660 [3] NCCL INFO Channel 00/0 : 20[3] -> 21[1] [send] via NET/IB/3
g31n05:704347:704660 [3] NCCL INFO Channel 01/0 : 20[3] -> 21[1] [send] via NET/IB/3
g31n05:704347:704660 [3] NCCL INFO Connected all rings
g31n05:704347:704660 [3] NCCL INFO Channel 01/0 : 18[1] -> 20[3] [receive] via NET/IB/3
g31n05:704347:704660 [3] NCCL INFO Channel 00/0 : 20[3] -> 22[5] [send] via NET/IB/3
g31n05:704347:704660 [3] NCCL INFO Channel 01/0 : 17[3] -> 20[3] [receive] via NET/IB/3
g31n05:704347:704660 [3] NCCL INFO Channel 01/0 : 20[3] -> 17[3] [send] via NET/IB/3
g31n05:704347:704660 [3] NCCL INFO Channel 00/0 : 22[5] -> 20[3] [receive] via NET/IB/3
g31n05:704347:704660 [3] NCCL INFO Channel 01/0 : 20[3] -> 18[1] [send] via NET/IB/3
g31n05:704347:704660 [3] NCCL INFO Channel 01/0 : 21[1] -> 20[3] [receive] via NET/IB/3
g31n05:704347:704660 [3] NCCL INFO Connected all trees
g31n05:704347:704660 [3] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g31n05:704347:704660 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n05:704347:704660 [3] NCCL INFO comm 0x16f9150f0 rank 20 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xeee4c143812ce6e4 - Init COMPLETE
g31n05:704347:704679 [3] NCCL INFO Using network IB
g31n05:704347:704679 [3] NCCL INFO comm 0x16f28dac0 rank 10 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x17fc85f125de53ec - Init START
g31n05:704347:704679 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g31n05:704347:704679 [3] NCCL INFO Trees [0] 9/11/-1->10->8 [1] -1/-1/-1->10->9
g31n05:704347:704679 [3] NCCL INFO P2P Chunksize set to 131072
g31n05:704347:704679 [3] NCCL INFO Channel 00/0 : 9[1] -> 10[3] [receive] via NET/IB/3
g31n05:704347:704679 [3] NCCL INFO Channel 01/0 : 9[1] -> 10[3] [receive] via NET/IB/3
g31n05:704347:704679 [3] NCCL INFO Channel 00/0 : 10[3] -> 11[5] [send] via NET/IB/3
g31n05:704347:704679 [3] NCCL INFO Channel 01/0 : 10[3] -> 11[5] [send] via NET/IB/3
g31n05:704347:704679 [3] NCCL INFO Connected all rings
g31n05:704347:704679 [3] NCCL INFO Channel 00/0 : 8[5] -> 10[3] [receive] via NET/IB/3
g31n05:704347:704679 [3] NCCL INFO Channel 00/0 : 10[3] -> 8[5] [send] via NET/IB/3
g31n05:704347:704679 [3] NCCL INFO Channel 00/0 : 11[5] -> 10[3] [receive] via NET/IB/3
g31n05:704347:704679 [3] NCCL INFO Channel 00/0 : 10[3] -> 9[1] [send] via NET/IB/3
g31n05:704347:704679 [3] NCCL INFO Channel 01/0 : 10[3] -> 9[1] [send] via NET/IB/3
g31n05:704347:704679 [3] NCCL INFO Connected all trees
g31n05:704347:704679 [3] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g31n05:704347:704679 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n05:704347:704679 [3] NCCL INFO comm 0x16f28dac0 rank 10 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x17fc85f125de53ec - Init COMPLETE
05:704349:704681 [5] NCCL INFO Channel 01/0 : 9[3] -> 10[5] [receive] via NET/IB/3
g31n05:704349:704681 [5] NCCL INFO Channel 00/0 : 10[5] -> 11[1] [send] via NET/IB/3
g31n05:704349:704681 [5] NCCL INFO Channel 01/0 : 10[5] -> 11[1] [send] via NET/IB/3
g31n05:704349:704681 [5] NCCL INFO Connected all rings
g31n05:704349:704681 [5] NCCL INFO Channel 00/0 : 8[1] -> 10[5] [receive] via NET/IB/3
g31n05:704349:704681 [5] NCCL INFO Channel 00/0 : 10[5] -> 8[1] [send] via NET/IB/3
g31n05:704349:704681 [5] NCCL INFO Channel 00/0 : 11[1] -> 10[5] [receive] via NET/IB/3
g31n05:704349:704681 [5] NCCL INFO Channel 00/0 : 10[5] -> 9[3] [send] via NET/IB/3
g31n05:704349:704681 [5] NCCL INFO Channel 01/0 : 10[5] -> 9[3] [send] via NET/IB/3
g31n05:704349:704681 [5] NCCL INFO Connected all trees
g31n05:704349:704681 [5] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g31n05:704349:704681 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n05:704349:704681 [5] NCCL INFO comm 0x142a42100 rank 10 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x1542a3b966c3490b - Init COMPLETE
g32n03:753097:753097 [1] NCCL INFO cudaDriverVersion 12020
g32n03:753097:753097 [1] NCCL INFO Bootstrap : Using ib0:10.41.17.248<0>
g32n03:753097:753097 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g32n03:753097:753097 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g32n03:753097:753328 [1] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.248<0>
g32n03:753097:753328 [1] NCCL INFO Using network IB
g32n03:753097:753328 [1] NCCL INFO comm 0x189ec4290 rank 91 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x28395441218594c5 - Init START
g32n03:753097:753328 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g32n03:753097:753328 [1] NCCL INFO Trees [0] 92/-1/-1->91->90 [1] 90/-1/-1->91->95 [2] 92/-1/-1->91->90 [3] 90/-1/-1->91->95
g32n03:753097:753328 [1] NCCL INFO P2P Chunksize set to 131072
g32n03:753097:753328 [1] NCCL INFO Channel 00/0 : 91[1] -> 92[2] via P2P/IPC
g32n03:753097:753328 [1] NCCL INFO Channel 02/0 : 91[1] -> 92[2] via P2P/IPC
g32n03:753097:753328 [1] NCCL INFO Channel 01/0 : 91[1] -> 90[0] via P2P/IPC
g32n03:753097:753328 [1] NCCL INFO Channel 03/0 : 91[1] -> 90[0] via P2P/IPC
g32n03:753097:753328 [1] NCCL INFO Connected all rings
g32n03:753097:753328 [1] NCCL INFO Channel 01/0 : 91[1] -> 95[5] via P2P/IPC
g32n03:753097:753328 [1] NCCL INFO Channel 03/0 : 91[1] -> 95[5] via P2P/IPC
g32n03:753097:753328 [1] NCCL INFO Channel 00/0 : 91[1] -> 90[0] via P2P/IPC
g32n03:753097:753328 [1] NCCL INFO Channel 02/0 : 91[1] -> 90[0] via P2P/IPC
g32n03:753097:753328 [1] NCCL INFO Connected all trees
g32n03:753097:753328 [1] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g32n03:753097:753328 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g32n03:753097:753328 [1] NCCL INFO comm 0x189ec4290 rank 91 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x28395441218594c5 - Init COMPLETE
g32n03:753097:753414 [1] NCCL INFO Using network IB
g32n03:753097:753414 [1] NCCL INFO comm 0x18cc04870 rank 22 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0x15c93a2ee1bf52db - Init START
g32n03:753097:753414 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g32n03:753097:753414 [1] NCCL INFO Trees [0] 23/-1/-1->22->21 [1] 23/10/-1->22->-1
g32n03:753097:753414 [1] NCCL INFO P2P Chunksize set to 131072
g32n03:753097:753414 [1] NCCL INFO Channel 00/0 : 21[3] -> 22[1] [receive] via NET/IB/2
g32n03:753097:753414 [1] NCCL INFO Channel 01/0 : 21[3] -> 22[1] [receive] via NET/IB/2
g32n03:753097:753414 [1] NCCL INFO Channel 00/0 : 22[1] -> 23[5] via P2P/IPC
g32n03:753097:753414 [1] NCCL INFO Channel 01/0 : 22[1] -> 23[5] via P2P/IPC
g32n03:753097:753414 [1] NCCL INFO Connected all rings
g32n03:753097:753414 [1] NCCL INFO Channel 01/0 : 10[1] -> 22[1] [receive] via NET/IB/2
g32n03:753097:753414 [1] NCCL INFO Channel 01/0 : 22[1] -> 10[1] [send] via NET/IB/2
g32n03:753097:753414 [1] NCCL INFO Channel 00/0 : 22[1] -> 21[3] [send] via NET/IB/2
g32n03:753097:753414 [1] NCCL INFO Connected all trees
g32n03:753097:753414 [1] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g32n03:753097:753414 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g32n03:753097:753414 [1] NCCL INFO comm 0x18cc04870 rank 22 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0x15c93a2ee1bf52db - Init COMPLETE
g32n03:753097:753433 [1] NCCL INFO Using network IB
g32n03:753097:753433 [1] NCCL INFO comm 0x18cc33710 rank 11 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x1542a3b966c3490b - Init START
g32n03:753097:753433 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g32n03:753097:753433 [1] NCCL INFO Trees [0] -1/-1/-1->11->10 [1] 3/-1/-1->11->-1
g32n03:753097:753433 [1] NCCL INFO P2P Chunksize set to 131072
g32n03:753097:753433 [1] NCCL INFO Channel 00/0 : 10[5] -> 11[1] [receive] via NET/IB/2
g32n03:753097:753433 [1] NCCL INFO Channel 01/0 : 10[5] -> 11[1] [receive] via NET/IB/2
g32n03:753097:g32n03:753100:753100 [4] NCCL INFO cudaDriverVersion 12020
g32n03:753100:753100 [4] NCCL INFO Bootstrap : Using ib0:10.41.17.248<0>
g32n03:753100:753100 [4] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g32n03:753100:753100 [4] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g32n03:753100:753331 [4] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.248<0>
g32n03:753100:753331 [4] NCCL INFO Using network IB
g32n03:753100:753331 [4] NCCL INFO comm 0x133d03f70 rank 94 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x28395441218594c5 - Init START
g32n03:753100:753331 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g32n03:753100:753331 [4] NCCL INFO Trees [0] 95/-1/-1->94->93 [1] 93/-1/-1->94->88 [2] 95/-1/-1->94->93 [3] 93/46/-1->94->-1
g32n03:753100:753331 [4] NCCL INFO P2P Chunksize set to 131072
g32n03:753100:753331 [4] NCCL INFO Channel 00/0 : 94[4] -> 95[5] via P2P/IPC
g32n03:753100:753331 [4] NCCL INFO Channel 01/0 : 94[4] -> 95[5] via P2P/IPC
g32n03:753100:753331 [4] NCCL INFO Channel 02/0 : 94[4] -> 95[5] via P2P/IPC
g32n03:753100:753331 [4] NCCL INFO Channel 03/0 : 94[4] -> 95[5] via P2P/IPC
g32n03:753100:753331 [4] NCCL INFO Connected all rings
g32n03:753100:753331 [4] NCCL INFO Channel 01/0 : 88[4] -> 94[4] [receive] via NET/IB/3
g32n03:753100:753331 [4] NCCL INFO Channel 03/0 : 46[4] -> 94[4] [receive] via NET/IB/3
g32n03:753100:753331 [4] NCCL INFO Channel 03/0 : 94[4] -> 46[4] [send] via NET/IB/3
g32n03:753100:753331 [4] NCCL INFO Channel 01/0 : 94[4] -> 88[4] [send] via NET/IB/3
g32n03:753100:753331 [4] NCCL INFO Channel 00/0 : 94[4] -> 93[3] via P2P/IPC
g32n03:753100:753331 [4] NCCL INFO Channel 01/0 : 94[4] -> 93[3] via P2P/IPC
g32n03:753100:753331 [4] NCCL INFO Channel 02/0 : 94[4] -> 93[3] via P2P/IPC
g32n03:753100:753331 [4] NCCL INFO Channel 03/0 : 94[4] -> 93[3] via P2P/IPC
g32n03:753100:753331 [4] NCCL INFO Connected all trees
g32n03:753100:753331 [4] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g32n03:753100:753331 [4] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g32n03:753100:753331 [4] NCCL INFO comm 0x133d03f70 rank 94 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x28395441218594c5 - Init COMPLETE
g32n03:753100:753411 [4] NCCL INFO Using network IB
g32n03:753100:753411 [4] NCCL INFO comm 0x1365987c0 rank 23 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xe45164e09488020a - Init START
g32n03:753100:753411 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g32n03:753100:753411 [4] NCCL INFO Trees [0] -1/-1/-1->23->22 [1] -1/-1/-1->23->22
g32n03:753100:753411 [4] NCCL INFO P2P Chunksize set to 131072
g32n03:753100:753411 [4] NCCL INFO Channel 00/0 : 23[4] -> 0[2] [send] via NET/IB/1
g32n03:753100:753411 [4] NCCL INFO Channel 01/0 : 23[4] -> 0[2] [send] via NET/IB/1
g32n03:753100:753411 [4] NCCL INFO Connected all rings
g32n03:753100:753411 [4] NCCL INFO Channel 00/0 : 23[4] -> 22[0] via P2P/IPC
g32n03:753100:753411 [4] NCCL INFO Channel 01/0 : 23[4] -> 22[0] via P2P/IPC
g32n03:753100:753411 [4] NCCL INFO Connected all trees
g32n03:753100:753411 [4] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g32n03:753100:753411 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g32n03:753100:753411 [4] NCCL INFO comm 0x1365987c0 rank 23 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xe45164e09488020a - Init COMPLETE
g32n03:753100:753434 [4] NCCL INFO Using network IB
g32n03:753100:753434 [4] NCCL INFO comm 0x137a28000 rank 11 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x8442a1c07b0a8edb - Init START
g32n03:753100:753434 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g32n03:753100:753434 [4] NCCL INFO Trees [0] -1/-1/-1->11->10 [1] 3/-1/-1->11->-1
g32n03:753100:753434 [4] NCCL INFO P2P Chunksize set to 131072
g32n03:753100:753434 [4] NCCL 753433 [1] NCCL INFO Channel 00/0 : 11[1] -> 0[3] [send] via NET/IB/2
g32n03:753097:753433 [1] NCCL INFO Channel 01/0 : 11[1] -> 0[3] [send] via NET/IB/2
g32n03:753097:753433 [1] NCCL INFO Connected all rings
g32n03:753097:753433 [1] NCCL INFO Channel 01/0 : 11[1] -> 3[3] [send] via NET/IB/2
g32n03:753097:753433 [1] NCCL INFO Channel 01/0 : 3[3] -> 11[1] [receive] via NET/IB/2
g32n03:753097:753433 [1] NCCL INFO Channel 00/0 : 11[1] -> 10[5] [send] via NET/IB/2
g32n03:753097:753433 [1] NCCL INFO Connected all trees
g32n03:753097:753433 [1] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g32n03:753097:753433 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g32n03:753097:753433 [1] NCCL INFO comm 0x18cc33710 rank 11 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x1542a3b966c3490b - Init COMPLETE
INFO Channel 00/0 : 10[2] -> 11[4] [receive] via NET/IB/1
g32n03:753100:753434 [4] NCCL INFO Channel 01/0 : 10[2] -> 11[4] [receive] via NET/IB/1
g32n03:753100:753434 [4] NCCL INFO Channel 00/0 : 11[4] -> 0[0] [send] via NET/IB/1
g32n03:753100:753434 [4] NCCL INFO Channel 01/0 : 11[4] -> 0[0] [send] via NET/IB/1
g32n03:753100:753434 [4] NCCL INFO Connected all rings
g32n03:753100:753434 [4] NCCL INFO Channel 01/0 : 11[4] -> 3[0] [send] via NET/IB/1
g32n03:753100:753434 [4] NCCL INFO Channel 01/0 : 3[0] -> 11[4] [receive] via NET/IB/1
g32n03:753100:753434 [4] NCCL INFO Channel 00/0 : 11[4] -> 10[2] [send] via NET/IB/1
g32n03:753100:753434 [4] NCCL INFO Connected all trees
g32n03:753100:753434 [4] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g32n03:753100:753434 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g32n03:753100:753434 [4] NCCL INFO comm 0x137a28000 rank 11 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x8442a1c07b0a8edb - Init COMPLETE
g26n02:851592:851592 [3] NCCL INFO cudaDriverVersion 12020
g26n02:851592:851592 [3] NCCL INFO Bootstrap : Using ib0:10.41.17.139<0>
g26n02:851592:851592 [3] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g26n02:851592:851592 [3] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g26n02:851592:851823 [3] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.139<0>
g26n02:851592:851823 [3] NCCL INFO Using network IB
g26n02:851592:851823 [3] NCCL INFO comm 0x13f8546f0 rank 15 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x28395441218594c5 - Init START
g26n02:851592:851823 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g26n02:851592:851823 [3] NCCL INFO Trees [0] 16/-1/-1->15->14 [1] 17/10/-1->15->16 [2] 16/-1/-1->15->14 [3] 17/-1/-1->15->16
g26n02:851592:851823 [3] NCCL INFO P2P Chunksize set to 131072
g26n02:851592:851823 [3] NCCL INFO Channel 00/0 : 15[3] -> 16[4] via P2P/IPC
g26n02:851592:851823 [3] NCCL INFO Channel 01/0 : 15[3] -> 16[4] via P2P/IPC
g26n02:851592:851823 [3] NCCL INFO Channel 02/0 : 15[3] -> 16[4] via P2P/IPC
g26n02:851592:851823 [3] NCCL INFO Channel 03/0 : 15[3] -> 16[4] via P2P/IPC
g26n02:851592:851823 [3] NCCL INFO Channel 01/0 : 6[0] -> 15[3] [receive] via NET/IB/3
g26n02:851592:851823 [3] NCCL INFO Channel 03/0 : 6[0] -> 15[3] [receive] via NET/IB/3
g26n02:851592:851823 [3] NCCL INFO Connected all rings
g26n02:851592:851823 [3] NCCL INFO Channel 01/0 : 15[3] -> 17[5] via P2P/IPC
g26n02:851592:851823 [3] NCCL INFO Channel 03/0 : 15[3] -> 17[5] via P2P/IPC
g26n02:851592:851823 [3] NCCL INFO Channel 01/0 : 10[4] -> 15[3] [receive] via NET/IB/3
g26n02:851592:851823 [3] NCCL INFO Channel 01/0 : 15[3] -> 10[4] [send] via NET/IB/3
g26n02:851592:851823 [3] NCCL INFO Channel 00/0 : 15[3] -> 14[2] via P2P/IPC
g26n02:851592:851823 [3] NCCL INFO Channel 02/0 : 15[3] -> 14[2] via P2P/IPC
g26n02:851592:851823 [3] NCCL INFO Connected all trees
g26n02:851592:851823 [3] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g26n02:851592:851823 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g26n02:851592:851823 [3] NCCL INFO comm 0x13f8546f0 rank 15 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x28395441218594c5 - Init COMPLETE
g26n02:851592:851905 [3] NCCL INFO Using network IB
g26n02:851592:851905 [3] NCCL INFO comm 0x142643af0 rank 3 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x15c93a2ee1bf52db - Init START
g26n02:851592:851905 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g26n02:851592:851905 [3] NCCL INFO Trees [0] 1/4/-1->3->6 [1] -1/-1/-1->3->2
g26n02:851592:851905 [3] NCCL INFO P2P Chunksize set to 131072
g26n02:851592:851905 [3] NCCL INFO Channel 00/0 : 2[5] -> 3[3] [receive] via NET/IB/3
g26n02:851592:851905 [3] NCCL INFO Channel 01/0 : 2[5] -> 3[3] [receive] via NET/IB/3
g26n02:851592:851905 [3] NCCL INFO Channel 00/0 : 3[3] -> 4[1] [send] via NET/IB/3
g26n02:851592:851905 [3] NCCL INFO Channel 01/0 : 3[3] -> 4[1] [send] via NET/IB/3
g26n02:851592:851905 [3] NCCL INFO Connected all rings
g26n02:851592:851905 [3] NCCL INFO Channel 00/0 : 1[1] -> 3[3] [receive] via NET/IB/3
g26n02:851592:851905 [3] NCCL INFO Channel 00/0 : 3[3] -> 6[3] [send] via NET/IB/3
g26n02:851592:851905 [3] NCCL INFO Channel 00/0 : 6[3] -> 3[3] [receive] via NET/IB/3
g26n02:851592:851905 [3] NCCL INFO Channel 00/0 : 3[3] -> 1[1] [send] via NET/IB/3
g26n02:851592:851905 [3] NCCL INFO Channel 00/0 : 4[1] -> 3[3] [receive] via NET/IB/3
g26n02:851592:851905 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[5] [send] via NET/IB/3
g26n02:851592:851905 [3] NCCL INFO Connected all trees
g26n02:851592:851905 [3] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g26n02:851592:851905 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g26n02:851592:851905 [3] NCCL INFO comm 0x142643af0 rank 3 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xg26n02:851594:851594 [5] NCCL INFO cudaDriverVersion 12020
g26n02:851594:851594 [5] NCCL INFO Bootstrap : Using ib0:10.41.17.139<0>
g26n02:851594:851594 [5] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g26n02:851594:851594 [5] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g26n02:851594:851821 [5] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.139<0>
g26n02:851594:851821 [5] NCCL INFO Using network IB
g26n02:851594:851821 [5] NCCL INFO comm 0x170523b30 rank 17 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x28395441218594c5 - Init START
g26n02:851594:851821 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g26n02:851594:851821 [5] NCCL INFO Trees [0] -1/-1/-1->17->16 [1] 13/-1/-1->17->15 [2] -1/-1/-1->17->16 [3] 13/-1/-1->17->15
g26n02:851594:851821 [5] NCCL INFO P2P Chunksize set to 131072
g26n02:851594:851821 [5] NCCL INFO Channel 00/0 : 17[5] -> 18[0] [send] via NET/IB/1
g26n02:851594:851821 [5] NCCL INFO Channel 02/0 : 17[5] -> 18[0] [send] via NET/IB/1
g26n02:851594:851821 [5] NCCL INFO Channel 01/0 : 17[5] -> 14[2] via P2P/IPC
g26n02:851594:851821 [5] NCCL INFO Channel 03/0 : 17[5] -> 14[2] via P2P/IPC
g26n02:851594:851821 [5] NCCL INFO Connected all rings
g26n02:851594:851821 [5] NCCL INFO Channel 01/0 : 17[5] -> 13[1] via P2P/IPC
g26n02:851594:851821 [5] NCCL INFO Channel 03/0 : 17[5] -> 13[1] via P2P/IPC
g26n02:851594:851821 [5] NCCL INFO Channel 01/0 : 17[5] -> 15[3] via P2P/IPC
g26n02:851594:851821 [5] NCCL INFO Channel 03/0 : 17[5] -> 15[3] via P2P/IPC
g26n02:851594:851821 [5] NCCL INFO Channel 00/0 : 17[5] -> 16[4] via P2P/IPC
g26n02:851594:851821 [5] NCCL INFO Channel 02/0 : 17[5] -> 16[4] via P2P/IPC
g26n02:851594:851821 [5] NCCL INFO Connected all trees
g26n02:851594:851821 [5] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g26n02:851594:851821 [5] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g26n02:851594:851821 [5] NCCL INFO comm 0x170523b30 rank 17 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x28395441218594c5 - Init COMPLETE
g26n02:851594:851904 [5] NCCL INFO Using network IB
g26n02:851594:851904 [5] NCCL INFO comm 0x174250590 rank 4 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xeee4c143812ce6e4 - Init START
g26n02:851594:851904 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g26n02:851594:851904 [5] NCCL INFO Trees [0] 2/-1/-1->4->3 [1] -1/-1/-1->4->3
g26n02:851594:851904 [5] NCCL INFO P2P Chunksize set to 131072
g26n02:851594:851904 [5] NCCL INFO Channel 00/0 : 4[5] -> 5[3] [send] via NET/IB/3
g26n02:851594:851904 [5] NCCL INFO Channel 01/0 : 4[5] -> 5[3] [send] via NET/IB/3
g26n02:851594:851904 [5] NCCL INFO Connected all rings
g26n02:851594:851904 [5] NCCL INFO Channel 00/0 : 2[3] -> 4[5] [receive] via NET/IB/2
g26n02:851594:851904 [5] NCCL INFO Channel 00/0 : 4[5] -> 2[3] [send] via NET/IB/2
g26n02:851594:851904 [5] NCCL INFO Channel 00/0 : 4[5] -> 3[1] via P2P/IPC
g26n02:851594:851904 [5] NCCL INFO Channel 01/0 : 4[5] -> 3[1] via P2P/IPC
g26n02:851594:851904 [5] NCCL INFO Connected all trees
g26n02:851594:851904 [5] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g26n02:851594:851904 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g26n02:851594:851904 [5] NCCL INFO comm 0x174250590 rank 4 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xeee4c143812ce6e4 - Init COMPLETE
g26n02:851594:851926 [5] NCCL INFO Using network IB
g26n02:851594:851926 [5] NCCL INFO comm 0x172e08450 rank 2 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x17fc85f125de53ec - Init START
g26n02:851594:851926 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g26n02:851594:851926 [5] NCCL INFO Trees [0] 1/3/-1->2->4 [1] -1/-1/-1->2->1
g26n02:851594:851926 [5] NCCL INFO P2P Chunksize set to 131072
g26n02:851594:851926 [5] NCCL15c93a2ee1bf52db - Init COMPLETE
g26n02:851592:851928 [3] NCCL INFO Using network IB
g26n02:851592:851928 [3] NCCL INFO comm 0x1421a8720 rank 1 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x6876d4a326688ade - Init START
g26n02:851592:851928 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g26n02:851592:851928 [3] NCCL INFO Trees [0] -1/-1/-1->1->2 [1] 2/0/-1->1->3
g26n02:851592:851928 [3] NCCL INFO P2P Chunksize set to 131072
g26n02:851592:851928 [3] NCCL INFO Channel 00/0 : 0[1] -> 1[3] [receive] via NET/IB/3
g26n02:851592:851928 [3] NCCL INFO Channel 01/0 : 0[1] -> 1[3] [receive] via NET/IB/3
g26n02:851592:851928 [3] NCCL INFO Channel 00/0 : 1[3] -> 2[5] [send] via NET/IB/3
g26n02:851592:851928 [3] NCCL INFO Channel 01/0 : 1[3] -> 2[5] [send] via NET/IB/3
g26n02:851592:851928 [3] NCCL INFO Connected all rings
g26n02:851592:851928 [3] NCCL INFO Channel 01/0 : 1[3] -> 3[1] [send] via NET/IB/3
g26n02:851592:851928 [3] NCCL INFO Channel 01/0 : 3[1] -> 1[3] [receive] via NET/IB/3
g26n02:851592:851928 [3] NCCL INFO Channel 00/0 : 2[5] -> 1[3] [receive] via NET/IB/3
g26n02:851592:851928 [3] NCCL INFO Channel 01/0 : 2[5] -> 1[3] [receive] via NET/IB/3
g26n02:851592:851928 [3] NCCL INFO Channel 01/0 : 1[3] -> 0[1] [send] via NET/IB/3
g26n02:851592:851928 [3] NCCL INFO Connected all trees
g26n02:851592:851928 [3] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g26n02:851592:851928 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g26n02:851592:851928 [3] NCCL INFO comm 0x1421a8720 rank 1 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x6876d4a326688ade - Init COMPLETE
 INFO Channel 00/0 : 1[3] -> 2[5] [receive] via NET/IB/3
g26n02:851594:851926 [5] NCCL INFO Channel 01/0 : 1[3] -> 2[5] [receive] via NET/IB/3
g26n02:851594:851926 [5] NCCL INFO Channel 00/0 : 2[5] -> 3[1] [send] via NET/IB/3
g26n02:851594:851926 [5] NCCL INFO Channel 01/0 : 2[5] -> 3[1] [send] via NET/IB/3
g26n02:851594:851926 [5] NCCL INFO Connected all rings
g26n02:851594:851926 [5] NCCL INFO Channel 00/0 : 2[5] -> 4[3] [send] via NET/IB/3
g26n02:851594:851926 [5] NCCL INFO Channel 00/0 : 4[3] -> 2[5] [receive] via NET/IB/3
g26n02:851594:851926 [5] NCCL INFO Channel 00/0 : 3[1] -> 2[5] [receive] via NET/IB/3
g26n02:851594:851926 [5] NCCL INFO Channel 00/0 : 2[5] -> 1[3] [send] via NET/IB/3
g26n02:851594:851926 [5] NCCL INFO Channel 01/0 : 2[5] -> 1[3] [send] via NET/IB/3
g26n02:851594:851926 [5] NCCL INFO Connected all trees
g26n02:851594:851926 [5] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g26n02:851594:851926 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g26n02:851594:851926 [5] NCCL INFO comm 0x172e08450 rank 2 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x17fc85f125de53ec - Init COMPLETE
g26n02:851593:851593 [4] NCCL INFO cudaDriverVersion 12020
g26n02:851593:851593 [4] NCCL INFO Bootstrap : Using ib0:10.41.17.139<0>
g26n02:851593:851593 [4] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g26n02:851593:851593 [4] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g26n02:851593:851822 [4] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.139<0>
g26n02:851593:851822 [4] NCCL INFO Using network IB
g26n02:851593:851822 [4] NCCL INFO comm 0x147e04190 rank 16 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x28395441218594c5 - Init START
g26n02:851593:851822 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g26n02:851593:851822 [4] NCCL INFO Trees [0] 17/-1/-1->16->15 [1] 15/22/-1->16->27 [2] 17/-1/-1->16->15 [3] 15/-1/-1->16->9
g26n02:851593:851822 [4] NCCL INFO P2P Chunksize set to 131072
g26n02:851593:851822 [4] NCCL INFO Channel 00/0 : 16[4] -> 17[5] via P2P/IPC
g26n02:851593:851822 [4] NCCL INFO Channel 01/0 : 16[4] -> 17[5] via P2P/IPC
g26n02:851593:851822 [4] NCCL INFO Channel 02/0 : 16[4] -> 17[5] via P2P/IPC
g26n02:851593:851822 [4] NCCL INFO Channel 03/0 : 16[4] -> 17[5] via P2P/IPC
g26n02:851593:851822 [4] NCCL INFO Connected all rings
g26n02:851593:851822 [4] NCCL INFO Channel 01/0 : 16[4] -> 22[4] [send] via NET/IB/3
g26n02:851593:851822 [4] NCCL INFO Channel 03/0 : 9[3] -> 16[4] [receive] via NET/IB/3
g26n02:851593:851822 [4] NCCL INFO Channel 01/0 : 16[4] -> 27[3] [send] via NET/IB/3
g26n02:851593:851822 [4] NCCL INFO Channel 01/0 : 27[3] -> 16[4] [receive] via NET/IB/3
g26n02:851593:851822 [4] NCCL INFO Channel 03/0 : 16[4] -> 9[3] [send] via NET/IB/3
g26n02:851593:851822 [4] NCCL INFO Channel 01/0 : 22[4] -> 16[4] [receive] via NET/IB/3
g26n02:851593:851822 [4] NCCL INFO Channel 00/0 : 16[4] -> 15[3] via P2P/IPC
g26n02:851593:851822 [4] NCCL INFO Channel 01/0 : 16[4] -> 15[3] via P2P/IPC
g26n02:851593:851822 [4] NCCL INFO Channel 02/0 : 16[4] -> 15[3] via P2P/IPC
g26n02:851593:851822 [4] NCCL INFO Channel 03/0 : 16[4] -> 15[3] via P2P/IPC
g26n02:851593:851822 [4] NCCL INFO Connected all trees
g26n02:851593:851822 [4] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g26n02:851593:851822 [4] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g26n02:851593:851822 [4] NCCL INFO comm 0x147e04190 rank 16 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x28395441218594c5 - Init COMPLETE
g26n02:851593:851908 [4] NCCL INFO Using network IB
g26n02:851593:851908 [4] NCCL INFO comm 0x14aa98ff0 rank 4 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x864be1a757d9f276 - Init START
g26n02:851593:851908 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g26n02:851593:851908 [4] NCCL INFO Trees [0] 2/-1/-1->4->3 [1] -1/-1/-1->4->3
g26n02:851593:851908 [4] NCCL INFO P2P Chunksize set to 131072
g26n02:851593:851908 [4] NCCL INFO Channel 00/0 : 4[4] -> 5[2] [send] via NET/IB/1
g26n02:851593:851908 [4] NCCL INFO Channel 01/0 : 4[4] -> 5[2] [send] via NET/IB/1
g26n02:851593:851908 [4] NCCL INFO Connected all rings
g26n02:851593:851908 [4] NCCL INFO Channel 00/0 : 2[2] -> 4[4] [receive] via NET/IB/0
g26n02:851593:851908 [4] NCCL INFO Channel 00/0 : 4[4] -> 2[2] [send] via NET/IB/0
g26n02:851593:851908 [4] NCCL INFO Channel 00/0 : 4[4] -> 3[0] via P2P/IPC
g26n02:851593:851908 [4] NCCL INFO Channel 01/0 : 4[4] -> 3[0] via P2P/IPC
g26n02:851593:851908 [4] NCCL INFO Connected all trees
g26n02:851593:851908 [4] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g26n02:851593:851908 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g26n02:851593:851908 [4] NCCL INFO comm 0x14aa98ff0 rank 4 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x864be1a757d9f276 - Init COMPLETE
g26n02:851593:851924 [4] NCCL INFO Using network IB
g26n02:851593:851924 [4] NCCL INFO comm 0x14aaf48d0 rank 2 nranks 12 cudaDev 4 nvmlDev 4 bug26n02:851590:851590 [1] NCCL INFO cudaDriverVersion 12020
g26n02:851590:851590 [1] NCCL INFO Bootstrap : Using ib0:10.41.17.139<0>
g26n02:851590:851590 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g26n02:851590:851590 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g26n02:851590:851819 [1] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.139<0>
g26n02:851590:851819 [1] NCCL INFO Using network IB
g26n02:851590:851819 [1] NCCL INFO comm 0x14abf3e70 rank 13 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x28395441218594c5 - Init START
g26n02:851590:851819 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g26n02:851590:851819 [1] NCCL INFO Trees [0] 14/6/-1->13->12 [1] 12/-1/-1->13->17 [2] 14/-1/-1->13->12 [3] 12/-1/-1->13->17
g26n02:851590:851819 [1] NCCL INFO P2P Chunksize set to 131072
g26n02:851590:851819 [1] NCCL INFO Channel 00/0 : 13[1] -> 14[2] via P2P/IPC
g26n02:851590:851819 [1] NCCL INFO Channel 02/0 : 13[1] -> 14[2] via P2P/IPC
g26n02:851590:851819 [1] NCCL INFO Channel 01/0 : 13[1] -> 12[0] via P2P/IPC
g26n02:851590:851819 [1] NCCL INFO Channel 03/0 : 13[1] -> 12[0] via P2P/IPC
g26n02:851590:851819 [1] NCCL INFO Connected all rings
g26n02:851590:851819 [1] NCCL INFO Channel 01/0 : 13[1] -> 17[5] via P2P/IPC
g26n02:851590:851819 [1] NCCL INFO Channel 03/0 : 13[1] -> 17[5] via P2P/IPC
g26n02:851590:851819 [1] NCCL INFO Channel 00/0 : 6[0] -> 13[1] [receive] via NET/IB/0
g26n02:851590:851819 [1] NCCL INFO Channel 00/0 : 13[1] -> 6[0] [send] via NET/IB/0
g26n02:851590:851819 [1] NCCL INFO Channel 00/0 : 13[1] -> 12[0] via P2P/IPC
g26n02:851590:851819 [1] NCCL INFO Channel 02/0 : 13[1] -> 12[0] via P2P/IPC
g26n02:851590:851819 [1] NCCL INFO Connected all trees
g26n02:851590:851819 [1] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g26n02:851590:851819 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g26n02:851590:851819 [1] NCCL INFO comm 0x14abf3e70 rank 13 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x28395441218594c5 - Init COMPLETE
g26n02:851590:851907 [1] NCCL INFO Using network IB
g26n02:851590:851907 [1] NCCL INFO comm 0x14d916580 rank 3 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0xeee4c143812ce6e4 - Init START
g26n02:851590:851907 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g26n02:851590:851907 [1] NCCL INFO Trees [0] 4/5/-1->3->7 [1] 4/-1/-1->3->2
g26n02:851590:851907 [1] NCCL INFO P2P Chunksize set to 131072
g26n02:851590:851907 [1] NCCL INFO Channel 00/0 : 2[3] -> 3[1] [receive] via NET/IB/2
g26n02:851590:851907 [1] NCCL INFO Channel 01/0 : 2[3] -> 3[1] [receive] via NET/IB/2
g26n02:851590:851907 [1] NCCL INFO Channel 00/0 : 3[1] -> 4[5] via P2P/IPC
g26n02:851590:851907 [1] NCCL INFO Channel 01/0 : 3[1] -> 4[5] via P2P/IPC
g26n02:851590:851907 [1] NCCL INFO Connected all rings
g26n02:851590:851907 [1] NCCL INFO Channel 00/0 : 3[1] -> 5[3] [send] via NET/IB/2
g26n02:851590:851907 [1] NCCL INFO Channel 00/0 : 3[1] -> 7[5] [send] via NET/IB/2
g26n02:851590:851907 [1] NCCL INFO Channel 00/0 : 7[5] -> 3[1] [receive] via NET/IB/2
g26n02:851590:851907 [1] NCCL INFO Channel 00/0 : 5[3] -> 3[1] [receive] via NET/IB/2
g26n02:851590:851907 [1] NCCL INFO Channel 01/0 : 3[1] -> 2[3] [send] via NET/IB/2
g26n02:851590:851907 [1] NCCL INFO Connected all trees
g26n02:851590:851907 [1] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g26n02:851590:851907 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g26n02:851590:851907 [1] NCCL INFO comm 0x14d916580 rank 3 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0xeee4c143812ce6e4 - Init COMPLETE
g26n02:851590:851925 [1] NCCL INFO Using network IB
g26n02:851590:851925 [1] NCCL INFO comm 0x14e9ff190 rank 1 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0xad5995bf820e7340 - Init START
g26n02:851590:851925 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g26n02:851590:851925sId 3504000 commId 0x76a32220a008087c - Init START
g26n02:851593:851924 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g26n02:851593:851924 [4] NCCL INFO Trees [0] 1/3/-1->2->4 [1] -1/-1/-1->2->1
g26n02:851593:851924 [4] NCCL INFO P2P Chunksize set to 131072
g26n02:851593:851924 [4] NCCL INFO Channel 00/0 : 1[2] -> 2[4] [receive] via NET/IB/1
g26n02:851593:851924 [4] NCCL INFO Channel 01/0 : 1[2] -> 2[4] [receive] via NET/IB/1
g26n02:851593:851924 [4] NCCL INFO Channel 00/0 : 2[4] -> 3[0] [send] via NET/IB/1
g26n02:851593:851924 [4] NCCL INFO Channel 01/0 : 2[4] -> 3[0] [send] via NET/IB/1
g26n02:851593:851924 [4] NCCL INFO Connected all rings
g26n02:851593:851924 [4] NCCL INFO Channel 00/0 : 2[4] -> 4[2] [send] via NET/IB/1
g26n02:851593:851924 [4] NCCL INFO Channel 00/0 : 4[2] -> 2[4] [receive] via NET/IB/1
g26n02:851593:851924 [4] NCCL INFO Channel 00/0 : 3[0] -> 2[4] [receive] via NET/IB/1
g26n02:851593:851924 [4] NCCL INFO Channel 00/0 : 2[4] -> 1[2] [send] via NET/IB/1
g26n02:851593:851924 [4] NCCL INFO Channel 01/0 : 2[4] -> 1[2] [send] via NET/IB/1
g26n02:851593:851924 [4] NCCL INFO Connected all trees
g26n02:851593:851924 [4] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g26n02:851593:851924 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g26n02:851593:851924 [4] NCCL INFO comm 0x14aaf48d0 rank 2 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x76a32220a008087c - Init COMPLETE
 [1] NCCL INFO Trees [0] -1/-1/-1->1->2 [1] 2/0/-1->1->3
g26n02:851590:851925 [1] NCCL INFO P2P Chunksize set to 131072
g26n02:851590:851925 [1] NCCL INFO Channel 00/0 : 0[5] -> 1[1] [receive] via NET/IB/2
g26n02:851590:851925 [1] NCCL INFO Channel 01/0 : 0[5] -> 1[1] [receive] via NET/IB/2
g26n02:851590:851925 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[3] [send] via NET/IB/2
g26n02:851590:851925 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[3] [send] via NET/IB/2
g26n02:851590:851925 [1] NCCL INFO Connected all rings
g26n02:851590:851925 [1] NCCL INFO Channel 01/0 : 1[1] -> 3[5] [send] via NET/IB/2
g26n02:851590:851925 [1] NCCL INFO Channel 01/0 : 3[5] -> 1[1] [receive] via NET/IB/2
g26n02:851590:851925 [1] NCCL INFO Channel 00/0 : 2[3] -> 1[1] [receive] via NET/IB/2
g26n02:851590:851925 [1] NCCL INFO Channel 01/0 : 2[3] -> 1[1] [receive] via NET/IB/2
g26n02:851590:851925 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[5] [send] via NET/IB/2
g26n02:851590:851925 [1] NCCL INFO Connected all trees
g26n02:851590:851925 [1] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g26n02:851590:851925 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g26n02:851590:851925 [1] NCCL INFO comm 0x14e9ff190 rank 1 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0xad5995bf820e7340 - Init COMPLETE
g27n17:842068:842068 [5] NCCL INFO cudaDriverVersion 12020
g27n17:842068:842068 [5] NCCL INFO Bootstrap : Using ib0:10.41.17.172<0>
g27n17:842068:842068 [5] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g27n17:842068:842068 [5] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g27n17:842068:842297 [5] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.172<0>
g27n17:842068:842297 [5] NCCL INFO Using network IB
g27n17:842068:842297 [5] NCCL INFO comm 0x153c43af0 rank 23 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x28395441218594c5 - Init START
g27n17:842068:842297 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g27n17:842068:842297 [5] NCCL INFO Trees [0] -1/-1/-1->23->22 [1] 19/-1/-1->23->21 [2] -1/-1/-1->23->22 [3] 19/-1/-1->23->21
g27n17:842068:842297 [5] NCCL INFO P2P Chunksize set to 131072
g27n17:842068:842297 [5] NCCL INFO Channel 00/0 : 23[5] -> 24[0] [send] via NET/IB/1
g27n17:842068:842297 [5] NCCL INFO Channel 02/0 : 23[5] -> 24[0] [send] via NET/IB/1
g27n17:842068:842297 [5] NCCL INFO Channel 01/0 : 23[5] -> 20[2] via P2P/IPC
g27n17:842068:842297 [5] NCCL INFO Channel 03/0 : 23[5] -> 20[2] via P2P/IPC
g27n17:842068:842297 [5] NCCL INFO Connected all rings
g27n17:842068:842297 [5] NCCL INFO Channel 01/0 : 23[5] -> 19[1] via P2P/IPC
g27n17:842068:842297 [5] NCCL INFO Channel 03/0 : 23[5] -> 19[1] via P2P/IPC
g27n17:842068:842297 [5] NCCL INFO Channel 01/0 : 23[5] -> 21[3] via P2P/IPC
g27n17:842068:842297 [5] NCCL INFO Channel 03/0 : 23[5] -> 21[3] via P2P/IPC
g27n17:842068:842297 [5] NCCL INFO Channel 00/0 : 23[5] -> 22[4] via P2P/IPC
g27n17:842068:842297 [5] NCCL INFO Channel 02/0 : 23[5] -> 22[4] via P2P/IPC
g27n17:842068:842297 [5] NCCL INFO Connected all trees
g27n17:842068:842297 [5] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g27n17:842068:842297 [5] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g27n17:842068:842297 [5] NCCL INFO comm 0x153c43af0 rank 23 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x28395441218594c5 - Init COMPLETE
g27n17:842068:842386 [5] NCCL INFO Using network IB
g27n17:842068:842386 [5] NCCL INFO comm 0x1565b76f0 rank 5 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x15c93a2ee1bf52db - Init START
g27n17:842068:842386 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g27n17:842068:842386 [5] NCCL INFO Trees [0] -1/-1/-1->5->4 [1] 7/-1/-1->5->4
g27n17:842068:842386 [5] NCCL INFO P2P Chunksize set to 131072
g27n17:842068:842386 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[3] [send] via NET/IB/3
g27n17:842068:842386 [5] NCCL INFO Channel 01/0 : 5[5] -> 6[3] [send] via NET/IB/3
g27n17:842068:842386 [5] NCCL INFO Connected all rings
g27n17:842068:842386 [5] NCCL INFO Channel 01/0 : 5[5] -> 7[1] [send] via NET/IB/2
g27n17:842068:842386 [5] NCCL INFO Channel 01/0 : 7[1] -> 5[5] [receive] via NET/IB/2
g27n17:842068:842386 [5] NCCL INFO Channel 00/0 : 5[5] -> 4[1] via P2P/IPC
g27n17:842068:842386 [5] NCCL INFO Channel 01/0 : 5[5] -> 4[1] via P2P/IPC
g27n17:842068:842386 [5] NCCL INFO Connected all trees
g27n17:842068:842386 [5] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g27n17:842068:842386 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g27n17:842068:842386 [5] NCCL INFO comm 0x1565b76f0 rank 5 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x15c93a2ee1bf52db - Init COMPLETE
g27n17:842068:842402 [5] NCCL INFO Using network IB
g27n17:842068:842402 [5] NCCL INFO comm 0x156595850 rank 2 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x6876d4a326688ade - Init START
g27n17:842068:842402 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g27n17:842068:842402 [5] NCCL INFO Trees [0] 1/3/-1->2->4 [1] -1/-1/-1->2->1
g27n17:842068:842402 [5] NCCL INFO P2P Chunksize set to 131072
g27n17:842068:842402 [5] NCCLg27n17:842064:842064 [3] NCCL INFO cudaDriverVersion 12020
g27n17:842064:842064 [3] NCCL INFO Bootstrap : Using ib0:10.41.17.172<0>
g27n17:842064:842064 [3] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g27n17:842064:842064 [3] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g27n17:842064:842298 [3] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.172<0>
g27n17:842064:842298 [3] NCCL INFO Using network IB
g27n17:842064:842298 [3] NCCL INFO comm 0x156844100 rank 21 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x28395441218594c5 - Init START
g27n17:842064:842298 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g27n17:842064:842298 [3] NCCL INFO Trees [0] 22/-1/-1->21->20 [1] 23/-1/-1->21->22 [2] 22/-1/-1->21->20 [3] 23/34/-1->21->22
g27n17:842064:842298 [3] NCCL INFO P2P Chunksize set to 131072
g27n17:842064:842298 [3] NCCL INFO Channel 00/0 : 21[3] -> 22[4] via P2P/IPC
g27n17:842064:842298 [3] NCCL INFO Channel 01/0 : 21[3] -> 22[4] via P2P/IPC
g27n17:842064:842298 [3] NCCL INFO Channel 02/0 : 21[3] -> 22[4] via P2P/IPC
g27n17:842064:842298 [3] NCCL INFO Channel 03/0 : 21[3] -> 22[4] via P2P/IPC
g27n17:842064:842298 [3] NCCL INFO Channel 01/0 : 12[0] -> 21[3] [receive] via NET/IB/3
g27n17:842064:842298 [3] NCCL INFO Channel 03/0 : 12[0] -> 21[3] [receive] via NET/IB/3
g27n17:842064:842298 [3] NCCL INFO Connected all rings
g27n17:842064:842298 [3] NCCL INFO Channel 01/0 : 21[3] -> 23[5] via P2P/IPC
g27n17:842064:842298 [3] NCCL INFO Channel 03/0 : 21[3] -> 23[5] via P2P/IPC
g27n17:842064:842298 [3] NCCL INFO Channel 03/0 : 21[3] -> 34[4] [send] via NET/IB/3
g27n17:842064:842298 [3] NCCL INFO Channel 03/0 : 34[4] -> 21[3] [receive] via NET/IB/3
g27n17:842064:842298 [3] NCCL INFO Channel 00/0 : 21[3] -> 20[2] via P2P/IPC
g27n17:842064:842298 [3] NCCL INFO Channel 02/0 : 21[3] -> 20[2] via P2P/IPC
g27n17:842064:842298 [3] NCCL INFO Connected all trees
g27n17:842064:842298 [3] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g27n17:842064:842298 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g27n17:842064:842298 [3] NCCL INFO comm 0x156844100 rank 21 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x28395441218594c5 - Init COMPLETE
g27n17:842064:842382 [3] NCCL INFO Using network IB
g27n17:842064:842382 [3] NCCL INFO comm 0x15919f6f0 rank 5 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xeee4c143812ce6e4 - Init START
g27n17:842064:842382 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g27n17:842064:842382 [3] NCCL INFO Trees [0] -1/-1/-1->5->3 [1] 8/2/-1->5->11
g27n17:842064:842382 [3] NCCL INFO P2P Chunksize set to 131072
g27n17:842064:842382 [3] NCCL INFO Channel 00/0 : 4[5] -> 5[3] [receive] via NET/IB/3
g27n17:842064:842382 [3] NCCL INFO Channel 01/0 : 4[5] -> 5[3] [receive] via NET/IB/3
g27n17:842064:842382 [3] NCCL INFO Channel 00/0 : 5[3] -> 6[1] [send] via NET/IB/3
g27n17:842064:842382 [3] NCCL INFO Channel 01/0 : 5[3] -> 6[1] [send] via NET/IB/3
g27n17:842064:842382 [3] NCCL INFO Connected all rings
g27n17:842064:842382 [3] NCCL INFO Channel 00/0 : 3[1] -> 5[3] [receive] via NET/IB/3
g27n17:842064:842382 [3] NCCL INFO Channel 01/0 : 2[3] -> 5[3] [receive] via NET/IB/3
g27n17:842064:842382 [3] NCCL INFO Channel 01/0 : 5[3] -> 8[3] [send] via NET/IB/3
g27n17:842064:842382 [3] NCCL INFO Channel 01/0 : 5[3] -> 11[3] [send] via NET/IB/3
g27n17:842064:842382 [3] NCCL INFO Channel 01/0 : 11[3] -> 5[3] [receive] via NET/IB/3
g27n17:842064:842382 [3] NCCL INFO Channel 01/0 : 8[3] -> 5[3] [receive] via NET/IB/3
g27n17:842064:842382 [3] NCCL INFO Channel 01/0 : 5[3] -> 2[3] [send] via NET/IB/3
g27n17:842064:842382 [3] NCCL INFO Channel 00/0 : 5[3] -> 3[1] [send] via NET/IB/3
g27n17:842064:842382 [3] NCCL INFO Connected all trees
g27n17:842064:842382 [3] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g27n17:842064:842382 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g27n17:842064:842382 [3] NCCL INFO comm 0x15919f6f0 rank 5 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xeee4c143812ce6e4 - Init COMPLETE
g27n17:842064:842403 [3] NCCL INFO Using network IB
g27n17:842064:842403 [3] NCCL INFO comm 0x159504420 rank 2 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xad5995bf820e7340 - Init START
g27n17:842064:842403 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g27n17:842064:842403 [3] NCCL INFO Trees [0] 1/3/-1->2->4 [1] -1/-1/-1->2->1
g27n17:842064:842403 [3] NCCL INFO P2P Chunksize set to 131072
g27n17:842064:842403 [3] NCCL INFO Channel 00/0 : 1[1] -> 2[3] [receive] via NET/IB/3
g27n17:842064:842403 [3] NCCL INFO Channel 01/0 : 1[1] -> 2[3] [receive] via NET/IB/3
g27n17:842064:842403 [3] NCCL INFO Channel 00/0 : 2[3] -> 3[5] [send] via NET/IB/3
g27n17:842064:842403 [3] NCCL INFO Channel 01/0 : 2[3] -> 3[5] [send] via NET/IB/3
g27n17:842064:842403 [3] NCCL INFO Connected all rings
g27n17:842064:842403 [3] NCCL INFO Channel 00/0 : 2[3] -> 4[1] [send] via NET/IB/3
g27n17:842064:842403 [3] NCCL INFO Channel 00/0 : 4[1] -> 2[3] [receive] via NET/IB/3
g27n17:842064:842403 [3] NCCL INFO Channel 00/0 : 3[5] -> 2[3] [receive] via NET/IB/3
g27n17:842064:842403 [3] NCCL INFO Channel 00/0 : 2[3] -> 1[1] [send] via NET/IB/3
g27n17:842064:842403 [3] NCCL INFO Channel 01/0 : 2[3] -> 1[1] [send] via NET/IB/3
g27n17:842064:842403 [3] NCCL INFO Connected all trees
g27n17:842064:842403 [3] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g27n17:842064:842403 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g27n17:842064:842403 [3] NCCL INFO comm 0x159504420 rank 2 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xad5995bf820e7340 - Init COMPLETE
 INFO Channel 00/0 : 1[3] -> 2[5] [receive] via NET/IB/3
g27n17:842068:842402 [5] NCCL INFO Channel 01/0 : 1[3] -> 2[5] [receive] via NET/IB/3
g27n17:842068:842402 [5] NCCL INFO Channel 00/0 : 2[5] -> 3[1] [send] via NET/IB/3
g27n17:842068:842402 [5] NCCL INFO Channel 01/0 : 2[5] -> 3[1] [send] via NET/IB/3
g27n17:842068:842402 [5] NCCL INFO Connected all rings
g27n17:842068:842402 [5] NCCL INFO Channel 00/0 : 2[5] -> 4[3] [send] via NET/IB/3
g27n17:842068:842402 [5] NCCL INFO Channel 00/0 : 4[3] -> 2[5] [receive] via NET/IB/3
g27n17:842068:842402 [5] NCCL INFO Channel 00/0 : 3[1] -> 2[5] [receive] via NET/IB/3
g27n17:842068:842402 [5] NCCL INFO Channel 00/0 : 2[5] -> 1[3] [send] via NET/IB/3
g27n17:842068:842402 [5] NCCL INFO Channel 01/0 : 2[5] -> 1[3] [send] via NET/IB/3
g27n17:842068:842402 [5] NCCL INFO Connected all trees
g27n17:842068:842402 [5] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g27n17:842068:842402 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g27n17:842068:842402 [5] NCCL INFO comm 0x156595850 rank 2 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x6876d4a326688ade - Init COMPLETE
g32n03:753098:753098 [2] NCCL INFO cudaDriverVersion 12020
g32n03:753098:753098 [2] NCCL INFO Bootstrap : Using ib0:10.41.17.248<0>
g32n03:753098:753098 [2] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g32n03:753098:753098 [2] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g32n03:753098:753327 [2] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.248<0>
g32n03:753098:753327 [2] NCCL INFO Using network IB
g32n03:753098:753327 [2] NCCL INFO comm 0x1404146e0 rank 92 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x28395441218594c5 - Init START
g32n03:753098:753327 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g32n03:753098:753327 [2] NCCL INFO Trees [0] 93/-1/-1->92->91 [1] -1/-1/-1->92->90 [2] 93/-1/-1->92->91 [3] -1/-1/-1->92->90
g32n03:753098:753327 [2] NCCL INFO P2P Chunksize set to 131072
g32n03:753098:753327 [2] NCCL INFO Channel 00/0 : 92[2] -> 93[3] via P2P/IPC
g32n03:753098:753327 [2] NCCL INFO Channel 02/0 : 92[2] -> 93[3] via P2P/IPC
g32n03:753098:753327 [2] NCCL INFO Channel 01/0 : 92[2] -> 91[1] via P2P/IPC
g32n03:753098:753327 [2] NCCL INFO Channel 03/0 : 92[2] -> 91[1] via P2P/IPC
g32n03:753098:753327 [2] NCCL INFO Connected all rings
g32n03:753098:753327 [2] NCCL INFO Channel 01/0 : 92[2] -> 90[0] via P2P/IPC
g32n03:753098:753327 [2] NCCL INFO Channel 03/0 : 92[2] -> 90[0] via P2P/IPC
g32n03:753098:753327 [2] NCCL INFO Channel 00/0 : 92[2] -> 91[1] via P2P/IPC
g32n03:753098:753327 [2] NCCL INFO Channel 02/0 : 92[2] -> 91[1] via P2P/IPC
g32n03:753098:753327 [2] NCCL INFO Connected all trees
g32n03:753098:753327 [2] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g32n03:753098:753327 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g32n03:753098:753327 [2] NCCL INFO comm 0x1404146e0 rank 92 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x28395441218594c5 - Init COMPLETE
g32n03:753098:753412 [2] NCCL INFO Using network IB
g32n03:753098:753412 [2] NCCL INFO comm 0x1431fc8b0 rank 23 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0x864be1a757d9f276 - Init START
g32n03:753098:753412 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g32n03:753098:753412 [2] NCCL INFO Trees [0] -1/-1/-1->23->21 [1] 11/-1/-1->23->-1
g32n03:753098:753412 [2] NCCL INFO P2P Chunksize set to 131072
g32n03:753098:753412 [2] NCCL INFO Channel 00/0 : 22[4] -> 23[2] [receive] via NET/IB/0
g32n03:753098:753412 [2] NCCL INFO Channel 01/0 : 22[4] -> 23[2] [receive] via NET/IB/0
g32n03:753098:753412 [2] NCCL INFO Channel 00/0 : 23[2] -> 0[0] [send] via NET/IB/0
g32n03:753098:753412 [2] NCCL INFO Channel 01/0 : 23[2] -> 0[0] [send] via NET/IB/0
g32n03:753098:753412 [2] NCCL INFO Connected all rings
g32n03:753098:753412 [2] NCCL INFO Channel 00/0 : 21[0] -> 23[2] [receive] via NET/IB/0
g32n03:753098:753412 [2] NCCL INFO Channel 01/0 : 11[2] -> 23[2] [receive] via NET/IB/0
g32n03:753098:753412 [2] NCCL INFO Channel 01/0 : 23[2] -> 11[2] [send] via NET/IB/0
g32n03:753098:753412 [2] NCCL INFO Channel 00/0 : 23[2] -> 21[0] [send] via NET/IB/0
g32n03:753098:753412 [2] NCCL INFO Connected all trees
g32n03:753098:753412 [2] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g32n03:753098:753412 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g32n03:753098:753412 [2] NCCL INFO comm 0x1431fc8b0 rank 23 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0x864be1a757d9f276 - Init COMPLETE
g32n03:753098:753429 [2] NCCL INFO Using network IB
g32n03:753098:753429 [2] NCCL INFO comm 0x14222a490 rank 11 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0x51bdf7b6b8c3aecb - Init START
g32n03:753098:753429 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g32n03:753098:753429 [2] NCCL INFO Trees [0] -1/-1/-1->11->10 [1] 3/-1/-1->11->-1
g32n03:753098:753429 [2] NCCL INFO P2P Chunksize set to 131072
g32n03:753098:753429 [2] NCCL INFO Channel 00/0 : 10[0] -> 11[2] [receive] via NET/IB/0
g32n03:753098:753429 [2] NCCL INFO Channel 01/0 : 10[0] -> 11[2] [receive] via NET/IB/0
g32n03:753098:753429 [2] NCCL INFO Channel 00/0 : 11[2] -> 0[4] [send] via NET/IB/0
g32n03:753098:753429 [2] NCCL INFO Channel 01/0 : 11[2] -> 0[4] [send] via NET/IB/0
g32n03:753098:753429 [2] NCCL INFO Connected all rings
g32n03:753098:753429 [2] NCCL INFO Channel 01/0 : 11[2] -> 3[4] [send] via NET/IB/0
g32n03:753098:753429 [2] NCCL INFO Channel 01/0 : 3[4] -> 11[2] [receive] via NET/IB/0
g32n03:753098:753429 [2] NCCL INFO Channel 00/0 : 11[2] -> 10[0] [send] via NET/IB/0
g32n03:753098:753429 [2] NCCL INFO Connected all trees
g32n03:753098:753429 [2] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g32n03:753098:753429 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g32n03:753098:753429 [2] NCCL INFO comm 0x14222a490 rank 11 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0x51bdf7b6b8c3aecb - Init COMPLETE
g25n18:898853:899261 [1] NCCL INFO Using network IB
g25n18:898853:899261 [1] NCCL INFO comm 0x143eed4b0 rank 0 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0xeee4c143812ce6e4 - Init START
g25n18:898853:899261 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g25n18:898853:899261 [1] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
g25n18:898853:899261 [1] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
g25n18:898853:899261 [1] NCCL INFO Trees [0] 1/12/-1->0->-1 [1] 1/-1/-1->0->2
g25n18:898853:899261 [1] NCCL INFO P2P Chunksize set to 131072
g25n18:898853:899261 [1] NCCL INFO Channel 00/0 : 23[3] -> 0[1] [receive] via NET/IB/2
g25n18:898853:899261 [1] NCCL INFO Channel 01/0 : 23[3] -> 0[1] [receive] via NET/IB/2
g25n18:898853:899261 [1] NCCL INFO Channel 00/0 : 0[1] -> 1[5] via P2P/IPC
g25n18:898853:899261 [1] NCCL INFO Channel 01/0 : 0[1] -> 1[5] via P2P/IPC
g25n18:898853:899261 [1] NCCL INFO Connected all rings
g25n18:898853:899261 [1] NCCL INFO Channel 01/0 : 0[1] -> 2[3] [send] via NET/IB/2
g25n18:898853:899261 [1] NCCL INFO Channel 00/0 : 12[1] -> 0[1] [receive] via NET/IB/2
g25n18:898853:899261 [1] NCCL INFO Channel 00/0 : 0[1] -> 12[1] [send] via NET/IB/2
g25n18:898853:899261 [1] NCCL INFO Channel 01/0 : 2[3] -> 0[1] [receive] via NET/IB/2
g25n18:898853:899261 [1] NCCL INFO Connected all trees
g25n18:898853:899261 [1] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g25n18:898853:899261 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g25n18:898853:899261 [1] NCCL INFO comm 0x143eed4b0 rank 0 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0xeee4c143812ce6e4 - Init COMPLETE
g25n18:898853:899286 [1] NCCL INFO Using network IB
g25n18:898853:899286 [1] NCCL INFO comm 0x143e002f0 rank 0 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x17fc85f125de53ec - Init START
g25n18:898853:899286 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g25n18:898853:899286 [1] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7   8   9  10  11
g25n18:898853:899286 [1] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7   8   9  10  11
g25n18:898853:899286 [1] NCCL INFO Trees [0] 8/-1/-1->0->-1 [1] -1/-1/-1->0->1
g25n18:898853:899286 [1] NCCL INFO P2P Chunksize set to 131072
g25n18:898853:899286 [1] NCCL INFO Channel 00/0 : 11[5] -> 0[1] [receive] via NET/IB/2
g25n18:898853:899286 [1] NCCL INFO Channel 01/0 : 11[5] -> 0[1] [receive] via NET/IB/2
g25n18:898853:899286 [1] NCCL INFO Channel 00/0 : 0[1] -> 1[3] [send] via NET/IB/2
g25n18:898853:899286 [1] NCCL INFO Channel 01/0 : 0[1] -> 1[3] [send] via NET/IB/2
g25n18:898853:899286 [1] NCCL INFO Connected all rings
g25n18:898853:899286 [1] NCCL INFO Channel 00/0 : 8[5] -> 0[1] [receive] via NET/IB/2
g25n18:898853:899286 [1] NCCL INFO Channel 00/0 : 0[1] -> 8[5] [send] via NET/IB/2
g25n18:898853:899286 [1] NCCL INFO Channel 01/0 : 1[3] -> 0[1] [receive] via NET/IB/2
g25n18:898853:899286 [1] NCCL INFO Connected all trees
g25n18:898853:899286 [1] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g25n18:898853:899286 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g25n18:898853:899286 [1] NCCL INFO comm 0x143e002f0 rank 0 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x17fc85f125de53ec - Init COMPLETE
g31n04:688228:688228 [4] NCCL INFO cudaDriverVersion 12020
g31n04:688228:688228 [4] NCCL INFO Bootstrap : Using ib0:10.41.17.231<0>
g31n04:688228:688228 [4] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g31n04:688228:688228 [4] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g31n04:688228:688456 [4] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.231<0>
g31n04:688228:688456 [4] NCCL INFO Using network IB
g31n04:688228:688456 [4] NCCL INFO comm 0x153e44510 rank 76 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x28395441218594c5 - Init START
g31n04:688228:688456 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g31n04:688228:688456 [4] NCCL INFO Trees [0] 77/-1/-1->76->75 [1] 75/88/-1->76->52 [2] 77/-1/-1->76->75 [3] 75/-1/-1->76->82
g31n04:688228:688456 [4] NCCL INFO P2P Chunksize set to 131072
g31n04:688228:688456 [4] NCCL INFO Channel 00/0 : 76[4] -> 77[5] via P2P/IPC
g31n04:688228:688456 [4] NCCL INFO Channel 01/0 : 76[4] -> 77[5] via P2P/IPC
g31n04:688228:688456 [4] NCCL INFO Channel 02/0 : 76[4] -> 77[5] via P2P/IPC
g31n04:688228:688456 [4] NCCL INFO Channel 03/0 : 76[4] -> 77[5] via P2P/IPC
g31n04:688228:688456 [4] NCCL INFO Connected all rings
g31n04:688228:688456 [4] NCCL INFO Channel 03/0 : 76[4] -> 82[4] [send] via NET/IB/3
g31n04:688228:688456 [4] NCCL INFO Channel 01/0 : 76[4] -> 88[4] [send] via NET/IB/3
g31n04:688228:688456 [4] NCCL INFO Channel 01/0 : 52[4] -> 76[4] [receive] via NET/IB/3
g31n04:688228:688456 [4] NCCL INFO Channel 01/0 : 76[4] -> 52[4] [send] via NET/IB/3
g31n04:688228:688456 [4] NCCL INFO Channel 01/0 : 88[4] -> 76[4] [receive] via NET/IB/3
g31n04:688228:688456 [4] NCCL INFO Channel 03/0 : 82[4] -> 76[4] [receive] via NET/IB/3
g31n04:688228:688456 [4] NCCL INFO Channel 00/0 : 76[4] -> 75[3] via P2P/IPC
g31n04:688228:688456 [4] NCCL INFO Channel 01/0 : 76[4] -> 75[3] via P2P/IPC
g31n04:688228:688456 [4] NCCL INFO Channel 02/0 : 76[4] -> 75[3] via P2P/IPC
g31n04:688228:688456 [4] NCCL INFO Channel 03/0 : 76[4] -> 75[3] via P2P/IPC
g31n04:688228:688456 [4] NCCL INFO Connected all trees
g31n04:688228:688456 [4] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g31n04:688228:688456 [4] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g31n04:688228:688456 [4] NCCL INFO comm 0x153e44510 rank 76 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x28395441218594c5 - Init COMPLETE
g31n04:688228:688544 [4] NCCL INFO Using network IB
g31n04:688228:688544 [4] NCCL INFO comm 0x15673f340 rank 19 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x864be1a757d9f276 - Init START
g31n04:688228:688544 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g31n04:688228:688544 [4] NCCL INFO Trees [0] 15/-1/-1->19->18 [1] -1/-1/-1->19->18
g31n04:688228:688544 [4] NCCL INFO P2P Chunksize set to 131072
g31n04:688228:688544 [4] NCCL INFO Channel 00/0 : 19[4] -> 20[2] [send] via NET/IB/1
g31n04:688228:688544 [4] NCCL INFO Channel 01/0 : 19[4] -> 20[2] [send] via NET/IB/1
g31n04:688228:688544 [4] NCCL INFO Connected all rings
g31n04:688228:688544 [4] NCCL INFO Channel 00/0 : 15[0] -> 19[4] [receive] via NET/IB/0
g31n04:688228:688544 [4] NCCL INFO Channel 00/0 : 19[4] -> 15[0] [send] via NET/IB/0
g31n04:688228:688544 [4] NCCL INFO Channel 00/0 : 19[4] -> 18[0] via P2P/IPC
g31n04:688228:688544 [4] NCCL INFO Channel 01/0 : 19[4] -> 18[0] via P2P/IPC
g31n04:688228:688544 [4] NCCL INFO Connected all trees
g31n04:688228:688544 [4] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g31n04:688228:688544 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n04:688228:688544 [4] NCCL INFO comm 0x15673f340 rank 19 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x864be1a757d9f276 - Init COMPLETE
g31n04:688228:688560 [4] NCCL INFO Using network IB
g31n04:688228:688560 [4] NCCL INFO comm 0x156aa1760 rank 9 nranks 12 g31n04:688227:688227 [3] NCCL INFO cudaDriverVersion 12020
g31n04:688227:688227 [3] NCCL INFO Bootstrap : Using ib0:10.41.17.231<0>
g31n04:688227:688227 [3] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g31n04:688227:688227 [3] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g31n04:688227:688460 [3] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.231<0>
g31n04:688227:688460 [3] NCCL INFO Using network IB
g31n04:688227:688460 [3] NCCL INFO comm 0x12de142a0 rank 75 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x28395441218594c5 - Init START
g31n04:688227:688460 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g31n04:688227:688460 [3] NCCL INFO Trees [0] 76/-1/-1->75->74 [1] 77/64/-1->75->76 [2] 76/-1/-1->75->74 [3] 77/-1/-1->75->76
g31n04:688227:688460 [3] NCCL INFO P2P Chunksize set to 131072
g31n04:688227:688460 [3] NCCL INFO Channel 00/0 : 75[3] -> 76[4] via P2P/IPC
g31n04:688227:688460 [3] NCCL INFO Channel 01/0 : 75[3] -> 76[4] via P2P/IPC
g31n04:688227:688460 [3] NCCL INFO Channel 02/0 : 75[3] -> 76[4] via P2P/IPC
g31n04:688227:688460 [3] NCCL INFO Channel 03/0 : 75[3] -> 76[4] via P2P/IPC
g31n04:688227:688460 [3] NCCL INFO Channel 01/0 : 66[0] -> 75[3] [receive] via NET/IB/3
g31n04:688227:688460 [3] NCCL INFO Channel 03/0 : 66[0] -> 75[3] [receive] via NET/IB/3
g31n04:688227:688460 [3] NCCL INFO Connected all rings
g31n04:688227:688460 [3] NCCL INFO Channel 01/0 : 75[3] -> 77[5] via P2P/IPC
g31n04:688227:688460 [3] NCCL INFO Channel 03/0 : 75[3] -> 77[5] via P2P/IPC
g31n04:688227:688460 [3] NCCL INFO Channel 01/0 : 64[4] -> 75[3] [receive] via NET/IB/3
g31n04:688227:688460 [3] NCCL INFO Channel 01/0 : 75[3] -> 64[4] [send] via NET/IB/3
g31n04:688227:688460 [3] NCCL INFO Channel 00/0 : 75[3] -> 74[2] via P2P/IPC
g31n04:688227:688460 [3] NCCL INFO Channel 02/0 : 75[3] -> 74[2] via P2P/IPC
g31n04:688227:688460 [3] NCCL INFO Connected all trees
g31n04:688227:688460 [3] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g31n04:688227:688460 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g31n04:688227:688460 [3] NCCL INFO comm 0x12de142a0 rank 75 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x28395441218594c5 - Init COMPLETE
g31n04:688227:688542 [3] NCCL INFO Using network IB
g31n04:688227:688542 [3] NCCL INFO comm 0x1306ef2f0 rank 18 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x15c93a2ee1bf52db - Init START
g31n04:688227:688542 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g31n04:688227:688542 [3] NCCL INFO Trees [0] 15/21/-1->18->12 [1] -1/-1/-1->18->19
g31n04:688227:688542 [3] NCCL INFO P2P Chunksize set to 131072
g31n04:688227:688542 [3] NCCL INFO Channel 00/0 : 17[5] -> 18[3] [receive] via NET/IB/3
g31n04:688227:688542 [3] NCCL INFO Channel 01/0 : 17[5] -> 18[3] [receive] via NET/IB/3
g31n04:688227:688542 [3] NCCL INFO Channel 00/0 : 18[3] -> 19[1] [send] via NET/IB/3
g31n04:688227:688542 [3] NCCL INFO Channel 01/0 : 18[3] -> 19[1] [send] via NET/IB/3
g31n04:688227:688542 [3] NCCL INFO Connected all rings
g31n04:688227:688542 [3] NCCL INFO Channel 00/0 : 15[3] -> 18[3] [receive] via NET/IB/3
g31n04:688227:688542 [3] NCCL INFO Channel 00/0 : 18[3] -> 21[3] [send] via NET/IB/3
g31n04:688227:688542 [3] NCCL INFO Channel 00/0 : 12[3] -> 18[3] [receive] via NET/IB/3
g31n04:688227:688542 [3] NCCL INFO Channel 00/0 : 18[3] -> 12[3] [send] via NET/IB/3
g31n04:688227:688542 [3] NCCL INFO Channel 00/0 : 21[3] -> 18[3] [receive] via NET/IB/3
g31n04:688227:688542 [3] NCCL INFO Channel 00/0 : 18[3] -> 15[3] [send] via NET/IB/3
g31n04:688227:688542 [3] NCCL INFO Channel 01/0 : 19[1] -> 18[3] [receive] via NET/IB/3
g31n04:688227:688542 [3] NCCL INFO Connected all trees
g31n04:688227:688542 [3] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g31n04:688227:688542 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per cudaDev 4 nvmlDev 4 busId 3504000 commId 0x51bdf7b6b8c3aecb - Init START
g31n04:688228:688560 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g31n04:688228:688560 [4] NCCL INFO Trees [0] -1/-1/-1->9->10 [1] 10/8/-1->9->7
g31n04:688228:688560 [4] NCCL INFO P2P Chunksize set to 131072
g31n04:688228:688560 [4] NCCL INFO Channel 00/0 : 8[2] -> 9[4] [receive] via NET/IB/1
g31n04:688228:688560 [4] NCCL INFO Channel 01/0 : 8[2] -> 9[4] [receive] via NET/IB/1
g31n04:688228:688560 [4] NCCL INFO Channel 00/0 : 9[4] -> 10[0] [send] via NET/IB/1
g31n04:688228:688560 [4] NCCL INFO Channel 01/0 : 9[4] -> 10[0] [send] via NET/IB/1
g31n04:688228:688560 [4] NCCL INFO Connected all rings
g31n04:688228:688560 [4] NCCL INFO Channel 01/0 : 7[0] -> 9[4] [receive] via NET/IB/1
g31n04:688228:688560 [4] NCCL INFO Channel 01/0 : 9[4] -> 7[0] [send] via NET/IB/1
g31n04:688228:688560 [4] NCCL INFO Channel 00/0 : 10[0] -> 9[4] [receive] via NET/IB/1
g31n04:688228:688560 [4] NCCL INFO Channel 01/0 : 10[0] -> 9[4] [receive] via NET/IB/1
g31n04:688228:688560 [4] NCCL INFO Channel 01/0 : 9[4] -> 8[2] [send] via NET/IB/1
g31n04:688228:688560 [4] NCCL INFO Connected all trees
g31n04:688228:688560 [4] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g31n04:688228:688560 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n04:688228:688560 [4] NCCL INFO comm 0x156aa1760 rank 9 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x51bdf7b6b8c3aecb - Init COMPLETE
peer
g31n04:688227:688542 [3] NCCL INFO comm 0x1306ef2f0 rank 18 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x15c93a2ee1bf52db - Init COMPLETE
g31n04:688227:688564 [3] NCCL INFO Using network IB
g31n04:688227:688564 [3] NCCL INFO comm 0x130cad5f0 rank 9 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x1542a3b966c3490b - Init START
g31n04:688227:688564 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g31n04:688227:688564 [3] NCCL INFO Trees [0] -1/-1/-1->9->10 [1] 10/8/-1->9->7
g31n04:688227:688564 [3] NCCL INFO P2P Chunksize set to 131072
g31n04:688227:688564 [3] NCCL INFO Channel 00/0 : 8[1] -> 9[3] [receive] via NET/IB/3
g31n04:688227:688564 [3] NCCL INFO Channel 01/0 : 8[1] -> 9[3] [receive] via NET/IB/3
g31n04:688227:688564 [3] NCCL INFO Channel 00/0 : 9[3] -> 10[5] [send] via NET/IB/3
g31n04:688227:688564 [3] NCCL INFO Channel 01/0 : 9[3] -> 10[5] [send] via NET/IB/3
g31n04:688227:688564 [3] NCCL INFO Connected all rings
g31n04:688227:688564 [3] NCCL INFO Channel 01/0 : 7[5] -> 9[3] [receive] via NET/IB/3
g31n04:688227:688564 [3] NCCL INFO Channel 01/0 : 9[3] -> 7[5] [send] via NET/IB/3
g31n04:688227:688564 [3] NCCL INFO Channel 00/0 : 10[5] -> 9[3] [receive] via NET/IB/3
g31n04:688227:688564 [3] NCCL INFO Channel 01/0 : 10[5] -> 9[3] [receive] via NET/IB/3
g31n04:688227:688564 [3] NCCL INFO Channel 01/0 : 9[3] -> 8[1] [send] via NET/IB/3
g31n04:688227:688564 [3] NCCL INFO Connected all trees
g31n04:688227:688564 [3] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g31n04:688227:688564 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n04:688227:688564 [3] NCCL INFO comm 0x130cad5f0 rank 9 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x1542a3b966c3490b - Init COMPLETE
g31n04:688229:688229 [5] NCCL INFO cudaDriverVersion 12020
g31n04:688229:688229 [5] NCCL INFO Bootstrap : Using ib0:10.41.17.231<0>
g31n04:688229:688229 [5] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g31n04:688229:688229 [5] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g31n04:688229:688457 [5] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.231<0>
g31n04:688229:688457 [5] NCCL INFO Using network IB
g31n04:688229:688457 [5] NCCL INFO comm 0x128723b10 rank 77 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x28395441218594c5 - Init START
g31n04:688229:688457 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g31n04:688229:688457 [5] NCCL INFO Trees [0] -1/-1/-1->77->76 [1] 73/-1/-1->77->75 [2] -1/-1/-1->77->76 [3] 73/-1/-1->77->75
g31n04:688229:688457 [5] NCCL INFO P2P Chunksize set to 131072
g31n04:688229:688457 [5] NCCL INFO Channel 00/0 : 77[5] -> 78[0] [send] via NET/IB/1
g31n04:688229:688457 [5] NCCL INFO Channel 02/0 : 77[5] -> 78[0] [send] via NET/IB/1
g31n04:688229:688457 [5] NCCL INFO Channel 01/0 : 77[5] -> 74[2] via P2P/IPC
g31n04:688229:688457 [5] NCCL INFO Channel 03/0 : 77[5] -> 74[2] via P2P/IPC
g31n04:688229:688457 [5] NCCL INFO Connected all rings
g31n04:688229:688457 [5] NCCL INFO Channel 01/0 : 77[5] -> 73[1] via P2P/IPC
g31n04:688229:688457 [5] NCCL INFO Channel 03/0 : 77[5] -> 73[1] via P2P/IPC
g31n04:688229:688457 [5] NCCL INFO Channel 01/0 : 77[5] -> 75[3] via P2P/IPC
g31n04:688229:688457 [5] NCCL INFO Channel 03/0 : 77[5] -> 75[3] via P2P/IPC
g31n04:688229:688457 [5] NCCL INFO Channel 00/0 : 77[5] -> 76[4] via P2P/IPC
g31n04:688229:688457 [5] NCCL INFO Channel 02/0 : 77[5] -> 76[4] via P2P/IPC
g31n04:688229:688457 [5] NCCL INFO Connected all trees
g31n04:688229:688457 [5] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g31n04:688229:688457 [5] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g31n04:688229:688457 [5] NCCL INFO comm 0x128723b10 rank 77 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x28395441218594c5 - Init COMPLETE
g31n04:688229:688543 [5] NCCL INFO Using network IB
g31n04:688229:688543 [5] NCCL INFO comm 0x12b01b0f0 rank 19 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xeee4c143812ce6e4 - Init START
g31n04:688229:688543 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g31n04:688229:688543 [5] NCCL INFO Trees [0] 15/-1/-1->19->18 [1] -1/-1/-1->19->18
g31n04:688229:688543 [5] NCCL INFO P2P Chunksize set to 131072
g31n04:688229:688543 [5] NCCL INFO Channel 00/0 : 19[5] -> 20[3] [send] via NET/IB/3
g31n04:688229:688543 [5] NCCL INFO Channel 01/0 : 19[5] -> 20[3] [send] via NET/IB/3
g31n04:688229:688543 [5] NCCL INFO Connected all rings
g31n04:688229:688543 [5] NCCL INFO Channel 00/0 : 15[1] -> 19[5] [receive] via NET/IB/2
g31n04:688229:688543 [5] NCCL INFO Channel 00/0 : 19[5] -> 15[1] [send] via NET/IB/2
g31n04:688229:688543 [5] NCCL INFO Channel 00/0 : 19[5] -> 18[1] via P2P/IPC
g31n04:688229:688543 [5] NCCL INFO Channel 01/0 : 19[5] -> 18[1] via P2P/IPC
g31n04:688229:688543 [5] NCCL INFO Connected all trees
g31n04:688229:688543 [5] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g31n04:688229:688543 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n04:688229:688543 [5] NCCL INFO comm 0x12b01b0f0 rank 19 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xeee4c143812ce6e4 - Init COMPLETE
g31n04:688229:688562 [5] NCCL INFO Using network IB
g31n04:688229:688562 [5] NCCL INFO comm 0x12b413430 rank 9 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xad5995bf820e7340 - Init START
g31n04:688229:688562 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g31n04:688229:688562 [5] NCCL INFO Trees [0] -1/-1/-1->9->10 [1] 10/8/-1->9->7
g31n04:688229:688562 [5] NCCL INFO P2P Chunksize set to 131072
g31n04:688229:688562 [5] NCCL INFO Channel 00/0 : 8[3] -> 9[5] [receive] via NET/IB/3
g31n04:688229:688562 [5] NCCL INFO Channel 01/0 : 8[3] -> 9[5] [receive] via NET/IB/3
g31n04:688229:688562 [5] NCCL INFO Channel 00/0 : 9[5] -> 10[1] [send] via NET/IB/3
g31n04:688229:688562 [5] NCCL INFO Channel 01/0 : 9[5] -> 10[1] [send] via NET/IB/3
g31n04:688229:688562 [5] NCCL INFO Connected all rings
g31n04:688229:688562 [5] NCCL INFO Channel 01/0 : 7[1] -> 9[5] [receive] via NET/IB/3
g31n04:688229:688562 [5] NCCL INFO Channel 01/0 : 9[5] -> 7[1] [send] via NET/IB/3
g31n04:688229:688562 [5] NCCL INFO Channel 00/0 : 10[1] -> 9[5] [receive] via NET/IB/3
g31n04:688229:688562 [5] NCCL INFO Channel 01/0 : 10[1] -> 9[5] [receive] via NET/IB/3
g31n04:688229:688562 [5] NCCL INFO Channel 01/0 : 9[5] -> 8[3] [send] via NET/IB/3
g31n04:688229:688562 [5] NCCL INFO Connected all trees
g31n04:688229:688562 [5] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g31n04:688229:688562 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n04:688229:688562 [5] NCCL INFO comm 0x12b413430 rank 9 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xad5995bf820e7340 - Init COMPLETE
g26n01:811335:811335 [4] NCCL INFO cudaDriverVersion 12020
g26n01:811335:811335 [4] NCCL INFO Bootstrap : Using ib0:10.41.17.138<0>
g26n01:811335:811335 [4] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g26n01:811335:811335 [4] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g26n01:811335:811565 [4] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.138<0>
g26n01:811335:811565 [4] NCCL INFO Using network IB
g26n01:811335:811565 [4] NCCL INFO comm 0x118a842a0 rank 10 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x28395441218594c5 - Init START
g26n01:811335:811565 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g26n01:811335:811565 [4] NCCL INFO Trees [0] 11/-1/-1->10->9 [1] 9/-1/-1->10->15 [2] 11/-1/-1->10->9 [3] 9/4/-1->10->22
g26n01:811335:811565 [4] NCCL INFO P2P Chunksize set to 131072
g26n01:811335:811565 [4] NCCL INFO Channel 00/0 : 10[4] -> 11[5] via P2P/IPC
g26n01:811335:811565 [4] NCCL INFO Channel 01/0 : 10[4] -> 11[5] via P2P/IPC
g26n01:811335:811565 [4] NCCL INFO Channel 02/0 : 10[4] -> 11[5] via P2P/IPC
g26n01:811335:811565 [4] NCCL INFO Channel 03/0 : 10[4] -> 11[5] via P2P/IPC
g26n01:811335:811565 [4] NCCL INFO Connected all rings
g26n01:811335:811565 [4] NCCL INFO Channel 01/0 : 10[4] -> 15[3] [send] via NET/IB/3
g26n01:811335:811565 [4] NCCL INFO Channel 03/0 : 4[4] -> 10[4] [receive] via NET/IB/3
g26n01:811335:811565 [4] NCCL INFO Channel 03/0 : 10[4] -> 22[4] [send] via NET/IB/3
g26n01:811335:811565 [4] NCCL INFO Channel 03/0 : 22[4] -> 10[4] [receive] via NET/IB/3
g26n01:811335:811565 [4] NCCL INFO Channel 03/0 : 10[4] -> 4[4] [send] via NET/IB/3
g26n01:811335:811565 [4] NCCL INFO Channel 01/0 : 15[3] -> 10[4] [receive] via NET/IB/3
g26n01:811335:811565 [4] NCCL INFO Channel 00/0 : 10[4] -> 9[3] via P2P/IPC
g26n01:811335:811565 [4] NCCL INFO Channel 01/0 : 10[4] -> 9[3] via P2P/IPC
g26n01:811335:811565 [4] NCCL INFO Channel 02/0 : 10[4] -> 9[3] via P2P/IPC
g26n01:811335:811565 [4] NCCL INFO Channel 03/0 : 10[4] -> 9[3] via P2P/IPC
g26n01:811335:811565 [4] NCCL INFO Connected all trees
g26n01:811335:811565 [4] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g26n01:811335:811565 [4] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g26n01:811335:811565 [4] NCCL INFO comm 0x118a842a0 rank 10 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x28395441218594c5 - Init COMPLETE
g26n01:811335:811643 [4] NCCL INFO Using network IB
g26n01:811335:811643 [4] NCCL INFO comm 0x11b723c50 rank 2 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xe45164e09488020a - Init START
g26n01:811335:811643 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g26n01:811335:811643 [4] NCCL INFO Trees [0] -1/-1/-1->2->1 [1] 3/-1/-1->2->1
g26n01:811335:811643 [4] NCCL INFO P2P Chunksize set to 131072
g26n01:811335:811643 [4] NCCL INFO Channel 00/0 : 2[4] -> 3[2] [send] via NET/IB/1
g26n01:811335:811643 [4] NCCL INFO Channel 01/0 : 2[4] -> 3[2] [send] via NET/IB/1
g26n01:811335:811643 [4] NCCL INFO Connected all rings
g26n01:811335:811643 [4] NCCL INFO Channel 01/0 : 3[2] -> 2[4] [receive] via NET/IB/0
g26n01:811335:811643 [4] NCCL INFO Channel 00/0 : 2[4] -> 1[0] via P2P/IPC
g26n01:811335:811643 [4] NCCL INFO Channel 01/0 : 2[4] -> 1[0] via P2P/IPC
g26n01:811335:811643 [4] NCCL INFO Connected all trees
g26n01:811335:811643 [4] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g26n01:811335:811643 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g26n01:811335:811643 [4] NCCL INFO comm 0x11b723c50 rank 2 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xe45164e09488020a - Init COMPLETE
g26n01:811335:811668 [4] NCCL INFO Using network IB
g26n01:811335:811668 [4] NCCL INFO comm 0x11bed2070 rank 1 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xaca7b5a6b2aa6a19 - Init START
g26n01:811335:811668 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g26n01:811335:811668 [4] NCCL INFO Trees [0] -1/-1/-1->1->2 [1] 2/0/-1->1->3
g26n01:811335:811668 [4] NCCL INFO P2P Chunksize set to 131072
g26n01:811335:811668 [4] NCCL INFO Channel 00/0 : 0[2] -> 1[4] [receive] via NET/IB/1
g26n01:811335:811668 [4] NCCL INFO Channel 01/0 : 0[2] -> 1[4] [receive] via NET/IB/1
g26n01:811335:811668 [4] NCCL INFO Channel 00/0 : 1[4] -> 2[0] [send] via NET/IB/1
g26n01:811335:811668 [4] NCCL INFO Channel 01/0 : 1[4] -> 2[0] [send] via NET/IB/1
g26n01:811335:811668 [4] NCCL INFO Connected all rings
g26n01:811335:811668 [4] NCCL INFO Channel 01/0 : 1[4] -> 3[2] [send] via NET/IB/1
g26n01:811335:811668 [4] NCCL INFO Channel 01/0 : 3[2] -> 1[4] [receive] via NET/IB/1
g26n01:811335:811668 [4] NCCL INFO Channel 00/0 : 2[0] -> 1[4] [receive] via NET/IB/1
g26n01:811335:811668 [4] NCCL INFO Channel 01/0 : 2[0] -> 1[4] [receive] via NET/IB/1
g26n01:811335:811668 [4] NCCL INFO Channel 01/0 : 1[4] -> 0[2] [send] via NET/IB/1
g26n01:811335:811668 [4] NCCL INFO Connected all trees
g26n01:811335:811668 [4] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g26n01:811335:811668 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g26n01:811335:811668 [4] NCCL INFO comm 0x11bed2070 rank 1 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xaca7b5a6b2aa6a19 - Init COMPLETE
g31n04:688225:688225 [1] NCCL INFO cudaDriverVersion 12020
g31n04:688225:688225 [1] NCCL INFO Bootstrap : Using ib0:10.41.17.231<0>
g31n04:688225:688225 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g31n04:688225:688225 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g31n04:688225:688459 [1] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.231<0>
g31n04:688225:688459 [1] NCCL INFO Using network IB
g31n04:688225:688459 [1] NCCL INFO comm 0x14c1e4210 rank 73 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x28395441218594c5 - Init START
g31n04:688225:688459 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g31n04:688225:688459 [1] NCCL INFO Trees [0] 74/60/-1->73->72 [1] 72/-1/-1->73->77 [2] 74/-1/-1->73->72 [3] 72/-1/-1->73->77
g31n04:688225:688459 [1] NCCL INFO P2P Chunksize set to 131072
g31n04:688225:688459 [1] NCCL INFO Channel 00/0 : 73[1] -> 74[2] via P2P/IPC
g31n04:688225:688459 [1] NCCL INFO Channel 02/0 : 73[1] -> 74[2] via P2P/IPC
g31n04:688225:688459 [1] NCCL INFO Channel 01/0 : 73[1] -> 72[0] via P2P/IPC
g31n04:688225:688459 [1] NCCL INFO Channel 03/0 : 73[1] -> 72[0] via P2P/IPC
g31n04:688225:688459 [1] NCCL INFO Connected all rings
g31n04:688225:688459 [1] NCCL INFO Channel 01/0 : 73[1] -> 77[5] via P2P/IPC
g31n04:688225:688459 [1] NCCL INFO Channel 03/0 : 73[1] -> 77[5] via P2P/IPC
g31n04:688225:688459 [1] NCCL INFO Channel 00/0 : 60[0] -> 73[1] [receive] via NET/IB/0
g31n04:688225:688459 [1] NCCL INFO Channel 00/0 : 73[1] -> 60[0] [send] via NET/IB/0
g31n04:688225:688459 [1] NCCL INFO Channel 00/0 : 73[1] -> 72[0] via P2P/IPC
g31n04:688225:688459 [1] NCCL INFO Channel 02/0 : 73[1] -> 72[0] via P2P/IPC
g31n04:688225:688459 [1] NCCL INFO Connected all trees
g31n04:688225:688459 [1] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g31n04:688225:688459 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g31n04:688225:688459 [1] NCCL INFO comm 0x14c1e4210 rank 73 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x28395441218594c5 - Init COMPLETE
g31n04:688225:688546 [1] NCCL INFO Using network IB
g31n04:688225:688546 [1] NCCL INFO comm 0x14eb239a0 rank 18 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0xeee4c143812ce6e4 - Init START
g31n04:688225:688546 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g31n04:688225:688546 [1] NCCL INFO Trees [0] 19/21/-1->18->12 [1] 19/-1/-1->18->20
g31n04:688225:688546 [1] NCCL INFO P2P Chunksize set to 131072
g31n04:688225:688546 [1] NCCL INFO Channel 00/0 : 17[3] -> 18[1] [receive] via NET/IB/2
g31n04:688225:688546 [1] NCCL INFO Channel 01/0 : 17[3] -> 18[1] [receive] via NET/IB/2
g31n04:688225:688546 [1] NCCL INFO Channel 00/0 : 18[1] -> 19[5] via P2P/IPC
g31n04:688225:688546 [1] NCCL INFO Channel 01/0 : 18[1] -> 19[5] via P2P/IPC
g31n04:688225:688546 [1] NCCL INFO Connected all rings
g31n04:688225:688546 [1] NCCL INFO Channel 01/0 : 18[1] -> 20[3] [send] via NET/IB/2
g31n04:688225:688546 [1] NCCL INFO Channel 00/0 : 18[1] -> 21[1] [send] via NET/IB/2
g31n04:688225:688546 [1] NCCL INFO Channel 00/0 : 12[1] -> 18[1] [receive] via NET/IB/2
g31n04:688225:688546 [1] NCCL INFO Channel 00/0 : 18[1] -> 12[1] [send] via NET/IB/2
g31n04:688225:688546 [1] NCCL INFO Channel 00/0 : 21[1] -> 18[1] [receive] via NET/IB/2
g31n04:688225:688546 [1] NCCL INFO Channel 01/0 : 20[3] -> 18[1] [receive] via NET/IB/2
g31n04:688225:688546 [1] NCCL INFO Connected all trees
g31n04:688225:688546 [1] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g31n04:688225:688546 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n04:688225:688546 [1] NCCL INFO comm 0x14eb239a0 rank 18 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0xeee4c143812ce6e4 - Init COMPLETE
g31n04:688225:688561 [1] NCCL INFO Using network IB
g31n04:688225:688561 [1] NCCL INFO comm 0x14eacd440 rank 9 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x17fc85f125de53ec - Init START
g31n04:688225:688561 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g31n04:688225:688561 [1] NCCL INFO Trees [0] -1/-1/-1->9->10 [1] 10/8/-1->9->7
g31n04:688225:688561 [1] NCCL INFO P2P Chunksize set to 131072
g31n04:688225:688561 [1] NCCL INFO Channel 00/0 : 8[5] -> 9[1] [receive] via NET/IB/2
g31n04:688225:688561 [1] NCCL INFO Channel 01/0 : 8[5] -> 9[1] [receive] via NET/IB/2
g31n04:688225:688561 [1] NCCL INFO Channel 00/0 : 9[1] -> 10[3] [send] via NET/IB/2
g31n04:688225:688561 [1] NCCL INFO Channel 01/0 : 9[1] -> 10[3] [send] via NET/IB/2
g31n04:688225:688561 [1] NCCL INFO Connected all rings
g31n04:688225:688561 [1] NCCL INFO Channel 01/0 : 7[3] -> 9[1] [receive] via NET/IB/2
g31n04:688225:688561 [1] NCCL INFO Channel 01/0 : 9[1] -> 7[3] [send] via NET/IB/2
g31n04:688225:688561 [1] NCCL INFO Channel 00/0 : 10[3] -> 9[1] [receive] via NET/IB/2
g31n04:688225:688561 [1] NCCL INFO Channel 01/0 : 10[3] -> 9[1] [receive] via NET/IB/2
g31n04:688225:688561 [1] NCCL INFO Channel 01/0 : 9[1] -> 8[5] [send] via NET/IB/2
g31n04:688225:688561 [1] NCCL INFO Connected all trees
g31n04:688225:688561 [1] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g31n04:688225:688561 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n04:688225:688561 [1] NCCL INFO comm 0x14eacd440 rank 9 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x17fc85f125de53ec - Init COMPLETE
g25n18:898852:899148 [0] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.137<0>
g25n18:898852:899148 [0] NCCL INFO Using network IB
g25n18:898852:899148 [0] NCCL INFO comm 0x137675bc0 rank 0 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x28395441218594c5 - Init START
g25n18:898852:899148 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g25n18:898852:899148 [0] NCCL INFO Channel 00/04 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
g25n18:898852:899148 [0] NCCL INFO Channel 01/04 :    0   9  10  11   8   7   6  15  16  17  14  13  12  21  22  23  20  19  18  27
g25n18:898852:899148 [0] NCCL INFO Channel 02/04 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
g25n18:898852:899148 [0] NCCL INFO Channel 03/04 :    0   9  10  11   8   7   6  15  16  17  14  13  12  21  22  23  20  19  18  27
g25n18:898852:899148 [0] NCCL INFO Trees [0] 1/48/-1->0->-1 [1] 2/-1/-1->0->1 [2] 1/-1/-1->0->6 [3] 2/-1/-1->0->1
g25n18:898852:899148 [0] NCCL INFO P2P Chunksize set to 131072
g25n18:898852:899148 [0] NCCL INFO Channel 00/0 : 95[5] -> 0[0] [receive] via NET/IB/0
g25n18:898852:899148 [0] NCCL INFO Channel 02/0 : 95[5] -> 0[0] [receive] via NET/IB/0
g25n18:898852:899148 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/IPC
g25n18:898852:899148 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/IPC
g25n18:898852:899148 [0] NCCL INFO Channel 01/0 : 0[0] -> 9[3] [send] via NET/IB/2
g25n18:898852:899148 [0] NCCL INFO Channel 03/0 : 0[0] -> 9[3] [send] via NET/IB/2
g25n18:898852:899148 [0] NCCL INFO Connected all rings
g25n18:898852:899148 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/IPC
g25n18:898852:899148 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/IPC
g25n18:898852:899148 [0] NCCL INFO Channel 01/0 : 0[0] -> 2[2] via P2P/IPC
g25n18:898852:899148 [0] NCCL INFO Channel 03/0 : 0[0] -> 2[2] via P2P/IPC
g25n18:898852:899148 [0] NCCL INFO Channel 02/0 : 0[0] -> 6[0] [send] via NET/IB/0
g25n18:898852:899148 [0] NCCL INFO Channel 00/0 : 48[0] -> 0[0] [receive] via NET/IB/0
g25n18:898852:899148 [0] NCCL INFO Channel 00/0 : 0[0] -> 48[0] [send] via NET/IB/0
g25n18:898852:899148 [0] NCCL INFO Channel 02/0 : 6[0] -> 0[0] [receive] via NET/IB/0
g25n18:898852:899148 [0] NCCL INFO Connected all trees
g25n18:898852:899148 [0] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g25n18:898852:899148 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g25n18:898852:899148 [0] NCCL INFO comm 0x137675bc0 rank 0 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x28395441218594c5 - Init COMPLETE
g25n18:898852:899266 [0] NCCL INFO Using network IB
g25n18:898852:899266 [0] NCCL INFO comm 0x13a505760 rank 0 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0x864be1a757d9f276 - Init START
g25n18:898852:899266 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g25n18:898852:899266 [0] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
g25n18:898852:899266 [0] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
g25n18:898852:899266 [0] NCCL INFO Trees [0] 1/12/-1->0->-1 [1] 1/-1/-1->0->2
g25n18:898852:899266 [0] NCCL INFO P2P Chunksize set to 131072
g25n18:898852:899266 [0] NCCL INFO Channel 00/0 : 23[2] -> 0[0] [receive] via NET/IB/0
g25n18:898852:899266 [0] NCCL INFO Channel 01/0 : 23[2] -> 0[0] [receive] via NET/IB/0
g25n18:898852:899266 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[4] via P2P/IPC
g25n18:898852:899266 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[4] via P2P/IPC
g25n18:898852:899266 [0] NCCL INFO Connected all rings
g25n18:898852:899266 [0] NCCL INFO Channel 01/0 : 0[0] -> 2[2] [send] via NET/IB/0
g25n18:898852:899266 [0] NCCL INFO Channel 00/0 : 12[0] -> 0[0] [receive] via NET/IB/0
g25n18:898852:899266 [0] NCCL INFO Channel 00/0 : 0[0] -> 12[0] [send] via NET/IB/0
g25n18:898852:899266 [0] NCCL INFO Channel 01/0 : 2[2] -> 0[0] [receive] via NET/IB/0
g25n18:898852:899266 [0] NCCL INFO Connected all trees
g25n18:898852:899266 [0] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g25n18:898852:899266 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g25n18:898852:899266 [0] NCCL INFO comm 0x13a505760 rank 0 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0x864be1a757d9f276 - Init COMPLETE
g25n18:898852:899282 [0] NCCL INFO Using network IB
g25n18:898852:899282 [0] NCCL INFO comm 0x139fc53b0 rank 0 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x76a32220a008087c - Init START
g25n18:898852:899282 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g25n18:898852:899282 [0] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7   8   9  10  11
g25n18:898852:899282 [0] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7   8   9  10  11
g25n18:898852:899282 [0] NCCL INFO Trees [0] 8/-1/-1->0->-1 [1] -1/-1/-1->0->1
g25n18:898852:899282 [0] NCCL INFO P2P Chunksize set to 131072
g25n18:898852:899282 [0] NCCL INFO Channel 00/0 : 11[4] -> 0[0] [receive] via NET/IB/0
g25n18:898852:899282 [0] NCCL INFO Channel 01/0 : 11[4] -> 0[0] [receive] via NET/IB/0
g25n18:898852:899282 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[2] [send] via NET/IB/0
g25n18:898852:899282 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[2] [send] via NET/IB/0
g25n18:898852:899282 [0] NCCL INFO Connected all rings
g25n18:898852:899282 [0] NCCL INFO Channel 00/0 : 8[4] -> 0[0] [receive] via NET/IB/0
g25n18:898852:899282 [0] NCCL INFO Channel 00/0 : 0[0] -> 8[4] [send] via NET/IB/0
g25n18:898852:899282 [0] NCCL INFO Channel 01/0 : 1[2] -> 0[0] [receive] via NET/IB/0
g25n18:898852:899282 [0] NCCL INFO Connected all trees
g25n18:898852:899282 [0] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g25n18:898852:899282 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g25n18:898852:899282 [0] NCCL INFO comm 0x139fc53b0 rank 0 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x76a32220a008087c - Init COMPLETE
g32n02:753686:753686 [3] NCCL INFO cudaDriverVersion 12020
g32n02:753686:753686 [3] NCCL INFO Bootstrap : Using ib0:10.41.17.247<0>
g32n02:753686:753686 [3] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g32n02:753686:753686 [3] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g32n02:753686:753917 [3] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.247<0>
g32n02:753686:753917 [3] NCCL INFO Using network IB
g32n02:753686:753917 [3] NCCL INFO comm 0x113103b50 rank 87 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x28395441218594c5 - Init START
g32n02:753686:753917 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g32n02:753686:753917 [3] NCCL INFO Trees [0] 88/-1/-1->87->86 [1] 89/82/-1->87->88 [2] 88/-1/-1->87->86 [3] 89/-1/-1->87->88
g32n02:753686:753917 [3] NCCL INFO P2P Chunksize set to 131072
g32n02:753686:753917 [3] NCCL INFO Channel 00/0 : 87[3] -> 88[4] via P2P/IPC
g32n02:753686:753917 [3] NCCL INFO Channel 01/0 : 87[3] -> 88[4] via P2P/IPC
g32n02:753686:753917 [3] NCCL INFO Channel 02/0 : 87[3] -> 88[4] via P2P/IPC
g32n02:753686:753917 [3] NCCL INFO Channel 03/0 : 87[3] -> 88[4] via P2P/IPC
g32n02:753686:753917 [3] NCCL INFO Channel 01/0 : 78[0] -> 87[3] [receive] via NET/IB/3
g32n02:753686:753917 [3] NCCL INFO Channel 03/0 : 78[0] -> 87[3] [receive] via NET/IB/3
g32n02:753686:753917 [3] NCCL INFO Connected all rings
g32n02:753686:753917 [3] NCCL INFO Channel 01/0 : 87[3] -> 89[5] via P2P/IPC
g32n02:753686:753917 [3] NCCL INFO Channel 03/0 : 87[3] -> 89[5] via P2P/IPC
g32n02:753686:753917 [3] NCCL INFO Channel 01/0 : 82[4] -> 87[3] [receive] via NET/IB/3
g32n02:753686:753917 [3] NCCL INFO Channel 01/0 : 87[3] -> 82[4] [send] via NET/IB/3
g32n02:753686:753917 [3] NCCL INFO Channel 00/0 : 87[3] -> 86[2] via P2P/IPC
g32n02:753686:753917 [3] NCCL INFO Channel 02/0 : 87[3] -> 86[2] via P2P/IPC
g32n02:753686:753917 [3] NCCL INFO Connected all trees
g32n02:753686:753917 [3] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g32n02:753686:753917 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g32n02:753686:753917 [3] NCCL INFO comm 0x113103b50 rank 87 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x28395441218594c5 - Init COMPLETE
g32n02:753686:754001 [3] NCCL INFO Using network IB
g32n02:753686:754001 [3] NCCL INFO comm 0x1150a5c30 rank 21 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x15c93a2ee1bf52db - Init START
g32n02:753686:754001 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g32n02:753686:754001 [3] NCCL INFO Trees [0] 19/22/-1->21->18 [1] -1/-1/-1->21->20
g32n02:753686:754001 [3] NCCL INFO P2P Chunksize set to 131072
g32n02:753686:754001 [3] NCCL INFO Channel 00/0 : 20[5] -> 21[3] [receive] via NET/IB/3
g32n02:753686:754001 [3] NCCL INFO Channel 01/0 : 20[5] -> 21[3] [receive] via NET/IB/3
g32n02:753686:754001 [3] NCCL INFO Channel 00/0 : 21[3] -> 22[1] [send] via NET/IB/3
g32n02:753686:754001 [3] NCCL INFO Channel 01/0 : 21[3] -> 22[1] [send] via NET/IB/3
g32n02:753686:754001 [3] NCCL INFO Connected all rings
g32n02:753686:754001 [3] NCCL INFO Channel 00/0 : 19[1] -> 21[3] [receive] via NET/IB/3
g32n02:753686:754001 [3] NCCL INFO Channel 00/0 : 18[3] -> 21[3] [receive] via NET/IB/3
g32n02:753686:754001 [3] NCCL INFO Channel 00/0 : 21[3] -> 18[3] [send] via NET/IB/3
g32n02:753686:754001 [3] NCCL INFO Channel 00/0 : 21[3] -> 19[1] [send] via NET/IB/3
g32n02:753686:754001 [3] NCCL INFO Channel 00/0 : 22[1] -> 21[3] [receive] via NET/IB/3
g32n02:753686:754001 [3] NCCL INFO Channel 01/0 : 21[3] -> 20[5] [send] via NET/IB/3
g32n02:753686:754001 [3] NCCL INFO Connected all trees
g32n02:753686:754001 [3] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g32n02:753686:754001 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g32n02:753686:754001 [3] NCCL INFO comm 0x1150a5c30 rank 21 nranks 24 cudaDev 3 nvmg32n02:753687:753687 [4] NCCL INFO cudaDriverVersion 12020
g32n02:753687:753687 [4] NCCL INFO Bootstrap : Using ib0:10.41.17.247<0>
g32n02:753687:753687 [4] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g32n02:753687:753687 [4] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g32n02:753687:753915 [4] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.247<0>
g32n02:753687:753915 [4] NCCL INFO Using network IB
g32n02:753687:753915 [4] NCCL INFO comm 0x15e593d60 rank 88 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x28395441218594c5 - Init START
g32n02:753687:753915 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g32n02:753687:753915 [4] NCCL INFO Trees [0] 89/-1/-1->88->87 [1] 87/94/-1->88->76 [2] 89/-1/-1->88->87 [3] 87/-1/-1->88->81
g32n02:753687:753915 [4] NCCL INFO P2P Chunksize set to 131072
g32n02:753687:753915 [4] NCCL INFO Channel 00/0 : 88[4] -> 89[5] via P2P/IPC
g32n02:753687:753915 [4] NCCL INFO Channel 01/0 : 88[4] -> 89[5] via P2P/IPC
g32n02:753687:753915 [4] NCCL INFO Channel 02/0 : 88[4] -> 89[5] via P2P/IPC
g32n02:753687:753915 [4] NCCL INFO Channel 03/0 : 88[4] -> 89[5] via P2P/IPC
g32n02:753687:753915 [4] NCCL INFO Connected all rings
g32n02:753687:753915 [4] NCCL INFO Channel 01/0 : 88[4] -> 94[4] [send] via NET/IB/3
g32n02:753687:753915 [4] NCCL INFO Channel 03/0 : 81[3] -> 88[4] [receive] via NET/IB/3
g32n02:753687:753915 [4] NCCL INFO Channel 01/0 : 76[4] -> 88[4] [receive] via NET/IB/3
g32n02:753687:753915 [4] NCCL INFO Channel 01/0 : 88[4] -> 76[4] [send] via NET/IB/3
g32n02:753687:753915 [4] NCCL INFO Channel 03/0 : 88[4] -> 81[3] [send] via NET/IB/3
g32n02:753687:753915 [4] NCCL INFO Channel 01/0 : 94[4] -> 88[4] [receive] via NET/IB/3
g32n02:753687:753915 [4] NCCL INFO Channel 00/0 : 88[4] -> 87[3] via P2P/IPC
g32n02:753687:753915 [4] NCCL INFO Channel 01/0 : 88[4] -> 87[3] via P2P/IPC
g32n02:753687:753915 [4] NCCL INFO Channel 02/0 : 88[4] -> 87[3] via P2P/IPC
g32n02:753687:753915 [4] NCCL INFO Channel 03/0 : 88[4] -> 87[3] via P2P/IPC
g32n02:753687:753915 [4] NCCL INFO Connected all trees
g32n02:753687:753915 [4] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g32n02:753687:753915 [4] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g32n02:753687:753915 [4] NCCL INFO comm 0x15e593d60 rank 88 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x28395441218594c5 - Init COMPLETE
g32n02:753687:754005 [4] NCCL INFO Using network IB
g32n02:753687:754005 [4] NCCL INFO comm 0x16122c1f0 rank 22 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x864be1a757d9f276 - Init START
g32n02:753687:754005 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g32n02:753687:754005 [4] NCCL INFO Trees [0] 20/-1/-1->22->21 [1] -1/-1/-1->22->21
g32n02:753687:754005 [4] NCCL INFO P2P Chunksize set to 131072
g32n02:753687:754005 [4] NCCL INFO Channel 00/0 : 22[4] -> 23[2] [send] via NET/IB/1
g32n02:753687:754005 [4] NCCL INFO Channel 01/0 : 22[4] -> 23[2] [send] via NET/IB/1
g32n02:753687:754005 [4] NCCL INFO Connected all rings
g32n02:753687:754005 [4] NCCL INFO Channel 00/0 : 20[2] -> 22[4] [receive] via NET/IB/0
g32n02:753687:754005 [4] NCCL INFO Channel 00/0 : 22[4] -> 20[2] [send] via NET/IB/0
g32n02:753687:754005 [4] NCCL INFO Channel 00/0 : 22[4] -> 21[0] via P2P/IPC
g32n02:753687:754005 [4] NCCL INFO Channel 01/0 : 22[4] -> 21[0] via P2P/IPC
g32n02:753687:754005 [4] NCCL INFO Connected all trees
g32n02:753687:754005 [4] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g32n02:753687:754005 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g32n02:753687:754005 [4] NCCL INFO comm 0x16122c1f0 rank 22 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x864be1a757d9f276 - Init COMPLETE
g32n02:753687:754020 [4] NCCL INFO Using network IB
g32n02:753687:754020 [4] NCCL INFO comm 0x16236c810 rank 11 nranks 12g32n02:753688:753688 [5] NCCL INFO cudaDriverVersion 12020
g32n02:753688:753688 [5] NCCL INFO Bootstrap : Using ib0:10.41.17.247<0>
g32n02:753688:753688 [5] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g32n02:753688:753688 [5] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g32n02:753688:753916 [5] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.247<0>
g32n02:753688:753916 [5] NCCL INFO Using network IB
g32n02:753688:753916 [5] NCCL INFO comm 0x1676f39d0 rank 89 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x28395441218594c5 - Init START
g32n02:753688:753916 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g32n02:753688:753916 [5] NCCL INFO Trees [0] -1/-1/-1->89->88 [1] 85/-1/-1->89->87 [2] -1/-1/-1->89->88 [3] 85/-1/-1->89->87
g32n02:753688:753916 [5] NCCL INFO P2P Chunksize set to 131072
g32n02:753688:753916 [5] NCCL INFO Channel 00/0 : 89[5] -> 90[0] [send] via NET/IB/1
g32n02:753688:753916 [5] NCCL INFO Channel 02/0 : 89[5] -> 90[0] [send] via NET/IB/1
g32n02:753688:753916 [5] NCCL INFO Channel 01/0 : 89[5] -> 86[2] via P2P/IPC
g32n02:753688:753916 [5] NCCL INFO Channel 03/0 : 89[5] -> 86[2] via P2P/IPC
g32n02:753688:753916 [5] NCCL INFO Connected all rings
g32n02:753688:753916 [5] NCCL INFO Channel 01/0 : 89[5] -> 85[1] via P2P/IPC
g32n02:753688:753916 [5] NCCL INFO Channel 03/0 : 89[5] -> 85[1] via P2P/IPC
g32n02:753688:753916 [5] NCCL INFO Channel 01/0 : 89[5] -> 87[3] via P2P/IPC
g32n02:753688:753916 [5] NCCL INFO Channel 03/0 : 89[5] -> 87[3] via P2P/IPC
g32n02:753688:753916 [5] NCCL INFO Channel 00/0 : 89[5] -> 88[4] via P2P/IPC
g32n02:753688:753916 [5] NCCL INFO Channel 02/0 : 89[5] -> 88[4] via P2P/IPC
g32n02:753688:753916 [5] NCCL INFO Connected all trees
g32n02:753688:753916 [5] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g32n02:753688:753916 [5] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g32n02:753688:753916 [5] NCCL INFO comm 0x1676f39d0 rank 89 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x28395441218594c5 - Init COMPLETE
g32n02:753688:754003 [5] NCCL INFO Using network IB
g32n02:753688:754003 [5] NCCL INFO comm 0x16a4e82f0 rank 22 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xeee4c143812ce6e4 - Init START
g32n02:753688:754003 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g32n02:753688:754003 [5] NCCL INFO Trees [0] 20/-1/-1->22->21 [1] -1/-1/-1->22->21
g32n02:753688:754003 [5] NCCL INFO P2P Chunksize set to 131072
g32n02:753688:754003 [5] NCCL INFO Channel 00/0 : 22[5] -> 23[3] [send] via NET/IB/3
g32n02:753688:754003 [5] NCCL INFO Channel 01/0 : 22[5] -> 23[3] [send] via NET/IB/3
g32n02:753688:754003 [5] NCCL INFO Connected all rings
g32n02:753688:754003 [5] NCCL INFO Channel 00/0 : 20[3] -> 22[5] [receive] via NET/IB/2
g32n02:753688:754003 [5] NCCL INFO Channel 00/0 : 22[5] -> 20[3] [send] via NET/IB/2
g32n02:753688:754003 [5] NCCL INFO Channel 00/0 : 22[5] -> 21[1] via P2P/IPC
g32n02:753688:754003 [5] NCCL INFO Channel 01/0 : 22[5] -> 21[1] via P2P/IPC
g32n02:753688:754003 [5] NCCL INFO Connected all trees
g32n02:753688:754003 [5] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g32n02:753688:754003 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g32n02:753688:754003 [5] NCCL INFO comm 0x16a4e82f0 rank 22 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xeee4c143812ce6e4 - Init COMPLETE
g32n02:753688:754022 [5] NCCL INFO Using network IB
g32n02:753688:754022 [5] NCCL INFO comm 0x1694db2e0 rank 11 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x17fc85f125de53ec - Init START
g32n02:753688:754022 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g32n02:753688:754022 [5] NCCL INFO Trees [0] -1/-1/-1->11->10 [1] 3/-1/-1->11->-1
g32n02:753688:754022 [5] NCCL INFO P2P Chunksize set to 131072
g32n cudaDev 4 nvmlDev 4 busId 3504000 commId 0x76a32220a008087c - Init START
g32n02:753687:754020 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g32n02:753687:754020 [4] NCCL INFO Trees [0] -1/-1/-1->11->10 [1] 3/-1/-1->11->-1
g32n02:753687:754020 [4] NCCL INFO P2P Chunksize set to 131072
g32n02:753687:754020 [4] NCCL INFO Channel 00/0 : 10[2] -> 11[4] [receive] via NET/IB/1
g32n02:753687:754020 [4] NCCL INFO Channel 01/0 : 10[2] -> 11[4] [receive] via NET/IB/1
g32n02:753687:754020 [4] NCCL INFO Channel 00/0 : 11[4] -> 0[0] [send] via NET/IB/1
g32n02:753687:754020 [4] NCCL INFO Channel 01/0 : 11[4] -> 0[0] [send] via NET/IB/1
g32n02:753687:754020 [4] NCCL INFO Connected all rings
g32n02:753687:754020 [4] NCCL INFO Channel 01/0 : 11[4] -> 3[0] [send] via NET/IB/1
g32n02:753687:754020 [4] NCCL INFO Channel 01/0 : 3[0] -> 11[4] [receive] via NET/IB/1
g32n02:753687:754020 [4] NCCL INFO Channel 00/0 : 11[4] -> 10[2] [send] via NET/IB/1
g32n02:753687:754020 [4] NCCL INFO Connected all trees
g32n02:753687:754020 [4] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g32n02:753687:754020 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g32n02:753687:754020 [4] NCCL INFO comm 0x16236c810 rank 11 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x76a32220a008087c - Init COMPLETE
lDev 3 busId 3503000 commId 0x15c93a2ee1bf52db - Init COMPLETE
g32n02:753686:754024 [3] NCCL INFO Using network IB
g32n02:753686:754024 [3] NCCL INFO comm 0x114e6a8e0 rank 10 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x6876d4a326688ade - Init START
g32n02:753686:754024 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g32n02:753686:754024 [3] NCCL INFO Trees [0] 9/11/-1->10->8 [1] -1/-1/-1->10->9
g32n02:753686:754024 [3] NCCL INFO P2P Chunksize set to 131072
g32n02:753686:754024 [3] NCCL INFO Channel 00/0 : 9[1] -> 10[3] [receive] via NET/IB/3
g32n02:753686:754024 [3] NCCL INFO Channel 01/0 : 9[1] -> 10[3] [receive] via NET/IB/3
g32n02:753686:754024 [3] NCCL INFO Channel 00/0 : 10[3] -> 11[5] [send] via NET/IB/3
g32n02:753686:754024 [3] NCCL INFO Channel 01/0 : 10[3] -> 11[5] [send] via NET/IB/3
g32n02:753686:754024 [3] NCCL INFO Connected all rings
g32n02:753686:754024 [3] NCCL INFO Channel 00/0 : 8[5] -> 10[3] [receive] via NET/IB/3
g32n02:753686:754024 [3] NCCL INFO Channel 00/0 : 10[3] -> 8[5] [send] via NET/IB/3
g32n02:753686:754024 [3] NCCL INFO Channel 00/0 : 11[5] -> 10[3] [receive] via NET/IB/3
g32n02:753686:754024 [3] NCCL INFO Channel 00/0 : 10[3] -> 9[1] [send] via NET/IB/3
g32n02:753686:754024 [3] NCCL INFO Channel 01/0 : 10[3] -> 9[1] [send] via NET/IB/3
g32n02:753686:754024 [3] NCCL INFO Connected all trees
g32n02:753686:754024 [3] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g32n02:753686:754024 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g32n02:753686:754024 [3] NCCL INFO comm 0x114e6a8e0 rank 10 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x6876d4a326688ade - Init COMPLETE
02:753688:754022 [5] NCCL INFO Channel 00/0 : 10[3] -> 11[5] [receive] via NET/IB/3
g32n02:753688:754022 [5] NCCL INFO Channel 01/0 : 10[3] -> 11[5] [receive] via NET/IB/3
g32n02:753688:754022 [5] NCCL INFO Channel 00/0 : 11[5] -> 0[1] [send] via NET/IB/3
g32n02:753688:754022 [5] NCCL INFO Channel 01/0 : 11[5] -> 0[1] [send] via NET/IB/3
g32n02:753688:754022 [5] NCCL INFO Connected all rings
g32n02:753688:754022 [5] NCCL INFO Channel 01/0 : 11[5] -> 3[1] [send] via NET/IB/3
g32n02:753688:754022 [5] NCCL INFO Channel 01/0 : 3[1] -> 11[5] [receive] via NET/IB/3
g32n02:753688:754022 [5] NCCL INFO Channel 00/0 : 11[5] -> 10[3] [send] via NET/IB/3
g32n02:753688:754022 [5] NCCL INFO Connected all trees
g32n02:753688:754022 [5] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g32n02:753688:754022 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g32n02:753688:754022 [5] NCCL INFO comm 0x1694db2e0 rank 11 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x17fc85f125de53ec - Init COMPLETE
g26n01:811333:811333 [2] NCCL INFO cudaDriverVersion 12020
g26n01:811333:811333 [2] NCCL INFO Bootstrap : Using ib0:10.41.17.138<0>
g26n01:811333:811333 [2] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g26n01:811333:811333 [2] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g26n01:811333:811567 [2] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.138<0>
g26n01:811333:811567 [2] NCCL INFO Using network IB
g26n01:811333:811567 [2] NCCL INFO comm 0x137e840c0 rank 8 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x28395441218594c5 - Init START
g26n01:811333:811567 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g26n01:811333:811567 [2] NCCL INFO Trees [0] 9/-1/-1->8->7 [1] -1/-1/-1->8->6 [2] 9/-1/-1->8->7 [3] -1/-1/-1->8->6
g26n01:811333:811567 [2] NCCL INFO P2P Chunksize set to 131072
g26n01:811333:811567 [2] NCCL INFO Channel 00/0 : 8[2] -> 9[3] via P2P/IPC
g26n01:811333:811567 [2] NCCL INFO Channel 02/0 : 8[2] -> 9[3] via P2P/IPC
g26n01:811333:811567 [2] NCCL INFO Channel 01/0 : 8[2] -> 7[1] via P2P/IPC
g26n01:811333:811567 [2] NCCL INFO Channel 03/0 : 8[2] -> 7[1] via P2P/IPC
g26n01:811333:811567 [2] NCCL INFO Connected all rings
g26n01:811333:811567 [2] NCCL INFO Channel 01/0 : 8[2] -> 6[0] via P2P/IPC
g26n01:811333:811567 [2] NCCL INFO Channel 03/0 : 8[2] -> 6[0] via P2P/IPC
g26n01:811333:811567 [2] NCCL INFO Channel 00/0 : 8[2] -> 7[1] via P2P/IPC
g26n01:811333:811567 [2] NCCL INFO Channel 02/0 : 8[2] -> 7[1] via P2P/IPC
g26n01:811333:811567 [2] NCCL INFO Connected all trees
g26n01:811333:811567 [2] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g26n01:811333:811567 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g26n01:811333:811567 [2] NCCL INFO comm 0x137e840c0 rank 8 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x28395441218594c5 - Init COMPLETE
g26n01:811333:811648 [2] NCCL INFO Using network IB
g26n01:811333:811648 [2] NCCL INFO comm 0x13a76d630 rank 2 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0x864be1a757d9f276 - Init START
g26n01:811333:811648 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g26n01:811333:811648 [2] NCCL INFO Trees [0] -1/-1/-1->2->4 [1] 3/0/-1->2->5
g26n01:811333:811648 [2] NCCL INFO P2P Chunksize set to 131072
g26n01:811333:811648 [2] NCCL INFO Channel 00/0 : 1[4] -> 2[2] [receive] via NET/IB/0
g26n01:811333:811648 [2] NCCL INFO Channel 01/0 : 1[4] -> 2[2] [receive] via NET/IB/0
g26n01:811333:811648 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[0] [send] via NET/IB/0
g26n01:811333:811648 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[0] [send] via NET/IB/0
g26n01:811333:811648 [2] NCCL INFO Connected all rings
g26n01:811333:811648 [2] NCCL INFO Channel 01/0 : 0[0] -> 2[2] [receive] via NET/IB/0
g26n01:811333:811648 [2] NCCL INFO Channel 00/0 : 2[2] -> 4[4] [send] via NET/IB/0
g26n01:811333:811648 [2] NCCL INFO Channel 01/0 : 2[2] -> 5[2] [send] via NET/IB/0
g26n01:811333:811648 [2] NCCL INFO Channel 01/0 : 5[2] -> 2[2] [receive] via NET/IB/0
g26n01:811333:811648 [2] NCCL INFO Channel 00/0 : 4[4] -> 2[2] [receive] via NET/IB/0
g26n01:811333:811648 [2] NCCL INFO Channel 01/0 : 2[2] -> 0[0] [send] via NET/IB/0
g26n01:811333:811648 [2] NCCL INFO Channel 01/0 : 3[0] -> 2[2] [receive] via NET/IB/0
g26n01:811333:811648 [2] NCCL INFO Connected all trees
g26n01:811333:811648 [2] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g26n01:811333:811648 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g26n01:811333:811648 [2] NCCL INFO comm 0x13a76d630 rank 2 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0x864be1a757d9f276 - Init COMPLETE
g26n01:811333:811661 [2] NCCL INFO Using network IB
g26n01:811333:811661 [2] NCCL INFO comm 0x13abe7670 rank 1 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0x76a32220a008087c - Init START
g26n01:811333:811661 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g2g26n01:811332:811663 [1] NCCL INFO Using network IB
g26n01:811332:811663 [1] NCCL INFO comm 0x141955390 rank 0 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x6876d4a326688ade - Init START
g26n01:811332:811663 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g26n01:811332:811663 [1] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7   8   9  10  11
g26n01:811332:811663 [1] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7   8   9  10  11
g26n01:811332:811663 [1] NCCL INFO Trees [0] 8/-1/-1->0->-1 [1] -1/-1/-1->0->1
g26n01:811332:811663 [1] NCCL INFO P2P Chunksize set to 131072
g26n01:811332:811663 [1] NCCL INFO Channel 00/0 : 11[5] -> 0[1] [receive] via NET/IB/2
g26n01:811332:811663 [1] NCCL INFO Channel 01/0 : 11[5] -> 0[1] [receive] via NET/IB/2
g26n01:811332:811663 [1] NCCL INFO Channel 00/0 : 0[1] -> 1[3] [send] via NET/IB/2
g26n01:811332:811663 [1] NCCL INFO Channel 01/0 : 0[1] -> 1[3] [send] via NET/IB/2
g26n01:811332:811663 [1] NCCL INFO Connected all rings
g26n01:811332:811663 [1] NCCL INFO Channel 00/0 : 8[5] -> 0[1] [receive] via NET/IB/2
g26n01:811332:811663 [1] NCCL INFO Channel 00/0 : 0[1] -> 8[5] [send] via NET/IB/2
g26n01:811332:811663 [1] NCCL INFO Channel 01/0 : 1[3] -> 0[1] [receive] via NET/IB/2
g26n01:811332:811663 [1] NCCL INFO Connected all trees
g26n01:811332:811663 [1] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g26n01:811332:811663 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g26n01:811332:811663 [1] NCCL INFO comm 0x141955390 rank 0 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x6876d4a326688ade - Init COMPLETE
6n01:811333:811661 [2] NCCL INFO Trees [0] -1/-1/-1->1->2 [1] 2/0/-1->1->3
g26n01:811333:811661 [2] NCCL INFO P2P Chunksize set to 131072
g26n01:811333:811661 [2] NCCL INFO Channel 00/0 : 0[0] -> 1[2] [receive] via NET/IB/0
g26n01:811333:811661 [2] NCCL INFO Channel 01/0 : 0[0] -> 1[2] [receive] via NET/IB/0
g26n01:811333:811661 [2] NCCL INFO Channel 00/0 : 1[2] -> 2[4] [send] via NET/IB/0
g26n01:811333:811661 [2] NCCL INFO Channel 01/0 : 1[2] -> 2[4] [send] via NET/IB/0
g26n01:811333:811661 [2] NCCL INFO Connected all rings
g26n01:811333:811661 [2] NCCL INFO Channel 01/0 : 1[2] -> 3[0] [send] via NET/IB/0
g26n01:811333:811661 [2] NCCL INFO Channel 01/0 : 3[0] -> 1[2] [receive] via NET/IB/0
g26n01:811333:811661 [2] NCCL INFO Channel 00/0 : 2[4] -> 1[2] [receive] via NET/IB/0
g26n01:811333:811661 [2] NCCL INFO Channel 01/0 : 2[4] -> 1[2] [receive] via NET/IB/0
g26n01:811333:811661 [2] NCCL INFO Channel 01/0 : 1[2] -> 0[0] [send] via NET/IB/0
g26n01:811333:811661 [2] NCCL INFO Connected all trees
g26n01:811333:811661 [2] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g26n01:811333:811661 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g26n01:811333:811661 [2] NCCL INFO comm 0x13abe7670 rank 1 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0x76a32220a008087c - Init COMPLETE
g31n01:617761:617761 [5] NCCL INFO cudaDriverVersion 12020
g31n01:617761:617761 [5] NCCL INFO Bootstrap : Using ib0:10.41.17.228<0>
g31n01:617761:617761 [5] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g31n01:617761:617761 [5] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g31n01:617761:617997 [5] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.228<0>
g31n01:617761:617997 [5] NCCL INFO Using network IB
g31n01:617761:617997 [5] NCCL INFO comm 0x14f8a4510 rank 59 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x28395441218594c5 - Init START
g31n01:617761:617997 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g31n01:617761:617997 [5] NCCL INFO Trees [0] -1/-1/-1->59->58 [1] 55/-1/-1->59->57 [2] -1/-1/-1->59->58 [3] 55/-1/-1->59->57
g31n01:617761:617997 [5] NCCL INFO P2P Chunksize set to 131072
g31n01:617761:617997 [5] NCCL INFO Channel 00/0 : 59[5] -> 60[0] [send] via NET/IB/1
g31n01:617761:617997 [5] NCCL INFO Channel 02/0 : 59[5] -> 60[0] [send] via NET/IB/1
g31n01:617761:617997 [5] NCCL INFO Channel 01/0 : 59[5] -> 56[2] via P2P/IPC
g31n01:617761:617997 [5] NCCL INFO Channel 03/0 : 59[5] -> 56[2] via P2P/IPC
g31n01:617761:617997 [5] NCCL INFO Connected all rings
g31n01:617761:617997 [5] NCCL INFO Channel 01/0 : 59[5] -> 55[1] via P2P/IPC
g31n01:617761:617997 [5] NCCL INFO Channel 03/0 : 59[5] -> 55[1] via P2P/IPC
g31n01:617761:617997 [5] NCCL INFO Channel 01/0 : 59[5] -> 57[3] via P2P/IPC
g31n01:617761:617997 [5] NCCL INFO Channel 03/0 : 59[5] -> 57[3] via P2P/IPC
g31n01:617761:617997 [5] NCCL INFO Channel 00/0 : 59[5] -> 58[4] via P2P/IPC
g31n01:617761:617997 [5] NCCL INFO Channel 02/0 : 59[5] -> 58[4] via P2P/IPC
g31n01:617761:617997 [5] NCCL INFO Connected all trees
g31n01:617761:617997 [5] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g31n01:617761:617997 [5] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g31n01:617761:617997 [5] NCCL INFO comm 0x14f8a4510 rank 59 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x28395441218594c5 - Init COMPLETE
g31n01:617761:618079 [5] NCCL INFO Using network IB
g31n01:617761:618079 [5] NCCL INFO comm 0x152167d40 rank 14 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x15c93a2ee1bf52db - Init START
g31n01:617761:618079 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g31n01:617761:618079 [5] NCCL INFO Trees [0] -1/-1/-1->14->13 [1] 15/-1/-1->14->13
g31n01:617761:618079 [5] NCCL INFO P2P Chunksize set to 131072
g31n01:617761:618079 [5] NCCL INFO Channel 00/0 : 14[5] -> 15[3] [send] via NET/IB/3
g31n01:617761:618079 [5] NCCL INFO Channel 01/0 : 14[5] -> 15[3] [send] via NET/IB/3
g31n01:617761:618079 [5] NCCL INFO Connected all rings
g31n01:617761:618079 [5] NCCL INFO Channel 01/0 : 15[3] -> 14[5] [receive] via NET/IB/2
g31n01:617761:618079 [5] NCCL INFO Channel 00/0 : 14[5] -> 13[1] via P2P/IPC
g31n01:617761:618079 [5] NCCL INFO Channel 01/0 : 14[5] -> 13[1] via P2P/IPC
g31n01:617761:618079 [5] NCCL INFO Connected all trees
g31n01:617761:618079 [5] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g31n01:617761:618079 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n01:617761:618079 [5] NCCL INFO comm 0x152167d40 rank 14 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x15c93a2ee1bf52db - Init COMPLETE
g31n01:617761:618098 [5] NCCL INFO Using network IB
g31n01:617761:618098 [5] NCCL INFO comm 0x151cb6140 rank 7 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x1542a3b966c3490b - Init START
g31n01:617761:618098 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g31n01:617761:618098 [5] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] 9/5/-1->7->3
g31n01:617761:618098 [5] NCCL INFO P2P Chunksize set to 131072
g31n01:617761:618098 [5] NCCL INFO Channel 00/0 : 6[3] -> 7[5] [receive] via NET/IB/3
g31n01:61g31n01:617759:617759 [3] NCCL INFO cudaDriverVersion 12020
g31n01:617759:617759 [3] NCCL INFO Bootstrap : Using ib0:10.41.17.228<0>
g31n01:617759:617759 [3] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g31n01:617759:617759 [3] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g31n01:617759:617994 [3] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.228<0>
g31n01:617759:617994 [3] NCCL INFO Using network IB
g31n01:617759:617994 [3] NCCL INFO comm 0x13d834250 rank 57 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x28395441218594c5 - Init START
g31n01:617759:617994 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g31n01:617759:617994 [3] NCCL INFO Trees [0] 58/-1/-1->57->56 [1] 59/-1/-1->57->58 [2] 58/-1/-1->57->56 [3] 59/64/-1->57->58
g31n01:617759:617994 [3] NCCL INFO P2P Chunksize set to 131072
g31n01:617759:617994 [3] NCCL INFO Channel 00/0 : 57[3] -> 58[4] via P2P/IPC
g31n01:617759:617994 [3] NCCL INFO Channel 01/0 : 57[3] -> 58[4] via P2P/IPC
g31n01:617759:617994 [3] NCCL INFO Channel 02/0 : 57[3] -> 58[4] via P2P/IPC
g31n01:617759:617994 [3] NCCL INFO Channel 03/0 : 57[3] -> 58[4] via P2P/IPC
g31n01:617759:617994 [3] NCCL INFO Channel 01/0 : 48[0] -> 57[3] [receive] via NET/IB/3
g31n01:617759:617994 [3] NCCL INFO Channel 03/0 : 48[0] -> 57[3] [receive] via NET/IB/3
g31n01:617759:617994 [3] NCCL INFO Connected all rings
g31n01:617759:617994 [3] NCCL INFO Channel 01/0 : 57[3] -> 59[5] via P2P/IPC
g31n01:617759:617994 [3] NCCL INFO Channel 03/0 : 57[3] -> 59[5] via P2P/IPC
g31n01:617759:617994 [3] NCCL INFO Channel 03/0 : 57[3] -> 64[4] [send] via NET/IB/3
g31n01:617759:617994 [3] NCCL INFO Channel 03/0 : 64[4] -> 57[3] [receive] via NET/IB/3
g31n01:617759:617994 [3] NCCL INFO Channel 00/0 : 57[3] -> 56[2] via P2P/IPC
g31n01:617759:617994 [3] NCCL INFO Channel 02/0 : 57[3] -> 56[2] via P2P/IPC
g31n01:617759:617994 [3] NCCL INFO Connected all trees
g31n01:617759:617994 [3] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g31n01:617759:617994 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g31n01:617759:617994 [3] NCCL INFO comm 0x13d834250 rank 57 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x28395441218594c5 - Init COMPLETE
g31n01:617759:618078 [3] NCCL INFO Using network IB
g31n01:617759:618078 [3] NCCL INFO comm 0x14014dfc0 rank 14 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xeee4c143812ce6e4 - Init START
g31n01:617759:618078 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g31n01:617759:618078 [3] NCCL INFO Trees [0] -1/-1/-1->14->16 [1] 15/12/-1->14->17
g31n01:617759:618078 [3] NCCL INFO P2P Chunksize set to 131072
g31n01:617759:618078 [3] NCCL INFO Channel 00/0 : 13[5] -> 14[3] [receive] via NET/IB/3
g31n01:617759:618078 [3] NCCL INFO Channel 01/0 : 13[5] -> 14[3] [receive] via NET/IB/3
g31n01:617759:618078 [3] NCCL INFO Channel 00/0 : 14[3] -> 15[1] [send] via NET/IB/3
g31n01:617759:618078 [3] NCCL INFO Channel 01/0 : 14[3] -> 15[1] [send] via NET/IB/3
g31n01:617759:618078 [3] NCCL INFO Connected all rings
g31n01:617759:618078 [3] NCCL INFO Channel 01/0 : 12[1] -> 14[3] [receive] via NET/IB/3
g31n01:617759:618078 [3] NCCL INFO Channel 00/0 : 14[3] -> 16[5] [send] via NET/IB/3
g31n01:617759:618078 [3] NCCL INFO Channel 01/0 : 14[3] -> 17[3] [send] via NET/IB/3
g31n01:617759:618078 [3] NCCL INFO Channel 01/0 : 17[3] -> 14[3] [receive] via NET/IB/3
g31n01:617759:618078 [3] NCCL INFO Channel 00/0 : 16[5] -> 14[3] [receive] via NET/IB/3
g31n01:617759:618078 [3] NCCL INFO Channel 01/0 : 14[3] -> 12[1] [send] via NET/IB/3
g31n01:617759:618078 [3] NCCL INFO Channel 01/0 : 15[1] -> 14[3] [receive] via NET/IB/3
g31n01:617759:618078 [3] NCCL INFO Connected all trees
g31n01:617759:618078 [3] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g31n01:617759:618078 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n01:617759:618078 [3] NCCL INFO comm 0x14014dfc0 rank 14 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xeee4c143812ce6e4 - Init COMPLETE
g31n01:617759:618096 [3] NCCL INFO Using network IB
g31n01:617759:618096 [3] NCCL INFO comm 0x140726f80 rank 7 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x17fc85f125de53ec - Init START
g31n01:617759:618096 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g31n01:617759:618096 [3] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] 9/5/-1->7->3
g31n01:617759:618096 [3] NCCL INFO P2P Chunksize set to 131072
g31n01:617759:618096 [3] NCCL INFO Channel 00/0 : 6[1] -> 7[3] [receive] via NET/IB/3
g31n01:617759:618096 [3] NCCL INFO Channel 01/0 : 6[1] -> 7[3] [receive] via NET/IB/3
g31n01:617759:618096 [3] NCCL INFO Channel 00/0 : 7[3] -> 8[5] [send] via NET/IB/3
g31n01:617759:618096 [3] NCCL INFO Channel 01/0 : 7[3] -> 8[5] [send] via NET/IB/3
g31n01:617759:618096 [3] NCCL INFO Connected all rings
g31n01:617759:618096 [3] NCCL INFO Channel 01/0 : 5[5] -> 7[3] [receive] via NET/IB/3
g31n01:617759:618096 [3] NCCL INFO Channel 01/0 : 7[3] -> 9[1] [send] via NET/IB/3
g31n01:617759:618096 [3] NCCL INFO Channel 01/0 : 3[1] -> 7[3] [receive] via NET/IB/3
g31n01:617759:618096 [3] NCCL INFO Channel 01/0 : 7[3] -> 3[1] [send] via NET/IB/3
g31n01:617759:618096 [3] NCCL INFO Channel 01/0 : 9[1] -> 7[3] [receive] via NET/IB/3
g31n01:617759:618096 [3] NCCL INFO Channel 01/0 : 7[3] -> 5[5] [send] via NET/IB/3
g31n01:617759:618096 [3] NCCL INFO Channel 00/0 : 7[3] -> 6[1] [send] via NET/IB/3
g31n01:617759:618096 [3] NCCL INFO Connected all trees
g31n01:617759:618096 [3] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g31n01:617759:618096 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n01:617759:618096 [3] NCCL INFO comm 0x140726f80 rank 7 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x17fc85f125de53ec - Init COMPLETE
7761:618098 [5] NCCL INFO Channel 01/0 : 6[3] -> 7[5] [receive] via NET/IB/3
g31n01:617761:618098 [5] NCCL INFO Channel 00/0 : 7[5] -> 8[1] [send] via NET/IB/3
g31n01:617761:618098 [5] NCCL INFO Channel 01/0 : 7[5] -> 8[1] [send] via NET/IB/3
g31n01:617761:618098 [5] NCCL INFO Connected all rings
g31n01:617761:618098 [5] NCCL INFO Channel 01/0 : 5[1] -> 7[5] [receive] via NET/IB/3
g31n01:617761:618098 [5] NCCL INFO Channel 01/0 : 7[5] -> 9[3] [send] via NET/IB/3
g31n01:617761:618098 [5] NCCL INFO Channel 01/0 : 3[3] -> 7[5] [receive] via NET/IB/3
g31n01:617761:618098 [5] NCCL INFO Channel 01/0 : 7[5] -> 3[3] [send] via NET/IB/3
g31n01:617761:618098 [5] NCCL INFO Channel 01/0 : 9[3] -> 7[5] [receive] via NET/IB/3
g31n01:617761:618098 [5] NCCL INFO Channel 01/0 : 7[5] -> 5[1] [send] via NET/IB/3
g31n01:617761:618098 [5] NCCL INFO Channel 00/0 : 7[5] -> 6[3] [send] via NET/IB/3
g31n01:617761:618098 [5] NCCL INFO Connected all trees
g31n01:617761:618098 [5] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g31n01:617761:618098 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n01:617761:618098 [5] NCCL INFO comm 0x151cb6140 rank 7 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x1542a3b966c3490b - Init COMPLETE
g31n03:688514:688514 [5] NCCL INFO cudaDriverVersion 12020
g31n03:688514:688514 [5] NCCL INFO Bootstrap : Using ib0:10.41.17.230<0>
g31n03:688514:688514 [5] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g31n03:688514:688514 [5] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g31n03:688514:688744 [5] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.230<0>
g31n03:688514:688744 [5] NCCL INFO Using network IB
g31n03:688514:688744 [5] NCCL INFO comm 0x176b13940 rank 71 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x28395441218594c5 - Init START
g31n03:688514:688744 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g31n03:688514:688744 [5] NCCL INFO Trees [0] -1/-1/-1->71->70 [1] 67/-1/-1->71->69 [2] -1/-1/-1->71->70 [3] 67/-1/-1->71->69
g31n03:688514:688744 [5] NCCL INFO P2P Chunksize set to 131072
g31n03:688514:688744 [5] NCCL INFO Channel 00/0 : 71[5] -> 72[0] [send] via NET/IB/1
g31n03:688514:688744 [5] NCCL INFO Channel 02/0 : 71[5] -> 72[0] [send] via NET/IB/1
g31n03:688514:688744 [5] NCCL INFO Channel 01/0 : 71[5] -> 68[2] via P2P/IPC
g31n03:688514:688744 [5] NCCL INFO Channel 03/0 : 71[5] -> 68[2] via P2P/IPC
g31n03:688514:688744 [5] NCCL INFO Connected all rings
g31n03:688514:688744 [5] NCCL INFO Channel 01/0 : 71[5] -> 67[1] via P2P/IPC
g31n03:688514:688744 [5] NCCL INFO Channel 03/0 : 71[5] -> 67[1] via P2P/IPC
g31n03:688514:688744 [5] NCCL INFO Channel 01/0 : 71[5] -> 69[3] via P2P/IPC
g31n03:688514:688744 [5] NCCL INFO Channel 03/0 : 71[5] -> 69[3] via P2P/IPC
g31n03:688514:688744 [5] NCCL INFO Channel 00/0 : 71[5] -> 70[4] via P2P/IPC
g31n03:688514:688744 [5] NCCL INFO Channel 02/0 : 71[5] -> 70[4] via P2P/IPC
g31n03:688514:688744 [5] NCCL INFO Connected all trees
g31n03:688514:688744 [5] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g31n03:688514:688744 [5] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g31n03:688514:688744 [5] NCCL INFO comm 0x176b13940 rank 71 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x28395441218594c5 - Init COMPLETE
g31n03:688514:688827 [5] NCCL INFO Using network IB
g31n03:688514:688827 [5] NCCL INFO comm 0x179788b00 rank 17 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x15c93a2ee1bf52db - Init START
g31n03:688514:688827 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g31n03:688514:688827 [5] NCCL INFO Trees [0] -1/-1/-1->17->16 [1] 19/-1/-1->17->16
g31n03:688514:688827 [5] NCCL INFO P2P Chunksize set to 131072
g31n03:688514:688827 [5] NCCL INFO Channel 00/0 : 17[5] -> 18[3] [send] via NET/IB/3
g31n03:688514:688827 [5] NCCL INFO Channel 01/0 : 17[5] -> 18[3] [send] via NET/IB/3
g31n03:688514:688827 [5] NCCL INFO Connected all rings
g31n03:688514:688827 [5] NCCL INFO Channel 01/0 : 17[5] -> 19[1] [send] via NET/IB/2
g31n03:688514:688827 [5] NCCL INFO Channel 01/0 : 19[1] -> 17[5] [receive] via NET/IB/2
g31n03:688514:688827 [5] NCCL INFO Channel 00/0 : 17[5] -> 16[1] via P2P/IPC
g31n03:688514:688827 [5] NCCL INFO Channel 01/0 : 17[5] -> 16[1] via P2P/IPC
g31n03:688514:688827 [5] NCCL INFO Connected all trees
g31n03:688514:688827 [5] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g31n03:688514:688827 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n03:688514:688827 [5] NCCL INFO comm 0x179788b00 rank 17 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x15c93a2ee1bf52db - Init COMPLETE
g31n03:688514:688844 [5] NCCL INFO Using network IB
g31n03:688514:688844 [5] NCCL INFO comm 0x179827310 rank 8 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x6876d4a326688ade - Init START
g31n03:688514:688844 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g31n03:688514:688844 [5] NCCL INFO Trees [0] 4/10/-1->8->0 [1] -1/-1/-1->8->9
g31n03:688514:688844 [5] NCCL INFO P2P Chunksize set to 131072
g31n03:68g31n03:688512:688512 [3] NCCL INFO cudaDriverVersion 12020
g31n03:688512:688512 [3] NCCL INFO Bootstrap : Using ib0:10.41.17.230<0>
g31n03:688512:688512 [3] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g31n03:688512:688512 [3] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g31n03:688512:688746 [3] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.230<0>
g31n03:688512:688746 [3] NCCL INFO Using network IB
g31n03:688512:688746 [3] NCCL INFO comm 0x172db45d0 rank 69 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x28395441218594c5 - Init START
g31n03:688512:688746 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g31n03:688512:688746 [3] NCCL INFO Trees [0] 70/-1/-1->69->68 [1] 71/-1/-1->69->70 [2] 70/-1/-1->69->68 [3] 71/82/-1->69->70
g31n03:688512:688746 [3] NCCL INFO P2P Chunksize set to 131072
g31n03:688512:688746 [3] NCCL INFO Channel 00/0 : 69[3] -> 70[4] via P2P/IPC
g31n03:688512:688746 [3] NCCL INFO Channel 01/0 : 69[3] -> 70[4] via P2P/IPC
g31n03:688512:688746 [3] NCCL INFO Channel 02/0 : 69[3] -> 70[4] via P2P/IPC
g31n03:688512:688746 [3] NCCL INFO Channel 03/0 : 69[3] -> 70[4] via P2P/IPC
g31n03:688512:688746 [3] NCCL INFO Channel 01/0 : 60[0] -> 69[3] [receive] via NET/IB/3
g31n03:688512:688746 [3] NCCL INFO Channel 03/0 : 60[0] -> 69[3] [receive] via NET/IB/3
g31n03:688512:688746 [3] NCCL INFO Connected all rings
g31n03:688512:688746 [3] NCCL INFO Channel 01/0 : 69[3] -> 71[5] via P2P/IPC
g31n03:688512:688746 [3] NCCL INFO Channel 03/0 : 69[3] -> 71[5] via P2P/IPC
g31n03:688512:688746 [3] NCCL INFO Channel 03/0 : 69[3] -> 82[4] [send] via NET/IB/3
g31n03:688512:688746 [3] NCCL INFO Channel 03/0 : 82[4] -> 69[3] [receive] via NET/IB/3
g31n03:688512:688746 [3] NCCL INFO Channel 00/0 : 69[3] -> 68[2] via P2P/IPC
g31n03:688512:688746 [3] NCCL INFO Channel 02/0 : 69[3] -> 68[2] via P2P/IPC
g31n03:688512:688746 [3] NCCL INFO Connected all trees
g31n03:688512:688746 [3] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g31n03:688512:688746 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g31n03:688512:688746 [3] NCCL INFO comm 0x172db45d0 rank 69 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x28395441218594c5 - Init COMPLETE
g31n03:688512:688825 [3] NCCL INFO Using network IB
g31n03:688512:688825 [3] NCCL INFO comm 0x175eec0b0 rank 17 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xeee4c143812ce6e4 - Init START
g31n03:688512:688825 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g31n03:688512:688825 [3] NCCL INFO Trees [0] -1/-1/-1->17->15 [1] 20/14/-1->17->11
g31n03:688512:688825 [3] NCCL INFO P2P Chunksize set to 131072
g31n03:688512:688825 [3] NCCL INFO Channel 00/0 : 16[5] -> 17[3] [receive] via NET/IB/3
g31n03:688512:688825 [3] NCCL INFO Channel 01/0 : 16[5] -> 17[3] [receive] via NET/IB/3
g31n03:688512:688825 [3] NCCL INFO Channel 00/0 : 17[3] -> 18[1] [send] via NET/IB/3
g31n03:688512:688825 [3] NCCL INFO Channel 01/0 : 17[3] -> 18[1] [send] via NET/IB/3
g31n03:688512:688825 [3] NCCL INFO Connected all rings
g31n03:688512:688825 [3] NCCL INFO Channel 00/0 : 15[1] -> 17[3] [receive] via NET/IB/3
g31n03:688512:688825 [3] NCCL INFO Channel 01/0 : 14[3] -> 17[3] [receive] via NET/IB/3
g31n03:688512:688825 [3] NCCL INFO Channel 01/0 : 17[3] -> 20[3] [send] via NET/IB/3
g31n03:688512:688825 [3] NCCL INFO Channel 01/0 : 11[3] -> 17[3] [receive] via NET/IB/3
g31n03:688512:688825 [3] NCCL INFO Channel 01/0 : 17[3] -> 11[3] [send] via NET/IB/3
g31n03:688512:688825 [3] NCCL INFO Channel 01/0 : 20[3] -> 17[3] [receive] via NET/IB/3
g31n03:688512:688825 [3] NCCL INFO Channel 01/0 : 17[3] -> 14[3] [send] via NET/IB/3
g31n03:688512:688825 [3] NCCL INFO Channel 00/0 : 17[3] -> 15[1] [send] via NET/IB/3
g31n03:688512:688825 [3] NCCL INFO Connected all trees
g31n03:688512:688825 [3] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g31n03:688512:688825 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n03:688512:688825 [3] NCCL INFO comm 0x175eec0b0 rank 17 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xeee4c143812ce6e4 - Init COMPLETE
g31n03:688512:688847 [3] NCCL INFO Using network IB
g31n03:688512:688847 [3] NCCL INFO comm 0x174c16430 rank 8 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xad5995bf820e7340 - Init START
g31n03:688512:688847 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g31n03:688512:688847 [3] NCCL INFO Trees [0] 4/10/-1->8->0 [1] -1/-1/-1->8->9
g31n03:688512:688847 [3] NCCL INFO P2P Chunksize set to 131072
g31n03:688512:688847 [3] NCCL INFO Channel 00/0 : 7[1] -> 8[3] [receive] via NET/IB/3
g31n03:688512:688847 [3] NCCL INFO Channel 01/0 : 7[1] -> 8[3] [receive] via NET/IB/3
g31n03:688512:688847 [3] NCCL INFO Channel 00/0 : 8[3] -> 9[5] [send] via NET/IB/3
g31n03:688512:688847 [3] NCCL INFO Channel 01/0 : 8[3] -> 9[5] [send] via NET/IB/3
g31n03:688512:688847 [3] NCCL INFO Connected all rings
g31n03:688512:688847 [3] NCCL INFO Channel 00/0 : 8[3] -> 10[1] [send] via NET/IB/3
g31n03:688512:688847 [3] NCCL INFO Channel 00/0 : 4[1] -> 8[3] [receive] via NET/IB/3
g31n03:688512:688847 [3] NCCL INFO Channel 00/0 : 8[3] -> 0[5] [send] via NET/IB/3
g31n03:688512:688847 [3] NCCL INFO Channel 00/0 : 0[5] -> 8[3] [receive] via NET/IB/3
g31n03:688512:688847 [3] NCCL INFO Channel 00/0 : 8[3] -> 4[1] [send] via NET/IB/3
g31n03:688512:688847 [3] NCCL INFO Channel 00/0 : 10[1] -> 8[3] [receive] via NET/IB/3
g31n03:688512:688847 [3] NCCL INFO Channel 01/0 : 9[5] -> 8[3] [receive] via NET/IB/3
g31n03:688512:688847 [3] NCCL INFO Connected all trees
g31n03:688512:688847 [3] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g31n03:688512:688847 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n03:688512:688847 [3] NCCL INFO comm 0x174c16430 rank 8 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xad5995bf820e7340 - Init COMPLETE
8514:688844 [5] NCCL INFO Channel 00/0 : 7[3] -> 8[5] [receive] via NET/IB/3
g31n03:688514:688844 [5] NCCL INFO Channel 01/0 : 7[3] -> 8[5] [receive] via NET/IB/3
g31n03:688514:688844 [5] NCCL INFO Channel 00/0 : 8[5] -> 9[1] [send] via NET/IB/3
g31n03:688514:688844 [5] NCCL INFO Channel 01/0 : 8[5] -> 9[1] [send] via NET/IB/3
g31n03:688514:688844 [5] NCCL INFO Connected all rings
g31n03:688514:688844 [5] NCCL INFO Channel 00/0 : 8[5] -> 10[3] [send] via NET/IB/3
g31n03:688514:688844 [5] NCCL INFO Channel 00/0 : 4[3] -> 8[5] [receive] via NET/IB/3
g31n03:688514:688844 [5] NCCL INFO Channel 00/0 : 8[5] -> 0[1] [send] via NET/IB/3
g31n03:688514:688844 [5] NCCL INFO Channel 00/0 : 0[1] -> 8[5] [receive] via NET/IB/3
g31n03:688514:688844 [5] NCCL INFO Channel 00/0 : 8[5] -> 4[3] [send] via NET/IB/3
g31n03:688514:688844 [5] NCCL INFO Channel 00/0 : 10[3] -> 8[5] [receive] via NET/IB/3
g31n03:688514:688844 [5] NCCL INFO Channel 01/0 : 9[1] -> 8[5] [receive] via NET/IB/3
g31n03:688514:688844 [5] NCCL INFO Connected all trees
g31n03:688514:688844 [5] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g31n03:688514:688844 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n03:688514:688844 [5] NCCL INFO comm 0x179827310 rank 8 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x6876d4a326688ade - Init COMPLETE
g31n05:704348:704348 [4] NCCL INFO cudaDriverVersion 12020
g31n05:704348:704348 [4] NCCL INFO Bootstrap : Using ib0:10.41.17.232<0>
g31n05:704348:704348 [4] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g31n05:704348:704348 [4] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g31n05:704348:704578 [4] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.232<0>
g31n05:704348:704578 [4] NCCL INFO Using network IB
g31n05:704348:704578 [4] NCCL INFO comm 0x127263e50 rank 82 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x28395441218594c5 - Init START
g31n05:704348:704578 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g31n05:704348:704578 [4] NCCL INFO Trees [0] 83/-1/-1->82->81 [1] 81/-1/-1->82->87 [2] 83/-1/-1->82->81 [3] 81/76/-1->82->69
g31n05:704348:704578 [4] NCCL INFO P2P Chunksize set to 131072
g31n05:704348:704578 [4] NCCL INFO Channel 00/0 : 82[4] -> 83[5] via P2P/IPC
g31n05:704348:704578 [4] NCCL INFO Channel 01/0 : 82[4] -> 83[5] via P2P/IPC
g31n05:704348:704578 [4] NCCL INFO Channel 02/0 : 82[4] -> 83[5] via P2P/IPC
g31n05:704348:704578 [4] NCCL INFO Channel 03/0 : 82[4] -> 83[5] via P2P/IPC
g31n05:704348:704578 [4] NCCL INFO Connected all rings
g31n05:704348:704578 [4] NCCL INFO Channel 01/0 : 82[4] -> 87[3] [send] via NET/IB/3
g31n05:704348:704578 [4] NCCL INFO Channel 03/0 : 76[4] -> 82[4] [receive] via NET/IB/3
g31n05:704348:704578 [4] NCCL INFO Channel 03/0 : 69[3] -> 82[4] [receive] via NET/IB/3
g31n05:704348:704578 [4] NCCL INFO Channel 03/0 : 82[4] -> 69[3] [send] via NET/IB/3
g31n05:704348:704578 [4] NCCL INFO Channel 03/0 : 82[4] -> 76[4] [send] via NET/IB/3
g31n05:704348:704578 [4] NCCL INFO Channel 01/0 : 87[3] -> 82[4] [receive] via NET/IB/3
g31n05:704348:704578 [4] NCCL INFO Channel 00/0 : 82[4] -> 81[3] via P2P/IPC
g31n05:704348:704578 [4] NCCL INFO Channel 01/0 : 82[4] -> 81[3] via P2P/IPC
g31n05:704348:704578 [4] NCCL INFO Channel 02/0 : 82[4] -> 81[3] via P2P/IPC
g31n05:704348:704578 [4] NCCL INFO Channel 03/0 : 82[4] -> 81[3] via P2P/IPC
g31n05:704348:704578 [4] NCCL INFO Connected all trees
g31n05:704348:704578 [4] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g31n05:704348:704578 [4] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g31n05:704348:704578 [4] NCCL INFO comm 0x127263e50 rank 82 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x28395441218594c5 - Init COMPLETE
g31n05:704348:704659 [4] NCCL INFO Using network IB
g31n05:704348:704659 [4] NCCL INFO comm 0x129adba30 rank 20 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xe45164e09488020a - Init START
g31n05:704348:704659 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g31n05:704348:704659 [4] NCCL INFO Trees [0] -1/-1/-1->20->19 [1] 21/-1/-1->20->19
g31n05:704348:704659 [4] NCCL INFO P2P Chunksize set to 131072
g31n05:704348:704659 [4] NCCL INFO Channel 00/0 : 20[4] -> 21[2] [send] via NET/IB/1
g31n05:704348:704659 [4] NCCL INFO Channel 01/0 : 20[4] -> 21[2] [send] via NET/IB/1
g31n05:704348:704659 [4] NCCL INFO Connected all rings
g31n05:704348:704659 [4] NCCL INFO Channel 01/0 : 21[2] -> 20[4] [receive] via NET/IB/0
g31n05:704348:704659 [4] NCCL INFO Channel 00/0 : 20[4] -> 19[0] via P2P/IPC
g31n05:704348:704659 [4] NCCL INFO Channel 01/0 : 20[4] -> 19[0] via P2P/IPC
g31n05:704348:704659 [4] NCCL INFO Connected all trees
g31n05:704348:704659 [4] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g31n05:704348:704659 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n05:704348:704659 [4] NCCL INFO comm 0x129adba30 rank 20 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xe45164e09488020a - Init COMPLETE
g31n05:704348:704680 [4] NCCL INFO Using network IB
g31n05:704348:704680 [4] NCCL INFO comm 0x12af27470 rank 10 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xaca7b5a6b2aa6a19 - Init START
g31n05:704348:704680 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g31n05:704348:704680 [4] NCCL INFO Trees [0] 9/11/-1->10->8 [1] -1/-1/-1->10->9
g31n05:704348:704680 [4] NCCL INFO P2P Chunksize set to 131072
g31n05:704348:704680 [4] NCCL INFO Channel 00/0 : 9[2] -> 10[4] [receive] via NET/IB/1
g31n05:704348:704680 [4] NCCL INFO Channel 01/0 : 9[2] -> 10[4] [receive] via NET/IB/1
g31n05:704348:704680 [4] NCCL INFO Channel 00/0 : 10[4] -> 11[0] [send] via NET/IB/1
g31n05:704348:704680 [4] NCCL INFO Channel 01/0 : 10[4] -> 11[0] [send] via NET/IB/1
g31n05:704348:704680 [4] NCCL INFO Connected all rings
g31n05:704348:704680 [4] NCCL INFO Channel 00/0 : 8[0] -> 10[4] [receive] via NET/IB/1
g31n05:704348:704680 [4] NCCL INFO Channel 00/0 : 10[4] -> 8[0] [send] via NET/IB/1
g31n05:704348:704680 [4] NCCL INFO Channel 00/0 : 11[0] -> 10[4] [receive] via NET/IB/1
g31n05:704348:704680 [4] NCCL INFO Channel 00/0 : 10[4] -> 9[2] [send] via NET/IB/1
g31n05:704348:704680 [4] NCCL INFO Channel 01/0 : 10[4] -> 9[2] [send] via NET/IB/1
g31n05:704348:704680 [4] NCCL INFO Connected all trees
g31n05:704348:704680 [4] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g31n05:704348:704680 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n05:704348:704680 [4] NCCL INFO comm 0x12af27470 rank 10 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xaca7b5a6b2aa6a19 - Init COMPLETE
g28n01:770099:770099 [4] NCCL INFO cudaDriverVersion 12020
g28n01:770099:770099 [4] NCCL INFO Bootstrap : Using ib0:10.41.17.174<0>
g28n01:770099:770099 [4] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g28n01:770099:770099 [4] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g28n01:770099:770323 [4] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.174<0>
g28n01:770099:770323 [4] NCCL INFO Using network IB
g28n01:770099:770323 [4] NCCL INFO comm 0x13ca44970 rank 34 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x28395441218594c5 - Init START
g28n01:770099:770323 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g28n01:770099:770323 [4] NCCL INFO Trees [0] 35/-1/-1->34->33 [1] 33/-1/-1->34->39 [2] 35/-1/-1->34->33 [3] 33/28/-1->34->21
g28n01:770099:770323 [4] NCCL INFO P2P Chunksize set to 131072
g28n01:770099:770323 [4] NCCL INFO Channel 00/0 : 34[4] -> 35[5] via P2P/IPC
g28n01:770099:770323 [4] NCCL INFO Channel 01/0 : 34[4] -> 35[5] via P2P/IPC
g28n01:770099:770323 [4] NCCL INFO Channel 02/0 : 34[4] -> 35[5] via P2P/IPC
g28n01:770099:770323 [4] NCCL INFO Channel 03/0 : 34[4] -> 35[5] via P2P/IPC
g28n01:770099:770323 [4] NCCL INFO Connected all rings
g28n01:770099:770323 [4] NCCL INFO Channel 01/0 : 34[4] -> 39[3] [send] via NET/IB/3
g28n01:770099:770323 [4] NCCL INFO Channel 03/0 : 28[4] -> 34[4] [receive] via NET/IB/3
g28n01:770099:770323 [4] NCCL INFO Channel 03/0 : 21[3] -> 34[4] [receive] via NET/IB/3
g28n01:770099:770323 [4] NCCL INFO Channel 03/0 : 34[4] -> 21[3] [send] via NET/IB/3
g28n01:770099:770323 [4] NCCL INFO Channel 03/0 : 34[4] -> 28[4] [send] via NET/IB/3
g28n01:770099:770323 [4] NCCL INFO Channel 01/0 : 39[3] -> 34[4] [receive] via NET/IB/3
g28n01:770099:770323 [4] NCCL INFO Channel 00/0 : 34[4] -> 33[3] via P2P/IPC
g28n01:770099:770323 [4] NCCL INFO Channel 01/0 : 34[4] -> 33[3] via P2P/IPC
g28n01:770099:770323 [4] NCCL INFO Channel 02/0 : 34[4] -> 33[3] via P2P/IPC
g28n01:770099:770323 [4] NCCL INFO Channel 03/0 : 34[4] -> 33[3] via P2P/IPC
g28n01:770099:770323 [4] NCCL INFO Connected all trees
g28n01:770099:770323 [4] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g28n01:770099:770323 [4] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g28n01:770099:770323 [4] NCCL INFO comm 0x13ca44970 rank 34 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x28395441218594c5 - Init COMPLETE
g28n01:770099:770411 [4] NCCL INFO Using network IB
g28n01:770099:770411 [4] NCCL INFO comm 0x13f355700 rank 8 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xe45164e09488020a - Init START
g28n01:770099:770411 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g28n01:770099:770411 [4] NCCL INFO Trees [0] -1/-1/-1->8->7 [1] 9/-1/-1->8->7
g28n01:770099:770411 [4] NCCL INFO P2P Chunksize set to 131072
g28n01:770099:770411 [4] NCCL INFO Channel 00/0 : 8[4] -> 9[2] [send] via NET/IB/1
g28n01:770099:770411 [4] NCCL INFO Channel 01/0 : 8[4] -> 9[2] [send] via NET/IB/1
g28n01:770099:770411 [4] NCCL INFO Connected all rings
g28n01:770099:770411 [4] NCCL INFO Channel 01/0 : 9[2] -> 8[4] [receive] via NET/IB/0
g28n01:770099:770411 [4] NCCL INFO Channel 00/0 : 8[4] -> 7[0] via P2P/IPC
g28n01:770099:770411 [4] NCCL INFO Channel 01/0 : 8[4] -> 7[0] via P2P/IPC
g28n01:770099:770411 [4] NCCL INFO Connected all trees
g28n01:770099:770411 [4] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g28n01:770099:770411 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n01:770099:770411 [4] NCCL INFO comm 0x13f355700 rank 8 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xe45164e09488020a - Init COMPLETE
g28n01:770099:770429 [4] NCCL INFO Using network IB
g28n01:770099:770429 [4] NCCL INFO comm 0x13e8ec970 rank 4 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xaca7b5a6b2aa6a19 - Init START
g28n01:770099:770429 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g28n01:770099:770429 [4] NCCL INFO Trees [0] 2/6/-1->4->8 [1] -1/-1/-1->4->5
g28n01:770099:770429 [4] NCCL INFO P2P Chunksize set to 131072
g28n01:770099:770429 [4] NCCL INFO Channel 00/0 : 3[2] -> 4[4] [receive] via NET/IB/1
g28n01:770099:770429 [4] NCCL INFO Channel 01/0 : 3[2] -> 4[4] [receive] via NET/IB/1
g28n01:770099:770429 [4] NCCL INFO Channel 00/0 : 4[4] -> 5[0] [send] via NET/IB/1
g28n01:770099:770429 [4] NCCL INFO Channel 01/0 : 4[4] -> 5[0] [send] via NET/IB/1
g28n01:770099:770429 [4] NCCL INFO Connected all rings
g28n01:770099:770429 [4] NCCL INFO Channel 00/0 : 2[0] -> 4[4] [receive] via NET/IB/1
g28n01:770099:770429 [4] NCCL INFO Channel 00/0 : 4[4] -> 6[2] [send] via NET/IB/1
g28n01:770099:770429 [4] NCCL INFO Channel 00/0 : 4[4] -> 8[0] [send] via NET/IB/1
g28n01:770099:770429 [4] NCCL INFO Channel 00/0 : 8[0] -> 4[4] [receive] via NET/IB/1
g28n01:770099:770429 [4] NCCL INFO Channel 00/0 : 6[2] -> 4[4] [receive] via NET/IB/1
g28n01:770099:770429 [4] NCCL INFO Channel 00/0 : 4[4] -> 2[0] [send] via NET/IB/1
g28n01:770099:770429 [4] NCCL INFO Channel 01/0 : 5[0] -> 4[4] [receive] via NET/IB/1
g28n01:770099:770429 [4] NCCL INFO Connected all trees
g28n01:770099:770429 [4] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g28n01:770099:770429 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n01:770099:770429 [4] NCCL INFO comm 0x13e8ec970 rank 4 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xaca7b5a6b2aa6a19 - Init COMPLETE
g31n04:688224:688224 [0] NCCL INFO cudaDriverVersion 12020
g31n04:688224:688224 [0] NCCL INFO Bootstrap : Using ib0:10.41.17.231<0>
g31n04:688224:688224 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g31n04:688224:688224 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g31n04:688224:688458 [0] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.231<0>
g31n04:688224:688458 [0] NCCL INFO Using network IB
g31n04:688224:688458 [0] NCCL INFO comm 0x128923b70 rank 72 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x28395441218594c5 - Init START
g31n04:688224:688458 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g31n04:688224:688458 [0] NCCL INFO Trees [0] 73/84/-1->72->48 [1] 74/-1/-1->72->73 [2] 73/-1/-1->72->78 [3] 74/-1/-1->72->73
g31n04:688224:688458 [0] NCCL INFO P2P Chunksize set to 131072
g31n04:688224:688458 [0] NCCL INFO Channel 00/0 : 71[5] -> 72[0] [receive] via NET/IB/0
g31n04:688224:688458 [0] NCCL INFO Channel 02/0 : 71[5] -> 72[0] [receive] via NET/IB/0
g31n04:688224:688458 [0] NCCL INFO Channel 00/0 : 72[0] -> 73[1] via P2P/IPC
g31n04:688224:688458 [0] NCCL INFO Channel 02/0 : 72[0] -> 73[1] via P2P/IPC
g31n04:688224:688458 [0] NCCL INFO Channel 01/0 : 72[0] -> 81[3] [send] via NET/IB/2
g31n04:688224:688458 [0] NCCL INFO Channel 03/0 : 72[0] -> 81[3] [send] via NET/IB/2
g31n04:688224:688458 [0] NCCL INFO Connected all rings
g31n04:688224:688458 [0] NCCL INFO Channel 01/0 : 72[0] -> 73[1] via P2P/IPC
g31n04:688224:688458 [0] NCCL INFO Channel 03/0 : 72[0] -> 73[1] via P2P/IPC
g31n04:688224:688458 [0] NCCL INFO Channel 01/0 : 72[0] -> 74[2] via P2P/IPC
g31n04:688224:688458 [0] NCCL INFO Channel 03/0 : 72[0] -> 74[2] via P2P/IPC
g31n04:688224:688458 [0] NCCL INFO Channel 02/0 : 72[0] -> 78[0] [send] via NET/IB/0
g31n04:688224:688458 [0] NCCL INFO Channel 00/0 : 72[0] -> 84[0] [send] via NET/IB/0
g31n04:688224:688458 [0] NCCL INFO Channel 00/0 : 48[0] -> 72[0] [receive] via NET/IB/0
g31n04:688224:688458 [0] NCCL INFO Channel 00/0 : 72[0] -> 48[0] [send] via NET/IB/0
g31n04:688224:688458 [0] NCCL INFO Channel 00/0 : 84[0] -> 72[0] [receive] via NET/IB/0
g31n04:688224:688458 [0] NCCL INFO Channel 02/0 : 78[0] -> 72[0] [receive] via NET/IB/0
g31n04:688224:688458 [0] NCCL INFO Connected all trees
g31n04:688224:688458 [0] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g31n04:688224:688458 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g31n04:688224:688458 [0] NCCL INFO comm 0x128923b70 rank 72 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x28395441218594c5 - Init COMPLETE
g31n04:688224:688545 [0] NCCL INFO Using network IB
g31n04:688224:688545 [0] NCCL INFO comm 0x12b5acf70 rank 18 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0x864be1a757d9f276 - Init START
g31n04:688224:688545 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g31n04:688224:688545 [0] NCCL INFO Trees [0] 19/21/-1->18->12 [1] 19/-1/-1->18->20
g31n04:688224:688545 [0] NCCL INFO P2P Chunksize set to 131072
g31n04:688224:688545 [0] NCCL INFO Channel 00/0 : 17[2] -> 18[0] [receive] via NET/IB/0
g31n04:688224:688545 [0] NCCL INFO Channel 01/0 : 17[2] -> 18[0] [receive] via NET/IB/0
g31n04:688224:688545 [0] NCCL INFO Channel 00/0 : 18[0] -> 19[4] via P2P/IPC
g31n04:688224:688545 [0] NCCL INFO Channel 01/0 : 18[0] -> 19[4] via P2P/IPC
g31n04:688224:688545 [0] NCCL INFO Connected all rings
g31n04:688224:688545 [0] NCCL INFO Channel 01/0 : 18[0] -> 20[2] [send] via NET/IB/0
g31n04:688224:688545 [0] NCCL INFO Channel 00/0 : 18[0] -> 21[0] [send] via NET/IB/0
g31n04:688224:688545 [0] NCCL INFO Channel 00/0 : 12[0] -> 18[0] [receive] via NET/IB/0
g31n04:688224:688545 [0] NCCL INFO Channel 00/0 : 18[0] -> 12[0] [send] via NET/IB/0
g31n04:688224:688545 [0] NCCL INFO Channel 00/0 : 21[0] -> 18[0] [receive] via NET/IB/0
g31n04:688224:688545 [0] NCCL INFO Channel 01/0 : 20[2] -> 18[0] [receive] via NET/IB/0
g31n04:688224:688545 [0] NCCL INFO Connected all trees
g31n04:688224:688545 [0] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g31n04:688224:688545 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n04:688224:688545 [0] NCCL INFO comm 0x12b5acf70 rank 18 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0x864be1a757d9f276 - Init COMPLETE
g31n04:688224:688559 [0] NCCL INFO Using network IB
g31n04:688224:688559 [0] NCCL INFO comm 0x12b7c65b0 rank 9 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x76a32220a008087c - Init START
g31n04:688224:688559 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g31n04:688224:688559 [0] NCCL INFO Trees [0] -1/-1/-1->9->10 [1] 10/8/-1->9->7
g31n04:688224:688559 [0] NCCL INFO P2P Chunksize set to 131072
g31n04:688224:688559 [0] NCCL INFO Channel 00/0 : 8[4] -> 9[0] [receive] via NET/IB/0
g31n04:688224:688559 [0] NCCL INFO Channel 01/0 : 8[4] -> 9[0] [receive] via NET/IB/0
g31n04:688224:688559 [0] NCCL INFO Channel 00/0 : 9[0] -> 10[2] [send] via NET/IB/0
g31n04:688224:688559 [0] NCCL INFO Channel 01/0 : 9[0] -> 10[2] [send] via NET/IB/0
g31n04:688224:688559 [0] NCCL INFO Connected all rings
g31n04:688224:688559 [0] NCCL INFO Channel 01/0 : 7[2] -> 9[0] [receive] via NET/IB/0
g31n04:688224:688559 [0] NCCL INFO Channel 01/0 : 9[0] -> 7[2] [send] via NET/IB/0
g31n04:688224:688559 [0] NCCL INFO Channel 00/0 : 10[2] -> 9[0] [receive] via NET/IB/0
g31n04:688224:688559 [0] NCCL INFO Channel 01/0 : 10[2] -> 9[0] [receive] via NET/IB/0
g31n04:688224:688559 [0] NCCL INFO Channel 01/0 : 9[0] -> 8[4] [send] via NET/IB/0
g31n04:688224:688559 [0] NCCL INFO Connected all trees
g31n04:688224:688559 [0] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g31n04:688224:688559 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n04:688224:688559 [0] NCCL INFO comm 0x12b7c65b0 rank 9 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x76a32220a008087c - Init COMPLETE
g28n01:770095:770095 [2] NCCL INFO cudaDriverVersion 12020
g28n01:770095:770095 [2] NCCL INFO Bootstrap : Using ib0:10.41.17.174<0>
g28n01:770095:770095 [2] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g28n01:770095:770095 [2] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g28n01:770095:770325 [2] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.174<0>
g28n01:770095:770325 [2] NCCL INFO Using network IB
g28n01:770095:770325 [2] NCCL INFO comm 0x14e033a00 rank 32 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x28395441218594c5 - Init START
g28n01:770095:770325 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g28n01:770095:770325 [2] NCCL INFO Trees [0] 33/-1/-1->32->31 [1] -1/-1/-1->32->30 [2] 33/-1/-1->32->31 [3] -1/-1/-1->32->30
g28n01:770095:770325 [2] NCCL INFO P2P Chunksize set to 131072
g28n01:770095:770325 [2] NCCL INFO Channel 00/0 : 32[2] -> 33[3] via P2P/IPC
g28n01:770095:770325 [2] NCCL INFO Channel 02/0 : 32[2] -> 33[3] via P2P/IPC
g28n01:770095:770325 [2] NCCL INFO Channel 01/0 : 32[2] -> 31[1] via P2P/IPC
g28n01:770095:770325 [2] NCCL INFO Channel 03/0 : 32[2] -> 31[1] via P2P/IPC
g28n01:770095:770325 [2] NCCL INFO Connected all rings
g28n01:770095:770325 [2] NCCL INFO Channel 01/0 : 32[2] -> 30[0] via P2P/IPC
g28n01:770095:770325 [2] NCCL INFO Channel 03/0 : 32[2] -> 30[0] via P2P/IPC
g28n01:770095:770325 [2] NCCL INFO Channel 00/0 : 32[2] -> 31[1] via P2P/IPC
g28n01:770095:770325 [2] NCCL INFO Channel 02/0 : 32[2] -> 31[1] via P2P/IPC
g28n01:770095:770325 [2] NCCL INFO Connected all trees
g28n01:770095:770325 [2] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g28n01:770095:770325 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g28n01:770095:770325 [2] NCCL INFO comm 0x14e033a00 rank 32 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x28395441218594c5 - Init COMPLETE
g28n01:770095:770413 [2] NCCL INFO Using network IB
g28n01:770095:770413 [2] NCCL INFO comm 0x150d00930 rank 8 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0x864be1a757d9f276 - Init START
g28n01:770095:770413 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g28n01:770095:770413 [2] NCCL INFO Trees [0] -1/-1/-1->8->10 [1] 9/6/-1->8->5
g28n01:770095:770413 [2] NCCL INFO P2P Chunksize set to 131072
g28n01:770095:770413 [2] NCCL INFO Channel 00/0 : 7[4] -> 8[2] [receive] via NET/IB/0
g28n01:770095:770413 [2] NCCL INFO Channel 01/0 : 7[4] -> 8[2] [receive] via NET/IB/0
g28n01:770095:770413 [2] NCCL INFO Channel 00/0 : 8[2] -> 9[0] [send] via NET/IB/0
g28n01:770095:770413 [2] NCCL INFO Channel 01/0 : 8[2] -> 9[0] [send] via NET/IB/0
g28n01:770095:770413 [2] NCCL INFO Connected all rings
g28n01:770095:770413 [2] NCCL INFO Channel 01/0 : 6[0] -> 8[2] [receive] via NET/IB/0
g28n01:770095:770413 [2] NCCL INFO Channel 00/0 : 8[2] -> 10[4] [send] via NET/IB/0
g28n01:770095:770413 [2] NCCL INFO Channel 01/0 : 5[2] -> 8[2] [receive] via NET/IB/0
g28n01:770095:770413 [2] NCCL INFO Channel 01/0 : 8[2] -> 5[2] [send] via NET/IB/0
g28n01:770095:770413 [2] NCCL INFO Channel 00/0 : 10[4] -> 8[2] [receive] via NET/IB/0
g28n01:770095:770413 [2] NCCL INFO Channel 01/0 : 8[2] -> 6[0] [send] via NET/IB/0
g28n01:770095:770413 [2] NCCL INFO Channel 01/0 : 9[0] -> 8[2] [receive] via NET/IB/0
g28n01:770095:770413 [2] NCCL INFO Connected all trees
g28n01:770095:770413 [2] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g28n01:770095:770413 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n01:770095:770413 [2] NCCL INFO comm 0x150d00930 rank 8 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0x864be1a757d9f276 - Init COMPLETE
g28n01:770095:770428 [2] NCCL INFO Using network IB
g28n01:770095:770428 [2] NCCL INFO comm 0x150cf12b0 rank 4 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0x76a32220a008087c - Init START
g28n01:770095:770428 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g28n01:770095:770428 [2] NCCL INFO Trees [0] 2/6/-1->4->8 [1] -1/-1/-1->4->5
g28n01:770095:770428 [2] NCCL INFO P2P Chunksize set to 131072
g28n01:770095:770428 [2] NCCL INFO Channel 00/0 : 3[0] -> 4[2] [receive] via NET/IB/0
g28n01:770095:770428 [2] NCCL INFO Channel 01/0 : 3[0] -> 4[2] [receive] via NET/IB/0
g28n01:770095:770428 [2] NCCL INFO Channel 00/0 : 4[2] -> 5[4] [send] via NET/IB/0
g28n01:770095:770428 [2] NCCL INFO Channel 01/0 : 4[2] -> 5[4] [send] via NET/IB/0
g28n01:770095:770428 [2] NCCL INFO Connected all rings
g28n01:770095:770428 [2] NCCL INFO Channel 00/0 : 2[4] -> 4[2] [receive] via NET/IB/0
g28n01:770095:770428 [2] NCCL INFO Channel 00/0 : 4[2] -> 6[0] [send] via NET/IB/0
g28n01:770095:770428 [2] NCCL INFO Channel 00/0 : 4[2] -> 8[4] [send] via NET/IB/0
g28n01:770095:770428 [2] NCCL INFO Channel 00/0 : 8[4] -> 4[2] [receive] via NET/IB/0
g28n01:770095:770428 [2] NCCL INFO Channel 00/0 : 6[0] -> 4[2] [receive] via NET/IB/0
g28n01:770095:770428 [2] NCCL INFO Channel 00/0 : 4[2] -> 2[4] [send] via NET/IB/0
g28n01:770095:770428 [2] NCCL INFO Channel 01/0 : 5[4] -> 4[2] [receive] via NET/IB/0
g28n01:770095:770428 [2] NCCL INFO Connected all trees
g28n01:770095:770428 [2] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g28n01:770095:770428 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n01:770095:770428 [2] NCCL INFO comm 0x150cf12b0 rank 4 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0x76a32220a008087c - Init COMPLETE
g28n01:770093:770093 [1] NCCL INFO cudaDriverVersion 12020
g28n01:770093:770093 [1] NCCL INFO Bootstrap : Using ib0:10.41.17.174<0>
g28n01:770093:770093 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g28n01:770093:770093 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g28n01:770093:770326 [1] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.174<0>
g28n01:770093:770326 [1] NCCL INFO Using network IB
g28n01:770093:770326 [1] NCCL INFO comm 0x119e44ec0 rank 31 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x28395441218594c5 - Init START
g28n01:770093:770326 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g28n01:770093:770326 [1] NCCL INFO Trees [0] 32/-1/-1->31->30 [1] 30/-1/-1->31->35 [2] 32/36/-1->31->30 [3] 30/-1/-1->31->35
g28n01:770093:770326 [1] NCCL INFO P2P Chunksize set to 131072
g28n01:770093:770326 [1] NCCL INFO Channel 00/0 : 31[1] -> 32[2] via P2P/IPC
g28n01:770093:770326 [1] NCCL INFO Channel 02/0 : 31[1] -> 32[2] via P2P/IPC
g28n01:770093:770326 [1] NCCL INFO Channel 01/0 : 31[1] -> 30[0] via P2P/IPC
g28n01:770093:770326 [1] NCCL INFO Channel 03/0 : 31[1] -> 30[0] via P2P/IPC
g28n01:770093:770326 [1] NCCL INFO Connected all rings
g28n01:770093:770326 [1] NCCL INFO Channel 01/0 : 31[1] -> 35[5] via P2P/IPC
g28n01:770093:770326 [1] NCCL INFO Channel 03/0 : 31[1] -> 35[5] via P2P/IPC
g28n01:770093:770326 [1] NCCL INFO Channel 02/0 : 31[1] -> 36[0] [send] via NET/IB/0
g28n01:770093:770326 [1] NCCL INFO Channel 02/0 : 36[0] -> 31[1] [receive] via NET/IB/0
g28n01:770093:770326 [1] NCCL INFO Channel 00/0 : 31[1] -> 30[0] via P2P/IPC
g28n01:770093:770326 [1] NCCL INFO Channel 02/0 : 31[1] -> 30[0] via P2P/IPC
g28n01:770093:770326 [1] NCCL INFO Connected all trees
g28n01:770093:770326 [1] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g28n01:770093:770326 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g28n01:770093:770326 [1] NCCL INFO comm 0x119e44ec0 rank 31 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x28395441218594c5 - Init COMPLETE
g28n01:770093:770412 [1] NCCL INFO Using network IB
g28n01:770093:770412 [1] NCCL INFO comm 0x11cc2e100 rank 7 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0x15c93a2ee1bf52db - Init START
g28n01:770093:770412 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g28n01:770093:770412 [1] NCCL INFO Trees [0] 8/-1/-1->7->9 [1] 8/6/-1->7->5
g28n01:770093:770412 [1] NCCL INFO P2P Chunksize set to 131072
g28n01:770093:770412 [1] NCCL INFO Channel 00/0 : 6[3] -> 7[1] [receive] via NET/IB/2
g28n01:770093:770412 [1] NCCL INFO Channel 01/0 : 6[3] -> 7[1] [receive] via NET/IB/2
g28n01:770093:770412 [1] NCCL INFO Channel 00/0 : 7[1] -> 8[5] via P2P/IPC
g28n01:770093:770412 [1] NCCL INFO Channel 01/0 : 7[1] -> 8[5] via P2P/IPC
g28n01:770093:770412 [1] NCCL INFO Connected all rings
g28n01:770093:770412 [1] NCCL INFO Channel 01/0 : 5[5] -> 7[1] [receive] via NET/IB/2
g28n01:770093:770412 [1] NCCL INFO Channel 00/0 : 7[1] -> 9[3] [send] via NET/IB/2
g28n01:770093:770412 [1] NCCL INFO Channel 00/0 : 9[3] -> 7[1] [receive] via NET/IB/2
g28n01:770093:770412 [1] NCCL INFO Channel 01/0 : 7[1] -> 5[5] [send] via NET/IB/2
g28n01:770093:770412 [1] NCCL INFO Channel 01/0 : 7[1] -> 6[3] [send] via NET/IB/2
g28n01:770093:770412 [1] NCCL INFO Connected all trees
g28n01:770093:770412 [1] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g28n01:770093:770412 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n01:770093:770412 [1] NCCL INFO comm 0x11cc2e100 rank 7 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0x15c93a2ee1bf52db - Init COMPLETE
g28n01:770093:770432 [1] NCCL INFO Using network IB
g28n01:770093:770432 [1] NCCL INFO comm 0x11bde3d60 rank 3 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x6876d4a326688ade - Init START
g28n01:770093:770432 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g28n01:770093:770g31n02:736466:736466 [3] NCCL INFO cudaDriverVersion 12020
g31n02:736466:736466 [3] NCCL INFO Bootstrap : Using ib0:10.41.17.229<0>
g31n02:736466:736466 [3] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g31n02:736466:736466 [3] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g31n02:736466:736693 [3] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.229<0>
g31n02:736466:736693 [3] NCCL INFO Using network IB
g31n02:736466:736693 [3] NCCL INFO comm 0x14e2b4340 rank 63 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x28395441218594c5 - Init START
g31n02:736466:736693 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g31n02:736466:736693 [3] NCCL INFO Trees [0] 64/-1/-1->63->62 [1] 65/58/-1->63->64 [2] 64/-1/-1->63->62 [3] 65/-1/-1->63->64
g31n02:736466:736693 [3] NCCL INFO P2P Chunksize set to 131072
g31n02:736466:736693 [3] NCCL INFO Channel 00/0 : 63[3] -> 64[4] via P2P/IPC
g31n02:736466:736693 [3] NCCL INFO Channel 01/0 : 63[3] -> 64[4] via P2P/IPC
g31n02:736466:736693 [3] NCCL INFO Channel 02/0 : 63[3] -> 64[4] via P2P/IPC
g31n02:736466:736693 [3] NCCL INFO Channel 03/0 : 63[3] -> 64[4] via P2P/IPC
g31n02:736466:736693 [3] NCCL INFO Channel 01/0 : 54[0] -> 63[3] [receive] via NET/IB/3
g31n02:736466:736693 [3] NCCL INFO Channel 03/0 : 54[0] -> 63[3] [receive] via NET/IB/3
g31n02:736466:736693 [3] NCCL INFO Connected all rings
g31n02:736466:736693 [3] NCCL INFO Channel 01/0 : 63[3] -> 65[5] via P2P/IPC
g31n02:736466:736693 [3] NCCL INFO Channel 03/0 : 63[3] -> 65[5] via P2P/IPC
g31n02:736466:736693 [3] NCCL INFO Channel 01/0 : 58[4] -> 63[3] [receive] via NET/IB/3
g31n02:736466:736693 [3] NCCL INFO Channel 01/0 : 63[3] -> 58[4] [send] via NET/IB/3
g31n02:736466:736693 [3] NCCL INFO Channel 00/0 : 63[3] -> 62[2] via P2P/IPC
g31n02:736466:736693 [3] NCCL INFO Channel 02/0 : 63[3] -> 62[2] via P2P/IPC
g31n02:736466:736693 [3] NCCL INFO Connected all trees
g31n02:736466:736693 [3] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g31n02:736466:736693 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g31n02:736466:736693 [3] NCCL INFO comm 0x14e2b4340 rank 63 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x28395441218594c5 - Init COMPLETE
g31n02:736466:736781 [3] NCCL INFO Using network IB
g31n02:736466:736781 [3] NCCL INFO comm 0x150f4f840 rank 15 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x15c93a2ee1bf52db - Init START
g31n02:736466:736781 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g31n02:736466:736781 [3] NCCL INFO Trees [0] 13/16/-1->15->18 [1] -1/-1/-1->15->14
g31n02:736466:736781 [3] NCCL INFO P2P Chunksize set to 131072
g31n02:736466:736781 [3] NCCL INFO Channel 00/0 : 14[5] -> 15[3] [receive] via NET/IB/3
g31n02:736466:736781 [3] NCCL INFO Channel 01/0 : 14[5] -> 15[3] [receive] via NET/IB/3
g31n02:736466:736781 [3] NCCL INFO Channel 00/0 : 15[3] -> 16[1] [send] via NET/IB/3
g31n02:736466:736781 [3] NCCL INFO Channel 01/0 : 15[3] -> 16[1] [send] via NET/IB/3
g31n02:736466:736781 [3] NCCL INFO Connected all rings
g31n02:736466:736781 [3] NCCL INFO Channel 00/0 : 13[1] -> 15[3] [receive] via NET/IB/3
g31n02:736466:736781 [3] NCCL INFO Channel 00/0 : 15[3] -> 18[3] [send] via NET/IB/3
g31n02:736466:736781 [3] NCCL INFO Channel 00/0 : 18[3] -> 15[3] [receive] via NET/IB/3
g31n02:736466:736781 [3] NCCL INFO Channel 00/0 : 15[3] -> 13[1] [send] via NET/IB/3
g31n02:736466:736781 [3] NCCL INFO Channel 00/0 : 16[1] -> 15[3] [receive] via NET/IB/3
g31n02:736466:736781 [3] NCCL INFO Channel 01/0 : 15[3] -> 14[5] [send] via NET/IB/3
g31n02:736466:736781 [3] NCCL INFO Connected all trees
g31n02:736466:736781 [3] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g31n02:736466:736781 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n02:736466:736781 [3] NCCL INFO comm 0x150f4f840 rank 15 nranks 24 cudaDev 3 nvm432 [1] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] 7/1/-1->3->11
g28n01:770093:770432 [1] NCCL INFO P2P Chunksize set to 131072
g28n01:770093:770432 [1] NCCL INFO Channel 00/0 : 2[5] -> 3[1] [receive] via NET/IB/2
g28n01:770093:770432 [1] NCCL INFO Channel 01/0 : 2[5] -> 3[1] [receive] via NET/IB/2
g28n01:770093:770432 [1] NCCL INFO Channel 00/0 : 3[1] -> 4[3] [send] via NET/IB/2
g28n01:770093:770432 [1] NCCL INFO Channel 01/0 : 3[1] -> 4[3] [send] via NET/IB/2
g28n01:770093:770432 [1] NCCL INFO Connected all rings
g28n01:770093:770432 [1] NCCL INFO Channel 01/0 : 1[3] -> 3[1] [receive] via NET/IB/2
g28n01:770093:770432 [1] NCCL INFO Channel 01/0 : 11[5] -> 3[1] [receive] via NET/IB/2
g28n01:770093:770432 [1] NCCL INFO Channel 01/0 : 3[1] -> 7[3] [send] via NET/IB/2
g28n01:770093:770432 [1] NCCL INFO Channel 01/0 : 7[3] -> 3[1] [receive] via NET/IB/2
g28n01:770093:770432 [1] NCCL INFO Channel 01/0 : 3[1] -> 11[5] [send] via NET/IB/2
g28n01:770093:770432 [1] NCCL INFO Channel 01/0 : 3[1] -> 1[3] [send] via NET/IB/2
g28n01:770093:770432 [1] NCCL INFO Channel 00/0 : 3[1] -> 2[5] [send] via NET/IB/2
g28n01:770093:770432 [1] NCCL INFO Connected all trees
g28n01:770093:770432 [1] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g28n01:770093:770432 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n01:770093:770432 [1] NCCL INFO comm 0x11bde3d60 rank 3 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x6876d4a326688ade - Init COMPLETE
lDev 3 busId 3503000 commId 0x15c93a2ee1bf52db - Init COMPLETE
g31n02:736466:736802 [3] NCCL INFO Using network IB
g31n02:736466:736802 [3] NCCL INFO comm 0x1510287b0 rank 7 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x6876d4a326688ade - Init START
g31n02:736466:736802 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g31n02:736466:736802 [3] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] 9/5/-1->7->3
g31n02:736466:736802 [3] NCCL INFO P2P Chunksize set to 131072
g31n02:736466:736802 [3] NCCL INFO Channel 00/0 : 6[1] -> 7[3] [receive] via NET/IB/3
g31n02:736466:736802 [3] NCCL INFO Channel 01/0 : 6[1] -> 7[3] [receive] via NET/IB/3
g31n02:736466:736802 [3] NCCL INFO Channel 00/0 : 7[3] -> 8[5] [send] via NET/IB/3
g31n02:736466:736802 [3] NCCL INFO Channel 01/0 : 7[3] -> 8[5] [send] via NET/IB/3
g31n02:736466:736802 [3] NCCL INFO Connected all rings
g31n02:736466:736802 [3] NCCL INFO Channel 01/0 : 5[5] -> 7[3] [receive] via NET/IB/3
g31n02:736466:736802 [3] NCCL INFO Channel 01/0 : 7[3] -> 9[1] [send] via NET/IB/3
g31n02:736466:736802 [3] NCCL INFO Channel 01/0 : 3[1] -> 7[3] [receive] via NET/IB/3
g31n02:736466:736802 [3] NCCL INFO Channel 01/0 : 7[3] -> 3[1] [send] via NET/IB/3
g31n02:736466:736802 [3] NCCL INFO Channel 01/0 : 9[1] -> 7[3] [receive] via NET/IB/3
g31n02:736466:736802 [3] NCCL INFO Channel 01/0 : 7[3] -> 5[5] [send] via NET/IB/3
g31n02:736466:736802 [3] NCCL INFO Channel 00/0 : 7[3] -> 6[1] [send] via NET/IB/3
g31n02:736466:736802 [3] NCCL INFO Connected all trees
g31n02:736466:736802 [3] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g31n02:736466:736802 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n02:736466:736802 [3] NCCL INFO comm 0x1510287b0 rank 7 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x6876d4a326688ade - Init COMPLETE
g31n05:704345:704345 [1] NCCL INFO cudaDriverVersion 12020
g31n05:704345:704345 [1] NCCL INFO Bootstrap : Using ib0:10.41.17.232<0>
g31n05:704345:704345 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g31n05:704345:704345 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g31n05:704345:704581 [1] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.232<0>
g31n05:704345:704581 [1] NCCL INFO Using network IB
g31n05:704345:704581 [1] NCCL INFO comm 0x12e0e4100 rank 79 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x28395441218594c5 - Init START
g31n05:704345:704581 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g31n05:704345:704581 [1] NCCL INFO Trees [0] 80/-1/-1->79->78 [1] 78/-1/-1->79->83 [2] 80/84/-1->79->78 [3] 78/-1/-1->79->83
g31n05:704345:704581 [1] NCCL INFO P2P Chunksize set to 131072
g31n05:704345:704581 [1] NCCL INFO Channel 00/0 : 79[1] -> 80[2] via P2P/IPC
g31n05:704345:704581 [1] NCCL INFO Channel 02/0 : 79[1] -> 80[2] via P2P/IPC
g31n05:704345:704581 [1] NCCL INFO Channel 01/0 : 79[1] -> 78[0] via P2P/IPC
g31n05:704345:704581 [1] NCCL INFO Channel 03/0 : 79[1] -> 78[0] via P2P/IPC
g31n05:704345:704581 [1] NCCL INFO Connected all rings
g31n05:704345:704581 [1] NCCL INFO Channel 01/0 : 79[1] -> 83[5] via P2P/IPC
g31n05:704345:704581 [1] NCCL INFO Channel 03/0 : 79[1] -> 83[5] via P2P/IPC
g31n05:704345:704581 [1] NCCL INFO Channel 02/0 : 79[1] -> 84[0] [send] via NET/IB/0
g31n05:704345:704581 [1] NCCL INFO Channel 02/0 : 84[0] -> 79[1] [receive] via NET/IB/0
g31n05:704345:704581 [1] NCCL INFO Channel 00/0 : 79[1] -> 78[0] via P2P/IPC
g31n05:704345:704581 [1] NCCL INFO Channel 02/0 : 79[1] -> 78[0] via P2P/IPC
g31n05:704345:704581 [1] NCCL INFO Connected all trees
g31n05:704345:704581 [1] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g31n05:704345:704581 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g31n05:704345:704581 [1] NCCL INFO comm 0x12e0e4100 rank 79 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x28395441218594c5 - Init COMPLETE
g31n05:704345:704662 [1] NCCL INFO Using network IB
g31n05:704345:704662 [1] NCCL INFO comm 0x13148cf20 rank 19 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0x15c93a2ee1bf52db - Init START
g31n05:704345:704662 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g31n05:704345:704662 [1] NCCL INFO Trees [0] 20/-1/-1->19->21 [1] 20/18/-1->19->17
g31n05:704345:704662 [1] NCCL INFO P2P Chunksize set to 131072
g31n05:704345:704662 [1] NCCL INFO Channel 00/0 : 18[3] -> 19[1] [receive] via NET/IB/2
g31n05:704345:704662 [1] NCCL INFO Channel 01/0 : 18[3] -> 19[1] [receive] via NET/IB/2
g31n05:704345:704662 [1] NCCL INFO Channel 00/0 : 19[1] -> 20[5] via P2P/IPC
g31n05:704345:704662 [1] NCCL INFO Channel 01/0 : 19[1] -> 20[5] via P2P/IPC
g31n05:704345:704662 [1] NCCL INFO Connected all rings
g31n05:704345:704662 [1] NCCL INFO Channel 01/0 : 17[5] -> 19[1] [receive] via NET/IB/2
g31n05:704345:704662 [1] NCCL INFO Channel 00/0 : 19[1] -> 21[3] [send] via NET/IB/2
g31n05:704345:704662 [1] NCCL INFO Channel 00/0 : 21[3] -> 19[1] [receive] via NET/IB/2
g31n05:704345:704662 [1] NCCL INFO Channel 01/0 : 19[1] -> 17[5] [send] via NET/IB/2
g31n05:704345:704662 [1] NCCL INFO Channel 01/0 : 19[1] -> 18[3] [send] via NET/IB/2
g31n05:704345:704662 [1] NCCL INFO Connected all trees
g31n05:704345:704662 [1] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g31n05:704345:704662 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n05:704345:704662 [1] NCCL INFO comm 0x13148cf20 rank 19 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0x15c93a2ee1bf52db - Init COMPLETE
g31n05:704345:704682 [1] NCCL INFO Using network IB
g31n05:704345:704682 [1] NCCL INFO comm 0x130fb0a30 rank 9 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x6876d4a326688ade - Init START
g31n05:704345:704682 [1] NCCL INFO Setting affinity for GPU 1 tog31n05:704346:704346 [2] NCCL INFO cudaDriverVersion 12020
g31n05:704346:704346 [2] NCCL INFO Bootstrap : Using ib0:10.41.17.232<0>
g31n05:704346:704346 [2] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g31n05:704346:704346 [2] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g31n05:704346:704579 [2] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.232<0>
g31n05:704346:704579 [2] NCCL INFO Using network IB
g31n05:704346:704579 [2] NCCL INFO comm 0x158514850 rank 80 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x28395441218594c5 - Init START
g31n05:704346:704579 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g31n05:704346:704579 [2] NCCL INFO Trees [0] 81/-1/-1->80->79 [1] -1/-1/-1->80->78 [2] 81/-1/-1->80->79 [3] -1/-1/-1->80->78
g31n05:704346:704579 [2] NCCL INFO P2P Chunksize set to 131072
g31n05:704346:704579 [2] NCCL INFO Channel 00/0 : 80[2] -> 81[3] via P2P/IPC
g31n05:704346:704579 [2] NCCL INFO Channel 02/0 : 80[2] -> 81[3] via P2P/IPC
g31n05:704346:704579 [2] NCCL INFO Channel 01/0 : 80[2] -> 79[1] via P2P/IPC
g31n05:704346:704579 [2] NCCL INFO Channel 03/0 : 80[2] -> 79[1] via P2P/IPC
g31n05:704346:704579 [2] NCCL INFO Connected all rings
g31n05:704346:704579 [2] NCCL INFO Channel 01/0 : 80[2] -> 78[0] via P2P/IPC
g31n05:704346:704579 [2] NCCL INFO Channel 03/0 : 80[2] -> 78[0] via P2P/IPC
g31n05:704346:704579 [2] NCCL INFO Channel 00/0 : 80[2] -> 79[1] via P2P/IPC
g31n05:704346:704579 [2] NCCL INFO Channel 02/0 : 80[2] -> 79[1] via P2P/IPC
g31n05:704346:704579 [2] NCCL INFO Connected all trees
g31n05:704346:704579 [2] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g31n05:704346:704579 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g31n05:704346:704579 [2] NCCL INFO comm 0x158514850 rank 80 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x28395441218594c5 - Init COMPLETE
g31n05:704346:704663 [2] NCCL INFO Using network IB
g31n05:704346:704663 [2] NCCL INFO comm 0x15b15d3a0 rank 20 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0x864be1a757d9f276 - Init START
g31n05:704346:704663 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g31n05:704346:704663 [2] NCCL INFO Trees [0] -1/-1/-1->20->22 [1] 21/18/-1->20->17
g31n05:704346:704663 [2] NCCL INFO P2P Chunksize set to 131072
g31n05:704346:704663 [2] NCCL INFO Channel 00/0 : 19[4] -> 20[2] [receive] via NET/IB/0
g31n05:704346:704663 [2] NCCL INFO Channel 01/0 : 19[4] -> 20[2] [receive] via NET/IB/0
g31n05:704346:704663 [2] NCCL INFO Channel 00/0 : 20[2] -> 21[0] [send] via NET/IB/0
g31n05:704346:704663 [2] NCCL INFO Channel 01/0 : 20[2] -> 21[0] [send] via NET/IB/0
g31n05:704346:704663 [2] NCCL INFO Connected all rings
g31n05:704346:704663 [2] NCCL INFO Channel 01/0 : 18[0] -> 20[2] [receive] via NET/IB/0
g31n05:704346:704663 [2] NCCL INFO Channel 00/0 : 20[2] -> 22[4] [send] via NET/IB/0
g31n05:704346:704663 [2] NCCL INFO Channel 01/0 : 17[2] -> 20[2] [receive] via NET/IB/0
g31n05:704346:704663 [2] NCCL INFO Channel 01/0 : 20[2] -> 17[2] [send] via NET/IB/0
g31n05:704346:704663 [2] NCCL INFO Channel 00/0 : 22[4] -> 20[2] [receive] via NET/IB/0
g31n05:704346:704663 [2] NCCL INFO Channel 01/0 : 20[2] -> 18[0] [send] via NET/IB/0
g31n05:704346:704663 [2] NCCL INFO Channel 01/0 : 21[0] -> 20[2] [receive] via NET/IB/0
g31n05:704346:704663 [2] NCCL INFO Connected all trees
g31n05:704346:704663 [2] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g31n05:704346:704663 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n05:704346:704663 [2] NCCL INFO comm 0x15b15d3a0 rank 20 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0x864be1a757d9f276 - Init COMPLETE
g31n05:704346:704678 [2] NCCL INFO Using network IB
g31n05:704346:704678 [2] NCCL INFO comm 0x15adfa3e0 rank 10 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0x76a32220a008087c - Init START
g31n05:704346:704678 [2] N f0000000
g31n05:704345:704682 [1] NCCL INFO Trees [0] -1/-1/-1->9->10 [1] 10/8/-1->9->7
g31n05:704345:704682 [1] NCCL INFO P2P Chunksize set to 131072
g31n05:704345:704682 [1] NCCL INFO Channel 00/0 : 8[5] -> 9[1] [receive] via NET/IB/2
g31n05:704345:704682 [1] NCCL INFO Channel 01/0 : 8[5] -> 9[1] [receive] via NET/IB/2
g31n05:704345:704682 [1] NCCL INFO Channel 00/0 : 9[1] -> 10[3] [send] via NET/IB/2
g31n05:704345:704682 [1] NCCL INFO Channel 01/0 : 9[1] -> 10[3] [send] via NET/IB/2
g31n05:704345:704682 [1] NCCL INFO Connected all rings
g31n05:704345:704682 [1] NCCL INFO Channel 01/0 : 7[3] -> 9[1] [receive] via NET/IB/2
g31n05:704345:704682 [1] NCCL INFO Channel 01/0 : 9[1] -> 7[3] [send] via NET/IB/2
g31n05:704345:704682 [1] NCCL INFO Channel 00/0 : 10[3] -> 9[1] [receive] via NET/IB/2
g31n05:704345:704682 [1] NCCL INFO Channel 01/0 : 10[3] -> 9[1] [receive] via NET/IB/2
g31n05:704345:704682 [1] NCCL INFO Channel 01/0 : 9[1] -> 8[5] [send] via NET/IB/2
g31n05:704345:704682 [1] NCCL INFO Connected all trees
g31n05:704345:704682 [1] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g31n05:704345:704682 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n05:704345:704682 [1] NCCL INFO comm 0x130fb0a30 rank 9 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x6876d4a326688ade - Init COMPLETE
CCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g31n05:704346:704678 [2] NCCL INFO Trees [0] 9/11/-1->10->8 [1] -1/-1/-1->10->9
g31n05:704346:704678 [2] NCCL INFO P2P Chunksize set to 131072
g31n05:704346:704678 [2] NCCL INFO Channel 00/0 : 9[0] -> 10[2] [receive] via NET/IB/0
g31n05:704346:704678 [2] NCCL INFO Channel 01/0 : 9[0] -> 10[2] [receive] via NET/IB/0
g31n05:704346:704678 [2] NCCL INFO Channel 00/0 : 10[2] -> 11[4] [send] via NET/IB/0
g31n05:704346:704678 [2] NCCL INFO Channel 01/0 : 10[2] -> 11[4] [send] via NET/IB/0
g31n05:704346:704678 [2] NCCL INFO Connected all rings
g31n05:704346:704678 [2] NCCL INFO Channel 00/0 : 8[4] -> 10[2] [receive] via NET/IB/0
g31n05:704346:704678 [2] NCCL INFO Channel 00/0 : 10[2] -> 8[4] [send] via NET/IB/0
g31n05:704346:704678 [2] NCCL INFO Channel 00/0 : 11[4] -> 10[2] [receive] via NET/IB/0
g31n05:704346:704678 [2] NCCL INFO Channel 00/0 : 10[2] -> 9[0] [send] via NET/IB/0
g31n05:704346:704678 [2] NCCL INFO Channel 01/0 : 10[2] -> 9[0] [send] via NET/IB/0
g31n05:704346:704678 [2] NCCL INFO Connected all trees
g31n05:704346:704678 [2] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g31n05:704346:704678 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n05:704346:704678 [2] NCCL INFO comm 0x15adfa3e0 rank 10 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0x76a32220a008087c - Init COMPLETE
g31n01:617760:617760 [4] NCCL INFO cudaDriverVersion 12020
g31n01:617760:617760 [4] NCCL INFO Bootstrap : Using ib0:10.41.17.228<0>
g31n01:617760:617760 [4] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g31n01:617760:617760 [4] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g31n01:617760:617996 [4] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.228<0>
g31n01:617760:617996 [4] NCCL INFO Using network IB
g31n01:617760:617996 [4] NCCL INFO comm 0x155404680 rank 58 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x28395441218594c5 - Init START
g31n01:617760:617996 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g31n01:617760:617996 [4] NCCL INFO Trees [0] 59/-1/-1->58->57 [1] 57/-1/-1->58->63 [2] 59/-1/-1->58->57 [3] 57/52/-1->58->70
g31n01:617760:617996 [4] NCCL INFO P2P Chunksize set to 131072
g31n01:617760:617996 [4] NCCL INFO Channel 00/0 : 58[4] -> 59[5] via P2P/IPC
g31n01:617760:617996 [4] NCCL INFO Channel 01/0 : 58[4] -> 59[5] via P2P/IPC
g31n01:617760:617996 [4] NCCL INFO Channel 02/0 : 58[4] -> 59[5] via P2P/IPC
g31n01:617760:617996 [4] NCCL INFO Channel 03/0 : 58[4] -> 59[5] via P2P/IPC
g31n01:617760:617996 [4] NCCL INFO Connected all rings
g31n01:617760:617996 [4] NCCL INFO Channel 01/0 : 58[4] -> 63[3] [send] via NET/IB/3
g31n01:617760:617996 [4] NCCL INFO Channel 03/0 : 52[4] -> 58[4] [receive] via NET/IB/3
g31n01:617760:617996 [4] NCCL INFO Channel 03/0 : 58[4] -> 70[4] [send] via NET/IB/3
g31n01:617760:617996 [4] NCCL INFO Channel 03/0 : 70[4] -> 58[4] [receive] via NET/IB/3
g31n01:617760:617996 [4] NCCL INFO Channel 03/0 : 58[4] -> 52[4] [send] via NET/IB/3
g31n01:617760:617996 [4] NCCL INFO Channel 01/0 : 63[3] -> 58[4] [receive] via NET/IB/3
g31n01:617760:617996 [4] NCCL INFO Channel 00/0 : 58[4] -> 57[3] via P2P/IPC
g31n01:617760:617996 [4] NCCL INFO Channel 01/0 : 58[4] -> 57[3] via P2P/IPC
g31n01:617760:617996 [4] NCCL INFO Channel 02/0 : 58[4] -> 57[3] via P2P/IPC
g31n01:617760:617996 [4] NCCL INFO Channel 03/0 : 58[4] -> 57[3] via P2P/IPC
g31n01:617760:617996 [4] NCCL INFO Connected all trees
g31n01:617760:617996 [4] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g31n01:617760:617996 [4] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g31n01:617760:617996 [4] NCCL INFO comm 0x155404680 rank 58 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x28395441218594c5 - Init COMPLETE
g31n01:617760:618077 [4] NCCL INFO Using network IB
g31n01:617760:618077 [4] NCCL INFO comm 0x157250b40 rank 14 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xe45164e09488020a - Init START
g31n01:617760:618077 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g31n01:617760:618077 [4] NCCL INFO Trees [0] -1/-1/-1->14->13 [1] 15/-1/-1->14->13
g31n01:617760:618077 [4] NCCL INFO P2P Chunksize set to 131072
g31n01:617760:618077 [4] NCCL INFO Channel 00/0 : 14[4] -> 15[2] [send] via NET/IB/1
g31n01:617760:618077 [4] NCCL INFO Channel 01/0 : 14[4] -> 15[2] [send] via NET/IB/1
g31n01:617760:618077 [4] NCCL INFO Connected all rings
g31n01:617760:618077 [4] NCCL INFO Channel 01/0 : 15[2] -> 14[4] [receive] via NET/IB/0
g31n01:617760:618077 [4] NCCL INFO Channel 00/0 : 14[4] -> 13[0] via P2P/IPC
g31n01:617760:618077 [4] NCCL INFO Channel 01/0 : 14[4] -> 13[0] via P2P/IPC
g31n01:617760:618077 [4] NCCL INFO Connected all trees
g31n01:617760:618077 [4] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g31n01:617760:618077 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n01:617760:618077 [4] NCCL INFO comm 0x157250b40 rank 14 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xe45164e09488020a - Init COMPLETE
g31n01:617760:618100 [4] NCCL INFO Using network IB
g31n01:617760:618100 [4] NCCL INFO comm 0x15912d640 rank 7 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xaca7b5a6b2aa6a19 - Init START
g31n01:617760:618100 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g31n01:617760:618100 [4] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] 9/5/-1->7->3
g31n01:617760:618100 [4] NCCL INFO P2P Chunksize set to 131072
g31n01:617760:618100 [4] NCCL INFO Channel 00/0 : 6[2] -> 7[4] [receive] via NET/IB/1
g31n01:617760:618100 [4] NCCL INFO Channel 01/0 : 6[2] -> 7[4] [receive] via NET/IB/1
g31n01:617760:618100 [4] NCCL INFO Channel 00/0 : 7[4] -> 8[0] [send] via NET/IB/1
g31n01:617760:618100 [4] NCCL INFO Channel 01/0 : 7[4] -> 8[0] [send] via NET/IB/1
g31n01:617760:618100 [4] NCCL INFO Connected all rings
g31n01:617760:618100 [4] NCCL INFO Channel 01/0 : 5[0] -> 7[4] [receive] via NET/IB/1
g31n01:617760:618100 [4] NCCL INFO Channel 01/0 : 7[4] -> 9[2] [send] via NET/IB/1
g31n01:617760:618100 [4] NCCL INFO Channel 01/0 : 3[2] -> 7[4] [receive] via NET/IB/1
g31n01:617760:618100 [4] NCCL INFO Channel 01/0 : 7[4] -> 3[2] [send] via NET/IB/1
g31n01:617760:618100 [4] NCCL INFO Channel 01/0 : 9[2] -> 7[4] [receive] via NET/IB/1
g31n01:617760:618100 [4] NCCL INFO Channel 01/0 : 7[4] -> 5[0] [send] via NET/IB/1
g31n01:617760:618100 [4] NCCL INFO Channel 00/0 : 7[4] -> 6[2] [send] via NET/IB/1
g31n01:617760:618100 [4] NCCL INFO Connected all trees
g31n01:617760:618100 [4] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g31n01:617760:618100 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n01:617760:618100 [4] NCCL INFO comm 0x15912d640 rank 7 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xaca7b5a6b2aa6a19 - Init COMPLETE
g26n02:851591:851591 [2] NCCL INFO cudaDriverVersion 12020
g26n02:851591:851591 [2] NCCL INFO Bootstrap : Using ib0:10.41.17.139<0>
g26n02:851591:851591 [2] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g26n02:851591:851591 [2] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g26n02:851591:851820 [2] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.139<0>
g26n02:851591:851820 [2] NCCL INFO Using network IB
g26n02:851591:851820 [2] NCCL INFO comm 0x153b73e70 rank 14 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x28395441218594c5 - Init START
g26n02:851591:851820 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g26n02:851591:851820 [2] NCCL INFO Trees [0] 15/-1/-1->14->13 [1] -1/-1/-1->14->12 [2] 15/-1/-1->14->13 [3] -1/-1/-1->14->12
g26n02:851591:851820 [2] NCCL INFO P2P Chunksize set to 131072
g26n02:851591:851820 [2] NCCL INFO Channel 00/0 : 14[2] -> 15[3] via P2P/IPC
g26n02:851591:851820 [2] NCCL INFO Channel 02/0 : 14[2] -> 15[3] via P2P/IPC
g26n02:851591:851820 [2] NCCL INFO Channel 01/0 : 14[2] -> 13[1] via P2P/IPC
g26n02:851591:851820 [2] NCCL INFO Channel 03/0 : 14[2] -> 13[1] via P2P/IPC
g26n02:851591:851820 [2] NCCL INFO Connected all rings
g26n02:851591:851820 [2] NCCL INFO Channel 01/0 : 14[2] -> 12[0] via P2P/IPC
g26n02:851591:851820 [2] NCCL INFO Channel 03/0 : 14[2] -> 12[0] via P2P/IPC
g26n02:851591:851820 [2] NCCL INFO Channel 00/0 : 14[2] -> 13[1] via P2P/IPC
g26n02:851591:851820 [2] NCCL INFO Channel 02/0 : 14[2] -> 13[1] via P2P/IPC
g26n02:851591:851820 [2] NCCL INFO Connected all trees
g26n02:851591:851820 [2] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g26n02:851591:851820 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g26n02:851591:851820 [2] NCCL INFO comm 0x153b73e70 rank 14 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x28395441218594c5 - Init COMPLETE
g26n02:851591:851906 [2] NCCL INFO Using network IB
g26n02:851591:851906 [2] NCCL INFO comm 0x156e639f0 rank 3 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0xe45164e09488020a - Init START
g26n02:851591:851906 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g26n02:851591:851906 [2] NCCL INFO Trees [0] 1/4/-1->3->6 [1] -1/-1/-1->3->2
g26n02:851591:851906 [2] NCCL INFO P2P Chunksize set to 131072
g26n02:851591:851906 [2] NCCL INFO Channel 00/0 : 2[4] -> 3[2] [receive] via NET/IB/0
g26n02:851591:851906 [2] NCCL INFO Channel 01/0 : 2[4] -> 3[2] [receive] via NET/IB/0
g26n02:851591:851906 [2] NCCL INFO Channel 00/0 : 3[2] -> 4[0] [send] via NET/IB/0
g26n02:851591:851906 [2] NCCL INFO Channel 01/0 : 3[2] -> 4[0] [send] via NET/IB/0
g26n02:851591:851906 [2] NCCL INFO Connected all rings
g26n02:851591:851906 [2] NCCL INFO Channel 00/0 : 1[0] -> 3[2] [receive] via NET/IB/0
g26n02:851591:851906 [2] NCCL INFO Channel 00/0 : 3[2] -> 6[2] [send] via NET/IB/0
g26n02:851591:851906 [2] NCCL INFO Channel 00/0 : 6[2] -> 3[2] [receive] via NET/IB/0
g26n02:851591:851906 [2] NCCL INFO Channel 00/0 : 3[2] -> 1[0] [send] via NET/IB/0
g26n02:851591:851906 [2] NCCL INFO Channel 00/0 : 4[0] -> 3[2] [receive] via NET/IB/0
g26n02:851591:851906 [2] NCCL INFO Channel 01/0 : 3[2] -> 2[4] [send] via NET/IB/0
g26n02:851591:851906 [2] NCCL INFO Connected all trees
g26n02:851591:851906 [2] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g26n02:851591:851906 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g26n02:851591:851906 [2] NCCL INFO comm 0x156e639f0 rank 3 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0xe45164e09488020a - Init COMPLETE
g26n02:851591:851927 [2] NCCL INFO Using network IB
g26n02:851591:851927 [2] NCCL INFO comm 0x1564d9600 rank 1 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0x8442a1c07b0a8edb - Init START
g26n02:851591:851927 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g26n02:851591:851927 [2] NCCL INFO Trees [0] -1/-1/-1->1->2 g26n02:851589:851589 [0] NCCL INFO cudaDriverVersion 12020
g26n02:851589:851589 [0] NCCL INFO Bootstrap : Using ib0:10.41.17.139<0>
g26n02:851589:851589 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g26n02:851589:851589 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g26n02:851589:851818 [0] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.139<0>
g26n02:851589:851818 [0] NCCL INFO Using network IB
g26n02:851589:851818 [0] NCCL INFO comm 0x15fc445b0 rank 12 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x28395441218594c5 - Init START
g26n02:851589:851818 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g26n02:851589:851818 [0] NCCL INFO Trees [0] 13/18/-1->12->25 [1] 14/-1/-1->12->13 [2] 13/-1/-1->12->7 [3] 14/-1/-1->12->13
g26n02:851589:851818 [0] NCCL INFO P2P Chunksize set to 131072
g26n02:851589:851818 [0] NCCL INFO Channel 00/0 : 11[5] -> 12[0] [receive] via NET/IB/0
g26n02:851589:851818 [0] NCCL INFO Channel 02/0 : 11[5] -> 12[0] [receive] via NET/IB/0
g26n02:851589:851818 [0] NCCL INFO Channel 00/0 : 12[0] -> 13[1] via P2P/IPC
g26n02:851589:851818 [0] NCCL INFO Channel 02/0 : 12[0] -> 13[1] via P2P/IPC
g26n02:851589:851818 [0] NCCL INFO Channel 01/0 : 12[0] -> 21[3] [send] via NET/IB/2
g26n02:851589:851818 [0] NCCL INFO Channel 03/0 : 12[0] -> 21[3] [send] via NET/IB/2
g26n02:851589:851818 [0] NCCL INFO Connected all rings
g26n02:851589:851818 [0] NCCL INFO Channel 01/0 : 12[0] -> 13[1] via P2P/IPC
g26n02:851589:851818 [0] NCCL INFO Channel 03/0 : 12[0] -> 13[1] via P2P/IPC
g26n02:851589:851818 [0] NCCL INFO Channel 01/0 : 12[0] -> 14[2] via P2P/IPC
g26n02:851589:851818 [0] NCCL INFO Channel 03/0 : 12[0] -> 14[2] via P2P/IPC
g26n02:851589:851818 [0] NCCL INFO Channel 02/0 : 7[1] -> 12[0] [receive] via NET/IB/0
g26n02:851589:851818 [0] NCCL INFO Channel 00/0 : 12[0] -> 18[0] [send] via NET/IB/0
g26n02:851589:851818 [0] NCCL INFO Channel 00/0 : 12[0] -> 25[1] [send] via NET/IB/0
g26n02:851589:851818 [0] NCCL INFO Channel 00/0 : 25[1] -> 12[0] [receive] via NET/IB/0
g26n02:851589:851818 [0] NCCL INFO Channel 00/0 : 18[0] -> 12[0] [receive] via NET/IB/0
g26n02:851589:851818 [0] NCCL INFO Channel 02/0 : 12[0] -> 7[1] [send] via NET/IB/0
g26n02:851589:851818 [0] NCCL INFO Connected all trees
g26n02:851589:851818 [0] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g26n02:851589:851818 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g26n02:851589:851818 [0] NCCL INFO comm 0x15fc445b0 rank 12 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x28395441218594c5 - Init COMPLETE
g26n02:851589:851909 [0] NCCL INFO Using network IB
g26n02:851589:851909 [0] NCCL INFO comm 0x163091f30 rank 3 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0x864be1a757d9f276 - Init START
g26n02:851589:851909 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g26n02:851589:851909 [0] NCCL INFO Trees [0] 4/5/-1->3->7 [1] 4/-1/-1->3->2
g26n02:851589:851909 [0] NCCL INFO P2P Chunksize set to 131072
g26n02:851589:851909 [0] NCCL INFO Channel 00/0 : 2[2] -> 3[0] [receive] via NET/IB/0
g26n02:851589:851909 [0] NCCL INFO Channel 01/0 : 2[2] -> 3[0] [receive] via NET/IB/0
g26n02:851589:851909 [0] NCCL INFO Channel 00/0 : 3[0] -> 4[4] via P2P/IPC
g26n02:851589:851909 [0] NCCL INFO Channel 01/0 : 3[0] -> 4[4] via P2P/IPC
g26n02:851589:851909 [0] NCCL INFO Connected all rings
g26n02:851589:851909 [0] NCCL INFO Channel 00/0 : 3[0] -> 5[2] [send] via NET/IB/0
g26n02:851589:851909 [0] NCCL INFO Channel 00/0 : 3[0] -> 7[4] [send] via NET/IB/0
g26n02:851589:851909 [0] NCCL INFO Channel 00/0 : 7[4] -> 3[0] [receive] via NET/IB/0
g26n02:851589:851909 [0] NCCL INFO Channel 00/0 : 5[2] -> 3[0] [receive] via NET/IB/0
g26n02:851589:851909 [0] NCCL INFO Channel 01/0 : 3[0] -> 2[2] [send] via NET/IB/0
g26n02:851589:851909 [0] NCCL INFO Connected all trees
g26n02:851589:851909 [0] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g26n02:851589:851909 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g26n02:851589:851909 [0] NCCL INFO comm 0x163091f30 rank 3 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0x864be1a757d9f276 - Init COMPLETE
g26n02:851589:851923 [0] NCCL INFO Using network IB
g26n02:851589:851923 [0] NCCL INFO comm 0x162b2be20 rank 1 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x51bdf7b6b8c3aecb - Init START
g26n02:851589:851923 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g26n02:851589:851923 [0] NCCL INFO Trees [0] -1/-1/-1->1->2 [1] 2/0/-1->1->3
g26n02:851589:851923 [0] NCCL INFO P2P Chunksize set to 131072
g26n02:851589:851923 [0] NCCL INFO Channel 00/0 : 0[4] -> 1[0] [receive] via NET/IB/0
g26n02:851589:851923 [0] NCCL INFO Channel 01/0 : 0[4] -> 1[0] [receive] via NET/IB/0
g26n02:851589:851923 [0] NCCL INFO Channel 00/0 : 1[0] -> 2[2] [send] via NET/IB/0
g26n02:851589:851923 [0] NCCL INFO Channel 01/0 : 1[0] -> 2[2] [send] via NET/IB/0
g26n02:851589:851923 [0] NCCL INFO Connected all rings
g26n02:851589:851923 [0] NCCL INFO Channel 01/0 : 1[0] -> 3[4] [send] via NET/IB/0
g26n02:851589:851923 [0] NCCL INFO Channel 01/0 : 3[4] -> 1[0] [receive] via NET/IB/0
g26n02:851589:851923 [0] NCCL INFO Channel 00/0 : 2[2] -> 1[0] [receive] via NET/IB/0
g26n02:851589:851923 [0] NCCL INFO Channel 01/0 : 2[2] -> 1[0] [receive] via NET/IB/0
g26n02:851589:851923 [0] NCCL INFO Channel 01/0 : 1[0] -> 0[4] [send] via NET/IB/0
g26n02:851589:851923 [0] NCCL INFO Connected all trees
g26n02:851589:851923 [0] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g26n02:851589:851923 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g26n02:851589:851923 [0] NCCL INFO comm 0x162b2be20 rank 1 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x51bdf7b6b8c3aecb - Init COMPLETE
[1] 2/0/-1->1->3
g26n02:851591:851927 [2] NCCL INFO P2P Chunksize set to 131072
g26n02:851591:851927 [2] NCCL INFO Channel 00/0 : 0[0] -> 1[2] [receive] via NET/IB/0
g26n02:851591:851927 [2] NCCL INFO Channel 01/0 : 0[0] -> 1[2] [receive] via NET/IB/0
g26n02:851591:851927 [2] NCCL INFO Channel 00/0 : 1[2] -> 2[4] [send] via NET/IB/0
g26n02:851591:851927 [2] NCCL INFO Channel 01/0 : 1[2] -> 2[4] [send] via NET/IB/0
g26n02:851591:851927 [2] NCCL INFO Connected all rings
g26n02:851591:851927 [2] NCCL INFO Channel 01/0 : 1[2] -> 3[0] [send] via NET/IB/0
g26n02:851591:851927 [2] NCCL INFO Channel 01/0 : 3[0] -> 1[2] [receive] via NET/IB/0
g26n02:851591:851927 [2] NCCL INFO Channel 00/0 : 2[4] -> 1[2] [receive] via NET/IB/0
g26n02:851591:851927 [2] NCCL INFO Channel 01/0 : 2[4] -> 1[2] [receive] via NET/IB/0
g26n02:851591:851927 [2] NCCL INFO Channel 01/0 : 1[2] -> 0[0] [send] via NET/IB/0
g26n02:851591:851927 [2] NCCL INFO Connected all trees
g26n02:851591:851927 [2] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g26n02:851591:851927 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g26n02:851591:851927 [2] NCCL INFO comm 0x1564d9600 rank 1 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0x8442a1c07b0a8edb - Init COMPLETE
g31n01:617758:617758 [2] NCCL INFO cudaDriverVersion 12020
g31n01:617758:617758 [2] NCCL INFO Bootstrap : Using ib0:10.41.17.228<0>
g31n01:617758:617758 [2] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g31n01:617758:617758 [2] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g31n01:617758:617993 [2] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.228<0>
g31n01:617758:617993 [2] NCCL INFO Using network IB
g31n01:617758:617993 [2] NCCL INFO comm 0x163903b90 rank 56 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x28395441218594c5 - Init START
g31n01:617758:617993 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g31n01:617758:617993 [2] NCCL INFO Trees [0] 57/-1/-1->56->55 [1] -1/-1/-1->56->54 [2] 57/-1/-1->56->55 [3] -1/-1/-1->56->54
g31n01:617758:617993 [2] NCCL INFO P2P Chunksize set to 131072
g31n01:617758:617993 [2] NCCL INFO Channel 00/0 : 56[2] -> 57[3] via P2P/IPC
g31n01:617758:617993 [2] NCCL INFO Channel 02/0 : 56[2] -> 57[3] via P2P/IPC
g31n01:617758:617993 [2] NCCL INFO Channel 01/0 : 56[2] -> 55[1] via P2P/IPC
g31n01:617758:617993 [2] NCCL INFO Channel 03/0 : 56[2] -> 55[1] via P2P/IPC
g31n01:617758:617993 [2] NCCL INFO Connected all rings
g31n01:617758:617993 [2] NCCL INFO Channel 01/0 : 56[2] -> 54[0] via P2P/IPC
g31n01:617758:617993 [2] NCCL INFO Channel 03/0 : 56[2] -> 54[0] via P2P/IPC
g31n01:617758:617993 [2] NCCL INFO Channel 00/0 : 56[2] -> 55[1] via P2P/IPC
g31n01:617758:617993 [2] NCCL INFO Channel 02/0 : 56[2] -> 55[1] via P2P/IPC
g31n01:617758:617993 [2] NCCL INFO Connected all trees
g31n01:617758:617993 [2] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g31n01:617758:617993 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g31n01:617758:617993 [2] NCCL INFO comm 0x163903b90 rank 56 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x28395441218594c5 - Init COMPLETE
g31n01:617758:618082 [2] NCCL INFO Using network IB
g31n01:617758:618082 [2] NCCL INFO comm 0x165684730 rank 14 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0x864be1a757d9f276 - Init START
g31n01:617758:618082 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g31n01:617758:618082 [2] NCCL INFO Trees [0] -1/-1/-1->14->16 [1] 15/12/-1->14->17
g31n01:617758:618082 [2] NCCL INFO P2P Chunksize set to 131072
g31n01:617758:618082 [2] NCCL INFO Channel 00/0 : 13[4] -> 14[2] [receive] via NET/IB/0
g31n01:617758:618082 [2] NCCL INFO Channel 01/0 : 13[4] -> 14[2] [receive] via NET/IB/0
g31n01:617758:618082 [2] NCCL INFO Channel 00/0 : 14[2] -> 15[0] [send] via NET/IB/0
g31n01:617758:618082 [2] NCCL INFO Channel 01/0 : 14[2] -> 15[0] [send] via NET/IB/0
g31n01:617758:618082 [2] NCCL INFO Connected all rings
g31n01:617758:618082 [2] NCCL INFO Channel 01/0 : 12[0] -> 14[2] [receive] via NET/IB/0
g31n01:617758:618082 [2] NCCL INFO Channel 00/0 : 14[2] -> 16[4] [send] via NET/IB/0
g31n01:617758:618082 [2] NCCL INFO Channel 01/0 : 14[2] -> 17[2] [send] via NET/IB/0
g31n01:617758:618082 [2] NCCL INFO Channel 01/0 : 17[2] -> 14[2] [receive] via NET/IB/0
g31n01:617758:618082 [2] NCCL INFO Channel 00/0 : 16[4] -> 14[2] [receive] via NET/IB/0
g31n01:617758:618082 [2] NCCL INFO Channel 01/0 : 14[2] -> 12[0] [send] via NET/IB/0
g31n01:617758:618082 [2] NCCL INFO Channel 01/0 : 15[0] -> 14[2] [receive] via NET/IB/0
g31n01:617758:618082 [2] NCCL INFO Connected all trees
g31n01:617758:618082 [2] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g31n01:617758:618082 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n01:617758:618082 [2] NCCL INFO comm 0x165684730 rank 14 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0x864be1a757d9f276 - Init COMPLETE
g31n01:617758:618095 [2] NCCL INFO Using network IB
g31n01:617758:618095 [2] NCCL INFO comm 0x16655cdf0 rank 7 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0x76a32220a008087c - Init START
g31n01:617758:618095 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g31n01:617758:618095 [2] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] 9/5/-1->7->3
g31n01:617758:618095 [2] NCCL INFO P2P Chunksize set to 131072
g31n01:617758:618095 [2] NCCL INFO Channel 00/0 : 6[0] -> 7[2] [receive] via NET/IB/0
g31n01:617758:618095 [2] NCCL INFO Channel 01/0 : 6[0] -> 7[2] [receive] via NET/IB/0
g31n01:617758:618095 [2] NCCL INFO Channel 00/0 : 7[2] -> 8[4] [send] via NET/IB/0
g31n01:617758:618095 [2] NCCL INFO Channel 01/0 : 7[2] -> 8[4] [send] via NET/IB/0
g31n01:617758:618095 [2] NCCL INFO Connected all rings
g31n01:617758:618095 [2] NCCL INFO Channel 01/0 : 5[4] -> 7[2] [receive] via NET/IB/0
g31n01:617758:618095 [2] NCCL INFO Channel 01/0 : 7[2] -> 9[0] [send] via NET/IB/0
g31n01:617758:618095 [2] NCCL INFO Channel 01/0 : 3[0] -> 7[2] [receive] via NET/IB/0
g31n01:617758:618095 [2] NCCL INFO Channel 01/0 : 7[2] -> 3[0] [send] via NET/IB/0
g31n01:617758:618095 [2] NCCL INFO Channel 01/0 : 9[0] -> 7[2] [receive] via NET/IB/0
g31n01:617758:618095 [2] NCCL INFO Channel 01/0 : 7[2] -> 5[4] [send] via NET/IB/0
g31n01:617758:618095 [2] NCCL INFO Channel 00/0 : 7[2] -> 6[0] [send] via NET/IB/0
g31n01:617758:618095 [2] NCCL INFO Connected all trees
g31n01:617758:618095 [2] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g31n01:617758:618095 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n01:617758:618095 [2] NCCL INFO comm 0x16655cdf0 rank 7 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0x76a32220a008087c - Init COMPLETE
g31n01:617757:617757 [1] NCCL INFO cudaDriverVersion 12020
g31n01:617757:617757 [1] NCCL INFO Bootstrap : Using ib0:10.41.17.228<0>
g31n01:617757:617757 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g31n01:617757:617757 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g31n01:617757:617995 [1] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.228<0>
g31n01:617757:617995 [1] NCCL INFO Using network IB
g31n01:617757:617995 [1] NCCL INFO comm 0x1356e49e0 rank 55 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x28395441218594c5 - Init START
g31n01:617757:617995 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g31n01:617757:617995 [1] NCCL INFO Trees [0] 56/-1/-1->55->54 [1] 54/-1/-1->55->59 [2] 56/60/-1->55->54 [3] 54/-1/-1->55->59
g31n01:617757:617995 [1] NCCL INFO P2P Chunksize set to 131072
g31n01:617757:617995 [1] NCCL INFO Channel 00/0 : 55[1] -> 56[2] via P2P/IPC
g31n01:617757:617995 [1] NCCL INFO Channel 02/0 : 55[1] -> 56[2] via P2P/IPC
g31n01:617757:617995 [1] NCCL INFO Channel 01/0 : 55[1] -> 54[0] via P2P/IPC
g31n01:617757:617995 [1] NCCL INFO Channel 03/0 : 55[1] -> 54[0] via P2P/IPC
g31n01:617757:617995 [1] NCCL INFO Connected all rings
g31n01:617757:617995 [1] NCCL INFO Channel 01/0 : 55[1] -> 59[5] via P2P/IPC
g31n01:617757:617995 [1] NCCL INFO Channel 03/0 : 55[1] -> 59[5] via P2P/IPC
g31n01:617757:617995 [1] NCCL INFO Channel 02/0 : 55[1] -> 60[0] [send] via NET/IB/0
g31n01:617757:617995 [1] NCCL INFO Channel 02/0 : 60[0] -> 55[1] [receive] via NET/IB/0
g31n01:617757:617995 [1] NCCL INFO Channel 00/0 : 55[1] -> 54[0] via P2P/IPC
g31n01:617757:617995 [1] NCCL INFO Channel 02/0 : 55[1] -> 54[0] via P2P/IPC
g31n01:617757:617995 [1] NCCL INFO Connected all trees
g31n01:617757:617995 [1] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g31n01:617757:617995 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g31n01:617757:617995 [1] NCCL INFO comm 0x1356e49e0 rank 55 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x28395441218594c5 - Init COMPLETE
g31n01:617757:618080 [1] NCCL INFO Using network IB
g31n01:617757:618080 [1] NCCL INFO comm 0x1385b46f0 rank 13 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0x15c93a2ee1bf52db - Init START
g31n01:617757:618080 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g31n01:617757:618080 [1] NCCL INFO Trees [0] 14/-1/-1->13->15 [1] 14/12/-1->13->16
g31n01:617757:618080 [1] NCCL INFO P2P Chunksize set to 131072
g31n01:617757:618080 [1] NCCL INFO Channel 00/0 : 12[3] -> 13[1] [receive] via NET/IB/2
g31n01:617757:618080 [1] NCCL INFO Channel 01/0 : 12[3] -> 13[1] [receive] via NET/IB/2
g31n01:617757:618080 [1] NCCL INFO Channel 00/0 : 13[1] -> 14[5] via P2P/IPC
g31n01:617757:618080 [1] NCCL INFO Channel 01/0 : 13[1] -> 14[5] via P2P/IPC
g31n01:617757:618080 [1] NCCL INFO Connected all rings
g31n01:617757:618080 [1] NCCL INFO Channel 00/0 : 13[1] -> 15[3] [send] via NET/IB/2
g31n01:617757:618080 [1] NCCL INFO Channel 01/0 : 13[1] -> 16[1] [send] via NET/IB/2
g31n01:617757:618080 [1] NCCL INFO Channel 01/0 : 16[1] -> 13[1] [receive] via NET/IB/2
g31n01:617757:618080 [1] NCCL INFO Channel 00/0 : 15[3] -> 13[1] [receive] via NET/IB/2
g31n01:617757:618080 [1] NCCL INFO Channel 01/0 : 13[1] -> 12[3] [send] via NET/IB/2
g31n01:617757:618080 [1] NCCL INFO Connected all trees
g31n01:617757:618080 [1] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g31n01:617757:618080 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n01:617757:618080 [1] NCCL INFO comm 0x1385b46f0 rank 13 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0x15c93a2ee1bf52db - Init COMPLETE
g31n01:617757:618097 [1] NCCL INFO Using network IB
g31n01:617757:618097 [1] NCCL INFO comm 0x137fd9df0 rank 6 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x6876d4a326688ade - Init START
g31n01:617757:618097 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g31n01:617757:618097 [1] NCCL INFO Trees [0] 5/7/-1->6->4 [1] -1/-1/-1->6->5
g31n01:617757:618097 [1] NCCL INFO P2P Chunksize set to 131072
g31n01:617757:618097 [1] NCCL INFO Channel 00/0 : 5[5] -> 6[1] [receive] via NET/IB/2
g31n01:617757:618097 [1] NCCL INFO Channel 01/0 : 5[5] -> 6[1] [receive] via NET/IB/2
g31n01:617757:618097 [1] NCCL INFO Channel 00/0 : 6[1] -> 7[3] [send] via NET/IB/2
g31n01:617757:618097 [1] NCCL INFO Channel 01/0 : 6[1] -> 7[3] [send] via NET/IB/2
g31n01:617757:618097 [1] NCCL INFO Connected all rings
g31n01:617757:618097 [1] NCCL INFO Channel 00/0 : 4[3] -> 6[1] [receive] via NET/IB/2
g31n01:617757:618097 [1] NCCL INFO Channel 00/0 : 6[1] -> 4[3] [send] via NET/IB/2
g31n01:617757:618097 [1] NCCL INFO Channel 00/0 : 7[3] -> 6[1] [receive] via NET/IB/2
g31n01:617757:618097 [1] NCCL INFO Channel 00/0 : 6[1] -> 5[5] [send] via NET/IB/2
g31n01:617757:618097 [1] NCCL INFO Channel 01/0 : 6[1] -> 5[5] [send] via NET/IB/2
g31n01:617757:618097 [1] NCCL INFO Connected all trees
g31n01:617757:618097 [1] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g31n01:617757:618097 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n01:617757:618097 [1] NCCL INFO comm 0x137fd9df0 rank 6 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x6876d4a326688ade - Init COMPLETE
g25n18:898854:899259 [2] NCCL INFO Using network IB
g25n18:898854:899259 [2] NCCL INFO comm 0x150a9f360 rank 0 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0xe45164e09488020a - Init START
g25n18:898854:899259 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g25n18:898854:899259 [2] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
g25n18:898854:899259 [2] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
g25n18:898854:899259 [2] NCCL INFO Trees [0] 12/-1/-1->0->-1 [1] -1/-1/-1->0->1
g25n18:898854:899259 [2] NCCL INFO P2P Chunksize set to 131072
g25n18:898854:899259 [2] NCCL INFO Channel 00/0 : 23[4] -> 0[2] [receive] via NET/IB/0
g25n18:898854:899259 [2] NCCL INFO Channel 01/0 : 23[4] -> 0[2] [receive] via NET/IB/0
g25n18:898854:899259 [2] NCCL INFO Channel 00/0 : 0[2] -> 1[0] [send] via NET/IB/0
g25n18:898854:899259 [2] NCCL INFO Channel 01/0 : 0[2] -> 1[0] [send] via NET/IB/0
g25n18:898854:899259 [2] NCCL INFO Connected all rings
g25n18:898854:899259 [2] NCCL INFO Channel 00/0 : 12[2] -> 0[2] [receive] via NET/IB/0
g25n18:898854:899259 [2] NCCL INFO Channel 00/0 : 0[2] -> 12[2] [send] via NET/IB/0
g25n18:898854:899259 [2] NCCL INFO Channel 01/0 : 1[0] -> 0[2] [receive] via NET/IB/0
g25n18:898854:899259 [2] NCCL INFO Connected all trees
g25n18:898854:899259 [2] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g25n18:898854:899259 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g25n18:898854:899259 [2] NCCL INFO comm 0x150a9f360 rank 0 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0xe45164e09488020a - Init COMPLETE
g25n18:898854:899290 [2] NCCL INFO Using network IB
g25n18:898854:899290 [2] NCCL INFO comm 0x15071de90 rank 0 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0xaca7b5a6b2aa6a19 - Init START
g25n18:898854:899290 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g25n18:898854:899290 [2] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7   8   9  10  11
g25n18:898854:899290 [2] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7   8   9  10  11
g25n18:898854:899290 [2] NCCL INFO Trees [0] 8/-1/-1->0->-1 [1] -1/-1/-1->0->1
g25n18:898854:899290 [2] NCCL INFO P2P Chunksize set to 131072
g25n18:898854:899290 [2] NCCL INFO Channel 00/0 : 11[0] -> 0[2] [receive] via NET/IB/0
g25n18:898854:899290 [2] NCCL INFO Channel 01/0 : 11[0] -> 0[2] [receive] via NET/IB/0
g25n18:898854:899290 [2] NCCL INFO Channel 00/0 : 0[2] -> 1[4] [send] via NET/IB/0
g25n18:898854:899290 [2] NCCL INFO Channel 01/0 : 0[2] -> 1[4] [send] via NET/IB/0
g25n18:898854:899290 [2] NCCL INFO Connected all rings
g25n18:898854:899290 [2] NCCL INFO Channel 00/0 : 8[0] -> 0[2] [receive] via NET/IB/0
g25n18:898854:899290 [2] NCCL INFO Channel 00/0 : 0[2] -> 8[0] [send] via NET/IB/0
g25n18:898854:899290 [2] NCCL INFO Channel 01/0 : 1[4] -> 0[2] [receive] via NET/IB/0
g25n18:898854:899290 [2] NCCL INFO Connected all trees
g25n18:898854:899290 [2] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g25n18:898854:899290 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g25n18:898854:899290 [2] NCCL INFO comm 0x15071de90 rank 0 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0xaca7b5a6b2aa6a19 - Init COMPLETE
g31n03:688513:688513 [4] NCCL INFO cudaDriverVersion 12020
g31n03:688513:688513 [4] NCCL INFO Bootstrap : Using ib0:10.41.17.230<0>
g31n03:688513:688513 [4] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g31n03:688513:688513 [4] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g31n03:688513:688747 [4] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.230<0>
g31n03:688513:688747 [4] NCCL INFO Using network IB
g31n03:688513:688747 [4] NCCL INFO comm 0x16a434a40 rank 70 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x28395441218594c5 - Init START
g31n03:688513:688747 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g31n03:688513:688747 [4] NCCL INFO Trees [0] 71/-1/-1->70->69 [1] 69/-1/-1->70->64 [2] 71/-1/-1->70->69 [3] 69/58/-1->70->45
g31n03:688513:688747 [4] NCCL INFO P2P Chunksize set to 131072
g31n03:688513:688747 [4] NCCL INFO Channel 00/0 : 70[4] -> 71[5] via P2P/IPC
g31n03:688513:688747 [4] NCCL INFO Channel 01/0 : 70[4] -> 71[5] via P2P/IPC
g31n03:688513:688747 [4] NCCL INFO Channel 02/0 : 70[4] -> 71[5] via P2P/IPC
g31n03:688513:688747 [4] NCCL INFO Channel 03/0 : 70[4] -> 71[5] via P2P/IPC
g31n03:688513:688747 [4] NCCL INFO Connected all rings
g31n03:688513:688747 [4] NCCL INFO Channel 01/0 : 64[4] -> 70[4] [receive] via NET/IB/3
g31n03:688513:688747 [4] NCCL INFO Channel 03/0 : 58[4] -> 70[4] [receive] via NET/IB/3
g31n03:688513:688747 [4] NCCL INFO Channel 03/0 : 45[3] -> 70[4] [receive] via NET/IB/3
g31n03:688513:688747 [4] NCCL INFO Channel 03/0 : 70[4] -> 45[3] [send] via NET/IB/3
g31n03:688513:688747 [4] NCCL INFO Channel 03/0 : 70[4] -> 58[4] [send] via NET/IB/3
g31n03:688513:688747 [4] NCCL INFO Channel 01/0 : 70[4] -> 64[4] [send] via NET/IB/3
g31n03:688513:688747 [4] NCCL INFO Channel 00/0 : 70[4] -> 69[3] via P2P/IPC
g31n03:688513:688747 [4] NCCL INFO Channel 01/0 : 70[4] -> 69[3] via P2P/IPC
g31n03:688513:688747 [4] NCCL INFO Channel 02/0 : 70[4] -> 69[3] via P2P/IPC
g31n03:688513:688747 [4] NCCL INFO Channel 03/0 : 70[4] -> 69[3] via P2P/IPC
g31n03:688513:688747 [4] NCCL INFO Connected all trees
g31n03:688513:688747 [4] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g31n03:688513:688747 [4] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g31n03:688513:688747 [4] NCCL INFO comm 0x16a434a40 rank 70 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x28395441218594c5 - Init COMPLETE
g31n03:688513:688828 [4] NCCL INFO Using network IB
g31n03:688513:688828 [4] NCCL INFO comm 0x16d1a3de0 rank 17 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xe45164e09488020a - Init START
g31n03:688513:688828 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g31n03:688513:688828 [4] NCCL INFO Trees [0] -1/-1/-1->17->16 [1] 19/-1/-1->17->16
g31n03:688513:688828 [4] NCCL INFO P2P Chunksize set to 131072
g31n03:688513:688828 [4] NCCL INFO Channel 00/0 : 17[4] -> 18[2] [send] via NET/IB/1
g31n03:688513:688828 [4] NCCL INFO Channel 01/0 : 17[4] -> 18[2] [send] via NET/IB/1
g31n03:688513:688828 [4] NCCL INFO Connected all rings
g31n03:688513:688828 [4] NCCL INFO Channel 01/0 : 17[4] -> 19[0] [send] via NET/IB/0
g31n03:688513:688828 [4] NCCL INFO Channel 01/0 : 19[0] -> 17[4] [receive] via NET/IB/0
g31n03:688513:688828 [4] NCCL INFO Channel 00/0 : 17[4] -> 16[0] via P2P/IPC
g31n03:688513:688828 [4] NCCL INFO Channel 01/0 : 17[4] -> 16[0] via P2P/IPC
g31n03:688513:688828 [4] NCCL INFO Connected all trees
g31n03:688513:688828 [4] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g31n03:688513:688828 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n03:688513:688828 [4] NCCL INFO comm 0x16d1a3de0 rank 17 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xe45164e09488020a - Init COMPLETE
g31n03:688513:688846 [4] NCCL INFO Using network IB
g31n03:688513:688846 [4] NCCL INFO comm 0x16e184020 rank 8 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x8442a1c07b0a8edb - Init START
g31n03:688513:688846 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g31n03:688513:688846 [4] NCCL INFO Trees [0] 4/10/-1->8->0 [1] -1/-1/-1->8->9
g31n03:688513:688846 [4] NCCL INFO P2P Chunksize set to 131072
g31n03:688513:688846 [4] NCCL INFO Channel 00/0 : 7[2] -> 8[4] [receive] via NET/IB/1
g31n03:688513:688846 [4] NCCL INFO Channel 01/0 : 7[2] -> 8[4] [receive] via NET/IB/1
g31n03:688513:688846 [4] NCCL INFO Channel 00/0 : 8[4] -> 9[0] [send] via NET/IB/1
g31n03:688513:688846 [4] NCCL INFO Channel 01/0 : 8[4] -> 9[0] [send] via NET/IB/1
g31n03:688513:688846 [4] NCCL INFO Connected all rings
g31n03:688513:688846 [4] NCCL INFO Channel 00/0 : 8[4] -> 10[2] [send] via NET/IB/1
g31n03:688513:688846 [4] NCCL INFO Channel 00/0 : 4[2] -> 8[4] [receive] via NET/IB/1
g31n03:688513:688846 [4] NCCL INFO Channel 00/0 : 8[4] -> 0[0] [send] via NET/IB/1
g31n03:688513:688846 [4] NCCL INFO Channel 00/0 : 0[0] -> 8[4] [receive] via NET/IB/1
g31n03:688513:688846 [4] NCCL INFO Channel 00/0 : 8[4] -> 4[2] [send] via NET/IB/1
g31n03:688513:688846 [4] NCCL INFO Channel 00/0 : 10[2] -> 8[4] [receive] via NET/IB/1
g31n03:688513:688846 [4] NCCL INFO Channel 01/0 : 9[0] -> 8[4] [receive] via NET/IB/1
g31n03:688513:688846 [4] NCCL INFO Connected all trees
g31n03:688513:688846 [4] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g31n03:688513:688846 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n03:688513:688846 [4] NCCL INFO comm 0x16e184020 rank 8 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x8442a1c07b0a8edb - Init COMPLETE
g31n03:688510:688510 [1] NCCL INFO cudaDriverVersion 12020
g31n03:688510:688510 [1] NCCL INFO Bootstrap : Using ib0:10.41.17.230<0>
g31n03:688510:688510 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g31n03:688510:688510 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g31n03:688510:688743 [1] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.230<0>
g31n03:688510:688743 [1] NCCL INFO Using network IB
g31n03:688510:688743 [1] NCCL INFO comm 0x138414230 rank 67 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x28395441218594c5 - Init START
g31n03:688510:688743 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g31n03:688510:688743 [1] NCCL INFO Trees [0] 68/-1/-1->67->66 [1] 66/-1/-1->67->71 [2] 68/78/-1->67->66 [3] 66/-1/-1->67->71
g31n03:688510:688743 [1] NCCL INFO P2P Chunksize set to 131072
g31n03:688510:688743 [1] NCCL INFO Channel 00/0 : 67[1] -> 68[2] via P2P/IPC
g31n03:688510:688743 [1] NCCL INFO Channel 02/0 : 67[1] -> 68[2] via P2P/IPC
g31n03:688510:688743 [1] NCCL INFO Channel 01/0 : 67[1] -> 66[0] via P2P/IPC
g31n03:688510:688743 [1] NCCL INFO Channel 03/0 : 67[1] -> 66[0] via P2P/IPC
g31n03:688510:688743 [1] NCCL INFO Connected all rings
g31n03:688510:688743 [1] NCCL INFO Channel 01/0 : 67[1] -> 71[5] via P2P/IPC
g31n03:688510:688743 [1] NCCL INFO Channel 03/0 : 67[1] -> 71[5] via P2P/IPC
g31n03:688510:688743 [1] NCCL INFO Channel 02/0 : 67[1] -> 78[0] [send] via NET/IB/0
g31n03:688510:688743 [1] NCCL INFO Channel 02/0 : 78[0] -> 67[1] [receive] via NET/IB/0
g31n03:688510:688743 [1] NCCL INFO Channel 00/0 : 67[1] -> 66[0] via P2P/IPC
g31n03:688510:688743 [1] NCCL INFO Channel 02/0 : 67[1] -> 66[0] via P2P/IPC
g31n03:688510:688743 [1] NCCL INFO Connected all trees
g31n03:688510:688743 [1] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g31n03:688510:688743 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g31n03:688510:688743 [1] NCCL INFO comm 0x138414230 rank 67 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x28395441218594c5 - Init COMPLETE
g31n03:688510:688826 [1] NCCL INFO Using network IB
g31n03:688510:688826 [1] NCCL INFO comm 0x13b089070 rank 16 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0x15c93a2ee1bf52db - Init START
g31n03:688510:688826 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g31n03:688510:688826 [1] NCCL INFO Trees [0] 17/-1/-1->16->15 [1] 17/13/-1->16->11
g31n03:688510:688826 [1] NCCL INFO P2P Chunksize set to 131072
g31n03:688510:688826 [1] NCCL INFO Channel 00/0 : 15[3] -> 16[1] [receive] via NET/IB/2
g31n03:688510:688826 [1] NCCL INFO Channel 01/0 : 15[3] -> 16[1] [receive] via NET/IB/2
g31n03:688510:688826 [1] NCCL INFO Channel 00/0 : 16[1] -> 17[5] via P2P/IPC
g31n03:688510:688826 [1] NCCL INFO Channel 01/0 : 16[1] -> 17[5] via P2P/IPC
g31n03:688510:688826 [1] NCCL INFO Connected all rings
g31n03:688510:688826 [1] NCCL INFO Channel 01/0 : 13[1] -> 16[1] [receive] via NET/IB/2
g31n03:688510:688826 [1] NCCL INFO Channel 01/0 : 11[5] -> 16[1] [receive] via NET/IB/2
g31n03:688510:688826 [1] NCCL INFO Channel 01/0 : 16[1] -> 11[5] [send] via NET/IB/2
g31n03:688510:688826 [1] NCCL INFO Channel 01/0 : 16[1] -> 13[1] [send] via NET/IB/2
g31n03:688510:688826 [1] NCCL INFO Channel 00/0 : 16[1] -> 15[3] [send] via NET/IB/2
g31n03:688510:688826 [1] NCCL INFO Connected all trees
g31n03:688510:688826 [1] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g31n03:688510:688826 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n03:688510:688826 [1] NCCL INFO comm 0x13b089070 rank 16 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0x15c93a2ee1bf52db - Init COMPLETE
g31n03:688510:688845 [1] NCCL INFO Using network IB
g31n03:688510:688845 [1] NCCL INFO comm 0x13b06a6e0 rank 8 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x1542a3b966c3490b - Init START
g31n03:688510:688845 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g31n03:688510:688845 [1] NCCL INFO Trees [0] 4/10/-1->8->0 [1] -1/-1/-1->8->9
g31n03:688510:688845 [1] NCCL INFO P2P Chunksize set to 131072
g31n03:688510:688845 [1] NCCL INFO Channel 00/0 : 7[5] -> 8[1] [receive] via NET/IB/2
g31n03:688510:688845 [1] NCCL INFO Channel 01/0 : 7[5] -> 8[1] [receive] via NET/IB/2
g31n03:688510:688845 [1] NCCL INFO Channel 00/0 : 8[1] -> 9[3] [send] via NET/IB/2
g31n03:688510:688845 [1] NCCL INFO Channel 01/0 : 8[1] -> 9[3] [send] via NET/IB/2
g31n03:688510:688845 [1] NCCL INFO Connected all rings
g31n03:688510:688845 [1] NCCL INFO Channel 00/0 : 8[1] -> 10[5] [send] via NET/IB/2
g31n03:688510:688845 [1] NCCL INFO Channel 00/0 : 4[5] -> 8[1] [receive] via NET/IB/2
g31n03:688510:688845 [1] NCCL INFO Channel 00/0 : 8[1] -> 0[3] [send] via NET/IB/2
g31n03:688510:688845 [1] NCCL INFO Channel 00/0 : 0[3] -> 8[1] [receive] via NET/IB/2
g31n03:688510:688845 [1] NCCL INFO Channel 00/0 : 8[1] -> 4[5] [send] via NET/IB/2
g31n03:688510:688845 [1] NCCL INFO Channel 00/0 : 10[5] -> 8[1] [receive] via NET/IB/2
g31n03:688510:688845 [1] NCCL INFO Channel 01/0 : 9[3] -> 8[1] [receive] via NET/IB/2
g31n03:688510:688845 [1] NCCL INFO Connected all trees
g31n03:688510:688845 [1] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g31n03:688510:688845 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n03:688510:688845 [1] NCCL INFO comm 0x13b06a6e0 rank 8 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x1542a3b966c3490b - Init COMPLETE
g31n03:688511:688511 [2] NCCL INFO cudaDriverVersion 12020
g31n03:688511:688511 [2] NCCL INFO Bootstrap : Using ib0:10.41.17.230<0>
g31n03:688511:688511 [2] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g31n03:688511:688511 [2] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g31n03:688511:688742 [2] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.230<0>
g31n03:688511:688742 [2] NCCL INFO Using network IB
g31n03:688511:688742 [2] NCCL INFO comm 0x139784260 rank 68 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x28395441218594c5 - Init START
g31n03:688511:688742 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g31n03:688511:688742 [2] NCCL INFO Trees [0] 69/-1/-1->68->67 [1] -1/-1/-1->68->66 [2] 69/-1/-1->68->67 [3] -1/-1/-1->68->66
g31n03:688511:688742 [2] NCCL INFO P2P Chunksize set to 131072
g31n03:688511:688742 [2] NCCL INFO Channel 00/0 : 68[2] -> 69[3] via P2P/IPC
g31n03:688511:688742 [2] NCCL INFO Channel 02/0 : 68[2] -> 69[3] via P2P/IPC
g31n03:688511:688742 [2] NCCL INFO Channel 01/0 : 68[2] -> 67[1] via P2P/IPC
g31n03:688511:688742 [2] NCCL INFO Channel 03/0 : 68[2] -> 67[1] via P2P/IPC
g31n03:688511:688742 [2] NCCL INFO Connected all rings
g31n03:688511:688742 [2] NCCL INFO Channel 01/0 : 68[2] -> 66[0] via P2P/IPC
g31n03:688511:688742 [2] NCCL INFO Channel 03/0 : 68[2] -> 66[0] via P2P/IPC
g31n03:688511:688742 [2] NCCL INFO Channel 00/0 : 68[2] -> 67[1] via P2P/IPC
g31n03:688511:688742 [2] NCCL INFO Channel 02/0 : 68[2] -> 67[1] via P2P/IPC
g31n03:688511:688742 [2] NCCL INFO Connected all trees
g31n03:688511:688742 [2] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g31n03:688511:688742 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g31n03:688511:688742 [2] NCCL INFO comm 0x139784260 rank 68 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x28395441218594c5 - Init COMPLETE
g31n03:688511:688829 [2] NCCL INFO Using network IB
g31n03:688511:688829 [2] NCCL INFO comm 0x13c65da60 rank 17 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0x864be1a757d9f276 - Init START
g31n03:688511:688829 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g31n03:688511:688829 [2] NCCL INFO Trees [0] -1/-1/-1->17->15 [1] 20/14/-1->17->11
g31n03:688511:688829 [2] NCCL INFO P2P Chunksize set to 131072
g31n03:688511:688829 [2] NCCL INFO Channel 00/0 : 16[4] -> 17[2] [receive] via NET/IB/0
g31n03:688511:688829 [2] NCCL INFO Channel 01/0 : 16[4] -> 17[2] [receive] via NET/IB/0
g31n03:688511:688829 [2] NCCL INFO Channel 00/0 : 17[2] -> 18[0] [send] via NET/IB/0
g31n03:688511:688829 [2] NCCL INFO Channel 01/0 : 17[2] -> 18[0] [send] via NET/IB/0
g31n03:688511:688829 [2] NCCL INFO Connected all rings
g31n03:688511:688829 [2] NCCL INFO Channel 00/0 : 15[0] -> 17[2] [receive] via NET/IB/0
g31n03:688511:688829 [2] NCCL INFO Channel 01/0 : 14[2] -> 17[2] [receive] via NET/IB/0
g31n03:688511:688829 [2] NCCL INFO Channel 01/0 : 17[2] -> 20[2] [send] via NET/IB/0
g31n03:688511:688829 [2] NCCL INFO Channel 01/0 : 11[2] -> 17[2] [receive] via NET/IB/0
g31n03:688511:688829 [2] NCCL INFO Channel 01/0 : 17[2] -> 11[2] [send] via NET/IB/0
g31n03:688511:688829 [2] NCCL INFO Channel 01/0 : 20[2] -> 17[2] [receive] via NET/IB/0
g31n03:688511:688829 [2] NCCL INFO Channel 01/0 : 17[2] -> 14[2] [send] via NET/IB/0
g31n03:688511:688829 [2] NCCL INFO Channel 00/0 : 17[2] -> 15[0] [send] via NET/IB/0
g31n03:688511:688829 [2] NCCL INFO Connected all trees
g31n03:688511:688829 [2] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g31n03:688511:688829 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n03:688511:688829 [2] NCCL INFO comm 0x13c65da60 rank 17 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0x864be1a757d9f276 - Init COMPLETE
g31n03:688511:688842 [2] NCCL INFO Using network IB
g31n03:688511:688842 [2] NCCL INFO comm 0x13b6797f0 rank 8 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0x51bdf7b6b8c3aecb - Init START
g31n03:688511:688842 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g31n03:688511:688842 [2] NCCL INFO Trees [0] 4/10/-1->8->0 [1] -1/-1/-1->8->9
g31n03:688511:688842 [2] NCCL INFO P2P Chunksize set to 131072
g31n03:688511:688842 [2] NCCL INFO Channel 00/0 : 7[0] -> 8[2] [receive] via NET/IB/0
g31n03:688511:688842 [2] NCCL INFO Channel 01/0 : 7[0] -> 8[2] [receive] via NET/IB/0
g31n03:688511:688842 [2] NCCL INFO Channel 00/0 : 8[2] -> 9[4] [send] via NET/IB/0
g31n03:688511:688842 [2] NCCL INFO Channel 01/0 : 8[2] -> 9[4] [send] via NET/IB/0
g31n03:688511:688842 [2] NCCL INFO Connected all rings
g31n03:688511:688842 [2] NCCL INFO Channel 00/0 : 8[2] -> 10[0] [send] via NET/IB/0
g31n03:688511:688842 [2] NCCL INFO Channel 00/0 : 4[0] -> 8[2] [receive] via NET/IB/0
g31n03:688511:688842 [2] NCCL INFO Channel 00/0 : 8[2] -> 0[4] [send] via NET/IB/0
g31n03:688511:688842 [2] NCCL INFO Channel 00/0 : 0[4] -> 8[2] [receive] via NET/IB/0
g31n03:688511:688842 [2] NCCL INFO Channel 00/0 : 8[2] -> 4[0] [send] via NET/IB/0
g31n03:688511:688842 [2] NCCL INFO Channel 00/0 : 10[0] -> 8[2] [receive] via NET/IB/0
g31n03:688511:688842 [2] NCCL INFO Channel 01/0 : 9[4] -> 8[2] [receive] via NET/IB/0
g31n03:688511:688842 [2] NCCL INFO Connected all trees
g31n03:688511:688842 [2] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g31n03:688511:688842 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n03:688511:688842 [2] NCCL INFO comm 0x13b6797f0 rank 8 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0x51bdf7b6b8c3aecb - Init COMPLETE
g32n03:753096:753096 [0] NCCL INFO cudaDriverVersion 12020
g32n03:753096:753096 [0] NCCL INFO Bootstrap : Using ib0:10.41.17.248<0>
g32n03:753096:753096 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g32n03:753096:753096 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g32n03:753096:753329 [0] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.248<0>
g32n03:753096:753329 [0] NCCL INFO Using network IB
g32n03:753096:753329 [0] NCCL INFO comm 0x13ade4100 rank 90 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x28395441218594c5 - Init START
g32n03:753096:753329 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g32n03:753096:753329 [0] NCCL INFO Trees [0] 91/-1/-1->90->84 [1] 92/-1/-1->90->91 [2] 91/42/-1->90->-1 [3] 92/-1/-1->90->91
g32n03:753096:753329 [0] NCCL INFO P2P Chunksize set to 131072
g32n03:753096:753329 [0] NCCL INFO Channel 00/0 : 89[5] -> 90[0] [receive] via NET/IB/0
g32n03:753096:753329 [0] NCCL INFO Channel 02/0 : 89[5] -> 90[0] [receive] via NET/IB/0
g32n03:753096:753329 [0] NCCL INFO Channel 00/0 : 90[0] -> 91[1] via P2P/IPC
g32n03:753096:753329 [0] NCCL INFO Channel 02/0 : 90[0] -> 91[1] via P2P/IPC
g32n03:753096:753329 [0] NCCL INFO Channel 01/0 : 90[0] -> 3[3] [send] via NET/IB/2
g32n03:753096:753329 [0] NCCL INFO Channel 03/0 : 90[0] -> 3[3] [send] via NET/IB/2
g32n03:753096:753329 [0] NCCL INFO Connected all rings
g32n03:753096:753329 [0] NCCL INFO Channel 01/0 : 90[0] -> 91[1] via P2P/IPC
g32n03:753096:753329 [0] NCCL INFO Channel 03/0 : 90[0] -> 91[1] via P2P/IPC
g32n03:753096:753329 [0] NCCL INFO Channel 01/0 : 90[0] -> 92[2] via P2P/IPC
g32n03:753096:753329 [0] NCCL INFO Channel 03/0 : 90[0] -> 92[2] via P2P/IPC
g32n03:753096:753329 [0] NCCL INFO Channel 00/0 : 84[0] -> 90[0] [receive] via NET/IB/0
g32n03:753096:753329 [0] NCCL INFO Channel 02/0 : 42[0] -> 90[0] [receive] via NET/IB/0
g32n03:753096:753329 [0] NCCL INFO Channel 02/0 : 90[0] -> 42[0] [send] via NET/IB/0
g32n03:753096:753329 [0] NCCL INFO Channel 00/0 : 90[0] -> 84[0] [send] via NET/IB/0
g32n03:753096:753329 [0] NCCL INFO Connected all trees
g32n03:753096:753329 [0] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g32n03:753096:753329 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g32n03:753096:753329 [0] NCCL INFO comm 0x13ade4100 rank 90 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x28395441218594c5 - Init COMPLETE
g32n03:753096:753413 [0] NCCL INFO Using network IB
g32n03:753096:753413 [0] NCCL INFO comm 0x13da374a0 rank 22 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0xe45164e09488020a - Init START
g32n03:753096:753413 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g32n03:753096:753413 [0] NCCL INFO Trees [0] 23/-1/-1->22->21 [1] 23/10/-1->22->-1
g32n03:753096:753413 [0] NCCL INFO P2P Chunksize set to 131072
g32n03:753096:753413 [0] NCCL INFO Channel 00/0 : 21[2] -> 22[0] [receive] via NET/IB/0
g32n03:753096:753413 [0] NCCL INFO Channel 01/0 : 21[2] -> 22[0] [receive] via NET/IB/0
g32n03:753096:753413 [0] NCCL INFO Channel 00/0 : 22[0] -> 23[4] via P2P/IPC
g32n03:753096:753413 [0] NCCL INFO Channel 01/0 : 22[0] -> 23[4] via P2P/IPC
g32n03:753096:753413 [0] NCCL INFO Connected all rings
g32n03:753096:753413 [0] NCCL INFO Channel 01/0 : 10[0] -> 22[0] [receive] via NET/IB/0
g32n03:753096:753413 [0] NCCL INFO Channel 01/0 : 22[0] -> 10[0] [send] via NET/IB/0
g32n03:753096:753413 [0] NCCL INFO Channel 00/0 : 22[0] -> 21[2] [send] via NET/IB/0
g32n03:753096:753413 [0] NCCL INFO Connected all trees
g32n03:753096:753413 [0] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g32n03:753096:753413 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g32n03:753096:753413 [0] NCCL INFO comm 0x13da374a0 rank 22 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0xe45164e09488020a - Init COMPLETE
g32n03:753096:753431 [0] NCCL INFO Using network IB
g32n03:753096:753431 [0] NCCL INFO comm 0x13cb032a0 rank 11 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0xaca7b5a6b2aa6a19 - Init START
g32n03:753096:753431 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g32n03:753096:753431 [0] NCCL INFO Trees [0] -1/-1/-1->11->10 [1] 3/-1/-1->11->-1
g32n03:753096:753431 [0] NCCL INFO P2P Chunksize set to 131072
g32n03:753096:753431 [0] NCCL INFO Channel 00/0 : 10[4] -> 11[0] [receive] via NET/IB/0
g32n03:753096:753431 [0] NCCL INFO Channel 01/0 : 10[4] -> 11[0] [receive] via NET/IB/0
g32n03:753096:753431 [0] NCCL INFO Channel 00/0 : 11[0] -> 0[2] [send] via NET/IB/0
g32n03:753096:753431 [0] NCCL INFO Channel 01/0 : 11[0] -> 0[2] [send] via NET/IB/0
g32n03:753096:753431 [0] NCCL INFO Connected all rings
g32n03:753096:753431 [0] NCCL INFO Channel 01/0 : 11[0] -> 3[2] [send] via NET/IB/0
g32n03:753096:753431 [0] NCCL INFO Channel 01/0 : 3[2] -> 11[0] [receive] via NET/IB/0
g32n03:753096:753431 [0] NCCL INFO Channel 00/0 : 11[0] -> 10[4] [send] via NET/IB/0
g32n03:753096:753431 [0] NCCL INFO Connected all trees
g32n03:753096:753431 [0] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g32n03:753096:753431 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g32n03:753096:753431 [0] NCCL INFO comm 0x13cb032a0 rank 11 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0xaca7b5a6b2aa6a19 - Init COMPLETE
g26n01:811331:811665 [0] NCCL INFO Using network IB
g26n01:811331:811665 [0] NCCL INFO comm 0x14c83c330 rank 0 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x8442a1c07b0a8edb - Init START
g26n01:811331:811665 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g26n01:811331:811665 [0] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7   8   9  10  11
g26n01:811331:811665 [0] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7   8   9  10  11
g26n01:811331:811665 [0] NCCL INFO Trees [0] 8/-1/-1->0->-1 [1] -1/-1/-1->0->1
g26n01:811331:811665 [0] NCCL INFO P2P Chunksize set to 131072
g26n01:811331:811665 [0] NCCL INFO Channel 00/0 : 11[4] -> 0[0] [receive] via NET/IB/0
g26n01:811331:811665 [0] NCCL INFO Channel 01/0 : 11[4] -> 0[0] [receive] via NET/IB/0
g26n01:811331:811665 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[2] [send] via NET/IB/0
g26n01:811331:811665 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[2] [send] via NET/IB/0
g26n01:811331:811665 [0] NCCL INFO Connected all rings
g26n01:811331:811665 [0] NCCL INFO Channel 00/0 : 8[4] -> 0[0] [receive] via NET/IB/0
g26n01:811331:811665 [0] NCCL INFO Channel 00/0 : 0[0] -> 8[4] [send] via NET/IB/0
g26n01:811331:811665 [0] NCCL INFO Channel 01/0 : 1[2] -> 0[0] [receive] via NET/IB/0
g26n01:811331:811665 [0] NCCL INFO Connected all trees
g26n01:811331:811665 [0] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g26n01:811331:811665 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g26n01:811331:811665 [0] NCCL INFO comm 0x14c83c330 rank 0 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x8442a1c07b0a8edb - Init COMPLETE
g27n17:842067:842067 [4] NCCL INFO cudaDriverVersion 12020
g27n17:842067:842067 [4] NCCL INFO Bootstrap : Using ib0:10.41.17.172<0>
g27n17:842067:842067 [4] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g27n17:842067:842067 [4] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g27n17:842067:842299 [4] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.172<0>
g27n17:842067:842299 [4] NCCL INFO Using network IB
g27n17:842067:842299 [4] NCCL INFO comm 0x1300439d0 rank 22 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x28395441218594c5 - Init START
g27n17:842067:842299 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g27n17:842067:842299 [4] NCCL INFO Trees [0] 23/-1/-1->22->21 [1] 21/-1/-1->22->16 [2] 23/-1/-1->22->21 [3] 21/10/-1->22->46
g27n17:842067:842299 [4] NCCL INFO P2P Chunksize set to 131072
g27n17:842067:842299 [4] NCCL INFO Channel 00/0 : 22[4] -> 23[5] via P2P/IPC
g27n17:842067:842299 [4] NCCL INFO Channel 01/0 : 22[4] -> 23[5] via P2P/IPC
g27n17:842067:842299 [4] NCCL INFO Channel 02/0 : 22[4] -> 23[5] via P2P/IPC
g27n17:842067:842299 [4] NCCL INFO Channel 03/0 : 22[4] -> 23[5] via P2P/IPC
g27n17:842067:842299 [4] NCCL INFO Connected all rings
g27n17:842067:842299 [4] NCCL INFO Channel 01/0 : 16[4] -> 22[4] [receive] via NET/IB/3
g27n17:842067:842299 [4] NCCL INFO Channel 03/0 : 10[4] -> 22[4] [receive] via NET/IB/3
g27n17:842067:842299 [4] NCCL INFO Channel 03/0 : 22[4] -> 46[4] [send] via NET/IB/3
g27n17:842067:842299 [4] NCCL INFO Channel 03/0 : 46[4] -> 22[4] [receive] via NET/IB/3
g27n17:842067:842299 [4] NCCL INFO Channel 03/0 : 22[4] -> 10[4] [send] via NET/IB/3
g27n17:842067:842299 [4] NCCL INFO Channel 01/0 : 22[4] -> 16[4] [send] via NET/IB/3
g27n17:842067:842299 [4] NCCL INFO Channel 00/0 : 22[4] -> 21[3] via P2P/IPC
g27n17:842067:842299 [4] NCCL INFO Channel 01/0 : 22[4] -> 21[3] via P2P/IPC
g27n17:842067:842299 [4] NCCL INFO Channel 02/0 : 22[4] -> 21[3] via P2P/IPC
g27n17:842067:842299 [4] NCCL INFO Channel 03/0 : 22[4] -> 21[3] via P2P/IPC
g27n17:842067:842299 [4] NCCL INFO Connected all trees
g27n17:842067:842299 [4] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g27n17:842067:842299 [4] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g27n17:842067:842299 [4] NCCL INFO comm 0x1300439d0 rank 22 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x28395441218594c5 - Init COMPLETE
g27n17:842067:842384 [4] NCCL INFO Using network IB
g27n17:842067:842384 [4] NCCL INFO comm 0x132dd4020 rank 5 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xe45164e09488020a - Init START
g27n17:842067:842384 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g27n17:842067:842384 [4] NCCL INFO Trees [0] -1/-1/-1->5->4 [1] 7/-1/-1->5->4
g27n17:842067:842384 [4] NCCL INFO P2P Chunksize set to 131072
g27n17:842067:842384 [4] NCCL INFO Channel 00/0 : 5[4] -> 6[2] [send] via NET/IB/1
g27n17:842067:842384 [4] NCCL INFO Channel 01/0 : 5[4] -> 6[2] [send] via NET/IB/1
g27n17:842067:842384 [4] NCCL INFO Connected all rings
g27n17:842067:842384 [4] NCCL INFO Channel 01/0 : 5[4] -> 7[0] [send] via NET/IB/0
g27n17:842067:842384 [4] NCCL INFO Channel 01/0 : 7[0] -> 5[4] [receive] via NET/IB/0
g27n17:842067:842384 [4] NCCL INFO Channel 00/0 : 5[4] -> 4[0] via P2P/IPC
g27n17:842067:842384 [4] NCCL INFO Channel 01/0 : 5[4] -> 4[0] via P2P/IPC
g27n17:842067:842384 [4] NCCL INFO Connected all trees
g27n17:842067:842384 [4] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g27n17:842067:842384 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g27n17:842067:842384 [4] NCCL INFO comm 0x132dd4020 rank 5 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xe45164e09488020a - Init COMPLETE
g27n17:842067:842405 [4] NCCL INFO Using network IB
g27n17:842067:842405 [4] NCCL INFO comm 0x131d0aaf0 rank 2 nranks 12 cudaDev 4 nvmlDev 4g27n17:842061:842061 [1] NCCL INFO cudaDriverVersion 12020
g27n17:842061:842061 [1] NCCL INFO Bootstrap : Using ib0:10.41.17.172<0>
g27n17:842061:842061 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g27n17:842061:842061 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g27n17:842061:842294 [1] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.172<0>
g27n17:842061:842294 [1] NCCL INFO Using network IB
g27n17:842061:842294 [1] NCCL INFO comm 0x1457f37a0 rank 19 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x28395441218594c5 - Init START
g27n17:842061:842294 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g27n17:842061:842294 [1] NCCL INFO Trees [0] 20/-1/-1->19->18 [1] 18/-1/-1->19->23 [2] 20/30/-1->19->18 [3] 18/-1/-1->19->23
g27n17:842061:842294 [1] NCCL INFO P2P Chunksize set to 131072
g27n17:842061:842294 [1] NCCL INFO Channel 00/0 : 19[1] -> 20[2] via P2P/IPC
g27n17:842061:842294 [1] NCCL INFO Channel 02/0 : 19[1] -> 20[2] via P2P/IPC
g27n17:842061:842294 [1] NCCL INFO Channel 01/0 : 19[1] -> 18[0] via P2P/IPC
g27n17:842061:842294 [1] NCCL INFO Channel 03/0 : 19[1] -> 18[0] via P2P/IPC
g27n17:842061:842294 [1] NCCL INFO Connected all rings
g27n17:842061:842294 [1] NCCL INFO Channel 01/0 : 19[1] -> 23[5] via P2P/IPC
g27n17:842061:842294 [1] NCCL INFO Channel 03/0 : 19[1] -> 23[5] via P2P/IPC
g27n17:842061:842294 [1] NCCL INFO Channel 02/0 : 19[1] -> 30[0] [send] via NET/IB/0
g27n17:842061:842294 [1] NCCL INFO Channel 02/0 : 30[0] -> 19[1] [receive] via NET/IB/0
g27n17:842061:842294 [1] NCCL INFO Channel 00/0 : 19[1] -> 18[0] via P2P/IPC
g27n17:842061:842294 [1] NCCL INFO Channel 02/0 : 19[1] -> 18[0] via P2P/IPC
g27n17:842061:842294 [1] NCCL INFO Connected all trees
g27n17:842061:842294 [1] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g27n17:842061:842294 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g27n17:842061:842294 [1] NCCL INFO comm 0x1457f37a0 rank 19 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x28395441218594c5 - Init COMPLETE
g27n17:842061:842383 [1] NCCL INFO Using network IB
g27n17:842061:842383 [1] NCCL INFO comm 0x1486a5830 rank 4 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0x15c93a2ee1bf52db - Init START
g27n17:842061:842383 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g27n17:842061:842383 [1] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/1/-1->4->10
g27n17:842061:842383 [1] NCCL INFO P2P Chunksize set to 131072
g27n17:842061:842383 [1] NCCL INFO Channel 00/0 : 3[3] -> 4[1] [receive] via NET/IB/2
g27n17:842061:842383 [1] NCCL INFO Channel 01/0 : 3[3] -> 4[1] [receive] via NET/IB/2
g27n17:842061:842383 [1] NCCL INFO Channel 00/0 : 4[1] -> 5[5] via P2P/IPC
g27n17:842061:842383 [1] NCCL INFO Channel 01/0 : 4[1] -> 5[5] via P2P/IPC
g27n17:842061:842383 [1] NCCL INFO Connected all rings
g27n17:842061:842383 [1] NCCL INFO Channel 01/0 : 1[1] -> 4[1] [receive] via NET/IB/2
g27n17:842061:842383 [1] NCCL INFO Channel 01/0 : 4[1] -> 10[1] [send] via NET/IB/2
g27n17:842061:842383 [1] NCCL INFO Channel 01/0 : 10[1] -> 4[1] [receive] via NET/IB/2
g27n17:842061:842383 [1] NCCL INFO Channel 01/0 : 4[1] -> 1[1] [send] via NET/IB/2
g27n17:842061:842383 [1] NCCL INFO Channel 00/0 : 4[1] -> 3[3] [send] via NET/IB/2
g27n17:842061:842383 [1] NCCL INFO Connected all trees
g27n17:842061:842383 [1] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g27n17:842061:842383 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g27n17:842061:842383 [1] NCCL INFO comm 0x1486a5830 rank 4 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0x15c93a2ee1bf52db - Init COMPLETE
g27n17:842061:842400 [1] NCCL INFO Using network IB
g27n17:842061:842400 [1] NCCL INFO comm 0x1484b05a0 rank 2 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x1542a3b966c3490b - Init START
g27n17:842061:842400 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g27n17:842061:g27n17:842060:842060 [0] NCCL INFO cudaDriverVersion 12020
g27n17:842060:842060 [0] NCCL INFO Bootstrap : Using ib0:10.41.17.172<0>
g27n17:842060:842060 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g27n17:842060:842060 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g27n17:842060:842296 [0] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.172<0>
g27n17:842060:842296 [0] NCCL INFO Using network IB
g27n17:842060:842296 [0] NCCL INFO comm 0x14a2743f0 rank 18 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x28395441218594c5 - Init START
g27n17:842060:842296 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g27n17:842060:842296 [0] NCCL INFO Trees [0] 19/-1/-1->18->12 [1] 20/-1/-1->18->19 [2] 19/6/-1->18->42 [3] 20/-1/-1->18->19
g27n17:842060:842296 [0] NCCL INFO P2P Chunksize set to 131072
g27n17:842060:842296 [0] NCCL INFO Channel 00/0 : 17[5] -> 18[0] [receive] via NET/IB/0
g27n17:842060:842296 [0] NCCL INFO Channel 02/0 : 17[5] -> 18[0] [receive] via NET/IB/0
g27n17:842060:842296 [0] NCCL INFO Channel 00/0 : 18[0] -> 19[1] via P2P/IPC
g27n17:842060:842296 [0] NCCL INFO Channel 02/0 : 18[0] -> 19[1] via P2P/IPC
g27n17:842060:842296 [0] NCCL INFO Channel 01/0 : 18[0] -> 27[3] [send] via NET/IB/2
g27n17:842060:842296 [0] NCCL INFO Channel 03/0 : 18[0] -> 27[3] [send] via NET/IB/2
g27n17:842060:842296 [0] NCCL INFO Connected all rings
g27n17:842060:842296 [0] NCCL INFO Channel 01/0 : 18[0] -> 19[1] via P2P/IPC
g27n17:842060:842296 [0] NCCL INFO Channel 03/0 : 18[0] -> 19[1] via P2P/IPC
g27n17:842060:842296 [0] NCCL INFO Channel 01/0 : 18[0] -> 20[2] via P2P/IPC
g27n17:842060:842296 [0] NCCL INFO Channel 03/0 : 18[0] -> 20[2] via P2P/IPC
g27n17:842060:842296 [0] NCCL INFO Channel 00/0 : 12[0] -> 18[0] [receive] via NET/IB/0
g27n17:842060:842296 [0] NCCL INFO Channel 02/0 : 6[0] -> 18[0] [receive] via NET/IB/0
g27n17:842060:842296 [0] NCCL INFO Channel 02/0 : 18[0] -> 42[0] [send] via NET/IB/0
g27n17:842060:842296 [0] NCCL INFO Channel 02/0 : 42[0] -> 18[0] [receive] via NET/IB/0
g27n17:842060:842296 [0] NCCL INFO Channel 02/0 : 18[0] -> 6[0] [send] via NET/IB/0
g27n17:842060:842296 [0] NCCL INFO Channel 00/0 : 18[0] -> 12[0] [send] via NET/IB/0
g27n17:842060:842296 [0] NCCL INFO Connected all trees
g27n17:842060:842296 [0] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g27n17:842060:842296 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g27n17:842060:842296 [0] NCCL INFO comm 0x14a2743f0 rank 18 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x28395441218594c5 - Init COMPLETE
g27n17:842060:842387 [0] NCCL INFO Using network IB
g27n17:842060:842387 [0] NCCL INFO comm 0x14d084110 rank 4 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0xe45164e09488020a - Init START
g27n17:842060:842387 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g27n17:842060:842387 [0] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/1/-1->4->10
g27n17:842060:842387 [0] NCCL INFO P2P Chunksize set to 131072
g27n17:842060:842387 [0] NCCL INFO Channel 00/0 : 3[2] -> 4[0] [receive] via NET/IB/0
g27n17:842060:842387 [0] NCCL INFO Channel 01/0 : 3[2] -> 4[0] [receive] via NET/IB/0
g27n17:842060:842387 [0] NCCL INFO Channel 00/0 : 4[0] -> 5[4] via P2P/IPC
g27n17:842060:842387 [0] NCCL INFO Channel 01/0 : 4[0] -> 5[4] via P2P/IPC
g27n17:842060:842387 [0] NCCL INFO Connected all rings
g27n17:842060:842387 [0] NCCL INFO Channel 01/0 : 1[0] -> 4[0] [receive] via NET/IB/0
g27n17:842060:842387 [0] NCCL INFO Channel 01/0 : 4[0] -> 10[0] [send] via NET/IB/0
g27n17:842060:842387 [0] NCCL INFO Channel 01/0 : 10[0] -> 4[0] [receive] via NET/IB/0
g27n17:842060:842387 [0] NCCL INFO Channel 01/0 : 4[0] -> 1[0] [send] via NET/IB/0
g27n17:842060:842387 [0] NCCL INFO Channel 00/0 : 4[0] -> 3[2] [send] via NET/IB/0
g27n17:842060:842387 [0] NCCL INFO Connected all trees
g27n17:842060:842387 [0] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g27n17:8420842400 [1] NCCL INFO Trees [0] 1/3/-1->2->4 [1] -1/-1/-1->2->1
g27n17:842061:842400 [1] NCCL INFO P2P Chunksize set to 131072
g27n17:842061:842400 [1] NCCL INFO Channel 00/0 : 1[5] -> 2[1] [receive] via NET/IB/2
g27n17:842061:842400 [1] NCCL INFO Channel 01/0 : 1[5] -> 2[1] [receive] via NET/IB/2
g27n17:842061:842400 [1] NCCL INFO Channel 00/0 : 2[1] -> 3[3] [send] via NET/IB/2
g27n17:842061:842400 [1] NCCL INFO Channel 01/0 : 2[1] -> 3[3] [send] via NET/IB/2
g27n17:842061:842400 [1] NCCL INFO Connected all rings
g27n17:842061:842400 [1] NCCL INFO Channel 00/0 : 2[1] -> 4[5] [send] via NET/IB/2
g27n17:842061:842400 [1] NCCL INFO Channel 00/0 : 4[5] -> 2[1] [receive] via NET/IB/2
g27n17:842061:842400 [1] NCCL INFO Channel 00/0 : 3[3] -> 2[1] [receive] via NET/IB/2
g27n17:842061:842400 [1] NCCL INFO Channel 00/0 : 2[1] -> 1[5] [send] via NET/IB/2
g27n17:842061:842400 [1] NCCL INFO Channel 01/0 : 2[1] -> 1[5] [send] via NET/IB/2
g27n17:842061:842400 [1] NCCL INFO Connected all trees
g27n17:842061:842400 [1] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g27n17:842061:842400 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g27n17:842061:842400 [1] NCCL INFO comm 0x1484b05a0 rank 2 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x1542a3b966c3490b - Init COMPLETE
 busId 3504000 commId 0x8442a1c07b0a8edb - Init START
g27n17:842067:842405 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g27n17:842067:842405 [4] NCCL INFO Trees [0] 1/3/-1->2->4 [1] -1/-1/-1->2->1
g27n17:842067:842405 [4] NCCL INFO P2P Chunksize set to 131072
g27n17:842067:842405 [4] NCCL INFO Channel 00/0 : 1[2] -> 2[4] [receive] via NET/IB/1
g27n17:842067:842405 [4] NCCL INFO Channel 01/0 : 1[2] -> 2[4] [receive] via NET/IB/1
g27n17:842067:842405 [4] NCCL INFO Channel 00/0 : 2[4] -> 3[0] [send] via NET/IB/1
g27n17:842067:842405 [4] NCCL INFO Channel 01/0 : 2[4] -> 3[0] [send] via NET/IB/1
g27n17:842067:842405 [4] NCCL INFO Connected all rings
g27n17:842067:842405 [4] NCCL INFO Channel 00/0 : 2[4] -> 4[2] [send] via NET/IB/1
g27n17:842067:842405 [4] NCCL INFO Channel 00/0 : 4[2] -> 2[4] [receive] via NET/IB/1
g27n17:842067:842405 [4] NCCL INFO Channel 00/0 : 3[0] -> 2[4] [receive] via NET/IB/1
g27n17:842067:842405 [4] NCCL INFO Channel 00/0 : 2[4] -> 1[2] [send] via NET/IB/1
g27n17:842067:842405 [4] NCCL INFO Channel 01/0 : 2[4] -> 1[2] [send] via NET/IB/1
g27n17:842067:842405 [4] NCCL INFO Connected all trees
g27n17:842067:842405 [4] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g27n17:842067:842405 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g27n17:842067:842405 [4] NCCL INFO comm 0x131d0aaf0 rank 2 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x8442a1c07b0a8edb - Init COMPLETE
g27n17:842063:842063 [2] NCCL INFO cudaDriverVersion 12020
g27n17:842063:842063 [2] NCCL INFO Bootstrap : Using ib0:10.41.17.172<0>
g27n17:842063:842063 [2] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g27n17:842063:842063 [2] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g27n17:842063:842295 [2] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.172<0>
g27n17:842063:842295 [2] NCCL INFO Using network IB
g27n17:842063:842295 [2] NCCL INFO comm 0x1276d42a0 rank 20 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x28395441218594c5 - Init START
g27n17:842063:842295 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g27n17:842063:842295 [2] NCCL INFO Trees [0] 21/-1/-1->20->19 [1] -1/-1/-1->20->18 [2] 21/-1/-1->20->19 [3] -1/-1/-1->20->18
g27n17:842063:842295 [2] NCCL INFO P2P Chunksize set to 131072
g27n17:842063:842295 [2] NCCL INFO Channel 00/0 : 20[2] -> 21[3] via P2P/IPC
g27n17:842063:842295 [2] NCCL INFO Channel 02/0 : 20[2] -> 21[3] via P2P/IPC
g27n17:842063:842295 [2] NCCL INFO Channel 01/0 : 20[2] -> 19[1] via P2P/IPC
g27n17:842063:842295 [2] NCCL INFO Channel 03/0 : 20[2] -> 19[1] via P2P/IPC
g27n17:842063:842295 [2] NCCL INFO Connected all rings
g27n17:842063:842295 [2] NCCL INFO Channel 01/0 : 20[2] -> 18[0] via P2P/IPC
g27n17:842063:842295 [2] NCCL INFO Channel 03/0 : 20[2] -> 18[0] via P2P/IPC
g27n17:842063:842295 [2] NCCL INFO Channel 00/0 : 20[2] -> 19[1] via P2P/IPC
g27n17:842063:842295 [2] NCCL INFO Channel 02/0 : 20[2] -> 19[1] via P2P/IPC
g27n17:842063:842295 [2] NCCL INFO Connected all trees
g27n17:842063:842295 [2] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g27n17:842063:842295 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g27n17:842063:842295 [2] NCCL INFO comm 0x1276d42a0 rank 20 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x28395441218594c5 - Init COMPLETE
g27n17:842063:842385 [2] NCCL INFO Using network IB
g27n17:842063:842385 [2] NCCL INFO comm 0x12a4ad9f0 rank 5 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0x864be1a757d9f276 - Init START
g27n17:842063:842385 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g27n17:842063:842385 [2] NCCL INFO Trees [0] -1/-1/-1->5->3 [1] 8/2/-1->5->11
g27n17:842063:842385 [2] NCCL INFO P2P Chunksize set to 131072
g27n17:842063:842385 [2] NCCL INFO Channel 00/0 : 4[4] -> 5[2] [receive] via NET/IB/0
g27n17:842063:842385 [2] NCCL INFO Channel 01/0 : 4[4] -> 5[2] [receive] via NET/IB/0
g27n17:842063:842385 [2] NCCL INFO Channel 00/0 : 5[2] -> 6[0] [send] via NET/IB/0
g27n17:842063:842385 [2] NCCL INFO Channel 01/0 : 5[2] -> 6[0] [send] via NET/IB/0
g27n17:842063:842385 [2] NCCL INFO Connected all rings
g27n17:842063:842385 [2] NCCL INFO Channel 00/0 : 3[0] -> 5[2] [receive] via NET/IB/0
g27n17:842063:842385 [2] NCCL INFO Channel 01/0 : 2[2] -> 5[2] [receive] via NET/IB/0
g27n17:842063:842385 [2] NCCL INFO Channel 01/0 : 5[2] -> 8[2] [send] via NET/IB/0
g27n17:842063:842385 [2] NCCL INFO Channel 01/0 : 5[2] -> 11[2] [send] via NET/IB/0
g27n17:842063:842385 [2] NCCL INFO Channel 01/0 : 11[2] -> 5[2] [receive] via NET/IB/0
g27n17:842063:842385 [2] NCCL INFO Channel 01/0 : 8[2] -> 5[2] [receive] via NET/IB/0
g27n17:842063:842385 [2] NCCL INFO Channel 01/0 : 5[2] -> 2[2] [send] via NET/IB/0
g27n17:842063:842385 [2] NCCL INFO Channel 00/0 : 5[2] -> 3[0] [send] via NET/IB/0
g27n17:842063:842385 [2] NCCL INFO Connected all trees
g27n17:842063:842385 [2] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g27n17:842063:842385 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g27n17:842063:842385 [2] NCCL INFO comm 0x12a4ad9f0 rank 5 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0x864be1a757d9f276 - Init COMPLETE
g27n17:842063:842401 [2] NCCL INFO Using network IB
g27n17:842063:842401 [2] NCCL INFO comm 0x129fb69a0 rank 2 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0x560:842387 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g27n17:842060:842387 [0] NCCL INFO comm 0x14d084110 rank 4 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0xe45164e09488020a - Init COMPLETE
g27n17:842060:842404 [0] NCCL INFO Using network IB
g27n17:842060:842404 [0] NCCL INFO comm 0x14ceccd50 rank 2 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0xaca7b5a6b2aa6a19 - Init START
g27n17:842060:842404 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g27n17:842060:842404 [0] NCCL INFO Trees [0] 1/3/-1->2->4 [1] -1/-1/-1->2->1
g27n17:842060:842404 [0] NCCL INFO P2P Chunksize set to 131072
g27n17:842060:842404 [0] NCCL INFO Channel 00/0 : 1[4] -> 2[0] [receive] via NET/IB/0
g27n17:842060:842404 [0] NCCL INFO Channel 01/0 : 1[4] -> 2[0] [receive] via NET/IB/0
g27n17:842060:842404 [0] NCCL INFO Channel 00/0 : 2[0] -> 3[2] [send] via NET/IB/0
g27n17:842060:842404 [0] NCCL INFO Channel 01/0 : 2[0] -> 3[2] [send] via NET/IB/0
g27n17:842060:842404 [0] NCCL INFO Connected all rings
g27n17:842060:842404 [0] NCCL INFO Channel 00/0 : 2[0] -> 4[4] [send] via NET/IB/0
g27n17:842060:842404 [0] NCCL INFO Channel 00/0 : 4[4] -> 2[0] [receive] via NET/IB/0
g27n17:842060:842404 [0] NCCL INFO Channel 00/0 : 3[2] -> 2[0] [receive] via NET/IB/0
g27n17:842060:842404 [0] NCCL INFO Channel 00/0 : 2[0] -> 1[4] [send] via NET/IB/0
g27n17:842060:842404 [0] NCCL INFO Channel 01/0 : 2[0] -> 1[4] [send] via NET/IB/0
g27n17:842060:842404 [0] NCCL INFO Connected all trees
g27n17:842060:842404 [0] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g27n17:842060:842404 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g27n17:842060:842404 [0] NCCL INFO comm 0x14ceccd50 rank 2 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0xaca7b5a6b2aa6a19 - Init COMPLETE
1bdf7b6b8c3aecb - Init START
g27n17:842063:842401 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g27n17:842063:842401 [2] NCCL INFO Trees [0] 1/3/-1->2->4 [1] -1/-1/-1->2->1
g27n17:842063:842401 [2] NCCL INFO P2P Chunksize set to 131072
g27n17:842063:842401 [2] NCCL INFO Channel 00/0 : 1[0] -> 2[2] [receive] via NET/IB/0
g27n17:842063:842401 [2] NCCL INFO Channel 01/0 : 1[0] -> 2[2] [receive] via NET/IB/0
g27n17:842063:842401 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[4] [send] via NET/IB/0
g27n17:842063:842401 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[4] [send] via NET/IB/0
g27n17:842063:842401 [2] NCCL INFO Connected all rings
g27n17:842063:842401 [2] NCCL INFO Channel 00/0 : 2[2] -> 4[0] [send] via NET/IB/0
g27n17:842063:842401 [2] NCCL INFO Channel 00/0 : 4[0] -> 2[2] [receive] via NET/IB/0
g27n17:842063:842401 [2] NCCL INFO Channel 00/0 : 3[4] -> 2[2] [receive] via NET/IB/0
g27n17:842063:842401 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[0] [send] via NET/IB/0
g27n17:842063:842401 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[0] [send] via NET/IB/0
g27n17:842063:842401 [2] NCCL INFO Connected all trees
g27n17:842063:842401 [2] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g27n17:842063:842401 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g27n17:842063:842401 [2] NCCL INFO comm 0x129fb69a0 rank 2 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0x51bdf7b6b8c3aecb - Init COMPLETE
g31n04:688226:688226 [2] NCCL INFO cudaDriverVersion 12020
g31n04:688226:688226 [2] NCCL INFO Bootstrap : Using ib0:10.41.17.231<0>
g31n04:688226:688226 [2] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g31n04:688226:688226 [2] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g31n04:688226:688455 [2] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.231<0>
g31n04:688226:688455 [2] NCCL INFO Using network IB
g31n04:688226:688455 [2] NCCL INFO comm 0x1762541e0 rank 74 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x28395441218594c5 - Init START
g31n04:688226:688455 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g31n04:688226:688455 [2] NCCL INFO Trees [0] 75/-1/-1->74->73 [1] -1/-1/-1->74->72 [2] 75/-1/-1->74->73 [3] -1/-1/-1->74->72
g31n04:688226:688455 [2] NCCL INFO P2P Chunksize set to 131072
g31n04:688226:688455 [2] NCCL INFO Channel 00/0 : 74[2] -> 75[3] via P2P/IPC
g31n04:688226:688455 [2] NCCL INFO Channel 02/0 : 74[2] -> 75[3] via P2P/IPC
g31n04:688226:688455 [2] NCCL INFO Channel 01/0 : 74[2] -> 73[1] via P2P/IPC
g31n04:688226:688455 [2] NCCL INFO Channel 03/0 : 74[2] -> 73[1] via P2P/IPC
g31n04:688226:688455 [2] NCCL INFO Connected all rings
g31n04:688226:688455 [2] NCCL INFO Channel 01/0 : 74[2] -> 72[0] via P2P/IPC
g31n04:688226:688455 [2] NCCL INFO Channel 03/0 : 74[2] -> 72[0] via P2P/IPC
g31n04:688226:688455 [2] NCCL INFO Channel 00/0 : 74[2] -> 73[1] via P2P/IPC
g31n04:688226:688455 [2] NCCL INFO Channel 02/0 : 74[2] -> 73[1] via P2P/IPC
g31n04:688226:688455 [2] NCCL INFO Connected all trees
g31n04:688226:688455 [2] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g31n04:688226:688455 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g31n04:688226:688455 [2] NCCL INFO comm 0x1762541e0 rank 74 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x28395441218594c5 - Init COMPLETE
g31n04:688226:688541 [2] NCCL INFO Using network IB
g31n04:688226:688541 [2] NCCL INFO comm 0x178f95470 rank 18 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0xe45164e09488020a - Init START
g31n04:688226:688541 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g31n04:688226:688541 [2] NCCL INFO Trees [0] 15/21/-1->18->12 [1] -1/-1/-1->18->19
g31n04:688226:688541 [2] NCCL INFO P2P Chunksize set to 131072
g31n04:688226:688541 [2] NCCL INFO Channel 00/0 : 17[4] -> 18[2] [receive] via NET/IB/0
g31n04:688226:688541 [2] NCCL INFO Channel 01/0 : 17[4] -> 18[2] [receive] via NET/IB/0
g31n04:688226:688541 [2] NCCL INFO Channel 00/0 : 18[2] -> 19[0] [send] via NET/IB/0
g31n04:688226:688541 [2] NCCL INFO Channel 01/0 : 18[2] -> 19[0] [send] via NET/IB/0
g31n04:688226:688541 [2] NCCL INFO Connected all rings
g31n04:688226:688541 [2] NCCL INFO Channel 00/0 : 15[2] -> 18[2] [receive] via NET/IB/0
g31n04:688226:688541 [2] NCCL INFO Channel 00/0 : 18[2] -> 21[2] [send] via NET/IB/0
g31n04:688226:688541 [2] NCCL INFO Channel 00/0 : 12[2] -> 18[2] [receive] via NET/IB/0
g31n04:688226:688541 [2] NCCL INFO Channel 00/0 : 18[2] -> 12[2] [send] via NET/IB/0
g31n04:688226:688541 [2] NCCL INFO Channel 00/0 : 21[2] -> 18[2] [receive] via NET/IB/0
g31n04:688226:688541 [2] NCCL INFO Channel 00/0 : 18[2] -> 15[2] [send] via NET/IB/0
g31n04:688226:688541 [2] NCCL INFO Channel 01/0 : 19[0] -> 18[2] [receive] via NET/IB/0
g31n04:688226:688541 [2] NCCL INFO Connected all trees
g31n04:688226:688541 [2] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g31n04:688226:688541 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n04:688226:688541 [2] NCCL INFO comm 0x178f95470 rank 18 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0xe45164e09488020a - Init COMPLETE
g31n04:688226:688563 [2] NCCL INFO Using network IB
g31n04:688226:688563 [2] NCCL INFO comm 0x178ebd120 rank 9 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0xaca7b5a6b2aa6a19 - Init START
g31n04:688226:688563 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g31n04:688226:688563 [2] NCCL INFO Trees [0] -1/-1/-1->9->10 [1] 10/8/-1->9->7
g31n04:688226:688563 [2] NCCL INFO P2P Chunksize set to 131072
g31n04:688226:688563 [2] NCCL INFO Channel 00/0 : 8[0] -> 9[2] [receive] via NET/IB/0
g31n04:688226:688563 [2] NCCL INFO Channel 01/0 : 8[0] -> 9[2] [receive] via NET/IB/0
g31n04:688226:688563 [2] NCCL INFO Channel 00/0 : 9[2] -> 10[4] [send] via NET/IB/0
g31n04:688226:688563 [2] NCCL INFO Channel 01/0 : 9[2] -> 10[4] [send] via NET/IB/0
g31n04:688226:688563 [2] NCCL INFO Connected all rings
g31n04:688226:688563 [2] NCCL INFO Channel 01/0 : 7[4] -> 9[2] [receive] via NET/IB/0
g31n04:688226:688563 [2] NCCL INFO Channel 01/0 : 9[2] -> 7[4] [send] via NET/IB/0
g31n04:688226:688563 [2] NCCL INFO Channel 00/0 : 10[4] -> 9[2] [receive] via NET/IB/0
g31n04:688226:688563 [2] NCCL INFO Channel 01/0 : 10[4] -> 9[2] [receive] via NET/IB/0
g31n04:688226:688563 [2] NCCL INFO Channel 01/0 : 9[2] -> 8[0] [send] via NET/IB/0
g31n04:688226:688563 [2] NCCL INFO Connected all trees
g31n04:688226:688563 [2] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g31n04:688226:688563 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n04:688226:688563 [2] NCCL INFO comm 0x178ebd120 rank 9 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0xaca7b5a6b2aa6a19 - Init COMPLETE
g31n02:736468:736468 [5] NCCL INFO cudaDriverVersion 12020
g31n02:736468:736468 [5] NCCL INFO Bootstrap : Using ib0:10.41.17.229<0>
g31n02:736468:736468 [5] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g31n02:736468:736468 [5] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g31n02:736468:736695 [5] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.229<0>
g31n02:736468:736695 [5] NCCL INFO Using network IB
g31n02:736468:736695 [5] NCCL INFO comm 0x155453490 rank 65 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x28395441218594c5 - Init START
g31n02:736468:736695 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g31n02:736468:736695 [5] NCCL INFO Trees [0] -1/-1/-1->65->64 [1] 61/-1/-1->65->63 [2] -1/-1/-1->65->64 [3] 61/-1/-1->65->63
g31n02:736468:736695 [5] NCCL INFO P2P Chunksize set to 131072
g31n02:736468:736695 [5] NCCL INFO Channel 00/0 : 65[5] -> 66[0] [send] via NET/IB/1
g31n02:736468:736695 [5] NCCL INFO Channel 02/0 : 65[5] -> 66[0] [send] via NET/IB/1
g31n02:736468:736695 [5] NCCL INFO Channel 01/0 : 65[5] -> 62[2] via P2P/IPC
g31n02:736468:736695 [5] NCCL INFO Channel 03/0 : 65[5] -> 62[2] via P2P/IPC
g31n02:736468:736695 [5] NCCL INFO Connected all rings
g31n02:736468:736695 [5] NCCL INFO Channel 01/0 : 65[5] -> 61[1] via P2P/IPC
g31n02:736468:736695 [5] NCCL INFO Channel 03/0 : 65[5] -> 61[1] via P2P/IPC
g31n02:736468:736695 [5] NCCL INFO Channel 01/0 : 65[5] -> 63[3] via P2P/IPC
g31n02:736468:736695 [5] NCCL INFO Channel 03/0 : 65[5] -> 63[3] via P2P/IPC
g31n02:736468:736695 [5] NCCL INFO Channel 00/0 : 65[5] -> 64[4] via P2P/IPC
g31n02:736468:736695 [5] NCCL INFO Channel 02/0 : 65[5] -> 64[4] via P2P/IPC
g31n02:736468:736695 [5] NCCL INFO Connected all trees
g31n02:736468:736695 [5] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g31n02:736468:736695 [5] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g31n02:736468:736695 [5] NCCL INFO comm 0x155453490 rank 65 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x28395441218594c5 - Init COMPLETE
g31n02:736468:736780 [5] NCCL INFO Using network IB
g31n02:736468:736780 [5] NCCL INFO comm 0x15816f530 rank 16 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xeee4c143812ce6e4 - Init START
g31n02:736468:736780 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g31n02:736468:736780 [5] NCCL INFO Trees [0] 14/-1/-1->16->15 [1] -1/-1/-1->16->15
g31n02:736468:736780 [5] NCCL INFO P2P Chunksize set to 131072
g31n02:736468:736780 [5] NCCL INFO Channel 00/0 : 16[5] -> 17[3] [send] via NET/IB/3
g31n02:736468:736780 [5] NCCL INFO Channel 01/0 : 16[5] -> 17[3] [send] via NET/IB/3
g31n02:736468:736780 [5] NCCL INFO Connected all rings
g31n02:736468:736780 [5] NCCL INFO Channel 00/0 : 14[3] -> 16[5] [receive] via NET/IB/2
g31n02:736468:736780 [5] NCCL INFO Channel 00/0 : 16[5] -> 14[3] [send] via NET/IB/2
g31n02:736468:736780 [5] NCCL INFO Channel 00/0 : 16[5] -> 15[1] via P2P/IPC
g31n02:736468:736780 [5] NCCL INFO Channel 01/0 : 16[5] -> 15[1] via P2P/IPC
g31n02:736468:736780 [5] NCCL INFO Connected all trees
g31n02:736468:736780 [5] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g31n02:736468:736780 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n02:736468:736780 [5] NCCL INFO comm 0x15816f530 rank 16 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xeee4c143812ce6e4 - Init COMPLETE
g31n02:736468:736800 [5] NCCL INFO Using network IB
g31n02:736468:736800 [5] NCCL INFO comm 0x1580fb1c0 rank 8 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x17fc85f125de53ec - Init START
g31n02:736468:736800 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g31n02:736468:736800 [5] NCCL INFO Trees [0] 4/10/-1->8->0 [1] -1/-1/-1->8->9
g31n02:736468:736800 [5] NCCL INFO P2P Chunksize set to 131072
g31n02:73g31n02:736467:736467 [4] NCCL INFO cudaDriverVersion 12020
g31n02:736467:736467 [4] NCCL INFO Bootstrap : Using ib0:10.41.17.229<0>
g31n02:736467:736467 [4] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g31n02:736467:736467 [4] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g31n02:736467:736696 [4] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.229<0>
g31n02:736467:736696 [4] NCCL INFO Using network IB
g31n02:736467:736696 [4] NCCL INFO comm 0x146e34620 rank 64 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x28395441218594c5 - Init START
g31n02:736467:736696 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g31n02:736467:736696 [4] NCCL INFO Trees [0] 65/-1/-1->64->63 [1] 63/70/-1->64->75 [2] 65/-1/-1->64->63 [3] 63/-1/-1->64->57
g31n02:736467:736696 [4] NCCL INFO P2P Chunksize set to 131072
g31n02:736467:736696 [4] NCCL INFO Channel 00/0 : 64[4] -> 65[5] via P2P/IPC
g31n02:736467:736696 [4] NCCL INFO Channel 01/0 : 64[4] -> 65[5] via P2P/IPC
g31n02:736467:736696 [4] NCCL INFO Channel 02/0 : 64[4] -> 65[5] via P2P/IPC
g31n02:736467:736696 [4] NCCL INFO Channel 03/0 : 64[4] -> 65[5] via P2P/IPC
g31n02:736467:736696 [4] NCCL INFO Connected all rings
g31n02:736467:736696 [4] NCCL INFO Channel 01/0 : 64[4] -> 70[4] [send] via NET/IB/3
g31n02:736467:736696 [4] NCCL INFO Channel 03/0 : 57[3] -> 64[4] [receive] via NET/IB/3
g31n02:736467:736696 [4] NCCL INFO Channel 01/0 : 64[4] -> 75[3] [send] via NET/IB/3
g31n02:736467:736696 [4] NCCL INFO Channel 01/0 : 75[3] -> 64[4] [receive] via NET/IB/3
g31n02:736467:736696 [4] NCCL INFO Channel 03/0 : 64[4] -> 57[3] [send] via NET/IB/3
g31n02:736467:736696 [4] NCCL INFO Channel 01/0 : 70[4] -> 64[4] [receive] via NET/IB/3
g31n02:736467:736696 [4] NCCL INFO Channel 00/0 : 64[4] -> 63[3] via P2P/IPC
g31n02:736467:736696 [4] NCCL INFO Channel 01/0 : 64[4] -> 63[3] via P2P/IPC
g31n02:736467:736696 [4] NCCL INFO Channel 02/0 : 64[4] -> 63[3] via P2P/IPC
g31n02:736467:736696 [4] NCCL INFO Channel 03/0 : 64[4] -> 63[3] via P2P/IPC
g31n02:736467:736696 [4] NCCL INFO Connected all trees
g31n02:736467:736696 [4] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g31n02:736467:736696 [4] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g31n02:736467:736696 [4] NCCL INFO comm 0x146e34620 rank 64 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x28395441218594c5 - Init COMPLETE
g31n02:736467:736783 [4] NCCL INFO Using network IB
g31n02:736467:736783 [4] NCCL INFO comm 0x149c0c8b0 rank 16 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x864be1a757d9f276 - Init START
g31n02:736467:736783 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g31n02:736467:736783 [4] NCCL INFO Trees [0] 14/-1/-1->16->15 [1] -1/-1/-1->16->15
g31n02:736467:736783 [4] NCCL INFO P2P Chunksize set to 131072
g31n02:736467:736783 [4] NCCL INFO Channel 00/0 : 16[4] -> 17[2] [send] via NET/IB/1
g31n02:736467:736783 [4] NCCL INFO Channel 01/0 : 16[4] -> 17[2] [send] via NET/IB/1
g31n02:736467:736783 [4] NCCL INFO Connected all rings
g31n02:736467:736783 [4] NCCL INFO Channel 00/0 : 14[2] -> 16[4] [receive] via NET/IB/0
g31n02:736467:736783 [4] NCCL INFO Channel 00/0 : 16[4] -> 14[2] [send] via NET/IB/0
g31n02:736467:736783 [4] NCCL INFO Channel 00/0 : 16[4] -> 15[0] via P2P/IPC
g31n02:736467:736783 [4] NCCL INFO Channel 01/0 : 16[4] -> 15[0] via P2P/IPC
g31n02:736467:736783 [4] NCCL INFO Connected all trees
g31n02:736467:736783 [4] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g31n02:736467:736783 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n02:736467:736783 [4] NCCL INFO comm 0x149c0c8b0 rank 16 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x864be1a757d9f276 - Init COMPLETE
g31n02:736467:736798 [4] NCCL INFO Using network IB
g31n02:736467:736798 [4] NCCL INFO comm 0x1496dbf20 rank 8 nranks 12 g28n01:770092:770092 [0] NCCL INFO cudaDriverVersion 12020
g28n01:770092:770092 [0] NCCL INFO Bootstrap : Using ib0:10.41.17.174<0>
g28n01:770092:770092 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g28n01:770092:770092 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g28n01:770092:770324 [0] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.174<0>
g28n01:770092:770324 [0] NCCL INFO Using network IB
g28n01:770092:770324 [0] NCCL INFO comm 0x15bec44a0 rank 30 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x28395441218594c5 - Init START
g28n01:770092:770324 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g28n01:770092:770324 [0] NCCL INFO Trees [0] 31/-1/-1->30->37 [1] 32/-1/-1->30->31 [2] 31/24/-1->30->19 [3] 32/-1/-1->30->31
g28n01:770092:770324 [0] NCCL INFO P2P Chunksize set to 131072
g28n01:770092:770324 [0] NCCL INFO Channel 00/0 : 29[5] -> 30[0] [receive] via NET/IB/0
g28n01:770092:770324 [0] NCCL INFO Channel 02/0 : 29[5] -> 30[0] [receive] via NET/IB/0
g28n01:770092:770324 [0] NCCL INFO Channel 00/0 : 30[0] -> 31[1] via P2P/IPC
g28n01:770092:770324 [0] NCCL INFO Channel 02/0 : 30[0] -> 31[1] via P2P/IPC
g28n01:770092:770324 [0] NCCL INFO Channel 01/0 : 30[0] -> 39[3] [send] via NET/IB/2
g28n01:770092:770324 [0] NCCL INFO Channel 03/0 : 30[0] -> 39[3] [send] via NET/IB/2
g28n01:770092:770324 [0] NCCL INFO Connected all rings
g28n01:770092:770324 [0] NCCL INFO Channel 01/0 : 30[0] -> 31[1] via P2P/IPC
g28n01:770092:770324 [0] NCCL INFO Channel 03/0 : 30[0] -> 31[1] via P2P/IPC
g28n01:770092:770324 [0] NCCL INFO Channel 01/0 : 30[0] -> 32[2] via P2P/IPC
g28n01:770092:770324 [0] NCCL INFO Channel 03/0 : 30[0] -> 32[2] via P2P/IPC
g28n01:770092:770324 [0] NCCL INFO Channel 02/0 : 24[0] -> 30[0] [receive] via NET/IB/0
g28n01:770092:770324 [0] NCCL INFO Channel 00/0 : 30[0] -> 37[1] [send] via NET/IB/0
g28n01:770092:770324 [0] NCCL INFO Channel 02/0 : 19[1] -> 30[0] [receive] via NET/IB/0
g28n01:770092:770324 [0] NCCL INFO Channel 02/0 : 30[0] -> 19[1] [send] via NET/IB/0
g28n01:770092:770324 [0] NCCL INFO Channel 00/0 : 37[1] -> 30[0] [receive] via NET/IB/0
g28n01:770092:770324 [0] NCCL INFO Channel 02/0 : 30[0] -> 24[0] [send] via NET/IB/0
g28n01:770092:770324 [0] NCCL INFO Connected all trees
g28n01:770092:770324 [0] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g28n01:770092:770324 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g28n01:770092:770324 [0] NCCL INFO comm 0x15bec44a0 rank 30 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x28395441218594c5 - Init COMPLETE
g28n01:770092:770410 [0] NCCL INFO Using network IB
g28n01:770092:770410 [0] NCCL INFO comm 0x15ebfb060 rank 7 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0xe45164e09488020a - Init START
g28n01:770092:770410 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g28n01:770092:770410 [0] NCCL INFO Trees [0] 8/-1/-1->7->9 [1] 8/6/-1->7->5
g28n01:770092:770410 [0] NCCL INFO P2P Chunksize set to 131072
g28n01:770092:770410 [0] NCCL INFO Channel 00/0 : 6[2] -> 7[0] [receive] via NET/IB/0
g28n01:770092:770410 [0] NCCL INFO Channel 01/0 : 6[2] -> 7[0] [receive] via NET/IB/0
g28n01:770092:770410 [0] NCCL INFO Channel 00/0 : 7[0] -> 8[4] via P2P/IPC
g28n01:770092:770410 [0] NCCL INFO Channel 01/0 : 7[0] -> 8[4] via P2P/IPC
g28n01:770092:770410 [0] NCCL INFO Connected all rings
g28n01:770092:770410 [0] NCCL INFO Channel 01/0 : 5[4] -> 7[0] [receive] via NET/IB/0
g28n01:770092:770410 [0] NCCL INFO Channel 00/0 : 7[0] -> 9[2] [send] via NET/IB/0
g28n01:770092:770410 [0] NCCL INFO Channel 00/0 : 9[2] -> 7[0] [receive] via NET/IB/0
g28n01:770092:770410 [0] NCCL INFO Channel 01/0 : 7[0] -> 5[4] [send] via NET/IB/0
g28n01:770092:770410 [0] NCCL INFO Channel 01/0 : 7[0] -> 6[2] [send] via NET/IB/0
g28n01:770092:770410 [0] NCCL INFO Connected all trees
g28n01:770092:770410 [0] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g28n01:7700cudaDev 4 nvmlDev 4 busId 3504000 commId 0x76a32220a008087c - Init START
g31n02:736467:736798 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g31n02:736467:736798 [4] NCCL INFO Trees [0] 4/10/-1->8->0 [1] -1/-1/-1->8->9
g31n02:736467:736798 [4] NCCL INFO P2P Chunksize set to 131072
g31n02:736467:736798 [4] NCCL INFO Channel 00/0 : 7[2] -> 8[4] [receive] via NET/IB/1
g31n02:736467:736798 [4] NCCL INFO Channel 01/0 : 7[2] -> 8[4] [receive] via NET/IB/1
g31n02:736467:736798 [4] NCCL INFO Channel 00/0 : 8[4] -> 9[0] [send] via NET/IB/1
g31n02:736467:736798 [4] NCCL INFO Channel 01/0 : 8[4] -> 9[0] [send] via NET/IB/1
g31n02:736467:736798 [4] NCCL INFO Connected all rings
g31n02:736467:736798 [4] NCCL INFO Channel 00/0 : 8[4] -> 10[2] [send] via NET/IB/1
g31n02:736467:736798 [4] NCCL INFO Channel 00/0 : 4[2] -> 8[4] [receive] via NET/IB/1
g31n02:736467:736798 [4] NCCL INFO Channel 00/0 : 8[4] -> 0[0] [send] via NET/IB/1
g31n02:736467:736798 [4] NCCL INFO Channel 00/0 : 0[0] -> 8[4] [receive] via NET/IB/1
g31n02:736467:736798 [4] NCCL INFO Channel 00/0 : 8[4] -> 4[2] [send] via NET/IB/1
g31n02:736467:736798 [4] NCCL INFO Channel 00/0 : 10[2] -> 8[4] [receive] via NET/IB/1
g31n02:736467:736798 [4] NCCL INFO Channel 01/0 : 9[0] -> 8[4] [receive] via NET/IB/1
g31n02:736467:736798 [4] NCCL INFO Connected all trees
g31n02:736467:736798 [4] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g31n02:736467:736798 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n02:736467:736798 [4] NCCL INFO comm 0x1496dbf20 rank 8 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x76a32220a008087c - Init COMPLETE
92:770410 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n01:770092:770410 [0] NCCL INFO comm 0x15ebfb060 rank 7 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0xe45164e09488020a - Init COMPLETE
g28n01:770092:770431 [0] NCCL INFO Using network IB
g28n01:770092:770431 [0] NCCL INFO comm 0x15e80ea30 rank 3 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x8442a1c07b0a8edb - Init START
g28n01:770092:770431 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g28n01:770092:770431 [0] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] 7/1/-1->3->11
g28n01:770092:770431 [0] NCCL INFO P2P Chunksize set to 131072
g28n01:770092:770431 [0] NCCL INFO Channel 00/0 : 2[4] -> 3[0] [receive] via NET/IB/0
g28n01:770092:770431 [0] NCCL INFO Channel 01/0 : 2[4] -> 3[0] [receive] via NET/IB/0
g28n01:770092:770431 [0] NCCL INFO Channel 00/0 : 3[0] -> 4[2] [send] via NET/IB/0
g28n01:770092:770431 [0] NCCL INFO Channel 01/0 : 3[0] -> 4[2] [send] via NET/IB/0
g28n01:770092:770431 [0] NCCL INFO Connected all rings
g28n01:770092:770431 [0] NCCL INFO Channel 01/0 : 1[2] -> 3[0] [receive] via NET/IB/0
g28n01:770092:770431 [0] NCCL INFO Channel 01/0 : 11[4] -> 3[0] [receive] via NET/IB/0
g28n01:770092:770431 [0] NCCL INFO Channel 01/0 : 3[0] -> 7[2] [send] via NET/IB/0
g28n01:770092:770431 [0] NCCL INFO Channel 01/0 : 7[2] -> 3[0] [receive] via NET/IB/0
g28n01:770092:770431 [0] NCCL INFO Channel 01/0 : 3[0] -> 11[4] [send] via NET/IB/0
g28n01:770092:770431 [0] NCCL INFO Channel 01/0 : 3[0] -> 1[2] [send] via NET/IB/0
g28n01:770092:770431 [0] NCCL INFO Channel 00/0 : 3[0] -> 2[4] [send] via NET/IB/0
g28n01:770092:770431 [0] NCCL INFO Connected all trees
g28n01:770092:770431 [0] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g28n01:770092:770431 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n01:770092:770431 [0] NCCL INFO comm 0x15e80ea30 rank 3 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x8442a1c07b0a8edb - Init COMPLETE
6468:736800 [5] NCCL INFO Channel 00/0 : 7[3] -> 8[5] [receive] via NET/IB/3
g31n02:736468:736800 [5] NCCL INFO Channel 01/0 : 7[3] -> 8[5] [receive] via NET/IB/3
g31n02:736468:736800 [5] NCCL INFO Channel 00/0 : 8[5] -> 9[1] [send] via NET/IB/3
g31n02:736468:736800 [5] NCCL INFO Channel 01/0 : 8[5] -> 9[1] [send] via NET/IB/3
g31n02:736468:736800 [5] NCCL INFO Connected all rings
g31n02:736468:736800 [5] NCCL INFO Channel 00/0 : 8[5] -> 10[3] [send] via NET/IB/3
g31n02:736468:736800 [5] NCCL INFO Channel 00/0 : 4[3] -> 8[5] [receive] via NET/IB/3
g31n02:736468:736800 [5] NCCL INFO Channel 00/0 : 8[5] -> 0[1] [send] via NET/IB/3
g31n02:736468:736800 [5] NCCL INFO Channel 00/0 : 0[1] -> 8[5] [receive] via NET/IB/3
g31n02:736468:736800 [5] NCCL INFO Channel 00/0 : 8[5] -> 4[3] [send] via NET/IB/3
g31n02:736468:736800 [5] NCCL INFO Channel 00/0 : 10[3] -> 8[5] [receive] via NET/IB/3
g31n02:736468:736800 [5] NCCL INFO Channel 01/0 : 9[1] -> 8[5] [receive] via NET/IB/3
g31n02:736468:736800 [5] NCCL INFO Connected all trees
g31n02:736468:736800 [5] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g31n02:736468:736800 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n02:736468:736800 [5] NCCL INFO comm 0x1580fb1c0 rank 8 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x17fc85f125de53ec - Init COMPLETE
g31n02:736464:736464 [1] NCCL INFO cudaDriverVersion 12020
g31n02:736464:736464 [1] NCCL INFO Bootstrap : Using ib0:10.41.17.229<0>
g31n02:736464:736464 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g31n02:736464:736464 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g31n02:736464:736694 [1] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.229<0>
g31n02:736464:736694 [1] NCCL INFO Using network IB
g31n02:736464:736694 [1] NCCL INFO comm 0x11d5a3f00 rank 61 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x28395441218594c5 - Init START
g31n02:736464:736694 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g31n02:736464:736694 [1] NCCL INFO Trees [0] 62/54/-1->61->60 [1] 60/-1/-1->61->65 [2] 62/-1/-1->61->60 [3] 60/-1/-1->61->65
g31n02:736464:736694 [1] NCCL INFO P2P Chunksize set to 131072
g31n02:736464:736694 [1] NCCL INFO Channel 00/0 : 61[1] -> 62[2] via P2P/IPC
g31n02:736464:736694 [1] NCCL INFO Channel 02/0 : 61[1] -> 62[2] via P2P/IPC
g31n02:736464:736694 [1] NCCL INFO Channel 01/0 : 61[1] -> 60[0] via P2P/IPC
g31n02:736464:736694 [1] NCCL INFO Channel 03/0 : 61[1] -> 60[0] via P2P/IPC
g31n02:736464:736694 [1] NCCL INFO Connected all rings
g31n02:736464:736694 [1] NCCL INFO Channel 01/0 : 61[1] -> 65[5] via P2P/IPC
g31n02:736464:736694 [1] NCCL INFO Channel 03/0 : 61[1] -> 65[5] via P2P/IPC
g31n02:736464:736694 [1] NCCL INFO Channel 00/0 : 54[0] -> 61[1] [receive] via NET/IB/0
g31n02:736464:736694 [1] NCCL INFO Channel 00/0 : 61[1] -> 54[0] [send] via NET/IB/0
g31n02:736464:736694 [1] NCCL INFO Channel 00/0 : 61[1] -> 60[0] via P2P/IPC
g31n02:736464:736694 [1] NCCL INFO Channel 02/0 : 61[1] -> 60[0] via P2P/IPC
g31n02:736464:736694 [1] NCCL INFO Connected all trees
g31n02:736464:736694 [1] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g31n02:736464:736694 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g31n02:736464:736694 [1] NCCL INFO comm 0x11d5a3f00 rank 61 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x28395441218594c5 - Init COMPLETE
g31n02:736464:736779 [1] NCCL INFO Using network IB
g31n02:736464:736779 [1] NCCL INFO comm 0x1202a8cc0 rank 15 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0xeee4c143812ce6e4 - Init START
g31n02:736464:736779 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g31n02:736464:736779 [1] NCCL INFO Trees [0] 16/17/-1->15->19 [1] 16/-1/-1->15->14
g31n02:736464:736779 [1] NCCL INFO P2P Chunksize set to 131072
g31n02:736464:736779 [1] NCCL INFO Channel 00/0 : 14[3] -> 15[1] [receive] via NET/IB/2
g31n02:736464:736779 [1] NCCL INFO Channel 01/0 : 14[3] -> 15[1] [receive] via NET/IB/2
g31n02:736464:736779 [1] NCCL INFO Channel 00/0 : 15[1] -> 16[5] via P2P/IPC
g31n02:736464:736779 [1] NCCL INFO Channel 01/0 : 15[1] -> 16[5] via P2P/IPC
g31n02:736464:736779 [1] NCCL INFO Connected all rings
g31n02:736464:736779 [1] NCCL INFO Channel 00/0 : 15[1] -> 17[3] [send] via NET/IB/2
g31n02:736464:736779 [1] NCCL INFO Channel 00/0 : 15[1] -> 19[5] [send] via NET/IB/2
g31n02:736464:736779 [1] NCCL INFO Channel 00/0 : 19[5] -> 15[1] [receive] via NET/IB/2
g31n02:736464:736779 [1] NCCL INFO Channel 00/0 : 17[3] -> 15[1] [receive] via NET/IB/2
g31n02:736464:736779 [1] NCCL INFO Channel 01/0 : 15[1] -> 14[3] [send] via NET/IB/2
g31n02:736464:736779 [1] NCCL INFO Connected all trees
g31n02:736464:736779 [1] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g31n02:736464:736779 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n02:736464:736779 [1] NCCL INFO comm 0x1202a8cc0 rank 15 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0xeee4c143812ce6e4 - Init COMPLETE
g31n02:736464:736799 [1] NCCL INFO Using network IB
g31n02:736464:736799 [1] NCCL INFO comm 0x11ff4fc40 rank 7 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0xad5995bf820e7340 - Init START
g31n02:736464:736799 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g31n02:736464:736799 [1] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] 9/5/-1->7->3
g31n02:736464:736799 [1] NCCL INFO P2P Chunksize set to 131072
g31n02:736464:736799 [1] NCCL INFO Channel 00/0 : 6[5] -> 7[1] [receive] via NET/IB/2
g31n02:736464:736799 [1] NCCL INFO Channel 01/0 : 6[5] -> 7[1] [receive] via NET/IB/2
g31n02:736464:736799 [1] NCCL INFO Channel 00/0 : 7[1] -> 8[3] [send] via NET/IB/2
g31n02:736464:736799 [1] NCCL INFO Channel 01/0 : 7[1] -> 8[3] [send] via NET/IB/2
g31n02:736464:736799 [1] NCCL INFO Connected all rings
g31n02:736464:736799 [1] NCCL INFO Channel 01/0 : 5[3] -> 7[1] [receive] via NET/IB/2
g31n02:736464:736799 [1] NCCL INFO Channel 01/0 : 7[1] -> 9[5] [send] via NET/IB/2
g31n02:736464:736799 [1] NCCL INFO Channel 01/0 : 3[5] -> 7[1] [receive] via NET/IB/2
g31n02:736464:736799 [1] NCCL INFO Channel 01/0 : 7[1] -> 3[5] [send] via NET/IB/2
g31n02:736464:736799 [1] NCCL INFO Channel 01/0 : 9[5] -> 7[1] [receive] via NET/IB/2
g31n02:736464:736799 [1] NCCL INFO Channel 01/0 : 7[1] -> 5[3] [send] via NET/IB/2
g31n02:736464:736799 [1] NCCL INFO Channel 00/0 : 7[1] -> 6[5] [send] via NET/IB/2
g31n02:736464:736799 [1] NCCL INFO Connected all trees
g31n02:736464:736799 [1] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g31n02:736464:736799 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n02:736464:736799 [1] NCCL INFO comm 0x11ff4fc40 rank 7 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0xad5995bf820e7340 - Init COMPLETE
g31n02:736465:736465 [2] NCCL INFO cudaDriverVersion 12020
g31n02:736465:736465 [2] NCCL INFO Bootstrap : Using ib0:10.41.17.229<0>
g31n02:736465:736465 [2] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g31n02:736465:736465 [2] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g31n02:736465:736691 [2] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.229<0>
g31n02:736465:736691 [2] NCCL INFO Using network IB
g31n02:736465:736691 [2] NCCL INFO comm 0x156c13aa0 rank 62 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x28395441218594c5 - Init START
g31n02:736465:736691 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g31n02:736465:736691 [2] NCCL INFO Trees [0] 63/-1/-1->62->61 [1] -1/-1/-1->62->60 [2] 63/-1/-1->62->61 [3] -1/-1/-1->62->60
g31n02:736465:736691 [2] NCCL INFO P2P Chunksize set to 131072
g31n02:736465:736691 [2] NCCL INFO Channel 00/0 : 62[2] -> 63[3] via P2P/IPC
g31n02:736465:736691 [2] NCCL INFO Channel 02/0 : 62[2] -> 63[3] via P2P/IPC
g31n02:736465:736691 [2] NCCL INFO Channel 01/0 : 62[2] -> 61[1] via P2P/IPC
g31n02:736465:736691 [2] NCCL INFO Channel 03/0 : 62[2] -> 61[1] via P2P/IPC
g31n02:736465:736691 [2] NCCL INFO Connected all rings
g31n02:736465:736691 [2] NCCL INFO Channel 01/0 : 62[2] -> 60[0] via P2P/IPC
g31n02:736465:736691 [2] NCCL INFO Channel 03/0 : 62[2] -> 60[0] via P2P/IPC
g31n02:736465:736691 [2] NCCL INFO Channel 00/0 : 62[2] -> 61[1] via P2P/IPC
g31n02:736465:736691 [2] NCCL INFO Channel 02/0 : 62[2] -> 61[1] via P2P/IPC
g31n02:736465:736691 [2] NCCL INFO Connected all trees
g31n02:736465:736691 [2] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g31n02:736465:736691 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g31n02:736465:736691 [2] NCCL INFO comm 0x156c13aa0 rank 62 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x28395441218594c5 - Init COMPLETE
g31n02:736465:736782 [2] NCCL INFO Using network IB
g31n02:736465:736782 [2] NCCL INFO comm 0x159507870 rank 15 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0xe45164e09488020a - Init START
g31n02:736465:736782 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g31n02:736465:736782 [2] NCCL INFO Trees [0] 13/16/-1->15->18 [1] -1/-1/-1->15->14
g31n02:736465:736782 [2] NCCL INFO P2P Chunksize set to 131072
g31n02:736465:736782 [2] NCCL INFO Channel 00/0 : 14[4] -> 15[2] [receive] via NET/IB/0
g31n02:736465:736782 [2] NCCL INFO Channel 01/0 : 14[4] -> 15[2] [receive] via NET/IB/0
g31n02:736465:736782 [2] NCCL INFO Channel 00/0 : 15[2] -> 16[0] [send] via NET/IB/0
g31n02:736465:736782 [2] NCCL INFO Channel 01/0 : 15[2] -> 16[0] [send] via NET/IB/0
g31n02:736465:736782 [2] NCCL INFO Connected all rings
g31n02:736465:736782 [2] NCCL INFO Channel 00/0 : 13[0] -> 15[2] [receive] via NET/IB/0
g31n02:736465:736782 [2] NCCL INFO Channel 00/0 : 15[2] -> 18[2] [send] via NET/IB/0
g31n02:736465:736782 [2] NCCL INFO Channel 00/0 : 18[2] -> 15[2] [receive] via NET/IB/0
g31n02:736465:736782 [2] NCCL INFO Channel 00/0 : 15[2] -> 13[0] [send] via NET/IB/0
g31n02:736465:736782 [2] NCCL INFO Channel 00/0 : 16[0] -> 15[2] [receive] via NET/IB/0
g31n02:736465:736782 [2] NCCL INFO Channel 01/0 : 15[2] -> 14[4] [send] via NET/IB/0
g31n02:736465:736782 [2] NCCL INFO Connected all trees
g31n02:736465:736782 [2] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g31n02:736465:736782 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n02:736465:736782 [2] NCCL INFO comm 0x159507870 rank 15 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0xe45164e09488020a - Init COMPLETE
g31n02:736465:736801 [2] NCCL INFO Using network IB
g31n02:736465:736801 [2] NCCL INFO comm 0x15996c3f0 rank 7 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0x8442a1c07b0a8edb - Init START
g31n02:736465:736801 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g31n02:736465:736801 [2] NCCL INg31n02:736463:736463 [0] NCCL INFO cudaDriverVersion 12020
g31n02:736463:736463 [0] NCCL INFO Bootstrap : Using ib0:10.41.17.229<0>
g31n02:736463:736463 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g31n02:736463:736463 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g31n02:736463:736692 [0] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.229<0>
g31n02:736463:736692 [0] NCCL INFO Using network IB
g31n02:736463:736692 [0] NCCL INFO comm 0x10ee64090 rank 60 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x28395441218594c5 - Init START
g31n02:736463:736692 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g31n02:736463:736692 [0] NCCL INFO Trees [0] 61/66/-1->60->73 [1] 62/-1/-1->60->61 [2] 61/-1/-1->60->55 [3] 62/-1/-1->60->61
g31n02:736463:736692 [0] NCCL INFO P2P Chunksize set to 131072
g31n02:736463:736692 [0] NCCL INFO Channel 00/0 : 59[5] -> 60[0] [receive] via NET/IB/0
g31n02:736463:736692 [0] NCCL INFO Channel 02/0 : 59[5] -> 60[0] [receive] via NET/IB/0
g31n02:736463:736692 [0] NCCL INFO Channel 00/0 : 60[0] -> 61[1] via P2P/IPC
g31n02:736463:736692 [0] NCCL INFO Channel 02/0 : 60[0] -> 61[1] via P2P/IPC
g31n02:736463:736692 [0] NCCL INFO Channel 01/0 : 60[0] -> 69[3] [send] via NET/IB/2
g31n02:736463:736692 [0] NCCL INFO Channel 03/0 : 60[0] -> 69[3] [send] via NET/IB/2
g31n02:736463:736692 [0] NCCL INFO Connected all rings
g31n02:736463:736692 [0] NCCL INFO Channel 01/0 : 60[0] -> 61[1] via P2P/IPC
g31n02:736463:736692 [0] NCCL INFO Channel 03/0 : 60[0] -> 61[1] via P2P/IPC
g31n02:736463:736692 [0] NCCL INFO Channel 01/0 : 60[0] -> 62[2] via P2P/IPC
g31n02:736463:736692 [0] NCCL INFO Channel 03/0 : 60[0] -> 62[2] via P2P/IPC
g31n02:736463:736692 [0] NCCL INFO Channel 02/0 : 55[1] -> 60[0] [receive] via NET/IB/0
g31n02:736463:736692 [0] NCCL INFO Channel 00/0 : 60[0] -> 66[0] [send] via NET/IB/0
g31n02:736463:736692 [0] NCCL INFO Channel 00/0 : 60[0] -> 73[1] [send] via NET/IB/0
g31n02:736463:736692 [0] NCCL INFO Channel 00/0 : 73[1] -> 60[0] [receive] via NET/IB/0
g31n02:736463:736692 [0] NCCL INFO Channel 00/0 : 66[0] -> 60[0] [receive] via NET/IB/0
g31n02:736463:736692 [0] NCCL INFO Channel 02/0 : 60[0] -> 55[1] [send] via NET/IB/0
g31n02:736463:736692 [0] NCCL INFO Connected all trees
g31n02:736463:736692 [0] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g31n02:736463:736692 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g31n02:736463:736692 [0] NCCL INFO comm 0x10ee64090 rank 60 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x28395441218594c5 - Init COMPLETE
g31n02:736463:736784 [0] NCCL INFO Using network IB
g31n02:736463:736784 [0] NCCL INFO comm 0x11173c330 rank 15 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0x864be1a757d9f276 - Init START
g31n02:736463:736784 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g31n02:736463:736784 [0] NCCL INFO Trees [0] 16/17/-1->15->19 [1] 16/-1/-1->15->14
g31n02:736463:736784 [0] NCCL INFO P2P Chunksize set to 131072
g31n02:736463:736784 [0] NCCL INFO Channel 00/0 : 14[2] -> 15[0] [receive] via NET/IB/0
g31n02:736463:736784 [0] NCCL INFO Channel 01/0 : 14[2] -> 15[0] [receive] via NET/IB/0
g31n02:736463:736784 [0] NCCL INFO Channel 00/0 : 15[0] -> 16[4] via P2P/IPC
g31n02:736463:736784 [0] NCCL INFO Channel 01/0 : 15[0] -> 16[4] via P2P/IPC
g31n02:736463:736784 [0] NCCL INFO Connected all rings
g31n02:736463:736784 [0] NCCL INFO Channel 00/0 : 15[0] -> 17[2] [send] via NET/IB/0
g31n02:736463:736784 [0] NCCL INFO Channel 00/0 : 15[0] -> 19[4] [send] via NET/IB/0
g31n02:736463:736784 [0] NCCL INFO Channel 00/0 : 19[4] -> 15[0] [receive] via NET/IB/0
g31n02:736463:736784 [0] NCCL INFO Channel 00/0 : 17[2] -> 15[0] [receive] via NET/IB/0
g31n02:736463:736784 [0] NCCL INFO Channel 01/0 : 15[0] -> 14[2] [send] via NET/IB/0
g31n02:736463:736784 [0] NCCL INFO Connected all trees
g31n02:736463:736784 [0] NCCL INFO threadThresholds 8/8/64 | 192/8/FO Trees [0] -1/-1/-1->7->6 [1] 9/5/-1->7->3
g31n02:736465:736801 [2] NCCL INFO P2P Chunksize set to 131072
g31n02:736465:736801 [2] NCCL INFO Channel 00/0 : 6[0] -> 7[2] [receive] via NET/IB/0
g31n02:736465:736801 [2] NCCL INFO Channel 01/0 : 6[0] -> 7[2] [receive] via NET/IB/0
g31n02:736465:736801 [2] NCCL INFO Channel 00/0 : 7[2] -> 8[4] [send] via NET/IB/0
g31n02:736465:736801 [2] NCCL INFO Channel 01/0 : 7[2] -> 8[4] [send] via NET/IB/0
g31n02:736465:736801 [2] NCCL INFO Connected all rings
g31n02:736465:736801 [2] NCCL INFO Channel 01/0 : 5[4] -> 7[2] [receive] via NET/IB/0
g31n02:736465:736801 [2] NCCL INFO Channel 01/0 : 7[2] -> 9[0] [send] via NET/IB/0
g31n02:736465:736801 [2] NCCL INFO Channel 01/0 : 3[0] -> 7[2] [receive] via NET/IB/0
g31n02:736465:736801 [2] NCCL INFO Channel 01/0 : 7[2] -> 3[0] [send] via NET/IB/0
g31n02:736465:736801 [2] NCCL INFO Channel 01/0 : 9[0] -> 7[2] [receive] via NET/IB/0
g31n02:736465:736801 [2] NCCL INFO Channel 01/0 : 7[2] -> 5[4] [send] via NET/IB/0
g31n02:736465:736801 [2] NCCL INFO Channel 00/0 : 7[2] -> 6[0] [send] via NET/IB/0
g31n02:736465:736801 [2] NCCL INFO Connected all trees
g31n02:736465:736801 [2] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g31n02:736465:736801 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n02:736465:736801 [2] NCCL INFO comm 0x15996c3f0 rank 7 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0x8442a1c07b0a8edb - Init COMPLETE
64 | 512 | 512
g31n02:736463:736784 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n02:736463:736784 [0] NCCL INFO comm 0x11173c330 rank 15 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0x864be1a757d9f276 - Init COMPLETE
g31n02:736463:736797 [0] NCCL INFO Using network IB
g31n02:736463:736797 [0] NCCL INFO comm 0x111b13130 rank 7 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x51bdf7b6b8c3aecb - Init START
g31n02:736463:736797 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g31n02:736463:736797 [0] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] 9/5/-1->7->3
g31n02:736463:736797 [0] NCCL INFO P2P Chunksize set to 131072
g31n02:736463:736797 [0] NCCL INFO Channel 00/0 : 6[4] -> 7[0] [receive] via NET/IB/0
g31n02:736463:736797 [0] NCCL INFO Channel 01/0 : 6[4] -> 7[0] [receive] via NET/IB/0
g31n02:736463:736797 [0] NCCL INFO Channel 00/0 : 7[0] -> 8[2] [send] via NET/IB/0
g31n02:736463:736797 [0] NCCL INFO Channel 01/0 : 7[0] -> 8[2] [send] via NET/IB/0
g31n02:736463:736797 [0] NCCL INFO Connected all rings
g31n02:736463:736797 [0] NCCL INFO Channel 01/0 : 5[2] -> 7[0] [receive] via NET/IB/0
g31n02:736463:736797 [0] NCCL INFO Channel 01/0 : 7[0] -> 9[4] [send] via NET/IB/0
g31n02:736463:736797 [0] NCCL INFO Channel 01/0 : 3[4] -> 7[0] [receive] via NET/IB/0
g31n02:736463:736797 [0] NCCL INFO Channel 01/0 : 7[0] -> 3[4] [send] via NET/IB/0
g31n02:736463:736797 [0] NCCL INFO Channel 01/0 : 9[4] -> 7[0] [receive] via NET/IB/0
g31n02:736463:736797 [0] NCCL INFO Channel 01/0 : 7[0] -> 5[2] [send] via NET/IB/0
g31n02:736463:736797 [0] NCCL INFO Channel 00/0 : 7[0] -> 6[4] [send] via NET/IB/0
g31n02:736463:736797 [0] NCCL INFO Connected all trees
g31n02:736463:736797 [0] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g31n02:736463:736797 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n02:736463:736797 [0] NCCL INFO comm 0x111b13130 rank 7 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x51bdf7b6b8c3aecb - Init COMPLETE
g31n05:704344:704344 [0] NCCL INFO cudaDriverVersion 12020
g31n05:704344:704344 [0] NCCL INFO Bootstrap : Using ib0:10.41.17.232<0>
g31n05:704344:704344 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g31n05:704344:704344 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g31n05:704344:704580 [0] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.232<0>
g31n05:704344:704580 [0] NCCL INFO Using network IB
g31n05:704344:704580 [0] NCCL INFO comm 0x174ee4490 rank 78 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x28395441218594c5 - Init START
g31n05:704344:704580 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g31n05:704344:704580 [0] NCCL INFO Trees [0] 79/-1/-1->78->85 [1] 80/-1/-1->78->79 [2] 79/72/-1->78->67 [3] 80/-1/-1->78->79
g31n05:704344:704580 [0] NCCL INFO P2P Chunksize set to 131072
g31n05:704344:704580 [0] NCCL INFO Channel 00/0 : 77[5] -> 78[0] [receive] via NET/IB/0
g31n05:704344:704580 [0] NCCL INFO Channel 02/0 : 77[5] -> 78[0] [receive] via NET/IB/0
g31n05:704344:704580 [0] NCCL INFO Channel 00/0 : 78[0] -> 79[1] via P2P/IPC
g31n05:704344:704580 [0] NCCL INFO Channel 02/0 : 78[0] -> 79[1] via P2P/IPC
g31n05:704344:704580 [0] NCCL INFO Channel 01/0 : 78[0] -> 87[3] [send] via NET/IB/2
g31n05:704344:704580 [0] NCCL INFO Channel 03/0 : 78[0] -> 87[3] [send] via NET/IB/2
g31n05:704344:704580 [0] NCCL INFO Connected all rings
g31n05:704344:704580 [0] NCCL INFO Channel 01/0 : 78[0] -> 79[1] via P2P/IPC
g31n05:704344:704580 [0] NCCL INFO Channel 03/0 : 78[0] -> 79[1] via P2P/IPC
g31n05:704344:704580 [0] NCCL INFO Channel 01/0 : 78[0] -> 80[2] via P2P/IPC
g31n05:704344:704580 [0] NCCL INFO Channel 03/0 : 78[0] -> 80[2] via P2P/IPC
g31n05:704344:704580 [0] NCCL INFO Channel 02/0 : 72[0] -> 78[0] [receive] via NET/IB/0
g31n05:704344:704580 [0] NCCL INFO Channel 00/0 : 78[0] -> 85[1] [send] via NET/IB/0
g31n05:704344:704580 [0] NCCL INFO Channel 02/0 : 67[1] -> 78[0] [receive] via NET/IB/0
g31n05:704344:704580 [0] NCCL INFO Channel 02/0 : 78[0] -> 67[1] [send] via NET/IB/0
g31n05:704344:704580 [0] NCCL INFO Channel 00/0 : 85[1] -> 78[0] [receive] via NET/IB/0
g31n05:704344:704580 [0] NCCL INFO Channel 02/0 : 78[0] -> 72[0] [send] via NET/IB/0
g31n05:704344:704580 [0] NCCL INFO Connected all trees
g31n05:704344:704580 [0] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g31n05:704344:704580 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g31n05:704344:704580 [0] NCCL INFO comm 0x174ee4490 rank 78 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x28395441218594c5 - Init COMPLETE
g31n05:704344:704664 [0] NCCL INFO Using network IB
g31n05:704344:704664 [0] NCCL INFO comm 0x177ba1b40 rank 19 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0xe45164e09488020a - Init START
g31n05:704344:704664 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g31n05:704344:704664 [0] NCCL INFO Trees [0] 20/-1/-1->19->21 [1] 20/18/-1->19->17
g31n05:704344:704664 [0] NCCL INFO P2P Chunksize set to 131072
g31n05:704344:704664 [0] NCCL INFO Channel 00/0 : 18[2] -> 19[0] [receive] via NET/IB/0
g31n05:704344:704664 [0] NCCL INFO Channel 01/0 : 18[2] -> 19[0] [receive] via NET/IB/0
g31n05:704344:704664 [0] NCCL INFO Channel 00/0 : 19[0] -> 20[4] via P2P/IPC
g31n05:704344:704664 [0] NCCL INFO Channel 01/0 : 19[0] -> 20[4] via P2P/IPC
g31n05:704344:704664 [0] NCCL INFO Connected all rings
g31n05:704344:704664 [0] NCCL INFO Channel 01/0 : 17[4] -> 19[0] [receive] via NET/IB/0
g31n05:704344:704664 [0] NCCL INFO Channel 00/0 : 19[0] -> 21[2] [send] via NET/IB/0
g31n05:704344:704664 [0] NCCL INFO Channel 00/0 : 21[2] -> 19[0] [receive] via NET/IB/0
g31n05:704344:704664 [0] NCCL INFO Channel 01/0 : 19[0] -> 17[4] [send] via NET/IB/0
g31n05:704344:704664 [0] NCCL INFO Channel 01/0 : 19[0] -> 18[2] [send] via NET/IB/0
g31n05:704344:704664 [0] NCCL INFO Connected all trees
g31n05:704344:704664 [0] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g31n05:704344:704664 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n05:704344:704664 [0] NCCL INFO comm 0x177ba1b40 rank 19 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0xe45164e09488020a - Init COMPLETE
g31n05:704344:704683 [0] NCCL INFO Using network IB
g31n05:704344:704683 [0] NCCL INFO comm 0x177ba85f0 rank 9 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x8442a1c07b0a8edb - Init START
g31n05:704344:704683 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g31n05:704344:704683 [0] NCCL INFO Trees [0] -1/-1/-1->9->10 [1] 10/8/-1->9->7
g31n05:704344:704683 [0] NCCL INFO P2P Chunksize set to 131072
g31n05:704344:704683 [0] NCCL INFO Channel 00/0 : 8[4] -> 9[0] [receive] via NET/IB/0
g31n05:704344:704683 [0] NCCL INFO Channel 01/0 : 8[4] -> 9[0] [receive] via NET/IB/0
g31n05:704344:704683 [0] NCCL INFO Channel 00/0 : 9[0] -> 10[2] [send] via NET/IB/0
g31n05:704344:704683 [0] NCCL INFO Channel 01/0 : 9[0] -> 10[2] [send] via NET/IB/0
g31n05:704344:704683 [0] NCCL INFO Connected all rings
g31n05:704344:704683 [0] NCCL INFO Channel 01/0 : 7[2] -> 9[0] [receive] via NET/IB/0
g31n05:704344:704683 [0] NCCL INFO Channel 01/0 : 9[0] -> 7[2] [send] via NET/IB/0
g31n05:704344:704683 [0] NCCL INFO Channel 00/0 : 10[2] -> 9[0] [receive] via NET/IB/0
g31n05:704344:704683 [0] NCCL INFO Channel 01/0 : 10[2] -> 9[0] [receive] via NET/IB/0
g31n05:704344:704683 [0] NCCL INFO Channel 01/0 : 9[0] -> 8[4] [send] via NET/IB/0
g31n05:704344:704683 [0] NCCL INFO Connected all trees
g31n05:704344:704683 [0] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g31n05:704344:704683 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n05:704344:704683 [0] NCCL INFO comm 0x177ba85f0 rank 9 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x8442a1c07b0a8edb - Init COMPLETE
g32n02:753685:753685 [2] NCCL INFO cudaDriverVersion 12020
g32n02:753685:753685 [2] NCCL INFO Bootstrap : Using ib0:10.41.17.247<0>
g32n02:753685:753685 [2] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g32n02:753685:753685 [2] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g32n02:753685:753918 [2] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.247<0>
g32n02:753685:753918 [2] NCCL INFO Using network IB
g32n02:753685:753918 [2] NCCL INFO comm 0x1393647e0 rank 86 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x28395441218594c5 - Init START
g32n02:753685:753918 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g32n02:753685:753918 [2] NCCL INFO Trees [0] 87/-1/-1->86->85 [1] -1/-1/-1->86->84 [2] 87/-1/-1->86->85 [3] -1/-1/-1->86->84
g32n02:753685:753918 [2] NCCL INFO P2P Chunksize set to 131072
g32n02:753685:753918 [2] NCCL INFO Channel 00/0 : 86[2] -> 87[3] via P2P/IPC
g32n02:753685:753918 [2] NCCL INFO Channel 02/0 : 86[2] -> 87[3] via P2P/IPC
g32n02:753685:753918 [2] NCCL INFO Channel 01/0 : 86[2] -> 85[1] via P2P/IPC
g32n02:753685:753918 [2] NCCL INFO Channel 03/0 : 86[2] -> 85[1] via P2P/IPC
g32n02:753685:753918 [2] NCCL INFO Connected all rings
g32n02:753685:753918 [2] NCCL INFO Channel 01/0 : 86[2] -> 84[0] via P2P/IPC
g32n02:753685:753918 [2] NCCL INFO Channel 03/0 : 86[2] -> 84[0] via P2P/IPC
g32n02:753685:753918 [2] NCCL INFO Channel 00/0 : 86[2] -> 85[1] via P2P/IPC
g32n02:753685:753918 [2] NCCL INFO Channel 02/0 : 86[2] -> 85[1] via P2P/IPC
g32n02:753685:753918 [2] NCCL INFO Connected all trees
g32n02:753685:753918 [2] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g32n02:753685:753918 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g32n02:753685:753918 [2] NCCL INFO comm 0x1393647e0 rank 86 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x28395441218594c5 - Init COMPLETE
g32n02:753685:754004 [2] NCCL INFO Using network IB
g32n02:753685:754004 [2] NCCL INFO comm 0x13c21d340 rank 21 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0xe45164e09488020a - Init START
g32n02:753685:754004 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g32n02:753685:754004 [2] NCCL INFO Trees [0] 19/22/-1->21->18 [1] -1/-1/-1->21->20
g32n02:753685:754004 [2] NCCL INFO P2P Chunksize set to 131072
g32n02:753685:754004 [2] NCCL INFO Channel 00/0 : 20[4] -> 21[2] [receive] via NET/IB/0
g32n02:753685:754004 [2] NCCL INFO Channel 01/0 : 20[4] -> 21[2] [receive] via NET/IB/0
g32n02:753685:754004 [2] NCCL INFO Channel 00/0 : 21[2] -> 22[0] [send] via NET/IB/0
g32n02:753685:754004 [2] NCCL INFO Channel 01/0 : 21[2] -> 22[0] [send] via NET/IB/0
g32n02:753685:754004 [2] NCCL INFO Connected all rings
g32n02:753685:754004 [2] NCCL INFO Channel 00/0 : 19[0] -> 21[2] [receive] via NET/IB/0
g32n02:753685:754004 [2] NCCL INFO Channel 00/0 : 18[2] -> 21[2] [receive] via NET/IB/0
g32n02:753685:754004 [2] NCCL INFO Channel 00/0 : 21[2] -> 18[2] [send] via NET/IB/0
g32n02:753685:754004 [2] NCCL INFO Channel 00/0 : 21[2] -> 19[0] [send] via NET/IB/0
g32n02:753685:754004 [2] NCCL INFO Channel 00/0 : 22[0] -> 21[2] [receive] via NET/IB/0
g32n02:753685:754004 [2] NCCL INFO Channel 01/0 : 21[2] -> 20[4] [send] via NET/IB/0
g32n02:753685:754004 [2] NCCL INFO Connected all trees
g32n02:753685:754004 [2] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g32n02:753685:754004 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g32n02:753685:754004 [2] NCCL INFO comm 0x13c21d340 rank 21 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0xe45164e09488020a - Init COMPLETE
g32n02:753685:754023 [2] NCCL INFO Using network IB
g32n02:753685:754023 [2] NCCL INFO comm 0x13c136350 rank 10 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0x8442a1c07b0a8edb - Init START
g32n02:753685:754023 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g32n02:753685:754023 [2] NCCL Ig32n02:753684:753684 [1] NCCL INFO cudaDriverVersion 12020
g32n02:753684:753684 [1] NCCL INFO Bootstrap : Using ib0:10.41.17.247<0>
g32n02:753684:753684 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g32n02:753684:753684 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g32n02:753684:753919 [1] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.247<0>
g32n02:753684:753919 [1] NCCL INFO Using network IB
g32n02:753684:753919 [1] NCCL INFO comm 0x145c44190 rank 85 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x28395441218594c5 - Init START
g32n02:753684:753919 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g32n02:753684:753919 [1] NCCL INFO Trees [0] 86/78/-1->85->84 [1] 84/-1/-1->85->89 [2] 86/-1/-1->85->84 [3] 84/-1/-1->85->89
g32n02:753684:753919 [1] NCCL INFO P2P Chunksize set to 131072
g32n02:753684:753919 [1] NCCL INFO Channel 00/0 : 85[1] -> 86[2] via P2P/IPC
g32n02:753684:753919 [1] NCCL INFO Channel 02/0 : 85[1] -> 86[2] via P2P/IPC
g32n02:753684:753919 [1] NCCL INFO Channel 01/0 : 85[1] -> 84[0] via P2P/IPC
g32n02:753684:753919 [1] NCCL INFO Channel 03/0 : 85[1] -> 84[0] via P2P/IPC
g32n02:753684:753919 [1] NCCL INFO Connected all rings
g32n02:753684:753919 [1] NCCL INFO Channel 01/0 : 85[1] -> 89[5] via P2P/IPC
g32n02:753684:753919 [1] NCCL INFO Channel 03/0 : 85[1] -> 89[5] via P2P/IPC
g32n02:753684:753919 [1] NCCL INFO Channel 00/0 : 78[0] -> 85[1] [receive] via NET/IB/0
g32n02:753684:753919 [1] NCCL INFO Channel 00/0 : 85[1] -> 78[0] [send] via NET/IB/0
g32n02:753684:753919 [1] NCCL INFO Channel 00/0 : 85[1] -> 84[0] via P2P/IPC
g32n02:753684:753919 [1] NCCL INFO Channel 02/0 : 85[1] -> 84[0] via P2P/IPC
g32n02:753684:753919 [1] NCCL INFO Connected all trees
g32n02:753684:753919 [1] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g32n02:753684:753919 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g32n02:753684:753919 [1] NCCL INFO comm 0x145c44190 rank 85 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x28395441218594c5 - Init COMPLETE
g32n02:753684:754002 [1] NCCL INFO Using network IB
g32n02:753684:754002 [1] NCCL INFO comm 0x148596ab0 rank 21 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0xeee4c143812ce6e4 - Init START
g32n02:753684:754002 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g32n02:753684:754002 [1] NCCL INFO Trees [0] 22/23/-1->21->18 [1] 22/-1/-1->21->20
g32n02:753684:754002 [1] NCCL INFO P2P Chunksize set to 131072
g32n02:753684:754002 [1] NCCL INFO Channel 00/0 : 20[3] -> 21[1] [receive] via NET/IB/2
g32n02:753684:754002 [1] NCCL INFO Channel 01/0 : 20[3] -> 21[1] [receive] via NET/IB/2
g32n02:753684:754002 [1] NCCL INFO Channel 00/0 : 21[1] -> 22[5] via P2P/IPC
g32n02:753684:754002 [1] NCCL INFO Channel 01/0 : 21[1] -> 22[5] via P2P/IPC
g32n02:753684:754002 [1] NCCL INFO Connected all rings
g32n02:753684:754002 [1] NCCL INFO Channel 00/0 : 21[1] -> 23[3] [send] via NET/IB/2
g32n02:753684:754002 [1] NCCL INFO Channel 00/0 : 18[1] -> 21[1] [receive] via NET/IB/2
g32n02:753684:754002 [1] NCCL INFO Channel 00/0 : 21[1] -> 18[1] [send] via NET/IB/2
g32n02:753684:754002 [1] NCCL INFO Channel 00/0 : 23[3] -> 21[1] [receive] via NET/IB/2
g32n02:753684:754002 [1] NCCL INFO Channel 01/0 : 21[1] -> 20[3] [send] via NET/IB/2
g32n02:753684:754002 [1] NCCL INFO Connected all trees
g32n02:753684:754002 [1] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g32n02:753684:754002 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g32n02:753684:754002 [1] NCCL INFO comm 0x148596ab0 rank 21 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0xeee4c143812ce6e4 - Init COMPLETE
g32n02:753684:754021 [1] NCCL INFO Using network IB
g32n02:753684:754021 [1] NCCL INFO comm 0x1488b1510 rank 10 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0xad5995bf820e7340 - Init START
g32n02:753684:754021 [1] NCCL INFO Setting affinity for GPU 1 tg32n02:753683:753683 [0] NCCL INFO cudaDriverVersion 12020
g32n02:753683:753683 [0] NCCL INFO Bootstrap : Using ib0:10.41.17.247<0>
g32n02:753683:753683 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g32n02:753683:753683 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g32n02:753683:753920 [0] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.247<0>
g32n02:753683:753920 [0] NCCL INFO Using network IB
g32n02:753683:753920 [0] NCCL INFO comm 0x1370b40c0 rank 84 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x28395441218594c5 - Init START
g32n02:753683:753920 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g32n02:753683:753920 [0] NCCL INFO Trees [0] 85/90/-1->84->72 [1] 86/-1/-1->84->85 [2] 85/-1/-1->84->79 [3] 86/-1/-1->84->85
g32n02:753683:753920 [0] NCCL INFO P2P Chunksize set to 131072
g32n02:753683:753920 [0] NCCL INFO Channel 00/0 : 83[5] -> 84[0] [receive] via NET/IB/0
g32n02:753683:753920 [0] NCCL INFO Channel 02/0 : 83[5] -> 84[0] [receive] via NET/IB/0
g32n02:753683:753920 [0] NCCL INFO Channel 00/0 : 84[0] -> 85[1] via P2P/IPC
g32n02:753683:753920 [0] NCCL INFO Channel 02/0 : 84[0] -> 85[1] via P2P/IPC
g32n02:753683:753920 [0] NCCL INFO Channel 01/0 : 84[0] -> 93[3] [send] via NET/IB/2
g32n02:753683:753920 [0] NCCL INFO Channel 03/0 : 84[0] -> 93[3] [send] via NET/IB/2
g32n02:753683:753920 [0] NCCL INFO Connected all rings
g32n02:753683:753920 [0] NCCL INFO Channel 01/0 : 84[0] -> 85[1] via P2P/IPC
g32n02:753683:753920 [0] NCCL INFO Channel 03/0 : 84[0] -> 85[1] via P2P/IPC
g32n02:753683:753920 [0] NCCL INFO Channel 01/0 : 84[0] -> 86[2] via P2P/IPC
g32n02:753683:753920 [0] NCCL INFO Channel 03/0 : 84[0] -> 86[2] via P2P/IPC
g32n02:753683:753920 [0] NCCL INFO Channel 02/0 : 79[1] -> 84[0] [receive] via NET/IB/0
g32n02:753683:753920 [0] NCCL INFO Channel 00/0 : 84[0] -> 90[0] [send] via NET/IB/0
g32n02:753683:753920 [0] NCCL INFO Channel 00/0 : 72[0] -> 84[0] [receive] via NET/IB/0
g32n02:753683:753920 [0] NCCL INFO Channel 00/0 : 84[0] -> 72[0] [send] via NET/IB/0
g32n02:753683:753920 [0] NCCL INFO Channel 00/0 : 90[0] -> 84[0] [receive] via NET/IB/0
g32n02:753683:753920 [0] NCCL INFO Channel 02/0 : 84[0] -> 79[1] [send] via NET/IB/0
g32n02:753683:753920 [0] NCCL INFO Connected all trees
g32n02:753683:753920 [0] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g32n02:753683:753920 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g32n02:753683:753920 [0] NCCL INFO comm 0x1370b40c0 rank 84 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x28395441218594c5 - Init COMPLETE
g32n02:753683:754006 [0] NCCL INFO Using network IB
g32n02:753683:754006 [0] NCCL INFO comm 0x139f9adc0 rank 21 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0x864be1a757d9f276 - Init START
g32n02:753683:754006 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g32n02:753683:754006 [0] NCCL INFO Trees [0] 22/23/-1->21->18 [1] 22/-1/-1->21->20
g32n02:753683:754006 [0] NCCL INFO P2P Chunksize set to 131072
g32n02:753683:754006 [0] NCCL INFO Channel 00/0 : 20[2] -> 21[0] [receive] via NET/IB/0
g32n02:753683:754006 [0] NCCL INFO Channel 01/0 : 20[2] -> 21[0] [receive] via NET/IB/0
g32n02:753683:754006 [0] NCCL INFO Channel 00/0 : 21[0] -> 22[4] via P2P/IPC
g32n02:753683:754006 [0] NCCL INFO Channel 01/0 : 21[0] -> 22[4] via P2P/IPC
g32n02:753683:754006 [0] NCCL INFO Connected all rings
g32n02:753683:754006 [0] NCCL INFO Channel 00/0 : 21[0] -> 23[2] [send] via NET/IB/0
g32n02:753683:754006 [0] NCCL INFO Channel 00/0 : 18[0] -> 21[0] [receive] via NET/IB/0
g32n02:753683:754006 [0] NCCL INFO Channel 00/0 : 21[0] -> 18[0] [send] via NET/IB/0
g32n02:753683:754006 [0] NCCL INFO Channel 00/0 : 23[2] -> 21[0] [receive] via NET/IB/0
g32n02:753683:754006 [0] NCCL INFO Channel 01/0 : 21[0] -> 20[2] [send] via NET/IB/0
g32n02:753683:754006 [0] NCCL INFO Connected all trees
g32n02:753683:754006 [0] NCCL INFO threadThresholds 8/8/64 | 192/8/o f0000000
g32n02:753684:754021 [1] NCCL INFO Trees [0] 9/11/-1->10->8 [1] -1/-1/-1->10->9
g32n02:753684:754021 [1] NCCL INFO P2P Chunksize set to 131072
g32n02:753684:754021 [1] NCCL INFO Channel 00/0 : 9[5] -> 10[1] [receive] via NET/IB/2
g32n02:753684:754021 [1] NCCL INFO Channel 01/0 : 9[5] -> 10[1] [receive] via NET/IB/2
g32n02:753684:754021 [1] NCCL INFO Channel 00/0 : 10[1] -> 11[3] [send] via NET/IB/2
g32n02:753684:754021 [1] NCCL INFO Channel 01/0 : 10[1] -> 11[3] [send] via NET/IB/2
g32n02:753684:754021 [1] NCCL INFO Connected all rings
g32n02:753684:754021 [1] NCCL INFO Channel 00/0 : 8[3] -> 10[1] [receive] via NET/IB/2
g32n02:753684:754021 [1] NCCL INFO Channel 00/0 : 10[1] -> 8[3] [send] via NET/IB/2
g32n02:753684:754021 [1] NCCL INFO Channel 00/0 : 11[3] -> 10[1] [receive] via NET/IB/2
g32n02:753684:754021 [1] NCCL INFO Channel 00/0 : 10[1] -> 9[5] [send] via NET/IB/2
g32n02:753684:754021 [1] NCCL INFO Channel 01/0 : 10[1] -> 9[5] [send] via NET/IB/2
g32n02:753684:754021 [1] NCCL INFO Connected all trees
g32n02:753684:754021 [1] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g32n02:753684:754021 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g32n02:753684:754021 [1] NCCL INFO comm 0x1488b1510 rank 10 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0xad5995bf820e7340 - Init COMPLETE
NFO Trees [0] 9/11/-1->10->8 [1] -1/-1/-1->10->9
g32n02:753685:754023 [2] NCCL INFO P2P Chunksize set to 131072
g32n02:753685:754023 [2] NCCL INFO Channel 00/0 : 9[0] -> 10[2] [receive] via NET/IB/0
g32n02:753685:754023 [2] NCCL INFO Channel 01/0 : 9[0] -> 10[2] [receive] via NET/IB/0
g32n02:753685:754023 [2] NCCL INFO Channel 00/0 : 10[2] -> 11[4] [send] via NET/IB/0
g32n02:753685:754023 [2] NCCL INFO Channel 01/0 : 10[2] -> 11[4] [send] via NET/IB/0
g32n02:753685:754023 [2] NCCL INFO Connected all rings
g32n02:753685:754023 [2] NCCL INFO Channel 00/0 : 8[4] -> 10[2] [receive] via NET/IB/0
g32n02:753685:754023 [2] NCCL INFO Channel 00/0 : 10[2] -> 8[4] [send] via NET/IB/0
g32n02:753685:754023 [2] NCCL INFO Channel 00/0 : 11[4] -> 10[2] [receive] via NET/IB/0
g32n02:753685:754023 [2] NCCL INFO Channel 00/0 : 10[2] -> 9[0] [send] via NET/IB/0
g32n02:753685:754023 [2] NCCL INFO Channel 01/0 : 10[2] -> 9[0] [send] via NET/IB/0
g32n02:753685:754023 [2] NCCL INFO Connected all trees
g32n02:753685:754023 [2] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g32n02:753685:754023 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g32n02:753685:754023 [2] NCCL INFO comm 0x13c136350 rank 10 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0x8442a1c07b0a8edb - Init COMPLETE
64 | 512 | 512
g32n02:753683:754006 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g32n02:753683:754006 [0] NCCL INFO comm 0x139f9adc0 rank 21 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0x864be1a757d9f276 - Init COMPLETE
g32n02:753683:754019 [0] NCCL INFO Using network IB
g32n02:753683:754019 [0] NCCL INFO comm 0x139a15560 rank 10 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x51bdf7b6b8c3aecb - Init START
g32n02:753683:754019 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g32n02:753683:754019 [0] NCCL INFO Trees [0] 9/11/-1->10->8 [1] -1/-1/-1->10->9
g32n02:753683:754019 [0] NCCL INFO P2P Chunksize set to 131072
g32n02:753683:754019 [0] NCCL INFO Channel 00/0 : 9[4] -> 10[0] [receive] via NET/IB/0
g32n02:753683:754019 [0] NCCL INFO Channel 01/0 : 9[4] -> 10[0] [receive] via NET/IB/0
g32n02:753683:754019 [0] NCCL INFO Channel 00/0 : 10[0] -> 11[2] [send] via NET/IB/0
g32n02:753683:754019 [0] NCCL INFO Channel 01/0 : 10[0] -> 11[2] [send] via NET/IB/0
g32n02:753683:754019 [0] NCCL INFO Connected all rings
g32n02:753683:754019 [0] NCCL INFO Channel 00/0 : 8[2] -> 10[0] [receive] via NET/IB/0
g32n02:753683:754019 [0] NCCL INFO Channel 00/0 : 10[0] -> 8[2] [send] via NET/IB/0
g32n02:753683:754019 [0] NCCL INFO Channel 00/0 : 11[2] -> 10[0] [receive] via NET/IB/0
g32n02:753683:754019 [0] NCCL INFO Channel 00/0 : 10[0] -> 9[4] [send] via NET/IB/0
g32n02:753683:754019 [0] NCCL INFO Channel 01/0 : 10[0] -> 9[4] [send] via NET/IB/0
g32n02:753683:754019 [0] NCCL INFO Connected all trees
g32n02:753683:754019 [0] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g32n02:753683:754019 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g32n02:753683:754019 [0] NCCL INFO comm 0x139a15560 rank 10 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x51bdf7b6b8c3aecb - Init COMPLETE
g28n04:841284:841284 [3] NCCL INFO cudaDriverVersion 12020
g28n04:841284:841284 [3] NCCL INFO Bootstrap : Using ib0:10.41.17.177<0>
g28n04:841284:841284 [3] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g28n04:841284:841284 [3] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g28n04:841284:841515 [3] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.177<0>
g28n04:841284:841515 [3] NCCL INFO Using network IB
g28n04:841284:841515 [3] NCCL INFO comm 0x16d2748b0 rank 51 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x28395441218594c5 - Init START
g28n04:841284:841515 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g28n04:841284:841515 [3] NCCL INFO Trees [0] 52/-1/-1->51->50 [1] 53/28/-1->51->52 [2] 52/-1/-1->51->50 [3] 53/-1/-1->51->52
g28n04:841284:841515 [3] NCCL INFO P2P Chunksize set to 131072
g28n04:841284:841515 [3] NCCL INFO Channel 00/0 : 51[3] -> 52[4] via P2P/IPC
g28n04:841284:841515 [3] NCCL INFO Channel 01/0 : 51[3] -> 52[4] via P2P/IPC
g28n04:841284:841515 [3] NCCL INFO Channel 02/0 : 51[3] -> 52[4] via P2P/IPC
g28n04:841284:841515 [3] NCCL INFO Channel 03/0 : 51[3] -> 52[4] via P2P/IPC
g28n04:841284:841515 [3] NCCL INFO Channel 01/0 : 42[0] -> 51[3] [receive] via NET/IB/3
g28n04:841284:841515 [3] NCCL INFO Channel 03/0 : 42[0] -> 51[3] [receive] via NET/IB/3
g28n04:841284:841515 [3] NCCL INFO Connected all rings
g28n04:841284:841515 [3] NCCL INFO Channel 01/0 : 51[3] -> 53[5] via P2P/IPC
g28n04:841284:841515 [3] NCCL INFO Channel 03/0 : 51[3] -> 53[5] via P2P/IPC
g28n04:841284:841515 [3] NCCL INFO Channel 01/0 : 28[4] -> 51[3] [receive] via NET/IB/3
g28n04:841284:841515 [3] NCCL INFO Channel 01/0 : 51[3] -> 28[4] [send] via NET/IB/3
g28n04:841284:841515 [3] NCCL INFO Channel 00/0 : 51[3] -> 50[2] via P2P/IPC
g28n04:841284:841515 [3] NCCL INFO Channel 02/0 : 51[3] -> 50[2] via P2P/IPC
g28n04:841284:841515 [3] NCCL INFO Connected all trees
g28n04:841284:841515 [3] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g28n04:841284:841515 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g28n04:841284:841515 [3] NCCL INFO comm 0x16d2748b0 rank 51 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x28395441218594c5 - Init COMPLETE
g28n04:841284:841599 [3] NCCL INFO Using network IB
g28n04:841284:841599 [3] NCCL INFO comm 0x16f0b9060 rank 12 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x15c93a2ee1bf52db - Init START
g28n04:841284:841599 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g28n04:841284:841599 [3] NCCL INFO Trees [0] 6/18/-1->12->0 [1] -1/-1/-1->12->13
g28n04:841284:841599 [3] NCCL INFO P2P Chunksize set to 131072
g28n04:841284:841599 [3] NCCL INFO Channel 00/0 : 11[5] -> 12[3] [receive] via NET/IB/3
g28n04:841284:841599 [3] NCCL INFO Channel 01/0 : 11[5] -> 12[3] [receive] via NET/IB/3
g28n04:841284:841599 [3] NCCL INFO Channel 00/0 : 12[3] -> 13[1] [send] via NET/IB/3
g28n04:841284:841599 [3] NCCL INFO Channel 01/0 : 12[3] -> 13[1] [send] via NET/IB/3
g28n04:841284:841599 [3] NCCL INFO Connected all rings
g28n04:841284:841599 [3] NCCL INFO Channel 00/0 : 6[3] -> 12[3] [receive] via NET/IB/3
g28n04:841284:841599 [3] NCCL INFO Channel 00/0 : 12[3] -> 18[3] [send] via NET/IB/3
g28n04:841284:841599 [3] NCCL INFO Channel 00/0 : 0[3] -> 12[3] [receive] via NET/IB/3
g28n04:841284:841599 [3] NCCL INFO Channel 00/0 : 12[3] -> 0[3] [send] via NET/IB/3
g28n04:841284:841599 [3] NCCL INFO Channel 00/0 : 18[3] -> 12[3] [receive] via NET/IB/3
g28n04:841284:841599 [3] NCCL INFO Channel 00/0 : 12[3] -> 6[3] [send] via NET/IB/3
g28n04:841284:841599 [3] NCCL INFO Channel 01/0 : 13[1] -> 12[3] [receive] via NET/IB/3
g28n04:841284:841599 [3] NCCL INFO Connected all trees
g28n04:841284:841599 [3] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g28n04:841284:841599 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n04:841284:841599 [3] NCCL INFO comm 0x16f0b9060 rank 12 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x15c93a2ee1bf52db - Init COMPLETE
g28n04:841284:841622 [3] NCCL INFO Using network IB
g28n04:841284:841622 [3] NCCL INFO comm 0x16fbcea00 rank 6 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x1542a3b966c3490b - Init START
g28n04:841284:841622 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g28n04:841284:841622 [3] NCCL INFO Trees [0] 5/7/-1->6->4 [1] -1/-1/-1->6->5
g28n04:841284:841622 [3] NCCL INFO P2P Chunksize set to 131072
g28n04:841284:841622 [3] NCCL INFO Channel 00/0 : 5[1] -> 6[3] [receive] via NET/IB/3
g28n04:841284:841622 [3] NCCL INFO Channel 01/0 : 5[1] -> 6[3] [receive] via NET/IB/3
g28n04:841284:841622 [3] NCCL INFO Channel 00/0 : 6[3] -> 7[5] [send] via NET/IB/3
g28n04:841284:841622 [3] NCCL INFO Channel 01/0 : 6[3] -> 7[5] [send] via NET/IB/3
g28n04:841284:841622 [3] NCCL INFO Connected all rings
g28n04:841284:841622 [3] NCCL INFO Channel 00/0 : 4[5] -> 6[3] [receive] via NET/IB/3
g28n04:841284:841622 [3] NCCL INFO Channel 00/0 : 6[3] -> 4[5] [send] via NET/IB/3
g28n04:841284:841622 [3] NCCL INFO Channel 00/0 : 7[5] -> 6[3] [receive] via NET/IB/3
g28n04:841284:841622 [3] NCCL INFO Channel 00/0 : 6[3] -> 5[1] [send] via NET/IB/3
g28n04:841284:841622 [3] NCCL INFO Channel 01/0 : 6[3] -> 5[1] [send] via NET/IB/3
g28n04:841284:841622 [3] NCCL INFO Connected all trees
g28n04:841284:841622 [3] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g28n04:841284:841622 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n04:841284:841622 [3] NCCL INFO comm 0x16fbcea00 rank 6 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x1542a3b966c3490b - Init COMPLETE
g28n04:841287:841287 [4] NCCL INFO cudaDriverVersion 12020
g28n04:841287:841287 [4] NCCL INFO Bootstrap : Using ib0:10.41.17.177<0>
g28n04:841287:841287 [4] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g28n04:841287:841287 [4] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g28n04:841287:841516 [4] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.177<0>
g28n04:841287:841516 [4] NCCL INFO Using network IB
g28n04:841287:841516 [4] NCCL INFO comm 0x1465b3d10 rank 52 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x28395441218594c5 - Init START
g28n04:841287:841516 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g28n04:841287:841516 [4] NCCL INFO Trees [0] 53/-1/-1->52->51 [1] 51/76/-1->52->4 [2] 53/-1/-1->52->51 [3] 51/-1/-1->52->58
g28n04:841287:841516 [4] NCCL INFO P2P Chunksize set to 131072
g28n04:841287:841516 [4] NCCL INFO Channel 00/0 : 52[4] -> 53[5] via P2P/IPC
g28n04:841287:841516 [4] NCCL INFO Channel 01/0 : 52[4] -> 53[5] via P2P/IPC
g28n04:841287:841516 [4] NCCL INFO Channel 02/0 : 52[4] -> 53[5] via P2P/IPC
g28n04:841287:841516 [4] NCCL INFO Channel 03/0 : 52[4] -> 53[5] via P2P/IPC
g28n04:841287:841516 [4] NCCL INFO Connected all rings
g28n04:841287:841516 [4] NCCL INFO Channel 03/0 : 52[4] -> 58[4] [send] via NET/IB/3
g28n04:841287:841516 [4] NCCL INFO Channel 01/0 : 52[4] -> 76[4] [send] via NET/IB/3
g28n04:841287:841516 [4] NCCL INFO Channel 01/0 : 4[4] -> 52[4] [receive] via NET/IB/3
g28n04:841287:841516 [4] NCCL INFO Channel 01/0 : 52[4] -> 4[4] [send] via NET/IB/3
g28n04:841287:841516 [4] NCCL INFO Channel 01/0 : 76[4] -> 52[4] [receive] via NET/IB/3
g28n04:841287:841516 [4] NCCL INFO Channel 03/0 : 58[4] -> 52[4] [receive] via NET/IB/3
g28n04:841287:841516 [4] NCCL INFO Channel 00/0 : 52[4] -> 51[3] via P2P/IPC
g28n04:841287:841516 [4] NCCL INFO Channel 01/0 : 52[4] -> 51[3] via P2P/IPC
g28n04:841287:841516 [4] NCCL INFO Channel 02/0 : 52[4] -> 51[3] via P2P/IPC
g28n04:841287:841516 [4] NCCL INFO Channel 03/0 : 52[4] -> 51[3] via P2P/IPC
g28n04:841287:841516 [4] NCCL INFO Connected all trees
g28n04:841287:841516 [4] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g28n04:841287:841516 [4] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g28n04:841287:841516 [4] NCCL INFO comm 0x1465b3d10 rank 52 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x28395441218594c5 - Init COMPLETE
g28n04:841287:841602 [4] NCCL INFO Using network IB
g28n04:841287:841602 [4] NCCL INFO comm 0x14932ff70 rank 13 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x864be1a757d9f276 - Init START
g28n04:841287:841602 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g28n04:841287:841602 [4] NCCL INFO Trees [0] 6/-1/-1->13->12 [1] -1/-1/-1->13->12
g28n04:841287:841602 [4] NCCL INFO P2P Chunksize set to 131072
g28n04:841287:841602 [4] NCCL INFO Channel 00/0 : 13[4] -> 14[2] [send] via NET/IB/1
g28n04:841287:841602 [4] NCCL INFO Channel 01/0 : 13[4] -> 14[2] [send] via NET/IB/1
g28n04:841287:841602 [4] NCCL INFO Connected all rings
g28n04:841287:841602 [4] NCCL INFO Channel 00/0 : 6[0] -> 13[4] [receive] via NET/IB/0
g28n04:841287:841602 [4] NCCL INFO Channel 00/0 : 13[4] -> 6[0] [send] via NET/IB/0
g28n04:841287:841602 [4] NCCL INFO Channel 00/0 : 13[4] -> 12[0] via P2P/IPC
g28n04:841287:841602 [4] NCCL INFO Channel 01/0 : 13[4] -> 12[0] via P2P/IPC
g28n04:841287:841602 [4] NCCL INFO Connected all trees
g28n04:841287:841602 [4] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g28n04:841287:841602 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n04:841287:841602 [4] NCCL INFO comm 0x14932ff70 rank 13 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x864be1a757d9f276 - Init COMPLETE
g28n04:841287:841618 [4] NCCL INFO Using network IB
g28n04:841287:841618 [4] NCCL INFO comm 0x149316890 rank 6 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x51bdf7b6b8c3aecb - Init START
g28n04:841287:841618 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g28n04:841287:841618 [4] NCCL INFO Trees [0] 5/7/-1->6->4 [1] -1/-1/-1->6->5
g28n04:841287:841618 [4] NCCL INFO P2P Chunksize set to 131072
g28n04:841287:841618 [4] NCCL INFO Channel 00/0 : 5[2] -> 6[4] [receive] via NET/IB/1
g28n04:841287:841618 [4] NCCL INFO Channel 01/0 : 5[2] -> 6[4] [receive] via NET/IB/1
g28n04:841287:841618 [4] NCCL INFO Channel 00/0 : 6[4] -> 7[0] [send] via NET/IB/1
g28n04:841287:841618 [4] NCCL INFO Channel 01/0 : 6[4] -> 7[0] [send] via NET/IB/1
g28n04:841287:841618 [4] NCCL INFO Connected all rings
g28n04:841287:841618 [4] NCCL INFO Channel 00/0 : 4[0] -> 6[4] [receive] via NET/IB/1
g28n04:841287:841618 [4] NCCL INFO Channel 00/0 : 6[4] -> 4[0] [send] via NET/IB/1
g28n04:841287:841618 [4] NCCL INFO Channel 00/0 : 7[0] -> 6[4] [receive] via NET/IB/1
g28n04:841287:841618 [4] NCCL INFO Channel 00/0 : 6[4] -> 5[2] [send] via NET/IB/1
g28n04:841287:841618 [4] NCCL INFO Channel 01/0 : 6[4] -> 5[2] [send] via NET/IB/1
g28n04:841287:841618 [4] NCCL INFO Connected all trees
g28n04:841287:841618 [4] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g28n04:841287:841618 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n04:841287:841618 [4] NCCL INFO comm 0x149316890 rank 6 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x51bdf7b6b8c3aecb - Init COMPLETE
g28n04:841288:841288 [5] NCCL INFO cudaDriverVersion 12020
g28n04:841288:841288 [5] NCCL INFO Bootstrap : Using ib0:10.41.17.177<0>
g28n04:841288:841288 [5] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g28n04:841288:841288 [5] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g28n04:841288:841514 [5] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.177<0>
g28n04:841288:841514 [5] NCCL INFO Using network IB
g28n04:841288:841514 [5] NCCL INFO comm 0x1679c3cf0 rank 53 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x28395441218594c5 - Init START
g28n04:841288:841514 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g28n04:841288:841514 [5] NCCL INFO Trees [0] -1/-1/-1->53->52 [1] 49/-1/-1->53->51 [2] -1/-1/-1->53->52 [3] 49/-1/-1->53->51
g28n04:841288:841514 [5] NCCL INFO P2P Chunksize set to 131072
g28n04:841288:841514 [5] NCCL INFO Channel 00/0 : 53[5] -> 54[0] [send] via NET/IB/1
g28n04:841288:841514 [5] NCCL INFO Channel 02/0 : 53[5] -> 54[0] [send] via NET/IB/1
g28n04:841288:841514 [5] NCCL INFO Channel 01/0 : 53[5] -> 50[2] via P2P/IPC
g28n04:841288:841514 [5] NCCL INFO Channel 03/0 : 53[5] -> 50[2] via P2P/IPC
g28n04:841288:841514 [5] NCCL INFO Connected all rings
g28n04:841288:841514 [5] NCCL INFO Channel 01/0 : 53[5] -> 49[1] via P2P/IPC
g28n04:841288:841514 [5] NCCL INFO Channel 03/0 : 53[5] -> 49[1] via P2P/IPC
g28n04:841288:841514 [5] NCCL INFO Channel 01/0 : 53[5] -> 51[3] via P2P/IPC
g28n04:841288:841514 [5] NCCL INFO Channel 03/0 : 53[5] -> 51[3] via P2P/IPC
g28n04:841288:841514 [5] NCCL INFO Channel 00/0 : 53[5] -> 52[4] via P2P/IPC
g28n04:841288:841514 [5] NCCL INFO Channel 02/0 : 53[5] -> 52[4] via P2P/IPC
g28n04:841288:841514 [5] NCCL INFO Connected all trees
g28n04:841288:841514 [5] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g28n04:841288:841514 [5] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g28n04:841288:841514 [5] NCCL INFO comm 0x1679c3cf0 rank 53 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x28395441218594c5 - Init COMPLETE
g28n04:841288:841597 [5] NCCL INFO Using network IB
g28n04:841288:841597 [5] NCCL INFO comm 0x1695a2130 rank 13 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xeee4c143812ce6e4 - Init START
g28n04:841288:841597 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g28n04:841288:841597 [5] NCCL INFO Trees [0] 6/-1/-1->13->12 [1] -1/-1/-1->13->12
g28n04:841288:841597 [5] NCCL INFO P2P Chunksize set to 131072
g28n04:841288:841597 [5] NCCL INFO Channel 00/0 : 13[5] -> 14[3] [send] via NET/IB/3
g28n04:841288:841597 [5] NCCL INFO Channel 01/0 : 13[5] -> 14[3] [send] via NET/IB/3
g28n04:841288:841597 [5] NCCL INFO Connected all rings
g28n04:841288:841597 [5] NCCL INFO Channel 00/0 : 6[1] -> 13[5] [receive] via NET/IB/2
g28n04:841288:841597 [5] NCCL INFO Channel 00/0 : 13[5] -> 6[1] [send] via NET/IB/2
g28n04:841288:841597 [5] NCCL INFO Channel 00/0 : 13[5] -> 12[1] via P2P/IPC
g28n04:841288:841597 [5] NCCL INFO Channel 01/0 : 13[5] -> 12[1] via P2P/IPC
g28n04:841288:841597 [5] NCCL INFO Connected all trees
g28n04:841288:841597 [5] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g28n04:841288:841597 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n04:841288:841597 [5] NCCL INFO comm 0x1695a2130 rank 13 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xeee4c143812ce6e4 - Init COMPLETE
g28n04:841288:841620 [5] NCCL INFO Using network IB
g28n04:841288:841620 [5] NCCL INFO comm 0x1696956b0 rank 6 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xad5995bf820e7340 - Init START
g28n04:841288:841620 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g28n04:841288:841620 [5] NCCL INFO Trees [0] 5/7/-1->6->4 [1] -1/-1/-1->6->5
g28n04:841288:841620 [5] NCCL INFO P2P Chunksize set to 131072
g28n04:841288:841620 [5] NCCL INFO Channel 00/0 : 5[3] -> 6[5] [receive] via NET/IB/3
g28n04:841288:841620 [5] NCCL INFO Channel 01/0 : 5[3] -> 6[5] [receive] via NET/IB/3
g28n04:841288:841620 [5] NCCL INFO Channel 00/0 : 6[5] -> 7[1] [send] via NET/IB/3
g28n04:841288:841620 [5] NCCL INFO Channel 01/0 : 6[5] -> 7[1] [send] via NET/IB/3
g28n04:841288:841620 [5] NCCL INFO Connected all rings
g28n04:841288:841620 [5] NCCL INFO Channel 00/0 : 4[1] -> 6[5] [receive] via NET/IB/3
g28n04:841288:841620 [5] NCCL INFO Channel 00/0 : 6[5] -> 4[1] [send] via NET/IB/3
g28n04:841288:841620 [5] NCCL INFO Channel 00/0 : 7[1] -> 6[5] [receive] via NET/IB/3
g28n04:841288:841620 [5] NCCL INFO Channel 00/0 : 6[5] -> 5[3] [send] via NET/IB/3
g28n04:841288:841620 [5] NCCL INFO Channel 01/0 : 6[5] -> 5[3] [send] via NET/IB/3
g28n04:841288:841620 [5] NCCL INFO Connected all trees
g28n04:841288:841620 [5] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g28n04:841288:841620 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n04:841288:841620 [5] NCCL INFO comm 0x1696956b0 rank 6 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xad5995bf820e7340 - Init COMPLETE
g28n04:841283:841283 [2] NCCL INFO cudaDriverVersion 12020
g28n04:841283:841283 [2] NCCL INFO Bootstrap : Using ib0:10.41.17.177<0>
g28n04:841283:841283 [2] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g28n04:841283:841283 [2] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g28n04:841283:841513 [2] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.177<0>
g28n04:841283:841513 [2] NCCL INFO Using network IB
g28n04:841283:841513 [2] NCCL INFO comm 0x12c883fd0 rank 50 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x28395441218594c5 - Init START
g28n04:841283:841513 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g28n04:841283:841513 [2] NCCL INFO Trees [0] 51/-1/-1->50->49 [1] -1/-1/-1->50->48 [2] 51/-1/-1->50->49 [3] -1/-1/-1->50->48
g28n04:841283:841513 [2] NCCL INFO P2P Chunksize set to 131072
g28n04:841283:841513 [2] NCCL INFO Channel 00/0 : 50[2] -> 51[3] via P2P/IPC
g28n04:841283:841513 [2] NCCL INFO Channel 02/0 : 50[2] -> 51[3] via P2P/IPC
g28n04:841283:841513 [2] NCCL INFO Channel 01/0 : 50[2] -> 49[1] via P2P/IPC
g28n04:841283:841513 [2] NCCL INFO Channel 03/0 : 50[2] -> 49[1] via P2P/IPC
g28n04:841283:841513 [2] NCCL INFO Connected all rings
g28n04:841283:841513 [2] NCCL INFO Channel 01/0 : 50[2] -> 48[0] via P2P/IPC
g28n04:841283:841513 [2] NCCL INFO Channel 03/0 : 50[2] -> 48[0] via P2P/IPC
g28n04:841283:841513 [2] NCCL INFO Channel 00/0 : 50[2] -> 49[1] via P2P/IPC
g28n04:841283:841513 [2] NCCL INFO Channel 02/0 : 50[2] -> 49[1] via P2P/IPC
g28n04:841283:841513 [2] NCCL INFO Connected all trees
g28n04:841283:841513 [2] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g28n04:841283:841513 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g28n04:841283:841513 [2] NCCL INFO comm 0x12c883fd0 rank 50 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x28395441218594c5 - Init COMPLETE
g28n04:841283:841601 [2] NCCL INFO Using network IB
g28n04:841283:841601 [2] NCCL INFO comm 0x12fcd14c0 rank 12 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0xe45164e09488020a - Init START
g28n04:841283:841601 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g28n04:841283:841601 [2] NCCL INFO Trees [0] 6/18/-1->12->0 [1] -1/-1/-1->12->13
g28n04:841283:841601 [2] NCCL INFO P2P Chunksize set to 131072
g28n04:841283:841601 [2] NCCL INFO Channel 00/0 : 11[4] -> 12[2] [receive] via NET/IB/0
g28n04:841283:841601 [2] NCCL INFO Channel 01/0 : 11[4] -> 12[2] [receive] via NET/IB/0
g28n04:841283:841601 [2] NCCL INFO Channel 00/0 : 12[2] -> 13[0] [send] via NET/IB/0
g28n04:841283:841601 [2] NCCL INFO Channel 01/0 : 12[2] -> 13[0] [send] via NET/IB/0
g28n04:841283:841601 [2] NCCL INFO Connected all rings
g28n04:841283:841601 [2] NCCL INFO Channel 00/0 : 6[2] -> 12[2] [receive] via NET/IB/0
g28n04:841283:841601 [2] NCCL INFO Channel 00/0 : 12[2] -> 18[2] [send] via NET/IB/0
g28n04:841283:841601 [2] NCCL INFO Channel 00/0 : 0[2] -> 12[2] [receive] via NET/IB/0
g28n04:841283:841601 [2] NCCL INFO Channel 00/0 : 12[2] -> 0[2] [send] via NET/IB/0
g28n04:841283:841601 [2] NCCL INFO Channel 00/0 : 18[2] -> 12[2] [receive] via NET/IB/0
g28n04:841283:841601 [2] NCCL INFO Channel 00/0 : 12[2] -> 6[2] [send] via NET/IB/0
g28n04:841283:841601 [2] NCCL INFO Channel 01/0 : 13[0] -> 12[2] [receive] via NET/IB/0
g28n04:841283:841601 [2] NCCL INFO Connected all trees
g28n04:841283:841601 [2] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g28n04:841283:841601 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n04:841283:841601 [2] NCCL INFO comm 0x12fcd14c0 rank 12 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0xe45164e09488020a - Init COMPLETE
g28n04:841283:841621 [2] NCCL INFO Using network IB
g28n04:841283:841621 [2] NCCL INFO comm 0x12e4c5a00 rank 6 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0xaca7b5a6b2aa6a19 - Init START
g28n04:841283:841621 [2] NCCL INFg28n04:841281:841281 [1] NCCL INFO cudaDriverVersion 12020
g28n04:841281:841281 [1] NCCL INFO Bootstrap : Using ib0:10.41.17.177<0>
g28n04:841281:841281 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g28n04:841281:841281 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g28n04:841281:841511 [1] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.177<0>
g28n04:841281:841511 [1] NCCL INFO Using network IB
g28n04:841281:841511 [1] NCCL INFO comm 0x10dc13ea0 rank 49 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x28395441218594c5 - Init START
g28n04:841281:841511 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g28n04:841281:841511 [1] NCCL INFO Trees [0] 50/24/-1->49->48 [1] 48/-1/-1->49->53 [2] 50/-1/-1->49->48 [3] 48/-1/-1->49->53
g28n04:841281:841511 [1] NCCL INFO P2P Chunksize set to 131072
g28n04:841281:841511 [1] NCCL INFO Channel 00/0 : 49[1] -> 50[2] via P2P/IPC
g28n04:841281:841511 [1] NCCL INFO Channel 02/0 : 49[1] -> 50[2] via P2P/IPC
g28n04:841281:841511 [1] NCCL INFO Channel 01/0 : 49[1] -> 48[0] via P2P/IPC
g28n04:841281:841511 [1] NCCL INFO Channel 03/0 : 49[1] -> 48[0] via P2P/IPC
g28n04:841281:841511 [1] NCCL INFO Connected all rings
g28n04:841281:841511 [1] NCCL INFO Channel 01/0 : 49[1] -> 53[5] via P2P/IPC
g28n04:841281:841511 [1] NCCL INFO Channel 03/0 : 49[1] -> 53[5] via P2P/IPC
g28n04:841281:841511 [1] NCCL INFO Channel 00/0 : 24[0] -> 49[1] [receive] via NET/IB/0
g28n04:841281:841511 [1] NCCL INFO Channel 00/0 : 49[1] -> 24[0] [send] via NET/IB/0
g28n04:841281:841511 [1] NCCL INFO Channel 00/0 : 49[1] -> 48[0] via P2P/IPC
g28n04:841281:841511 [1] NCCL INFO Channel 02/0 : 49[1] -> 48[0] via P2P/IPC
g28n04:841281:841511 [1] NCCL INFO Connected all trees
g28n04:841281:841511 [1] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g28n04:841281:841511 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g28n04:841281:841511 [1] NCCL INFO comm 0x10dc13ea0 rank 49 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x28395441218594c5 - Init COMPLETE
g28n04:841281:841598 [1] NCCL INFO Using network IB
g28n04:841281:841598 [1] NCCL INFO comm 0x1108996b0 rank 12 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0xeee4c143812ce6e4 - Init START
g28n04:841281:841598 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g28n04:841281:841598 [1] NCCL INFO Trees [0] 13/18/-1->12->0 [1] 13/-1/-1->12->14
g28n04:841281:841598 [1] NCCL INFO P2P Chunksize set to 131072
g28n04:841281:841598 [1] NCCL INFO Channel 00/0 : 11[3] -> 12[1] [receive] via NET/IB/2
g28n04:841281:841598 [1] NCCL INFO Channel 01/0 : 11[3] -> 12[1] [receive] via NET/IB/2
g28n04:841281:841598 [1] NCCL INFO Channel 00/0 : 12[1] -> 13[5] via P2P/IPC
g28n04:841281:841598 [1] NCCL INFO Channel 01/0 : 12[1] -> 13[5] via P2P/IPC
g28n04:841281:841598 [1] NCCL INFO Connected all rings
g28n04:841281:841598 [1] NCCL INFO Channel 01/0 : 12[1] -> 14[3] [send] via NET/IB/2
g28n04:841281:841598 [1] NCCL INFO Channel 00/0 : 12[1] -> 18[1] [send] via NET/IB/2
g28n04:841281:841598 [1] NCCL INFO Channel 00/0 : 0[1] -> 12[1] [receive] via NET/IB/2
g28n04:841281:841598 [1] NCCL INFO Channel 00/0 : 12[1] -> 0[1] [send] via NET/IB/2
g28n04:841281:841598 [1] NCCL INFO Channel 00/0 : 18[1] -> 12[1] [receive] via NET/IB/2
g28n04:841281:841598 [1] NCCL INFO Channel 01/0 : 14[3] -> 12[1] [receive] via NET/IB/2
g28n04:841281:841598 [1] NCCL INFO Connected all trees
g28n04:841281:841598 [1] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g28n04:841281:841598 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n04:841281:841598 [1] NCCL INFO comm 0x1108996b0 rank 12 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0xeee4c143812ce6e4 - Init COMPLETE
g28n04:841281:841619 [1] NCCL INFO Using network IB
g28n04:841281:841619 [1] NCCL INFO comm 0x11095a020 rank 6 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x17fc85f12g28n04:841280:841280 [0] NCCL INFO cudaDriverVersion 12020
g28n04:841280:841280 [0] NCCL INFO Bootstrap : Using ib0:10.41.17.177<0>
g28n04:841280:841280 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g28n04:841280:841280 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g28n04:841280:841512 [0] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.177<0>
g28n04:841280:841512 [0] NCCL INFO Using network IB
g28n04:841280:841512 [0] NCCL INFO comm 0x14b1540a0 rank 48 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x28395441218594c5 - Init START
g28n04:841280:841512 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g28n04:841280:841512 [0] NCCL INFO Trees [0] 49/72/-1->48->0 [1] 50/-1/-1->48->49 [2] 49/-1/-1->48->54 [3] 50/-1/-1->48->49
g28n04:841280:841512 [0] NCCL INFO P2P Chunksize set to 131072
g28n04:841280:841512 [0] NCCL INFO Channel 00/0 : 47[5] -> 48[0] [receive] via NET/IB/0
g28n04:841280:841512 [0] NCCL INFO Channel 02/0 : 47[5] -> 48[0] [receive] via NET/IB/0
g28n04:841280:841512 [0] NCCL INFO Channel 00/0 : 48[0] -> 49[1] via P2P/IPC
g28n04:841280:841512 [0] NCCL INFO Channel 02/0 : 48[0] -> 49[1] via P2P/IPC
g28n04:841280:841512 [0] NCCL INFO Channel 01/0 : 48[0] -> 57[3] [send] via NET/IB/2
g28n04:841280:841512 [0] NCCL INFO Channel 03/0 : 48[0] -> 57[3] [send] via NET/IB/2
g28n04:841280:841512 [0] NCCL INFO Connected all rings
g28n04:841280:841512 [0] NCCL INFO Channel 01/0 : 48[0] -> 49[1] via P2P/IPC
g28n04:841280:841512 [0] NCCL INFO Channel 03/0 : 48[0] -> 49[1] via P2P/IPC
g28n04:841280:841512 [0] NCCL INFO Channel 01/0 : 48[0] -> 50[2] via P2P/IPC
g28n04:841280:841512 [0] NCCL INFO Channel 03/0 : 48[0] -> 50[2] via P2P/IPC
g28n04:841280:841512 [0] NCCL INFO Channel 02/0 : 48[0] -> 54[0] [send] via NET/IB/0
g28n04:841280:841512 [0] NCCL INFO Channel 00/0 : 48[0] -> 72[0] [send] via NET/IB/0
g28n04:841280:841512 [0] NCCL INFO Channel 00/0 : 0[0] -> 48[0] [receive] via NET/IB/0
g28n04:841280:841512 [0] NCCL INFO Channel 00/0 : 48[0] -> 0[0] [send] via NET/IB/0
g28n04:841280:841512 [0] NCCL INFO Channel 00/0 : 72[0] -> 48[0] [receive] via NET/IB/0
g28n04:841280:841512 [0] NCCL INFO Channel 02/0 : 54[0] -> 48[0] [receive] via NET/IB/0
g28n04:841280:841512 [0] NCCL INFO Connected all trees
g28n04:841280:841512 [0] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g28n04:841280:841512 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g28n04:841280:841512 [0] NCCL INFO comm 0x14b1540a0 rank 48 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x28395441218594c5 - Init COMPLETE
g28n04:841280:841600 [0] NCCL INFO Using network IB
g28n04:841280:841600 [0] NCCL INFO comm 0x14e02a000 rank 12 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0x864be1a757d9f276 - Init START
g28n04:841280:841600 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g28n04:841280:841600 [0] NCCL INFO Trees [0] 13/18/-1->12->0 [1] 13/-1/-1->12->14
g28n04:841280:841600 [0] NCCL INFO P2P Chunksize set to 131072
g28n04:841280:841600 [0] NCCL INFO Channel 00/0 : 11[2] -> 12[0] [receive] via NET/IB/0
g28n04:841280:841600 [0] NCCL INFO Channel 01/0 : 11[2] -> 12[0] [receive] via NET/IB/0
g28n04:841280:841600 [0] NCCL INFO Channel 00/0 : 12[0] -> 13[4] via P2P/IPC
g28n04:841280:841600 [0] NCCL INFO Channel 01/0 : 12[0] -> 13[4] via P2P/IPC
g28n04:841280:841600 [0] NCCL INFO Connected all rings
g28n04:841280:841600 [0] NCCL INFO Channel 01/0 : 12[0] -> 14[2] [send] via NET/IB/0
g28n04:841280:841600 [0] NCCL INFO Channel 00/0 : 12[0] -> 18[0] [send] via NET/IB/0
g28n04:841280:841600 [0] NCCL INFO Channel 00/0 : 0[0] -> 12[0] [receive] via NET/IB/0
g28n04:841280:841600 [0] NCCL INFO Channel 00/0 : 12[0] -> 0[0] [send] via NET/IB/0
g28n04:841280:841600 [0] NCCL INFO Channel 00/0 : 18[0] -> 12[0] [receive] via NET/IB/0
g28n04:841280:841600 [0] NCCL INFO Channel 01/0 : 14[2] -> 12[0] [receive] via NET/IB/0
g28n04:841280:841600 [0] NCCL INFO Conne5de53ec - Init START
g28n04:841281:841619 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g28n04:841281:841619 [1] NCCL INFO Trees [0] 5/7/-1->6->4 [1] -1/-1/-1->6->5
g28n04:841281:841619 [1] NCCL INFO P2P Chunksize set to 131072
g28n04:841281:841619 [1] NCCL INFO Channel 00/0 : 5[5] -> 6[1] [receive] via NET/IB/2
g28n04:841281:841619 [1] NCCL INFO Channel 01/0 : 5[5] -> 6[1] [receive] via NET/IB/2
g28n04:841281:841619 [1] NCCL INFO Channel 00/0 : 6[1] -> 7[3] [send] via NET/IB/2
g28n04:841281:841619 [1] NCCL INFO Channel 01/0 : 6[1] -> 7[3] [send] via NET/IB/2
g28n04:841281:841619 [1] NCCL INFO Connected all rings
g28n04:841281:841619 [1] NCCL INFO Channel 00/0 : 4[3] -> 6[1] [receive] via NET/IB/2
g28n04:841281:841619 [1] NCCL INFO Channel 00/0 : 6[1] -> 4[3] [send] via NET/IB/2
g28n04:841281:841619 [1] NCCL INFO Channel 00/0 : 7[3] -> 6[1] [receive] via NET/IB/2
g28n04:841281:841619 [1] NCCL INFO Channel 00/0 : 6[1] -> 5[5] [send] via NET/IB/2
g28n04:841281:841619 [1] NCCL INFO Channel 01/0 : 6[1] -> 5[5] [send] via NET/IB/2
g28n04:841281:841619 [1] NCCL INFO Connected all trees
g28n04:841281:841619 [1] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g28n04:841281:841619 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n04:841281:841619 [1] NCCL INFO comm 0x11095a020 rank 6 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x17fc85f125de53ec - Init COMPLETE
O Setting affinity for GPU 2 to 0f000000,00000000
g28n04:841283:841621 [2] NCCL INFO Trees [0] 5/7/-1->6->4 [1] -1/-1/-1->6->5
g28n04:841283:841621 [2] NCCL INFO P2P Chunksize set to 131072
g28n04:841283:841621 [2] NCCL INFO Channel 00/0 : 5[0] -> 6[2] [receive] via NET/IB/0
g28n04:841283:841621 [2] NCCL INFO Channel 01/0 : 5[0] -> 6[2] [receive] via NET/IB/0
g28n04:841283:841621 [2] NCCL INFO Channel 00/0 : 6[2] -> 7[4] [send] via NET/IB/0
g28n04:841283:841621 [2] NCCL INFO Channel 01/0 : 6[2] -> 7[4] [send] via NET/IB/0
g28n04:841283:841621 [2] NCCL INFO Connected all rings
g28n04:841283:841621 [2] NCCL INFO Channel 00/0 : 4[4] -> 6[2] [receive] via NET/IB/0
g28n04:841283:841621 [2] NCCL INFO Channel 00/0 : 6[2] -> 4[4] [send] via NET/IB/0
g28n04:841283:841621 [2] NCCL INFO Channel 00/0 : 7[4] -> 6[2] [receive] via NET/IB/0
g28n04:841283:841621 [2] NCCL INFO Channel 00/0 : 6[2] -> 5[0] [send] via NET/IB/0
g28n04:841283:841621 [2] NCCL INFO Channel 01/0 : 6[2] -> 5[0] [send] via NET/IB/0
g28n04:841283:841621 [2] NCCL INFO Connected all trees
g28n04:841283:841621 [2] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g28n04:841283:841621 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n04:841283:841621 [2] NCCL INFO comm 0x12e4c5a00 rank 6 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0xaca7b5a6b2aa6a19 - Init COMPLETE
cted all trees
g28n04:841280:841600 [0] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g28n04:841280:841600 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n04:841280:841600 [0] NCCL INFO comm 0x14e02a000 rank 12 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0x864be1a757d9f276 - Init COMPLETE
g28n04:841280:841617 [0] NCCL INFO Using network IB
g28n04:841280:841617 [0] NCCL INFO comm 0x14cc36a70 rank 6 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x76a32220a008087c - Init START
g28n04:841280:841617 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g28n04:841280:841617 [0] NCCL INFO Trees [0] 5/7/-1->6->4 [1] -1/-1/-1->6->5
g28n04:841280:841617 [0] NCCL INFO P2P Chunksize set to 131072
g28n04:841280:841617 [0] NCCL INFO Channel 00/0 : 5[4] -> 6[0] [receive] via NET/IB/0
g28n04:841280:841617 [0] NCCL INFO Channel 01/0 : 5[4] -> 6[0] [receive] via NET/IB/0
g28n04:841280:841617 [0] NCCL INFO Channel 00/0 : 6[0] -> 7[2] [send] via NET/IB/0
g28n04:841280:841617 [0] NCCL INFO Channel 01/0 : 6[0] -> 7[2] [send] via NET/IB/0
g28n04:841280:841617 [0] NCCL INFO Connected all rings
g28n04:841280:841617 [0] NCCL INFO Channel 00/0 : 4[2] -> 6[0] [receive] via NET/IB/0
g28n04:841280:841617 [0] NCCL INFO Channel 00/0 : 6[0] -> 4[2] [send] via NET/IB/0
g28n04:841280:841617 [0] NCCL INFO Channel 00/0 : 7[2] -> 6[0] [receive] via NET/IB/0
g28n04:841280:841617 [0] NCCL INFO Channel 00/0 : 6[0] -> 5[4] [send] via NET/IB/0
g28n04:841280:841617 [0] NCCL INFO Channel 01/0 : 6[0] -> 5[4] [send] via NET/IB/0
g28n04:841280:841617 [0] NCCL INFO Connected all trees
g28n04:841280:841617 [0] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g28n04:841280:841617 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n04:841280:841617 [0] NCCL INFO comm 0x14cc36a70 rank 6 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x76a32220a008087c - Init COMPLETE
g28n03:859953:859953 [0] NCCL INFO cudaDriverVersion 12020
g28n03:859953:859953 [0] NCCL INFO Bootstrap : Using ib0:10.41.17.176<0>
g28n03:859953:859953 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g28n03:859953:859953 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g28n03:859953:860186 [0] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.176<0>
g28n03:859953:860186 [0] NCCL INFO Using network IB
g28n03:859953:860186 [0] NCCL INFO comm 0x169124800 rank 42 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x28395441218594c5 - Init START
g28n03:859953:860186 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g28n03:859953:860186 [0] NCCL INFO Trees [0] 43/-1/-1->42->36 [1] 44/-1/-1->42->43 [2] 43/18/-1->42->90 [3] 44/-1/-1->42->43
g28n03:859953:860186 [0] NCCL INFO P2P Chunksize set to 131072
g28n03:859953:860186 [0] NCCL INFO Channel 00/0 : 41[5] -> 42[0] [receive] via NET/IB/0
g28n03:859953:860186 [0] NCCL INFO Channel 02/0 : 41[5] -> 42[0] [receive] via NET/IB/0
g28n03:859953:860186 [0] NCCL INFO Channel 00/0 : 42[0] -> 43[1] via P2P/IPC
g28n03:859953:860186 [0] NCCL INFO Channel 02/0 : 42[0] -> 43[1] via P2P/IPC
g28n03:859953:860186 [0] NCCL INFO Channel 01/0 : 42[0] -> 51[3] [send] via NET/IB/2
g28n03:859953:860186 [0] NCCL INFO Channel 03/0 : 42[0] -> 51[3] [send] via NET/IB/2
g28n03:859953:860186 [0] NCCL INFO Connected all rings
g28n03:859953:860186 [0] NCCL INFO Channel 01/0 : 42[0] -> 43[1] via P2P/IPC
g28n03:859953:860186 [0] NCCL INFO Channel 03/0 : 42[0] -> 43[1] via P2P/IPC
g28n03:859953:860186 [0] NCCL INFO Channel 01/0 : 42[0] -> 44[2] via P2P/IPC
g28n03:859953:860186 [0] NCCL INFO Channel 03/0 : 42[0] -> 44[2] via P2P/IPC
g28n03:859953:860186 [0] NCCL INFO Channel 00/0 : 36[0] -> 42[0] [receive] via NET/IB/0
g28n03:859953:860186 [0] NCCL INFO Channel 02/0 : 18[0] -> 42[0] [receive] via NET/IB/0
g28n03:859953:860186 [0] NCCL INFO Channel 02/0 : 90[0] -> 42[0] [receive] via NET/IB/0
g28n03:859953:860186 [0] NCCL INFO Channel 02/0 : 42[0] -> 90[0] [send] via NET/IB/0
g28n03:859953:860186 [0] NCCL INFO Channel 02/0 : 42[0] -> 18[0] [send] via NET/IB/0
g28n03:859953:860186 [0] NCCL INFO Channel 00/0 : 42[0] -> 36[0] [send] via NET/IB/0
g28n03:859953:860186 [0] NCCL INFO Connected all trees
g28n03:859953:860186 [0] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g28n03:859953:860186 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g28n03:859953:860186 [0] NCCL INFO comm 0x169124800 rank 42 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x28395441218594c5 - Init COMPLETE
g28n03:859953:860272 [0] NCCL INFO Using network IB
g28n03:859953:860272 [0] NCCL INFO comm 0x16bea06a0 rank 10 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0xe45164e09488020a - Init START
g28n03:859953:860272 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g28n03:859953:860272 [0] NCCL INFO Trees [0] 11/-1/-1->10->9 [1] 11/4/-1->10->22
g28n03:859953:860272 [0] NCCL INFO P2P Chunksize set to 131072
g28n03:859953:860272 [0] NCCL INFO Channel 00/0 : 9[2] -> 10[0] [receive] via NET/IB/0
g28n03:859953:860272 [0] NCCL INFO Channel 01/0 : 9[2] -> 10[0] [receive] via NET/IB/0
g28n03:859953:860272 [0] NCCL INFO Channel 00/0 : 10[0] -> 11[4] via P2P/IPC
g28n03:859953:860272 [0] NCCL INFO Channel 01/0 : 10[0] -> 11[4] via P2P/IPC
g28n03:859953:860272 [0] NCCL INFO Connected all rings
g28n03:859953:860272 [0] NCCL INFO Channel 01/0 : 4[0] -> 10[0] [receive] via NET/IB/0
g28n03:859953:860272 [0] NCCL INFO Channel 01/0 : 22[0] -> 10[0] [receive] via NET/IB/0
g28n03:859953:860272 [0] NCCL INFO Channel 01/0 : 10[0] -> 22[0] [send] via NET/IB/0
g28n03:859953:860272 [0] NCCL INFO Channel 01/0 : 10[0] -> 4[0] [send] via NET/IB/0
g28n03:859953:860272 [0] NCCL INFO Channel 00/0 : 10[0] -> 9[2] [send] via NET/IB/0
g28n03:859953:860272 [0] NCCL INFO Connected all trees
g28n03:859953:860272 [0] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 51g28n03:859961:859961 [5] NCCL INFO cudaDriverVersion 12020
g28n03:859961:859961 [5] NCCL INFO Bootstrap : Using ib0:10.41.17.176<0>
g28n03:859961:859961 [5] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g28n03:859961:859961 [5] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g28n03:859961:860188 [5] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.176<0>
g28n03:859961:860188 [5] NCCL INFO Using network IB
g28n03:859961:860188 [5] NCCL INFO comm 0x162354120 rank 47 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x28395441218594c5 - Init START
g28n03:859961:860188 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g28n03:859961:860188 [5] NCCL INFO Trees [0] -1/-1/-1->47->46 [1] 43/-1/-1->47->45 [2] -1/-1/-1->47->46 [3] 43/-1/-1->47->45
g28n03:859961:860188 [5] NCCL INFO P2P Chunksize set to 131072
g28n03:859961:860188 [5] NCCL INFO Channel 00/0 : 47[5] -> 48[0] [send] via NET/IB/1
g28n03:859961:860188 [5] NCCL INFO Channel 02/0 : 47[5] -> 48[0] [send] via NET/IB/1
g28n03:859961:860188 [5] NCCL INFO Channel 01/0 : 47[5] -> 44[2] via P2P/IPC
g28n03:859961:860188 [5] NCCL INFO Channel 03/0 : 47[5] -> 44[2] via P2P/IPC
g28n03:859961:860188 [5] NCCL INFO Connected all rings
g28n03:859961:860188 [5] NCCL INFO Channel 01/0 : 47[5] -> 43[1] via P2P/IPC
g28n03:859961:860188 [5] NCCL INFO Channel 03/0 : 47[5] -> 43[1] via P2P/IPC
g28n03:859961:860188 [5] NCCL INFO Channel 01/0 : 47[5] -> 45[3] via P2P/IPC
g28n03:859961:860188 [5] NCCL INFO Channel 03/0 : 47[5] -> 45[3] via P2P/IPC
g28n03:859961:860188 [5] NCCL INFO Channel 00/0 : 47[5] -> 46[4] via P2P/IPC
g28n03:859961:860188 [5] NCCL INFO Channel 02/0 : 47[5] -> 46[4] via P2P/IPC
g28n03:859961:860188 [5] NCCL INFO Connected all trees
g28n03:859961:860188 [5] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g28n03:859961:860188 [5] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g28n03:859961:860188 [5] NCCL INFO comm 0x162354120 rank 47 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x28395441218594c5 - Init COMPLETE
g28n03:859961:860273 [5] NCCL INFO Using network IB
g28n03:859961:860273 [5] NCCL INFO comm 0x164cbbbe0 rank 11 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x15c93a2ee1bf52db - Init START
g28n03:859961:860273 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g28n03:859961:860273 [5] NCCL INFO Trees [0] -1/-1/-1->11->10 [1] 16/-1/-1->11->10
g28n03:859961:860273 [5] NCCL INFO P2P Chunksize set to 131072
g28n03:859961:860273 [5] NCCL INFO Channel 00/0 : 11[5] -> 12[3] [send] via NET/IB/3
g28n03:859961:860273 [5] NCCL INFO Channel 01/0 : 11[5] -> 12[3] [send] via NET/IB/3
g28n03:859961:860273 [5] NCCL INFO Connected all rings
g28n03:859961:860273 [5] NCCL INFO Channel 01/0 : 11[5] -> 16[1] [send] via NET/IB/2
g28n03:859961:860273 [5] NCCL INFO Channel 01/0 : 16[1] -> 11[5] [receive] via NET/IB/2
g28n03:859961:860273 [5] NCCL INFO Channel 00/0 : 11[5] -> 10[1] via P2P/IPC
g28n03:859961:860273 [5] NCCL INFO Channel 01/0 : 11[5] -> 10[1] via P2P/IPC
g28n03:859961:860273 [5] NCCL INFO Connected all trees
g28n03:859961:860273 [5] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g28n03:859961:860273 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n03:859961:860273 [5] NCCL INFO comm 0x164cbbbe0 rank 11 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x15c93a2ee1bf52db - Init COMPLETE
g28n03:859961:860295 [5] NCCL INFO Using network IB
g28n03:859961:860295 [5] NCCL INFO comm 0x166092ce0 rank 5 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x6876d4a326688ade - Init START
g28n03:859961:860295 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g28n03:859961:860295 [5] NCCL INFO Trees [0] -1/-1/-1->5->6 [1] 6/4/-1->5->7
g28n03:859961:860295 [5] NCCL INFO P2P Chunksize set to 131072
g28n03:8592 | 512
g28n03:859953:860272 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n03:859953:860272 [0] NCCL INFO comm 0x16bea06a0 rank 10 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0xe45164e09488020a - Init COMPLETE
g28n03:859953:860291 [0] NCCL INFO Using network IB
g28n03:859953:860291 [0] NCCL INFO comm 0x16bddb020 rank 5 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0xaca7b5a6b2aa6a19 - Init START
g28n03:859953:860291 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g28n03:859953:860291 [0] NCCL INFO Trees [0] -1/-1/-1->5->6 [1] 6/4/-1->5->7
g28n03:859953:860291 [0] NCCL INFO P2P Chunksize set to 131072
g28n03:859953:860291 [0] NCCL INFO Channel 00/0 : 4[4] -> 5[0] [receive] via NET/IB/0
g28n03:859953:860291 [0] NCCL INFO Channel 01/0 : 4[4] -> 5[0] [receive] via NET/IB/0
g28n03:859953:860291 [0] NCCL INFO Channel 00/0 : 5[0] -> 6[2] [send] via NET/IB/0
g28n03:859953:860291 [0] NCCL INFO Channel 01/0 : 5[0] -> 6[2] [send] via NET/IB/0
g28n03:859953:860291 [0] NCCL INFO Connected all rings
g28n03:859953:860291 [0] NCCL INFO Channel 01/0 : 5[0] -> 7[4] [send] via NET/IB/0
g28n03:859953:860291 [0] NCCL INFO Channel 01/0 : 7[4] -> 5[0] [receive] via NET/IB/0
g28n03:859953:860291 [0] NCCL INFO Channel 00/0 : 6[2] -> 5[0] [receive] via NET/IB/0
g28n03:859953:860291 [0] NCCL INFO Channel 01/0 : 6[2] -> 5[0] [receive] via NET/IB/0
g28n03:859953:860291 [0] NCCL INFO Channel 01/0 : 5[0] -> 4[4] [send] via NET/IB/0
g28n03:859953:860291 [0] NCCL INFO Connected all trees
g28n03:859953:860291 [0] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g28n03:859953:860291 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n03:859953:860291 [0] NCCL INFO comm 0x16bddb020 rank 5 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0xaca7b5a6b2aa6a19 - Init COMPLETE
g31n01:617756:617756 [0] NCCL INFO cudaDriverVersion 12020
g31n01:617756:617756 [0] NCCL INFO Bootstrap : Using ib0:10.41.17.228<0>
g31n01:617756:617756 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g31n01:617756:617756 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g31n01:617756:617992 [0] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.228<0>
g31n01:617756:617992 [0] NCCL INFO Using network IB
g31n01:617756:617992 [0] NCCL INFO comm 0x168e23c70 rank 54 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x28395441218594c5 - Init START
g31n01:617756:617992 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g31n01:617756:617992 [0] NCCL INFO Trees [0] 55/-1/-1->54->61 [1] 56/-1/-1->54->55 [2] 55/48/-1->54->66 [3] 56/-1/-1->54->55
g31n01:617756:617992 [0] NCCL INFO P2P Chunksize set to 131072
g31n01:617756:617992 [0] NCCL INFO Channel 00/0 : 53[5] -> 54[0] [receive] via NET/IB/0
g31n01:617756:617992 [0] NCCL INFO Channel 02/0 : 53[5] -> 54[0] [receive] via NET/IB/0
g31n01:617756:617992 [0] NCCL INFO Channel 00/0 : 54[0] -> 55[1] via P2P/IPC
g31n01:617756:617992 [0] NCCL INFO Channel 02/0 : 54[0] -> 55[1] via P2P/IPC
g31n01:617756:617992 [0] NCCL INFO Channel 01/0 : 54[0] -> 63[3] [send] via NET/IB/2
g31n01:617756:617992 [0] NCCL INFO Channel 03/0 : 54[0] -> 63[3] [send] via NET/IB/2
g31n01:617756:617992 [0] NCCL INFO Connected all rings
g31n01:617756:617992 [0] NCCL INFO Channel 01/0 : 54[0] -> 55[1] via P2P/IPC
g31n01:617756:617992 [0] NCCL INFO Channel 03/0 : 54[0] -> 55[1] via P2P/IPC
g31n01:617756:617992 [0] NCCL INFO Channel 01/0 : 54[0] -> 56[2] via P2P/IPC
g31n01:617756:617992 [0] NCCL INFO Channel 03/0 : 54[0] -> 56[2] via P2P/IPC
g31n01:617756:617992 [0] NCCL INFO Channel 02/0 : 48[0] -> 54[0] [receive] via NET/IB/0
g31n01:617756:617992 [0] NCCL INFO Channel 00/0 : 54[0] -> 61[1] [send] via NET/IB/0
g31n01:617756:617992 [0] NCCL INFO Channel 02/0 : 54[0] -> 66[0] [send] via NET/IB/0
g31n01:617756:617992 [0] NCCL INFO Channel 02/0 : 66[0] -> 54[0] [receive] via NET/IB/0
g31n01:617756:617992 [0] NCCL INFO Channel 00/0 : 61[1] -> 54[0] [receive] via NET/IB/0
g31n01:617756:617992 [0] NCCL INFO Channel 02/0 : 54[0] -> 48[0] [send] via NET/IB/0
g31n01:617756:617992 [0] NCCL INFO Connected all trees
g31n01:617756:617992 [0] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g31n01:617756:617992 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g31n01:617756:617992 [0] NCCL INFO comm 0x168e23c70 rank 54 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x28395441218594c5 - Init COMPLETE
g31n01:617756:618081 [0] NCCL INFO Using network IB
g31n01:617756:618081 [0] NCCL INFO comm 0x16b7582f0 rank 13 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0xe45164e09488020a - Init START
g31n01:617756:618081 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g31n01:617756:618081 [0] NCCL INFO Trees [0] 14/-1/-1->13->15 [1] 14/12/-1->13->16
g31n01:617756:618081 [0] NCCL INFO P2P Chunksize set to 131072
g31n01:617756:618081 [0] NCCL INFO Channel 00/0 : 12[2] -> 13[0] [receive] via NET/IB/0
g31n01:617756:618081 [0] NCCL INFO Channel 01/0 : 12[2] -> 13[0] [receive] via NET/IB/0
g31n01:617756:618081 [0] NCCL INFO Channel 00/0 : 13[0] -> 14[4] via P2P/IPC
g31n01:617756:618081 [0] NCCL INFO Channel 01/0 : 13[0] -> 14[4] via P2P/IPC
g31n01:617756:618081 [0] NCCL INFO Connected all rings
g31n01:617756:618081 [0] NCCL INFO Channel 00/0 : 13[0] -> 15[2] [send] via NET/IB/0
g31n01:617756:618081 [0] NCCL INFO Channel 01/0 : 13[0] -> 16[0] [send] via NET/IB/0
g31n01:617756:618081 [0] NCCL INFO Channel 01/0 : 16[0] -> 13[0] [receive] via NET/IB/0
g31n01:617756:618081 [0] NCCL INFO Channel 00/0 : 15[2] -> 13[0] [receive] via NET/IB/0
g31n01:617756:618081 [0] NCCL INFO Channel 01/0 : 13[0] -> 12[2] [send] via NET/IB/0
g31n01:617756:618081 [0] NCCL INFO Connected all trees
g31n01:617756:618081 [0] NCCL INFO threadThresholds 8/8/64 | 192/8/g28n03:859960:859960 [4] NCCL INFO cudaDriverVersion 12020
g28n03:859960:859960 [4] NCCL INFO Bootstrap : Using ib0:10.41.17.176<0>
g28n03:859960:859960 [4] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g28n03:859960:859960 [4] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g28n03:859960:860187 [4] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.176<0>
g28n03:859960:860187 [4] NCCL INFO Using network IB
g28n03:859960:860187 [4] NCCL INFO comm 0x15dfe4140 rank 46 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x28395441218594c5 - Init START
g28n03:859960:860187 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g28n03:859960:860187 [4] NCCL INFO Trees [0] 47/-1/-1->46->45 [1] 45/-1/-1->46->40 [2] 47/-1/-1->46->45 [3] 45/22/-1->46->94
g28n03:859960:860187 [4] NCCL INFO P2P Chunksize set to 131072
g28n03:859960:860187 [4] NCCL INFO Channel 00/0 : 46[4] -> 47[5] via P2P/IPC
g28n03:859960:860187 [4] NCCL INFO Channel 01/0 : 46[4] -> 47[5] via P2P/IPC
g28n03:859960:860187 [4] NCCL INFO Channel 02/0 : 46[4] -> 47[5] via P2P/IPC
g28n03:859960:860187 [4] NCCL INFO Channel 03/0 : 46[4] -> 47[5] via P2P/IPC
g28n03:859960:860187 [4] NCCL INFO Connected all rings
g28n03:859960:860187 [4] NCCL INFO Channel 01/0 : 40[4] -> 46[4] [receive] via NET/IB/3
g28n03:859960:860187 [4] NCCL INFO Channel 03/0 : 22[4] -> 46[4] [receive] via NET/IB/3
g28n03:859960:860187 [4] NCCL INFO Channel 03/0 : 94[4] -> 46[4] [receive] via NET/IB/3
g28n03:859960:860187 [4] NCCL INFO Channel 03/0 : 46[4] -> 94[4] [send] via NET/IB/3
g28n03:859960:860187 [4] NCCL INFO Channel 03/0 : 46[4] -> 22[4] [send] via NET/IB/3
g28n03:859960:860187 [4] NCCL INFO Channel 01/0 : 46[4] -> 40[4] [send] via NET/IB/3
g28n03:859960:860187 [4] NCCL INFO Channel 00/0 : 46[4] -> 45[3] via P2P/IPC
g28n03:859960:860187 [4] NCCL INFO Channel 01/0 : 46[4] -> 45[3] via P2P/IPC
g28n03:859960:860187 [4] NCCL INFO Channel 02/0 : 46[4] -> 45[3] via P2P/IPC
g28n03:859960:860187 [4] NCCL INFO Channel 03/0 : 46[4] -> 45[3] via P2P/IPC
g28n03:859960:860187 [4] NCCL INFO Connected all trees
g28n03:859960:860187 [4] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g28n03:859960:860187 [4] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g28n03:859960:860187 [4] NCCL INFO comm 0x15dfe4140 rank 46 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x28395441218594c5 - Init COMPLETE
g28n03:859960:860275 [4] NCCL INFO Using network IB
g28n03:859960:860275 [4] NCCL INFO comm 0x160d3e570 rank 11 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xe45164e09488020a - Init START
g28n03:859960:860275 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g28n03:859960:860275 [4] NCCL INFO Trees [0] -1/-1/-1->11->10 [1] 16/-1/-1->11->10
g28n03:859960:860275 [4] NCCL INFO P2P Chunksize set to 131072
g28n03:859960:860275 [4] NCCL INFO Channel 00/0 : 11[4] -> 12[2] [send] via NET/IB/1
g28n03:859960:860275 [4] NCCL INFO Channel 01/0 : 11[4] -> 12[2] [send] via NET/IB/1
g28n03:859960:860275 [4] NCCL INFO Connected all rings
g28n03:859960:860275 [4] NCCL INFO Channel 01/0 : 11[4] -> 16[0] [send] via NET/IB/0
g28n03:859960:860275 [4] NCCL INFO Channel 01/0 : 16[0] -> 11[4] [receive] via NET/IB/0
g28n03:859960:860275 [4] NCCL INFO Channel 00/0 : 11[4] -> 10[0] via P2P/IPC
g28n03:859960:860275 [4] NCCL INFO Channel 01/0 : 11[4] -> 10[0] via P2P/IPC
g28n03:859960:860275 [4] NCCL INFO Connected all trees
g28n03:859960:860275 [4] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g28n03:859960:860275 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n03:859960:860275 [4] NCCL INFO comm 0x160d3e570 rank 11 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xe45164e09488020a - Init COMPLETE
g28n03:859960:860292 [4] NCCL INFO Using network IB
g28n03:859960:860292 [4] NCCL INFO comm 0x15fcdf0f0 rank 5 nranks 12 961:860295 [5] NCCL INFO Channel 00/0 : 4[3] -> 5[5] [receive] via NET/IB/3
g28n03:859961:860295 [5] NCCL INFO Channel 01/0 : 4[3] -> 5[5] [receive] via NET/IB/3
g28n03:859961:860295 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[1] [send] via NET/IB/3
g28n03:859961:860295 [5] NCCL INFO Channel 01/0 : 5[5] -> 6[1] [send] via NET/IB/3
g28n03:859961:860295 [5] NCCL INFO Connected all rings
g28n03:859961:860295 [5] NCCL INFO Channel 01/0 : 5[5] -> 7[3] [send] via NET/IB/3
g28n03:859961:860295 [5] NCCL INFO Channel 01/0 : 7[3] -> 5[5] [receive] via NET/IB/3
g28n03:859961:860295 [5] NCCL INFO Channel 00/0 : 6[1] -> 5[5] [receive] via NET/IB/3
g28n03:859961:860295 [5] NCCL INFO Channel 01/0 : 6[1] -> 5[5] [receive] via NET/IB/3
g28n03:859961:860295 [5] NCCL INFO Channel 01/0 : 5[5] -> 4[3] [send] via NET/IB/3
g28n03:859961:860295 [5] NCCL INFO Connected all trees
g28n03:859961:860295 [5] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g28n03:859961:860295 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n03:859961:860295 [5] NCCL INFO comm 0x166092ce0 rank 5 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x6876d4a326688ade - Init COMPLETE
64 | 512 | 512
g31n01:617756:618081 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n01:617756:618081 [0] NCCL INFO comm 0x16b7582f0 rank 13 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0xe45164e09488020a - Init COMPLETE
g31n01:617756:618099 [0] NCCL INFO Using network IB
g31n01:617756:618099 [0] NCCL INFO comm 0x16b6e57b0 rank 6 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x8442a1c07b0a8edb - Init START
g31n01:617756:618099 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g31n01:617756:618099 [0] NCCL INFO Trees [0] 5/7/-1->6->4 [1] -1/-1/-1->6->5
g31n01:617756:618099 [0] NCCL INFO P2P Chunksize set to 131072
g31n01:617756:618099 [0] NCCL INFO Channel 00/0 : 5[4] -> 6[0] [receive] via NET/IB/0
g31n01:617756:618099 [0] NCCL INFO Channel 01/0 : 5[4] -> 6[0] [receive] via NET/IB/0
g31n01:617756:618099 [0] NCCL INFO Channel 00/0 : 6[0] -> 7[2] [send] via NET/IB/0
g31n01:617756:618099 [0] NCCL INFO Channel 01/0 : 6[0] -> 7[2] [send] via NET/IB/0
g31n01:617756:618099 [0] NCCL INFO Connected all rings
g31n01:617756:618099 [0] NCCL INFO Channel 00/0 : 4[2] -> 6[0] [receive] via NET/IB/0
g31n01:617756:618099 [0] NCCL INFO Channel 00/0 : 6[0] -> 4[2] [send] via NET/IB/0
g31n01:617756:618099 [0] NCCL INFO Channel 00/0 : 7[2] -> 6[0] [receive] via NET/IB/0
g31n01:617756:618099 [0] NCCL INFO Channel 00/0 : 6[0] -> 5[4] [send] via NET/IB/0
g31n01:617756:618099 [0] NCCL INFO Channel 01/0 : 6[0] -> 5[4] [send] via NET/IB/0
g31n01:617756:618099 [0] NCCL INFO Connected all trees
g31n01:617756:618099 [0] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g31n01:617756:618099 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n01:617756:618099 [0] NCCL INFO comm 0x16b6e57b0 rank 6 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x8442a1c07b0a8edb - Init COMPLETE
g28n03:859958:859958 [3] NCCL INFO cudaDriverVersion 12020
g28n03:859958:859958 [3] NCCL INFO Bootstrap : Using ib0:10.41.17.176<0>
g28n03:859958:859958 [3] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g28n03:859958:859958 [3] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g28n03:859958:860189 [3] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.176<0>
g28n03:859958:860189 [3] NCCL INFO Using network IB
g28n03:859958:860189 [3] NCCL INFO comm 0x1585f49b0 rank 45 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x28395441218594c5 - Init START
g28n03:859958:860189 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g28n03:859958:860189 [3] NCCL INFO Trees [0] 46/-1/-1->45->44 [1] 47/-1/-1->45->46 [2] 46/-1/-1->45->44 [3] 47/70/-1->45->46
g28n03:859958:860189 [3] NCCL INFO P2P Chunksize set to 131072
g28n03:859958:860189 [3] NCCL INFO Channel 00/0 : 45[3] -> 46[4] via P2P/IPC
g28n03:859958:860189 [3] NCCL INFO Channel 01/0 : 45[3] -> 46[4] via P2P/IPC
g28n03:859958:860189 [3] NCCL INFO Channel 02/0 : 45[3] -> 46[4] via P2P/IPC
g28n03:859958:860189 [3] NCCL INFO Channel 03/0 : 45[3] -> 46[4] via P2P/IPC
g28n03:859958:860189 [3] NCCL INFO Channel 01/0 : 36[0] -> 45[3] [receive] via NET/IB/3
g28n03:859958:860189 [3] NCCL INFO Channel 03/0 : 36[0] -> 45[3] [receive] via NET/IB/3
g28n03:859958:860189 [3] NCCL INFO Connected all rings
g28n03:859958:860189 [3] NCCL INFO Channel 01/0 : 45[3] -> 47[5] via P2P/IPC
g28n03:859958:860189 [3] NCCL INFO Channel 03/0 : 45[3] -> 47[5] via P2P/IPC
g28n03:859958:860189 [3] NCCL INFO Channel 03/0 : 45[3] -> 70[4] [send] via NET/IB/3
g28n03:859958:860189 [3] NCCL INFO Channel 03/0 : 70[4] -> 45[3] [receive] via NET/IB/3
g28n03:859958:860189 [3] NCCL INFO Channel 00/0 : 45[3] -> 44[2] via P2P/IPC
g28n03:859958:860189 [3] NCCL INFO Channel 02/0 : 45[3] -> 44[2] via P2P/IPC
g28n03:859958:860189 [3] NCCL INFO Connected all trees
g28n03:859958:860189 [3] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g28n03:859958:860189 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g28n03:859958:860189 [3] NCCL INFO comm 0x1585f49b0 rank 45 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x28395441218594c5 - Init COMPLETE
g28n03:859958:860276 [3] NCCL INFO Using network IB
g28n03:859958:860276 [3] NCCL INFO comm 0x15aeda6e0 rank 11 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xeee4c143812ce6e4 - Init START
g28n03:859958:860276 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g28n03:859958:860276 [3] NCCL INFO Trees [0] -1/-1/-1->11->9 [1] 17/5/-1->11->23
g28n03:859958:860276 [3] NCCL INFO P2P Chunksize set to 131072
g28n03:859958:860276 [3] NCCL INFO Channel 00/0 : 10[5] -> 11[3] [receive] via NET/IB/3
g28n03:859958:860276 [3] NCCL INFO Channel 01/0 : 10[5] -> 11[3] [receive] via NET/IB/3
g28n03:859958:860276 [3] NCCL INFO Channel 00/0 : 11[3] -> 12[1] [send] via NET/IB/3
g28n03:859958:860276 [3] NCCL INFO Channel 01/0 : 11[3] -> 12[1] [send] via NET/IB/3
g28n03:859958:860276 [3] NCCL INFO Connected all rings
g28n03:859958:860276 [3] NCCL INFO Channel 00/0 : 9[1] -> 11[3] [receive] via NET/IB/3
g28n03:859958:860276 [3] NCCL INFO Channel 01/0 : 5[3] -> 11[3] [receive] via NET/IB/3
g28n03:859958:860276 [3] NCCL INFO Channel 01/0 : 11[3] -> 17[3] [send] via NET/IB/3
g28n03:859958:860276 [3] NCCL INFO Channel 01/0 : 23[3] -> 11[3] [receive] via NET/IB/3
g28n03:859958:860276 [3] NCCL INFO Channel 01/0 : 11[3] -> 23[3] [send] via NET/IB/3
g28n03:859958:860276 [3] NCCL INFO Channel 01/0 : 17[3] -> 11[3] [receive] via NET/IB/3
g28n03:859958:860276 [3] NCCL INFO Channel 01/0 : 11[3] -> 5[3] [send] via NET/IB/3
g28n03:859958:860276 [3] NCCL INFO Channel 00/0 : 11[3] -> 9[1] [send] via NET/IB/3
g28n03:859958:860276 [3] NCCL INFO Connected all trees
g28n03:859958:860276 [3] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g28n03:859958:860276 [3] g28n03:859956:859956 [2] NCCL INFO cudaDriverVersion 12020
g28n03:859956:859956 [2] NCCL INFO Bootstrap : Using ib0:10.41.17.176<0>
g28n03:859956:859956 [2] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g28n03:859956:859956 [2] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g28n03:859956:860190 [2] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.176<0>
g28n03:859956:860190 [2] NCCL INFO Using network IB
g28n03:859956:860190 [2] NCCL INFO comm 0x142613cb0 rank 44 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x28395441218594c5 - Init START
g28n03:859956:860190 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g28n03:859956:860190 [2] NCCL INFO Trees [0] 45/-1/-1->44->43 [1] -1/-1/-1->44->42 [2] 45/-1/-1->44->43 [3] -1/-1/-1->44->42
g28n03:859956:860190 [2] NCCL INFO P2P Chunksize set to 131072
g28n03:859956:860190 [2] NCCL INFO Channel 00/0 : 44[2] -> 45[3] via P2P/IPC
g28n03:859956:860190 [2] NCCL INFO Channel 02/0 : 44[2] -> 45[3] via P2P/IPC
g28n03:859956:860190 [2] NCCL INFO Channel 01/0 : 44[2] -> 43[1] via P2P/IPC
g28n03:859956:860190 [2] NCCL INFO Channel 03/0 : 44[2] -> 43[1] via P2P/IPC
g28n03:859956:860190 [2] NCCL INFO Connected all rings
g28n03:859956:860190 [2] NCCL INFO Channel 01/0 : 44[2] -> 42[0] via P2P/IPC
g28n03:859956:860190 [2] NCCL INFO Channel 03/0 : 44[2] -> 42[0] via P2P/IPC
g28n03:859956:860190 [2] NCCL INFO Channel 00/0 : 44[2] -> 43[1] via P2P/IPC
g28n03:859956:860190 [2] NCCL INFO Channel 02/0 : 44[2] -> 43[1] via P2P/IPC
g28n03:859956:860190 [2] NCCL INFO Connected all trees
g28n03:859956:860190 [2] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g28n03:859956:860190 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g28n03:859956:860190 [2] NCCL INFO comm 0x142613cb0 rank 44 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x28395441218594c5 - Init COMPLETE
g28n03:859956:860274 [2] NCCL INFO Using network IB
g28n03:859956:860274 [2] NCCL INFO comm 0x1463e8bc0 rank 11 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0x864be1a757d9f276 - Init START
g28n03:859956:860274 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g28n03:859956:860274 [2] NCCL INFO Trees [0] -1/-1/-1->11->9 [1] 17/5/-1->11->23
g28n03:859956:860274 [2] NCCL INFO P2P Chunksize set to 131072
g28n03:859956:860274 [2] NCCL INFO Channel 00/0 : 10[4] -> 11[2] [receive] via NET/IB/0
g28n03:859956:860274 [2] NCCL INFO Channel 01/0 : 10[4] -> 11[2] [receive] via NET/IB/0
g28n03:859956:860274 [2] NCCL INFO Channel 00/0 : 11[2] -> 12[0] [send] via NET/IB/0
g28n03:859956:860274 [2] NCCL INFO Channel 01/0 : 11[2] -> 12[0] [send] via NET/IB/0
g28n03:859956:860274 [2] NCCL INFO Connected all rings
g28n03:859956:860274 [2] NCCL INFO Channel 00/0 : 9[0] -> 11[2] [receive] via NET/IB/0
g28n03:859956:860274 [2] NCCL INFO Channel 01/0 : 5[2] -> 11[2] [receive] via NET/IB/0
g28n03:859956:860274 [2] NCCL INFO Channel 01/0 : 11[2] -> 17[2] [send] via NET/IB/0
g28n03:859956:860274 [2] NCCL INFO Channel 01/0 : 23[2] -> 11[2] [receive] via NET/IB/0
g28n03:859956:860274 [2] NCCL INFO Channel 01/0 : 11[2] -> 23[2] [send] via NET/IB/0
g28n03:859956:860274 [2] NCCL INFO Channel 01/0 : 17[2] -> 11[2] [receive] via NET/IB/0
g28n03:859956:860274 [2] NCCL INFO Channel 01/0 : 11[2] -> 5[2] [send] via NET/IB/0
g28n03:859956:860274 [2] NCCL INFO Channel 00/0 : 11[2] -> 9[0] [send] via NET/IB/0
g28n03:859956:860274 [2] NCCL INFO Connected all trees
g28n03:859956:860274 [2] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g28n03:859956:860274 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n03:859956:860274 [2] NCCL INFO comm 0x1463e8bc0 rank 11 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0x864be1a757d9f276 - Init COMPLETE
g28n03:859956:860290 [2] NCCL INFO Using network IB
g28n03:859956:860290 [2] NCCL INFO comm 0x1464246e0 rank 5 nranks 12 cudaDev 2 nvmlDev 2 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x8442a1c07b0a8edb - Init START
g28n03:859960:860292 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g28n03:859960:860292 [4] NCCL INFO Trees [0] -1/-1/-1->5->6 [1] 6/4/-1->5->7
g28n03:859960:860292 [4] NCCL INFO P2P Chunksize set to 131072
g28n03:859960:860292 [4] NCCL INFO Channel 00/0 : 4[2] -> 5[4] [receive] via NET/IB/1
g28n03:859960:860292 [4] NCCL INFO Channel 01/0 : 4[2] -> 5[4] [receive] via NET/IB/1
g28n03:859960:860292 [4] NCCL INFO Channel 00/0 : 5[4] -> 6[0] [send] via NET/IB/1
g28n03:859960:860292 [4] NCCL INFO Channel 01/0 : 5[4] -> 6[0] [send] via NET/IB/1
g28n03:859960:860292 [4] NCCL INFO Connected all rings
g28n03:859960:860292 [4] NCCL INFO Channel 01/0 : 5[4] -> 7[2] [send] via NET/IB/1
g28n03:859960:860292 [4] NCCL INFO Channel 01/0 : 7[2] -> 5[4] [receive] via NET/IB/1
g28n03:859960:860292 [4] NCCL INFO Channel 00/0 : 6[0] -> 5[4] [receive] via NET/IB/1
g28n03:859960:860292 [4] NCCL INFO Channel 01/0 : 6[0] -> 5[4] [receive] via NET/IB/1
g28n03:859960:860292 [4] NCCL INFO Channel 01/0 : 5[4] -> 4[2] [send] via NET/IB/1
g28n03:859960:860292 [4] NCCL INFO Connected all trees
g28n03:859960:860292 [4] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g28n03:859960:860292 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n03:859960:860292 [4] NCCL INFO comm 0x15fcdf0f0 rank 5 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x8442a1c07b0a8edb - Init COMPLETE
NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n03:859958:860276 [3] NCCL INFO comm 0x15aeda6e0 rank 11 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xeee4c143812ce6e4 - Init COMPLETE
g28n03:859958:860293 [3] NCCL INFO Using network IB
g28n03:859958:860293 [3] NCCL INFO comm 0x15c33a4e0 rank 5 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xad5995bf820e7340 - Init START
g28n03:859958:860293 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g28n03:859958:860293 [3] NCCL INFO Trees [0] -1/-1/-1->5->6 [1] 6/4/-1->5->7
g28n03:859958:860293 [3] NCCL INFO P2P Chunksize set to 131072
g28n03:859958:860293 [3] NCCL INFO Channel 00/0 : 4[1] -> 5[3] [receive] via NET/IB/3
g28n03:859958:860293 [3] NCCL INFO Channel 01/0 : 4[1] -> 5[3] [receive] via NET/IB/3
g28n03:859958:860293 [3] NCCL INFO Channel 00/0 : 5[3] -> 6[5] [send] via NET/IB/3
g28n03:859958:860293 [3] NCCL INFO Channel 01/0 : 5[3] -> 6[5] [send] via NET/IB/3
g28n03:859958:860293 [3] NCCL INFO Connected all rings
g28n03:859958:860293 [3] NCCL INFO Channel 01/0 : 5[3] -> 7[1] [send] via NET/IB/3
g28n03:859958:860293 [3] NCCL INFO Channel 01/0 : 7[1] -> 5[3] [receive] via NET/IB/3
g28n03:859958:860293 [3] NCCL INFO Channel 00/0 : 6[5] -> 5[3] [receive] via NET/IB/3
g28n03:859958:860293 [3] NCCL INFO Channel 01/0 : 6[5] -> 5[3] [receive] via NET/IB/3
g28n03:859958:860293 [3] NCCL INFO Channel 01/0 : 5[3] -> 4[1] [send] via NET/IB/3
g28n03:859958:860293 [3] NCCL INFO Connected all trees
g28n03:859958:860293 [3] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g28n03:859958:860293 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n03:859958:860293 [3] NCCL INFO comm 0x15c33a4e0 rank 5 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xad5995bf820e7340 - Init COMPLETE
busId 406000 commId 0x51bdf7b6b8c3aecb - Init START
g28n03:859956:860290 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g28n03:859956:860290 [2] NCCL INFO Trees [0] -1/-1/-1->5->6 [1] 6/4/-1->5->7
g28n03:859956:860290 [2] NCCL INFO P2P Chunksize set to 131072
g28n03:859956:860290 [2] NCCL INFO Channel 00/0 : 4[0] -> 5[2] [receive] via NET/IB/0
g28n03:859956:860290 [2] NCCL INFO Channel 01/0 : 4[0] -> 5[2] [receive] via NET/IB/0
g28n03:859956:860290 [2] NCCL INFO Channel 00/0 : 5[2] -> 6[4] [send] via NET/IB/0
g28n03:859956:860290 [2] NCCL INFO Channel 01/0 : 5[2] -> 6[4] [send] via NET/IB/0
g28n03:859956:860290 [2] NCCL INFO Connected all rings
g28n03:859956:860290 [2] NCCL INFO Channel 01/0 : 5[2] -> 7[0] [send] via NET/IB/0
g28n03:859956:860290 [2] NCCL INFO Channel 01/0 : 7[0] -> 5[2] [receive] via NET/IB/0
g28n03:859956:860290 [2] NCCL INFO Channel 00/0 : 6[4] -> 5[2] [receive] via NET/IB/0
g28n03:859956:860290 [2] NCCL INFO Channel 01/0 : 6[4] -> 5[2] [receive] via NET/IB/0
g28n03:859956:860290 [2] NCCL INFO Channel 01/0 : 5[2] -> 4[0] [send] via NET/IB/0
g28n03:859956:860290 [2] NCCL INFO Connected all trees
g28n03:859956:860290 [2] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g28n03:859956:860290 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n03:859956:860290 [2] NCCL INFO comm 0x1464246e0 rank 5 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0x51bdf7b6b8c3aecb - Init COMPLETE
g31n03:688509:688509 [0] NCCL INFO cudaDriverVersion 12020
g31n03:688509:688509 [0] NCCL INFO Bootstrap : Using ib0:10.41.17.230<0>
g31n03:688509:688509 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g31n03:688509:688509 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g31n03:688509:688745 [0] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.230<0>
g31n03:688509:688745 [0] NCCL INFO Using network IB
g31n03:688509:688745 [0] NCCL INFO comm 0x146ff3f70 rank 66 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x28395441218594c5 - Init START
g31n03:688509:688745 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g31n03:688509:688745 [0] NCCL INFO Trees [0] 67/-1/-1->66->60 [1] 68/-1/-1->66->67 [2] 67/54/-1->66->43 [3] 68/-1/-1->66->67
g31n03:688509:688745 [0] NCCL INFO P2P Chunksize set to 131072
g31n03:688509:688745 [0] NCCL INFO Channel 00/0 : 65[5] -> 66[0] [receive] via NET/IB/0
g31n03:688509:688745 [0] NCCL INFO Channel 02/0 : 65[5] -> 66[0] [receive] via NET/IB/0
g31n03:688509:688745 [0] NCCL INFO Channel 00/0 : 66[0] -> 67[1] via P2P/IPC
g31n03:688509:688745 [0] NCCL INFO Channel 02/0 : 66[0] -> 67[1] via P2P/IPC
g31n03:688509:688745 [0] NCCL INFO Channel 01/0 : 66[0] -> 75[3] [send] via NET/IB/2
g31n03:688509:688745 [0] NCCL INFO Channel 03/0 : 66[0] -> 75[3] [send] via NET/IB/2
g31n03:688509:688745 [0] NCCL INFO Connected all rings
g31n03:688509:688745 [0] NCCL INFO Channel 01/0 : 66[0] -> 67[1] via P2P/IPC
g31n03:688509:688745 [0] NCCL INFO Channel 03/0 : 66[0] -> 67[1] via P2P/IPC
g31n03:688509:688745 [0] NCCL INFO Channel 01/0 : 66[0] -> 68[2] via P2P/IPC
g31n03:688509:688745 [0] NCCL INFO Channel 03/0 : 66[0] -> 68[2] via P2P/IPC
g31n03:688509:688745 [0] NCCL INFO Channel 00/0 : 60[0] -> 66[0] [receive] via NET/IB/0
g31n03:688509:688745 [0] NCCL INFO Channel 02/0 : 54[0] -> 66[0] [receive] via NET/IB/0
g31n03:688509:688745 [0] NCCL INFO Channel 02/0 : 43[1] -> 66[0] [receive] via NET/IB/0
g31n03:688509:688745 [0] NCCL INFO Channel 02/0 : 66[0] -> 43[1] [send] via NET/IB/0
g31n03:688509:688745 [0] NCCL INFO Channel 02/0 : 66[0] -> 54[0] [send] via NET/IB/0
g31n03:688509:688745 [0] NCCL INFO Channel 00/0 : 66[0] -> 60[0] [send] via NET/IB/0
g31n03:688509:688745 [0] NCCL INFO Connected all trees
g31n03:688509:688745 [0] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g31n03:688509:688745 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g31n03:688509:688745 [0] NCCL INFO comm 0x146ff3f70 rank 66 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x28395441218594c5 - Init COMPLETE
g31n03:688509:688824 [0] NCCL INFO Using network IB
g31n03:688509:688824 [0] NCCL INFO comm 0x148cdaef0 rank 16 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0xe45164e09488020a - Init START
g31n03:688509:688824 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g31n03:688509:688824 [0] NCCL INFO Trees [0] 17/-1/-1->16->15 [1] 17/13/-1->16->11
g31n03:688509:688824 [0] NCCL INFO P2P Chunksize set to 131072
g31n03:688509:688824 [0] NCCL INFO Channel 00/0 : 15[2] -> 16[0] [receive] via NET/IB/0
g31n03:688509:688824 [0] NCCL INFO Channel 01/0 : 15[2] -> 16[0] [receive] via NET/IB/0
g31n03:688509:688824 [0] NCCL INFO Channel 00/0 : 16[0] -> 17[4] via P2P/IPC
g31n03:688509:688824 [0] NCCL INFO Channel 01/0 : 16[0] -> 17[4] via P2P/IPC
g31n03:688509:688824 [0] NCCL INFO Connected all rings
g31n03:688509:688824 [0] NCCL INFO Channel 01/0 : 13[0] -> 16[0] [receive] via NET/IB/0
g31n03:688509:688824 [0] NCCL INFO Channel 01/0 : 11[4] -> 16[0] [receive] via NET/IB/0
g31n03:688509:688824 [0] NCCL INFO Channel 01/0 : 16[0] -> 11[4] [send] via NET/IB/0
g31n03:688509:688824 [0] NCCL INFO Channel 01/0 : 16[0] -> 13[0] [send] via NET/IB/0
g31n03:688509:688824 [0] NCCL INFO Channel 00/0 : 16[0] -> 15[2] [send] via NET/IB/0
g31n03:688509:688824 [0] NCCL INFO Connected all trees
g31n03:688509:688824 [0] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g31n03:688509:688824 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n03:688509:688824 [0] NCCL INFO comm 0x148cdaef0 rank 16 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0xe45164e09488020a - Init COMPLETE
g31n03:688509:688843 [0] NCCL INFO Using network IB
g31n03:688509:688843 [0] NCCL INFO comm 0x149c3c740 rank 8 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0xaca7b5a6b2aa6a19 - Init START
g31n03:688509:688843 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g31n03:688509:688843 [0] NCCL INFO Trees [0] 4/10/-1->8->0 [1] -1/-1/-1->8->9
g31n03:688509:688843 [0] NCCL INFO P2P Chunksize set to 131072
g31n03:688509:688843 [0] NCCL INFO Channel 00/0 : 7[4] -> 8[0] [receive] via NET/IB/0
g31n03:688509:688843 [0] NCCL INFO Channel 01/0 : 7[4] -> 8[0] [receive] via NET/IB/0
g31n03:688509:688843 [0] NCCL INFO Channel 00/0 : 8[0] -> 9[2] [send] via NET/IB/0
g31n03:688509:688843 [0] NCCL INFO Channel 01/0 : 8[0] -> 9[2] [send] via NET/IB/0
g31n03:688509:688843 [0] NCCL INFO Connected all rings
g31n03:688509:688843 [0] NCCL INFO Channel 00/0 : 8[0] -> 10[4] [send] via NET/IB/0
g31n03:688509:688843 [0] NCCL INFO Channel 00/0 : 4[4] -> 8[0] [receive] via NET/IB/0
g31n03:688509:688843 [0] NCCL INFO Channel 00/0 : 8[0] -> 0[2] [send] via NET/IB/0
g31n03:688509:688843 [0] NCCL INFO Channel 00/0 : 0[2] -> 8[0] [receive] via NET/IB/0
g31n03:688509:688843 [0] NCCL INFO Channel 00/0 : 8[0] -> 4[4] [send] via NET/IB/0
g31n03:688509:688843 [0] NCCL INFO Channel 00/0 : 10[4] -> 8[0] [receive] via NET/IB/0
g31n03:688509:688843 [0] NCCL INFO Channel 01/0 : 9[2] -> 8[0] [receive] via NET/IB/0
g31n03:688509:688843 [0] NCCL INFO Connected all trees
g31n03:688509:688843 [0] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g31n03:688509:688843 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n03:688509:688843 [0] NCCL INFO comm 0x149c3c740 rank 8 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0xaca7b5a6b2aa6a19 - Init COMPLETE
g28n03:859954:859954 [1] NCCL INFO cudaDriverVersion 12020
g28n03:859954:859954 [1] NCCL INFO Bootstrap : Using ib0:10.41.17.176<0>
g28n03:859954:859954 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g28n03:859954:859954 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g28n03:859954:860191 [1] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.176<0>
g28n03:859954:860191 [1] NCCL INFO Using network IB
g28n03:859954:860191 [1] NCCL INFO comm 0x143813f20 rank 43 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x28395441218594c5 - Init START
g28n03:859954:860191 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g28n03:859954:860191 [1] NCCL INFO Trees [0] 44/-1/-1->43->42 [1] 42/-1/-1->43->47 [2] 44/66/-1->43->42 [3] 42/-1/-1->43->47
g28n03:859954:860191 [1] NCCL INFO P2P Chunksize set to 131072
g28n03:859954:860191 [1] NCCL INFO Channel 00/0 : 43[1] -> 44[2] via P2P/IPC
g28n03:859954:860191 [1] NCCL INFO Channel 02/0 : 43[1] -> 44[2] via P2P/IPC
g28n03:859954:860191 [1] NCCL INFO Channel 01/0 : 43[1] -> 42[0] via P2P/IPC
g28n03:859954:860191 [1] NCCL INFO Channel 03/0 : 43[1] -> 42[0] via P2P/IPC
g28n03:859954:860191 [1] NCCL INFO Connected all rings
g28n03:859954:860191 [1] NCCL INFO Channel 01/0 : 43[1] -> 47[5] via P2P/IPC
g28n03:859954:860191 [1] NCCL INFO Channel 03/0 : 43[1] -> 47[5] via P2P/IPC
g28n03:859954:860191 [1] NCCL INFO Channel 02/0 : 43[1] -> 66[0] [send] via NET/IB/0
g28n03:859954:860191 [1] NCCL INFO Channel 02/0 : 66[0] -> 43[1] [receive] via NET/IB/0
g28n03:859954:860191 [1] NCCL INFO Channel 00/0 : 43[1] -> 42[0] via P2P/IPC
g28n03:859954:860191 [1] NCCL INFO Channel 02/0 : 43[1] -> 42[0] via P2P/IPC
g28n03:859954:860191 [1] NCCL INFO Connected all trees
g28n03:859954:860191 [1] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g28n03:859954:860191 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g28n03:859954:860191 [1] NCCL INFO comm 0x143813f20 rank 43 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x28395441218594c5 - Init COMPLETE
g28n03:859954:860271 [1] NCCL INFO Using network IB
g28n03:859954:860271 [1] NCCL INFO comm 0x145517980 rank 10 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0x15c93a2ee1bf52db - Init START
g28n03:859954:860271 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g28n03:859954:860271 [1] NCCL INFO Trees [0] 11/-1/-1->10->9 [1] 11/4/-1->10->22
g28n03:859954:860271 [1] NCCL INFO P2P Chunksize set to 131072
g28n03:859954:860271 [1] NCCL INFO Channel 00/0 : 9[3] -> 10[1] [receive] via NET/IB/2
g28n03:859954:860271 [1] NCCL INFO Channel 01/0 : 9[3] -> 10[1] [receive] via NET/IB/2
g28n03:859954:860271 [1] NCCL INFO Channel 00/0 : 10[1] -> 11[5] via P2P/IPC
g28n03:859954:860271 [1] NCCL INFO Channel 01/0 : 10[1] -> 11[5] via P2P/IPC
g28n03:859954:860271 [1] NCCL INFO Connected all rings
g28n03:859954:860271 [1] NCCL INFO Channel 01/0 : 4[1] -> 10[1] [receive] via NET/IB/2
g28n03:859954:860271 [1] NCCL INFO Channel 01/0 : 22[1] -> 10[1] [receive] via NET/IB/2
g28n03:859954:860271 [1] NCCL INFO Channel 01/0 : 10[1] -> 22[1] [send] via NET/IB/2
g28n03:859954:860271 [1] NCCL INFO Channel 01/0 : 10[1] -> 4[1] [send] via NET/IB/2
g28n03:859954:860271 [1] NCCL INFO Channel 00/0 : 10[1] -> 9[3] [send] via NET/IB/2
g28n03:859954:860271 [1] NCCL INFO Connected all trees
g28n03:859954:860271 [1] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g28n03:859954:860271 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n03:859954:860271 [1] NCCL INFO comm 0x145517980 rank 10 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0x15c93a2ee1bf52db - Init COMPLETE
g28n03:859954:860294 [1] NCCL INFO Using network IB
g28n03:859954:860294 [1] NCCL INFO comm 0x146497240 rank 5 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x1542a3b966c3490b - Init START
g28n03:859954:860294 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g28n03:859954:860294 [1] NCCL INFO Trees [0] -1/-1/-1->5->6 [1] 6/4/-1->5->7
g28n03:859954:860294 [1] NCCL INFO P2P Chunksize set to 131072
g28n03:859954:860294 [1] NCCL INFO Channel 00/0 : 4[5] -> 5[1] [receive] via NET/IB/2
g28n03:859954:860294 [1] NCCL INFO Channel 01/0 : 4[5] -> 5[1] [receive] via NET/IB/2
g28n03:859954:860294 [1] NCCL INFO Channel 00/0 : 5[1] -> 6[3] [send] via NET/IB/2
g28n03:859954:860294 [1] NCCL INFO Channel 01/0 : 5[1] -> 6[3] [send] via NET/IB/2
g28n03:859954:860294 [1] NCCL INFO Connected all rings
g28n03:859954:860294 [1] NCCL INFO Channel 01/0 : 5[1] -> 7[5] [send] via NET/IB/2
g28n03:859954:860294 [1] NCCL INFO Channel 01/0 : 7[5] -> 5[1] [receive] via NET/IB/2
g28n03:859954:860294 [1] NCCL INFO Channel 00/0 : 6[3] -> 5[1] [receive] via NET/IB/2
g28n03:859954:860294 [1] NCCL INFO Channel 01/0 : 6[3] -> 5[1] [receive] via NET/IB/2
g28n03:859954:860294 [1] NCCL INFO Channel 01/0 : 5[1] -> 4[5] [send] via NET/IB/2
g28n03:859954:860294 [1] NCCL INFO Connected all trees
g28n03:859954:860294 [1] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g28n03:859954:860294 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n03:859954:860294 [1] NCCL INFO comm 0x146497240 rank 5 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x1542a3b966c3490b - Init COMPLETE
g27n18:839789:839789 [4] NCCL INFO cudaDriverVersion 12020
g27n18:839789:839789 [4] NCCL INFO Bootstrap : Using ib0:10.41.17.173<0>
g27n18:839789:839789 [4] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g27n18:839789:839789 [4] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g27n18:839789:840022 [4] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.173<0>
g27n18:839789:840022 [4] NCCL INFO Using network IB
g27n18:839789:840022 [4] NCCL INFO comm 0x1461a3f30 rank 28 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x28395441218594c5 - Init START
g27n18:839789:840022 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g27n18:839789:840022 [4] NCCL INFO Trees [0] 29/-1/-1->28->27 [1] 27/40/-1->28->51 [2] 29/-1/-1->28->27 [3] 27/-1/-1->28->34
g27n18:839789:840022 [4] NCCL INFO P2P Chunksize set to 131072
g27n18:839789:840022 [4] NCCL INFO Channel 00/0 : 28[4] -> 29[5] via P2P/IPC
g27n18:839789:840022 [4] NCCL INFO Channel 01/0 : 28[4] -> 29[5] via P2P/IPC
g27n18:839789:840022 [4] NCCL INFO Channel 02/0 : 28[4] -> 29[5] via P2P/IPC
g27n18:839789:840022 [4] NCCL INFO Channel 03/0 : 28[4] -> 29[5] via P2P/IPC
g27n18:839789:840022 [4] NCCL INFO Connected all rings
g27n18:839789:840022 [4] NCCL INFO Channel 03/0 : 28[4] -> 34[4] [send] via NET/IB/3
g27n18:839789:840022 [4] NCCL INFO Channel 01/0 : 28[4] -> 40[4] [send] via NET/IB/3
g27n18:839789:840022 [4] NCCL INFO Channel 01/0 : 28[4] -> 51[3] [send] via NET/IB/3
g27n18:839789:840022 [4] NCCL INFO Channel 01/0 : 51[3] -> 28[4] [receive] via NET/IB/3
g27n18:839789:840022 [4] NCCL INFO Channel 01/0 : 40[4] -> 28[4] [receive] via NET/IB/3
g27n18:839789:840022 [4] NCCL INFO Channel 03/0 : 34[4] -> 28[4] [receive] via NET/IB/3
g27n18:839789:840022 [4] NCCL INFO Channel 00/0 : 28[4] -> 27[3] via P2P/IPC
g27n18:839789:840022 [4] NCCL INFO Channel 01/0 : 28[4] -> 27[3] via P2P/IPC
g27n18:839789:840022 [4] NCCL INFO Channel 02/0 : 28[4] -> 27[3] via P2P/IPC
g27n18:839789:840022 [4] NCCL INFO Channel 03/0 : 28[4] -> 27[3] via P2P/IPC
g27n18:839789:840022 [4] NCCL INFO Connected all trees
g27n18:839789:840022 [4] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g27n18:839789:840022 [4] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g27n18:839789:840022 [4] NCCL INFO comm 0x1461a3f30 rank 28 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x28395441218594c5 - Init COMPLETE
g27n18:839789:840100 [4] NCCL INFO Using network IB
g27n18:839789:840100 [4] NCCL INFO comm 0x148e08c60 rank 7 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x864be1a757d9f276 - Init START
g27n18:839789:840100 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g27n18:839789:840100 [4] NCCL INFO Trees [0] 3/-1/-1->7->6 [1] -1/-1/-1->7->6
g27n18:839789:840100 [4] NCCL INFO P2P Chunksize set to 131072
g27n18:839789:840100 [4] NCCL INFO Channel 00/0 : 7[4] -> 8[2] [send] via NET/IB/1
g27n18:839789:840100 [4] NCCL INFO Channel 01/0 : 7[4] -> 8[2] [send] via NET/IB/1
g27n18:839789:840100 [4] NCCL INFO Connected all rings
g27n18:839789:840100 [4] NCCL INFO Channel 00/0 : 3[0] -> 7[4] [receive] via NET/IB/0
g27n18:839789:840100 [4] NCCL INFO Channel 00/0 : 7[4] -> 3[0] [send] via NET/IB/0
g27n18:839789:840100 [4] NCCL INFO Channel 00/0 : 7[4] -> 6[0] via P2P/IPC
g27n18:839789:840100 [4] NCCL INFO Channel 01/0 : 7[4] -> 6[0] via P2P/IPC
g27n18:839789:840100 [4] NCCL INFO Connected all trees
g27n18:839789:840100 [4] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g27n18:839789:840100 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g27n18:839789:840100 [4] NCCL INFO comm 0x148e08c60 rank 7 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x864be1a757d9f276 - Init COMPLETE
g27n18:839789:840119 [4] NCCL INFO Using network IB
g27n18:839789:840119 [4] NCCL INFO comm 0x148df5f30 rank 3 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x51bdf7b6b8c3aecb - Init START
g27n18:839789:840119 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g27n18:839789:840119 [4] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] 7/1/-1->3->11
g27n18:839789:840119 [4] NCCL INFO P2P Chunksize set to 131072
g27n18:839789:840119 [4] NCCL INFO Channel 00/0 : 2[2] -> 3[4] [receive] via NET/IB/1
g27n18:839789:840119 [4] NCCL INFO Channel 01/0 : 2[2] -> 3[4] [receive] via NET/IB/1
g27n18:839789:840119 [4] NCCL INFO Channel 00/0 : 3[4] -> 4[0] [send] via NET/IB/1
g27n18:839789:840119 [4] NCCL INFO Channel 01/0 : 3[4] -> 4[0] [send] via NET/IB/1
g27n18:839789:840119 [4] NCCL INFO Connected all rings
g27n18:839789:840119 [4] NCCL INFO Channel 01/0 : 1[0] -> 3[4] [receive] via NET/IB/1
g27n18:839789:840119 [4] NCCL INFO Channel 01/0 : 11[2] -> 3[4] [receive] via NET/IB/1
g27n18:839789:840119 [4] NCCL INFO Channel 01/0 : 3[4] -> 7[0] [send] via NET/IB/1
g27n18:839789:840119 [4] NCCL INFO Channel 01/0 : 7[0] -> 3[4] [receive] via NET/IB/1
g27n18:839789:840119 [4] NCCL INFO Channel 01/0 : 3[4] -> 11[2] [send] via NET/IB/1
g27n18:839789:840119 [4] NCCL INFO Channel 01/0 : 3[4] -> 1[0] [send] via NET/IB/1
g27n18:839789:840119 [4] NCCL INFO Channel 00/0 : 3[4] -> 2[2] [send] via NET/IB/1
g27n18:839789:840119 [4] NCCL INFO Connected all trees
g27n18:839789:840119 [4] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g27n18:839789:840119 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g27n18:839789:840119 [4] NCCL INFO comm 0x148df5f30 rank 3 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x51bdf7b6b8c3aecb - Init COMPLETE
g27n18:839782:839782 [0] NCCL INFO cudaDriverVersion 12020
g27n18:839782:839782 [0] NCCL INFO Bootstrap : Using ib0:10.41.17.173<0>
g27n18:839782:839782 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g27n18:839782:839782 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g27n18:839782:840018 [0] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.173<0>
g27n18:839782:840018 [0] NCCL INFO Using network IB
g27n18:839782:840018 [0] NCCL INFO comm 0x17b834140 rank 24 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x28395441218594c5 - Init START
g27n18:839782:840018 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g27n18:839782:840018 [0] NCCL INFO Trees [0] 25/36/-1->24->49 [1] 26/-1/-1->24->25 [2] 25/-1/-1->24->30 [3] 26/-1/-1->24->25
g27n18:839782:840018 [0] NCCL INFO P2P Chunksize set to 131072
g27n18:839782:840018 [0] NCCL INFO Channel 00/0 : 23[5] -> 24[0] [receive] via NET/IB/0
g27n18:839782:840018 [0] NCCL INFO Channel 02/0 : 23[5] -> 24[0] [receive] via NET/IB/0
g27n18:839782:840018 [0] NCCL INFO Channel 00/0 : 24[0] -> 25[1] via P2P/IPC
g27n18:839782:840018 [0] NCCL INFO Channel 02/0 : 24[0] -> 25[1] via P2P/IPC
g27n18:839782:840018 [0] NCCL INFO Channel 01/0 : 24[0] -> 33[3] [send] via NET/IB/2
g27n18:839782:840018 [0] NCCL INFO Channel 03/0 : 24[0] -> 33[3] [send] via NET/IB/2
g27n18:839782:840018 [0] NCCL INFO Connected all rings
g27n18:839782:840018 [0] NCCL INFO Channel 01/0 : 24[0] -> 25[1] via P2P/IPC
g27n18:839782:840018 [0] NCCL INFO Channel 03/0 : 24[0] -> 25[1] via P2P/IPC
g27n18:839782:840018 [0] NCCL INFO Channel 01/0 : 24[0] -> 26[2] via P2P/IPC
g27n18:839782:840018 [0] NCCL INFO Channel 03/0 : 24[0] -> 26[2] via P2P/IPC
g27n18:839782:840018 [0] NCCL INFO Channel 02/0 : 24[0] -> 30[0] [send] via NET/IB/0
g27n18:839782:840018 [0] NCCL INFO Channel 00/0 : 24[0] -> 36[0] [send] via NET/IB/0
g27n18:839782:840018 [0] NCCL INFO Channel 00/0 : 24[0] -> 49[1] [send] via NET/IB/0
g27n18:839782:840018 [0] NCCL INFO Channel 00/0 : 49[1] -> 24[0] [receive] via NET/IB/0
g27n18:839782:840018 [0] NCCL INFO Channel 00/0 : 36[0] -> 24[0] [receive] via NET/IB/0
g27n18:839782:840018 [0] NCCL INFO Channel 02/0 : 30[0] -> 24[0] [receive] via NET/IB/0
g27n18:839782:840018 [0] NCCL INFO Connected all trees
g27n18:839782:840018 [0] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g27n18:839782:840018 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g27n18:839782:840018 [0] NCCL INFO comm 0x17b834140 rank 24 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x28395441218594c5 - Init COMPLETE
g27n18:839782:840099 [0] NCCL INFO Using network IB
g27n18:839782:840099 [0] NCCL INFO comm 0x17d6fe830 rank 6 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0x864be1a757d9f276 - Init START
g27n18:839782:840099 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g27n18:839782:840099 [0] NCCL INFO Trees [0] 7/9/-1->6->13 [1] 7/-1/-1->6->8
g27n18:839782:840099 [0] NCCL INFO P2P Chunksize set to 131072
g27n18:839782:840099 [0] NCCL INFO Channel 00/0 : 5[2] -> 6[0] [receive] via NET/IB/0
g27n18:839782:840099 [0] NCCL INFO Channel 01/0 : 5[2] -> 6[0] [receive] via NET/IB/0
g27n18:839782:840099 [0] NCCL INFO Channel 00/0 : 6[0] -> 7[4] via P2P/IPC
g27n18:839782:840099 [0] NCCL INFO Channel 01/0 : 6[0] -> 7[4] via P2P/IPC
g27n18:839782:840099 [0] NCCL INFO Connected all rings
g27n18:839782:840099 [0] NCCL INFO Channel 01/0 : 6[0] -> 8[2] [send] via NET/IB/0
g27n18:839782:840099 [0] NCCL INFO Channel 00/0 : 6[0] -> 9[0] [send] via NET/IB/0
g27n18:839782:840099 [0] NCCL INFO Channel 00/0 : 6[0] -> 13[4] [send] via NET/IB/0
g27n18:839782:840099 [0] NCCL INFO Channel 00/0 : 13[4] -> 6[0] [receive] via NET/IB/0
g27n18:839782:840099 [0] NCCL INFO Channel 00/0 : 9[0] -> 6[0] [receive] via NET/IB/0
g27n18:839782:840099 [0] NCCL INFO Channel 01/0 : 8[2] -> 6[0] [receive] via NET/IB/0
g27n18:839782:840099 [0] NCCL INFO Connected all trees
g27n18:839782:840099 [0] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g27n18:839782:840099 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g27n18:839782:840099 [0] NCCL INFO comm 0x17d6fe830 rank 6 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0x864be1a757d9f276 - Init COMPLETE
g27n18:839782:840118 [0] NCCL INFO Using network IB
g27n18:839782:840118 [0] NCCL INFO comm 0x17f572f20 rank 3 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x76a32220a008087c - Init START
g27n18:839782:840118 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g27n18:839782:840118 [0] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] 7/1/-1->3->11
g27n18:839782:840118 [0] NCCL INFO P2P Chunksize set to 131072
g27n18:839782:840118 [0] NCCL INFO Channel 00/0 : 2[4] -> 3[0] [receive] via NET/IB/0
g27n18:839782:840118 [0] NCCL INFO Channel 01/0 : 2[4] -> 3[0] [receive] via NET/IB/0
g27n18:839782:840118 [0] NCCL INFO Channel 00/0 : 3[0] -> 4[2] [send] via NET/IB/0
g27n18:839782:840118 [0] NCCL INFO Channel 01/0 : 3[0] -> 4[2] [send] via NET/IB/0
g27n18:839782:840118 [0] NCCL INFO Connected all rings
g27n18:839782:840118 [0] NCCL INFO Channel 01/0 : 1[2] -> 3[0] [receive] via NET/IB/0
g27n18:839782:840118 [0] NCCL INFO Channel 01/0 : 11[4] -> 3[0] [receive] via NET/IB/0
g27n18:839782:840118 [0] NCCL INFO Channel 01/0 : 3[0] -> 7[2] [send] via NET/IB/0
g27n18:839782:840118 [0] NCCL INFO Channel 01/0 : 7[2] -> 3[0] [receive] via NET/IB/0
g27n18:839782:840118 [0] NCCL INFO Channel 01/0 : 3[0] -> 11[4] [send] via NET/IB/0
g27n18:839782:840118 [0] NCCL INFO Channel 01/0 : 3[0] -> 1[2] [send] via NET/IB/0
g27n18:839782:840118 [0] NCCL INFO Channel 00/0 : 3[0] -> 2[4] [send] via NET/IB/0
g27n18:839782:840118 [0] NCCL INFO Connected all trees
g27n18:839782:840118 [0] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g27n18:839782:840118 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g27n18:839782:840118 [0] NCCL INFO comm 0x17f572f20 rank 3 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x76a32220a008087c - Init COMPLETE
g27n18:839787:839787 [3] NCCL INFO cudaDriverVersion 12020
g27n18:839787:839787 [3] NCCL INFO Bootstrap : Using ib0:10.41.17.173<0>
g27n18:839787:839787 [3] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g27n18:839787:839787 [3] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g27n18:839787:840021 [3] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.173<0>
g27n18:839787:840021 [3] NCCL INFO Using network IB
g27n18:839787:840021 [3] NCCL INFO comm 0x1587a3e30 rank 27 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x28395441218594c5 - Init START
g27n18:839787:840021 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g27n18:839787:840021 [3] NCCL INFO Trees [0] 28/-1/-1->27->26 [1] 29/16/-1->27->28 [2] 28/-1/-1->27->26 [3] 29/-1/-1->27->28
g27n18:839787:840021 [3] NCCL INFO P2P Chunksize set to 131072
g27n18:839787:840021 [3] NCCL INFO Channel 00/0 : 27[3] -> 28[4] via P2P/IPC
g27n18:839787:840021 [3] NCCL INFO Channel 01/0 : 27[3] -> 28[4] via P2P/IPC
g27n18:839787:840021 [3] NCCL INFO Channel 02/0 : 27[3] -> 28[4] via P2P/IPC
g27n18:839787:840021 [3] NCCL INFO Channel 03/0 : 27[3] -> 28[4] via P2P/IPC
g27n18:839787:840021 [3] NCCL INFO Channel 01/0 : 18[0] -> 27[3] [receive] via NET/IB/3
g27n18:839787:840021 [3] NCCL INFO Channel 03/0 : 18[0] -> 27[3] [receive] via NET/IB/3
g27n18:839787:840021 [3] NCCL INFO Connected all rings
g27n18:839787:840021 [3] NCCL INFO Channel 01/0 : 27[3] -> 29[5] via P2P/IPC
g27n18:839787:840021 [3] NCCL INFO Channel 03/0 : 27[3] -> 29[5] via P2P/IPC
g27n18:839787:840021 [3] NCCL INFO Channel 01/0 : 16[4] -> 27[3] [receive] via NET/IB/3
g27n18:839787:840021 [3] NCCL INFO Channel 01/0 : 27[3] -> 16[4] [send] via NET/IB/3
g27n18:839787:840021 [3] NCCL INFO Channel 00/0 : 27[3] -> 26[2] via P2P/IPC
g27n18:839787:840021 [3] NCCL INFO Channel 02/0 : 27[3] -> 26[2] via P2P/IPC
g27n18:839787:840021 [3] NCCL INFO Connected all trees
g27n18:839787:840021 [3] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g27n18:839787:840021 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g27n18:839787:840021 [3] NCCL INFO comm 0x1587a3e30 rank 27 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x28395441218594c5 - Init COMPLETE
g27n18:839787:840097 [3] NCCL INFO Using network IB
g27n18:839787:840097 [3] NCCL INFO comm 0x15b568870 rank 6 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x15c93a2ee1bf52db - Init START
g27n18:839787:840097 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g27n18:839787:840097 [3] NCCL INFO Trees [0] 3/9/-1->6->12 [1] -1/-1/-1->6->7
g27n18:839787:840097 [3] NCCL INFO P2P Chunksize set to 131072
g27n18:839787:840097 [3] NCCL INFO Channel 00/0 : 5[5] -> 6[3] [receive] via NET/IB/3
g27n18:839787:840097 [3] NCCL INFO Channel 01/0 : 5[5] -> 6[3] [receive] via NET/IB/3
g27n18:839787:840097 [3] NCCL INFO Channel 00/0 : 6[3] -> 7[1] [send] via NET/IB/3
g27n18:839787:840097 [3] NCCL INFO Channel 01/0 : 6[3] -> 7[1] [send] via NET/IB/3
g27n18:839787:840097 [3] NCCL INFO Connected all rings
g27n18:839787:840097 [3] NCCL INFO Channel 00/0 : 3[3] -> 6[3] [receive] via NET/IB/3
g27n18:839787:840097 [3] NCCL INFO Channel 00/0 : 6[3] -> 9[3] [send] via NET/IB/3
g27n18:839787:840097 [3] NCCL INFO Channel 00/0 : 6[3] -> 12[3] [send] via NET/IB/3
g27n18:839787:840097 [3] NCCL INFO Channel 00/0 : 12[3] -> 6[3] [receive] via NET/IB/3
g27n18:839787:840097 [3] NCCL INFO Channel 00/0 : 9[3] -> 6[3] [receive] via NET/IB/3
g27n18:839787:840097 [3] NCCL INFO Channel 00/0 : 6[3] -> 3[3] [send] via NET/IB/3
g27n18:839787:840097 [3] NCCL INFO Channel 01/0 : 7[1] -> 6[3] [receive] via NET/IB/3
g27n18:839787:840097 [3] NCCL INFO Connected all trees
g27n18:839787:840097 [3] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g27n18:839787:840097 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g27n18:839787:840097 [3] NCCL INFO comm 0x15b568870 rank 6 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x15c93a2ee1bf52db - Init COMPLETE
g27n18:839787:840121 [3] NCCL INFO Using network IB
g27n18:839787:840121 [3] NCCL INFO comm 0x15a712a30 rank 3 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x1542a3b966c3490b - Init START
g27n18:839787:840121 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g27n18:839787:840121 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] 7/1/-1->3->11
g27n18:839787:840121 [3] NCCL INFO P2P Chunksize set to 131072
g27n18:839787:840121 [3] NCCL INFO Channel 00/0 : 2[1] -> 3[3] [receive] via NET/IB/3
g27n18:839787:840121 [3] NCCL INFO Channel 01/0 : 2[1] -> 3[3] [receive] via NET/IB/3
g27n18:839787:840121 [3] NCCL INFO Channel 00/0 : 3[3] -> 4[5] [send] via NET/IB/3
g27n18:839787:840121 [3] NCCL INFO Channel 01/0 : 3[3] -> 4[5] [send] via NET/IB/3
g27n18:839787:840121 [3] NCCL INFO Connected all rings
g27n18:839787:840121 [3] NCCL INFO Channel 01/0 : 1[5] -> 3[3] [receive] via NET/IB/3
g27n18:839787:840121 [3] NCCL INFO Channel 01/0 : 11[1] -> 3[3] [receive] via NET/IB/3
g27n18:839787:840121 [3] NCCL INFO Channel 01/0 : 3[3] -> 7[5] [send] via NET/IB/3
g27n18:839787:840121 [3] NCCL INFO Channel 01/0 : 7[5] -> 3[3] [receive] via NET/IB/3
g27n18:839787:840121 [3] NCCL INFO Channel 01/0 : 3[3] -> 11[1] [send] via NET/IB/3
g27n18:839787:840121 [3] NCCL INFO Channel 01/0 : 3[3] -> 1[5] [send] via NET/IB/3
g27n18:839787:840121 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[1] [send] via NET/IB/3
g27n18:839787:840121 [3] NCCL INFO Connected all trees
g27n18:839787:840121 [3] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g27n18:839787:840121 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g27n18:839787:840121 [3] NCCL INFO comm 0x15a712a30 rank 3 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x1542a3b966c3490b - Init COMPLETE
g27n18:839790:839790 [5] NCCL INFO cudaDriverVersion 12020
g27n18:839790:839790 [5] NCCL INFO Bootstrap : Using ib0:10.41.17.173<0>
g27n18:839790:839790 [5] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g27n18:839790:839790 [5] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g27n18:839790:840020 [5] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.173<0>
g27n18:839790:840020 [5] NCCL INFO Using network IB
g27n18:839790:840020 [5] NCCL INFO comm 0x162aa3ef0 rank 29 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x28395441218594c5 - Init START
g27n18:839790:840020 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g27n18:839790:840020 [5] NCCL INFO Trees [0] -1/-1/-1->29->28 [1] 25/-1/-1->29->27 [2] -1/-1/-1->29->28 [3] 25/-1/-1->29->27
g27n18:839790:840020 [5] NCCL INFO P2P Chunksize set to 131072
g27n18:839790:840020 [5] NCCL INFO Channel 00/0 : 29[5] -> 30[0] [send] via NET/IB/1
g27n18:839790:840020 [5] NCCL INFO Channel 02/0 : 29[5] -> 30[0] [send] via NET/IB/1
g27n18:839790:840020 [5] NCCL INFO Channel 01/0 : 29[5] -> 26[2] via P2P/IPC
g27n18:839790:840020 [5] NCCL INFO Channel 03/0 : 29[5] -> 26[2] via P2P/IPC
g27n18:839790:840020 [5] NCCL INFO Connected all rings
g27n18:839790:840020 [5] NCCL INFO Channel 01/0 : 29[5] -> 25[1] via P2P/IPC
g27n18:839790:840020 [5] NCCL INFO Channel 03/0 : 29[5] -> 25[1] via P2P/IPC
g27n18:839790:840020 [5] NCCL INFO Channel 01/0 : 29[5] -> 27[3] via P2P/IPC
g27n18:839790:840020 [5] NCCL INFO Channel 03/0 : 29[5] -> 27[3] via P2P/IPC
g27n18:839790:840020 [5] NCCL INFO Channel 00/0 : 29[5] -> 28[4] via P2P/IPC
g27n18:839790:840020 [5] NCCL INFO Channel 02/0 : 29[5] -> 28[4] via P2P/IPC
g27n18:839790:840020 [5] NCCL INFO Connected all trees
g27n18:839790:840020 [5] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g27n18:839790:840020 [5] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g27n18:839790:840020 [5] NCCL INFO comm 0x162aa3ef0 rank 29 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x28395441218594c5 - Init COMPLETE
g27n18:839790:840098 [5] NCCL INFO Using network IB
g27n18:839790:840098 [5] NCCL INFO comm 0x164784db0 rank 7 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xeee4c143812ce6e4 - Init START
g27n18:839790:840098 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g27n18:839790:840098 [5] NCCL INFO Trees [0] 3/-1/-1->7->6 [1] -1/-1/-1->7->6
g27n18:839790:840098 [5] NCCL INFO P2P Chunksize set to 131072
g27n18:839790:840098 [5] NCCL INFO Channel 00/0 : 7[5] -> 8[3] [send] via NET/IB/3
g27n18:839790:840098 [5] NCCL INFO Channel 01/0 : 7[5] -> 8[3] [send] via NET/IB/3
g27n18:839790:840098 [5] NCCL INFO Connected all rings
g27n18:839790:840098 [5] NCCL INFO Channel 00/0 : 3[1] -> 7[5] [receive] via NET/IB/2
g27n18:839790:840098 [5] NCCL INFO Channel 00/0 : 7[5] -> 3[1] [send] via NET/IB/2
g27n18:839790:840098 [5] NCCL INFO Channel 00/0 : 7[5] -> 6[1] via P2P/IPC
g27n18:839790:840098 [5] NCCL INFO Channel 01/0 : 7[5] -> 6[1] via P2P/IPC
g27n18:839790:840098 [5] NCCL INFO Connected all trees
g27n18:839790:840098 [5] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g27n18:839790:840098 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g27n18:839790:840098 [5] NCCL INFO comm 0x164784db0 rank 7 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xeee4c143812ce6e4 - Init COMPLETE
g27n18:839790:840117 [5] NCCL INFO Using network IB
g27n18:839790:840117 [5] NCCL INFO comm 0x1667f1730 rank 3 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xad5995bf820e7340 - Init START
g27n18:839790:840117 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g27n18:839790:840117 [5] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] 7/1/-1->3->11
g27n18:839790:840117 [5] NCCL INFO P2P Chunksize set to 131072
g27n18:839790:840117 [5] NCCL INFO Channel 00/0 : 2[3] -> 3[5] [receive] via NET/IB/3
g27n18:839790:840117 [5] NCCL INFO Channel 01/0 : 2[3] -> 3[5] [receive] via NET/IB/3
g27n18:839790:840117 [5] NCCL INFO Channel 00/0 : 3[5] -> 4[1] [send] via NET/IB/3
g27n18:839790:840117 [5] NCCL INFO Channel 01/0 : 3[5] -> 4[1] [send] via NET/IB/3
g27n18:839790:840117 [5] NCCL INFO Connected all rings
g27n18:839790:840117 [5] NCCL INFO Channel 01/0 : 1[1] -> 3[5] [receive] via NET/IB/3
g27n18:839790:840117 [5] NCCL INFO Channel 01/0 : 11[3] -> 3[5] [receive] via NET/IB/3
g27n18:839790:840117 [5] NCCL INFO Channel 01/0 : 3[5] -> 7[1] [send] via NET/IB/3
g27n18:839790:840117 [5] NCCL INFO Channel 01/0 : 7[1] -> 3[5] [receive] via NET/IB/3
g27n18:839790:840117 [5] NCCL INFO Channel 01/0 : 3[5] -> 11[3] [send] via NET/IB/3
g27n18:839790:840117 [5] NCCL INFO Channel 01/0 : 3[5] -> 1[1] [send] via NET/IB/3
g27n18:839790:840117 [5] NCCL INFO Channel 00/0 : 3[5] -> 2[3] [send] via NET/IB/3
g27n18:839790:840117 [5] NCCL INFO Connected all trees
g27n18:839790:840117 [5] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g27n18:839790:840117 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g27n18:839790:840117 [5] NCCL INFO comm 0x1667f1730 rank 3 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xad5995bf820e7340 - Init COMPLETE
g27n18:839783:839783 [1] NCCL INFO cudaDriverVersion 12020
g27n18:839783:839783 [1] NCCL INFO Bootstrap : Using ib0:10.41.17.173<0>
g27n18:839783:839783 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g27n18:839783:839783 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g27n18:839783:840017 [1] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.173<0>
g27n18:839783:840017 [1] NCCL INFO Using network IB
g27n18:839783:840017 [1] NCCL INFO comm 0x16e294370 rank 25 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x28395441218594c5 - Init START
g27n18:839783:840017 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g27n18:839783:840017 [1] NCCL INFO Trees [0] 26/12/-1->25->24 [1] 24/-1/-1->25->29 [2] 26/-1/-1->25->24 [3] 24/-1/-1->25->29
g27n18:839783:840017 [1] NCCL INFO P2P Chunksize set to 131072
g27n18:839783:840017 [1] NCCL INFO Channel 00/0 : 25[1] -> 26[2] via P2P/IPC
g27n18:839783:840017 [1] NCCL INFO Channel 02/0 : 25[1] -> 26[2] via P2P/IPC
g27n18:839783:840017 [1] NCCL INFO Channel 01/0 : 25[1] -> 24[0] via P2P/IPC
g27n18:839783:840017 [1] NCCL INFO Channel 03/0 : 25[1] -> 24[0] via P2P/IPC
g27n18:839783:840017 [1] NCCL INFO Connected all rings
g27n18:839783:840017 [1] NCCL INFO Channel 01/0 : 25[1] -> 29[5] via P2P/IPC
g27n18:839783:840017 [1] NCCL INFO Channel 03/0 : 25[1] -> 29[5] via P2P/IPC
g27n18:839783:840017 [1] NCCL INFO Channel 00/0 : 12[0] -> 25[1] [receive] via NET/IB/0
g27n18:839783:840017 [1] NCCL INFO Channel 00/0 : 25[1] -> 12[0] [send] via NET/IB/0
g27n18:839783:840017 [1] NCCL INFO Channel 00/0 : 25[1] -> 24[0] via P2P/IPC
g27n18:839783:840017 [1] NCCL INFO Channel 02/0 : 25[1] -> 24[0] via P2P/IPC
g27n18:839783:840017 [1] NCCL INFO Connected all trees
g27n18:839783:840017 [1] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g27n18:839783:840017 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g27n18:839783:840017 [1] NCCL INFO comm 0x16e294370 rank 25 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x28395441218594c5 - Init COMPLETE
g27n18:839783:840102 [1] NCCL INFO Using network IB
g27n18:839783:840102 [1] NCCL INFO comm 0x170f5f220 rank 6 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0xeee4c143812ce6e4 - Init START
g27n18:839783:840102 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g27n18:839783:840102 [1] NCCL INFO Trees [0] 7/9/-1->6->13 [1] 7/-1/-1->6->8
g27n18:839783:840102 [1] NCCL INFO P2P Chunksize set to 131072
g27n18:839783:840102 [1] NCCL INFO Channel 00/0 : 5[3] -> 6[1] [receive] via NET/IB/2
g27n18:839783:840102 [1] NCCL INFO Channel 01/0 : 5[3] -> 6[1] [receive] via NET/IB/2
g27n18:839783:840102 [1] NCCL INFO Channel 00/0 : 6[1] -> 7[5] via P2P/IPC
g27n18:839783:840102 [1] NCCL INFO Channel 01/0 : 6[1] -> 7[5] via P2P/IPC
g27n18:839783:840102 [1] NCCL INFO Connected all rings
g27n18:839783:840102 [1] NCCL INFO Channel 01/0 : 6[1] -> 8[3] [send] via NET/IB/2
g27n18:839783:840102 [1] NCCL INFO Channel 00/0 : 6[1] -> 9[1] [send] via NET/IB/2
g27n18:839783:840102 [1] NCCL INFO Channel 00/0 : 6[1] -> 13[5] [send] via NET/IB/2
g27n18:839783:840102 [1] NCCL INFO Channel 00/0 : 13[5] -> 6[1] [receive] via NET/IB/2
g27n18:839783:840102 [1] NCCL INFO Channel 00/0 : 9[1] -> 6[1] [receive] via NET/IB/2
g27n18:839783:840102 [1] NCCL INFO Channel 01/0 : 8[3] -> 6[1] [receive] via NET/IB/2
g27n18:839783:840102 [1] NCCL INFO Connected all trees
g27n18:839783:840102 [1] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g27n18:839783:840102 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g27n18:839783:840102 [1] NCCL INFO comm 0x170f5f220 rank 6 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0xeee4c143812ce6e4 - Init COMPLETE
g27n18:839783:840116 [1] NCCL INFO Using network IB
g27n18:839783:840116 [1] NCCL INFO comm 0x172038300 rank 3 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x17fc85f125de53ec - Init START
g27n18:839783:840116 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g27n18:839783:840116 [1] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] 7/1/-1->3->11
g27n18:839783:840116 [1] NCCL INFO P2P Chunksize set to 131072
g27n18:839783:840116 [1] NCCL INFO Channel 00/0 : 2[5] -> 3[1] [receive] via NET/IB/2
g27n18:839783:840116 [1] NCCL INFO Channel 01/0 : 2[5] -> 3[1] [receive] via NET/IB/2
g27n18:839783:840116 [1] NCCL INFO Channel 00/0 : 3[1] -> 4[3] [send] via NET/IB/2
g27n18:839783:840116 [1] NCCL INFO Channel 01/0 : 3[1] -> 4[3] [send] via NET/IB/2
g27n18:839783:840116 [1] NCCL INFO Connected all rings
g27n18:839783:840116 [1] NCCL INFO Channel 01/0 : 1[3] -> 3[1] [receive] via NET/IB/2
g27n18:839783:840116 [1] NCCL INFO Channel 01/0 : 11[5] -> 3[1] [receive] via NET/IB/2
g27n18:839783:840116 [1] NCCL INFO Channel 01/0 : 3[1] -> 7[3] [send] via NET/IB/2
g27n18:839783:840116 [1] NCCL INFO Channel 01/0 : 7[3] -> 3[1] [receive] via NET/IB/2
g27n18:839783:840116 [1] NCCL INFO Channel 01/0 : 3[1] -> 11[5] [send] via NET/IB/2
g27n18:839783:840116 [1] NCCL INFO Channel 01/0 : 3[1] -> 1[3] [send] via NET/IB/2
g27n18:839783:840116 [1] NCCL INFO Channel 00/0 : 3[1] -> 2[5] [send] via NET/IB/2
g27n18:839783:840116 [1] NCCL INFO Connected all trees
g27n18:839783:840116 [1] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g27n18:839783:840116 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g27n18:839783:840116 [1] NCCL INFO comm 0x172038300 rank 3 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x17fc85f125de53ec - Init COMPLETE
g27n18:839785:839785 [2] NCCL INFO cudaDriverVersion 12020
g27n18:839785:839785 [2] NCCL INFO Bootstrap : Using ib0:10.41.17.173<0>
g27n18:839785:839785 [2] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g27n18:839785:839785 [2] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g27n18:839785:840019 [2] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.173<0>
g27n18:839785:840019 [2] NCCL INFO Using network IB
g27n18:839785:840019 [2] NCCL INFO comm 0x1620e3fc0 rank 26 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x28395441218594c5 - Init START
g27n18:839785:840019 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g27n18:839785:840019 [2] NCCL INFO Trees [0] 27/-1/-1->26->25 [1] -1/-1/-1->26->24 [2] 27/-1/-1->26->25 [3] -1/-1/-1->26->24
g27n18:839785:840019 [2] NCCL INFO P2P Chunksize set to 131072
g27n18:839785:840019 [2] NCCL INFO Channel 00/0 : 26[2] -> 27[3] via P2P/IPC
g27n18:839785:840019 [2] NCCL INFO Channel 02/0 : 26[2] -> 27[3] via P2P/IPC
g27n18:839785:840019 [2] NCCL INFO Channel 01/0 : 26[2] -> 25[1] via P2P/IPC
g27n18:839785:840019 [2] NCCL INFO Channel 03/0 : 26[2] -> 25[1] via P2P/IPC
g27n18:839785:840019 [2] NCCL INFO Connected all rings
g27n18:839785:840019 [2] NCCL INFO Channel 01/0 : 26[2] -> 24[0] via P2P/IPC
g27n18:839785:840019 [2] NCCL INFO Channel 03/0 : 26[2] -> 24[0] via P2P/IPC
g27n18:839785:840019 [2] NCCL INFO Channel 00/0 : 26[2] -> 25[1] via P2P/IPC
g27n18:839785:840019 [2] NCCL INFO Channel 02/0 : 26[2] -> 25[1] via P2P/IPC
g27n18:839785:840019 [2] NCCL INFO Connected all trees
g27n18:839785:840019 [2] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g27n18:839785:840019 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g27n18:839785:840019 [2] NCCL INFO comm 0x1620e3fc0 rank 26 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x28395441218594c5 - Init COMPLETE
g27n18:839785:840101 [2] NCCL INFO Using network IB
g27n18:839785:840101 [2] NCCL INFO comm 0x164d59970 rank 6 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0xe45164e09488020a - Init START
g27n18:839785:840101 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g27n18:839785:840101 [2] NCCL INFO Trees [0] 3/9/-1->6->12 [1] -1/-1/-1->6->7
g27n18:839785:840101 [2] NCCL INFO P2P Chunksize set to 131072
g27n18:839785:840101 [2] NCCL INFO Channel 00/0 : 5[4] -> 6[2] [receive] via NET/IB/0
g27n18:839785:840101 [2] NCCL INFO Channel 01/0 : 5[4] -> 6[2] [receive] via NET/IB/0
g27n18:839785:840101 [2] NCCL INFO Channel 00/0 : 6[2] -> 7[0] [send] via NET/IB/0
g27n18:839785:840101 [2] NCCL INFO Channel 01/0 : 6[2] -> 7[0] [send] via NET/IB/0
g27n18:839785:840101 [2] NCCL INFO Connected all rings
g27n18:839785:840101 [2] NCCL INFO Channel 00/0 : 3[2] -> 6[2] [receive] via NET/IB/0
g27n18:839785:840101 [2] NCCL INFO Channel 00/0 : 6[2] -> 9[2] [send] via NET/IB/0
g27n18:839785:840101 [2] NCCL INFO Channel 00/0 : 6[2] -> 12[2] [send] via NET/IB/0
g27n18:839785:840101 [2] NCCL INFO Channel 00/0 : 12[2] -> 6[2] [receive] via NET/IB/0
g27n18:839785:840101 [2] NCCL INFO Channel 00/0 : 9[2] -> 6[2] [receive] via NET/IB/0
g27n18:839785:840101 [2] NCCL INFO Channel 00/0 : 6[2] -> 3[2] [send] via NET/IB/0
g27n18:839785:840101 [2] NCCL INFO Channel 01/0 : 7[0] -> 6[2] [receive] via NET/IB/0
g27n18:839785:840101 [2] NCCL INFO Connected all trees
g27n18:839785:840101 [2] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g27n18:839785:840101 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g27n18:839785:840101 [2] NCCL INFO comm 0x164d59970 rank 6 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0xe45164e09488020a - Init COMPLETE
g27n18:839785:840120 [2] NCCL INFO Using network IB
g27n18:839785:840120 [2] NCCL INFO comm 0x16498be40 rank 3 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0xaca7b5a6b2aa6a19 - Init START
g27n18:839785:840120 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g27n18:839785:840120 [2] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] 7/1/-1->3->11
g27n18:839785:840120 [2] NCCL INFO P2P Chunksize set to 131072
g27n18:839785:840120 [2] NCCL INFO Channel 00/0 : 2[0] -> 3[2] [receive] via NET/IB/0
g27n18:839785:840120 [2] NCCL INFO Channel 01/0 : 2[0] -> 3[2] [receive] via NET/IB/0
g27n18:839785:840120 [2] NCCL INFO Channel 00/0 : 3[2] -> 4[4] [send] via NET/IB/0
g27n18:839785:840120 [2] NCCL INFO Channel 01/0 : 3[2] -> 4[4] [send] via NET/IB/0
g27n18:839785:840120 [2] NCCL INFO Connected all rings
g27n18:839785:840120 [2] NCCL INFO Channel 01/0 : 1[4] -> 3[2] [receive] via NET/IB/0
g27n18:839785:840120 [2] NCCL INFO Channel 01/0 : 11[0] -> 3[2] [receive] via NET/IB/0
g27n18:839785:840120 [2] NCCL INFO Channel 01/0 : 3[2] -> 7[4] [send] via NET/IB/0
g27n18:839785:840120 [2] NCCL INFO Channel 01/0 : 7[4] -> 3[2] [receive] via NET/IB/0
g27n18:839785:840120 [2] NCCL INFO Channel 01/0 : 3[2] -> 11[0] [send] via NET/IB/0
g27n18:839785:840120 [2] NCCL INFO Channel 01/0 : 3[2] -> 1[4] [send] via NET/IB/0
g27n18:839785:840120 [2] NCCL INFO Channel 00/0 : 3[2] -> 2[0] [send] via NET/IB/0
g27n18:839785:840120 [2] NCCL INFO Connected all trees
g27n18:839785:840120 [2] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g27n18:839785:840120 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g27n18:839785:840120 [2] NCCL INFO comm 0x16498be40 rank 3 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0xaca7b5a6b2aa6a19 - Init COMPLETE
g28n02:792072:792072 [3] NCCL INFO cudaDriverVersion 12020
g28n02:792072:792072 [3] NCCL INFO Bootstrap : Using ib0:10.41.17.175<0>
g28n02:792072:792072 [3] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g28n02:792072:792072 [3] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g28n02:792072:792309 [3] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.175<0>
g28n02:792072:792309 [3] NCCL INFO Using network IB
g28n02:792072:792309 [3] NCCL INFO comm 0x15d633a10 rank 39 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x28395441218594c5 - Init START
g28n02:792072:792309 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g28n02:792072:792309 [3] NCCL INFO Trees [0] 40/-1/-1->39->38 [1] 41/34/-1->39->40 [2] 40/-1/-1->39->38 [3] 41/-1/-1->39->40
g28n02:792072:792309 [3] NCCL INFO P2P Chunksize set to 131072
g28n02:792072:792309 [3] NCCL INFO Channel 00/0 : 39[3] -> 40[4] via P2P/IPC
g28n02:792072:792309 [3] NCCL INFO Channel 01/0 : 39[3] -> 40[4] via P2P/IPC
g28n02:792072:792309 [3] NCCL INFO Channel 02/0 : 39[3] -> 40[4] via P2P/IPC
g28n02:792072:792309 [3] NCCL INFO Channel 03/0 : 39[3] -> 40[4] via P2P/IPC
g28n02:792072:792309 [3] NCCL INFO Channel 01/0 : 30[0] -> 39[3] [receive] via NET/IB/3
g28n02:792072:792309 [3] NCCL INFO Channel 03/0 : 30[0] -> 39[3] [receive] via NET/IB/3
g28n02:792072:792309 [3] NCCL INFO Connected all rings
g28n02:792072:792309 [3] NCCL INFO Channel 01/0 : 39[3] -> 41[5] via P2P/IPC
g28n02:792072:792309 [3] NCCL INFO Channel 03/0 : 39[3] -> 41[5] via P2P/IPC
g28n02:792072:792309 [3] NCCL INFO Channel 01/0 : 34[4] -> 39[3] [receive] via NET/IB/3
g28n02:792072:792309 [3] NCCL INFO Channel 01/0 : 39[3] -> 34[4] [send] via NET/IB/3
g28n02:792072:792309 [3] NCCL INFO Channel 00/0 : 39[3] -> 38[2] via P2P/IPC
g28n02:792072:792309 [3] NCCL INFO Channel 02/0 : 39[3] -> 38[2] via P2P/IPC
g28n02:792072:792309 [3] NCCL INFO Connected all trees
g28n02:792072:792309 [3] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g28n02:792072:792309 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g28n02:792072:792309 [3] NCCL INFO comm 0x15d633a10 rank 39 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x28395441218594c5 - Init COMPLETE
g28n02:792072:792402 [3] NCCL INFO Using network IB
g28n02:792072:792402 [3] NCCL INFO comm 0x1604e5b60 rank 9 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x15c93a2ee1bf52db - Init START
g28n02:792072:792402 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g28n02:792072:792402 [3] NCCL INFO Trees [0] 7/10/-1->9->6 [1] -1/-1/-1->9->8
g28n02:792072:792402 [3] NCCL INFO P2P Chunksize set to 131072
g28n02:792072:792402 [3] NCCL INFO Channel 00/0 : 8[5] -> 9[3] [receive] via NET/IB/3
g28n02:792072:792402 [3] NCCL INFO Channel 01/0 : 8[5] -> 9[3] [receive] via NET/IB/3
g28n02:792072:792402 [3] NCCL INFO Channel 00/0 : 9[3] -> 10[1] [send] via NET/IB/3
g28n02:792072:792402 [3] NCCL INFO Channel 01/0 : 9[3] -> 10[1] [send] via NET/IB/3
g28n02:792072:792402 [3] NCCL INFO Connected all rings
g28n02:792072:792402 [3] NCCL INFO Channel 00/0 : 7[1] -> 9[3] [receive] via NET/IB/3
g28n02:792072:792402 [3] NCCL INFO Channel 00/0 : 6[3] -> 9[3] [receive] via NET/IB/3
g28n02:792072:792402 [3] NCCL INFO Channel 00/0 : 9[3] -> 6[3] [send] via NET/IB/3
g28n02:792072:792402 [3] NCCL INFO Channel 00/0 : 9[3] -> 7[1] [send] via NET/IB/3
g28n02:792072:792402 [3] NCCL INFO Channel 00/0 : 10[1] -> 9[3] [receive] via NET/IB/3
g28n02:792072:792402 [3] NCCL INFO Channel 01/0 : 9[3] -> 8[5] [send] via NET/IB/3
g28n02:792072:792402 [3] NCCL INFO Connected all trees
g28n02:792072:792402 [3] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g28n02:792072:792402 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n02:792072:792402 [3] NCCL INFO comm 0x1604e5b60 rank 9 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 comg28n02:792075:792075 [4] NCCL INFO cudaDriverVersion 12020
g28n02:792075:792075 [4] NCCL INFO Bootstrap : Using ib0:10.41.17.175<0>
g28n02:792075:792075 [4] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g28n02:792075:792075 [4] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g28n02:792075:792307 [4] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.175<0>
g28n02:792075:792307 [4] NCCL INFO Using network IB
g28n02:792075:792307 [4] NCCL INFO comm 0x12be93c10 rank 40 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x28395441218594c5 - Init START
g28n02:792075:792307 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g28n02:792075:792307 [4] NCCL INFO Trees [0] 41/-1/-1->40->39 [1] 39/46/-1->40->28 [2] 41/-1/-1->40->39 [3] 39/-1/-1->40->33
g28n02:792075:792307 [4] NCCL INFO P2P Chunksize set to 131072
g28n02:792075:792307 [4] NCCL INFO Channel 00/0 : 40[4] -> 41[5] via P2P/IPC
g28n02:792075:792307 [4] NCCL INFO Channel 01/0 : 40[4] -> 41[5] via P2P/IPC
g28n02:792075:792307 [4] NCCL INFO Channel 02/0 : 40[4] -> 41[5] via P2P/IPC
g28n02:792075:792307 [4] NCCL INFO Channel 03/0 : 40[4] -> 41[5] via P2P/IPC
g28n02:792075:792307 [4] NCCL INFO Connected all rings
g28n02:792075:792307 [4] NCCL INFO Channel 01/0 : 40[4] -> 46[4] [send] via NET/IB/3
g28n02:792075:792307 [4] NCCL INFO Channel 03/0 : 33[3] -> 40[4] [receive] via NET/IB/3
g28n02:792075:792307 [4] NCCL INFO Channel 01/0 : 28[4] -> 40[4] [receive] via NET/IB/3
g28n02:792075:792307 [4] NCCL INFO Channel 01/0 : 40[4] -> 28[4] [send] via NET/IB/3
g28n02:792075:792307 [4] NCCL INFO Channel 03/0 : 40[4] -> 33[3] [send] via NET/IB/3
g28n02:792075:792307 [4] NCCL INFO Channel 01/0 : 46[4] -> 40[4] [receive] via NET/IB/3
g28n02:792075:792307 [4] NCCL INFO Channel 00/0 : 40[4] -> 39[3] via P2P/IPC
g28n02:792075:792307 [4] NCCL INFO Channel 01/0 : 40[4] -> 39[3] via P2P/IPC
g28n02:792075:792307 [4] NCCL INFO Channel 02/0 : 40[4] -> 39[3] via P2P/IPC
g28n02:792075:792307 [4] NCCL INFO Channel 03/0 : 40[4] -> 39[3] via P2P/IPC
g28n02:792075:792307 [4] NCCL INFO Connected all trees
g28n02:792075:792307 [4] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g28n02:792075:792307 [4] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g28n02:792075:792307 [4] NCCL INFO comm 0x12be93c10 rank 40 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x28395441218594c5 - Init COMPLETE
g28n02:792075:792404 [4] NCCL INFO Using network IB
g28n02:792075:792404 [4] NCCL INFO comm 0x12db22630 rank 10 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x864be1a757d9f276 - Init START
g28n02:792075:792404 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g28n02:792075:792404 [4] NCCL INFO Trees [0] 8/-1/-1->10->9 [1] -1/-1/-1->10->9
g28n02:792075:792404 [4] NCCL INFO P2P Chunksize set to 131072
g28n02:792075:792404 [4] NCCL INFO Channel 00/0 : 10[4] -> 11[2] [send] via NET/IB/1
g28n02:792075:792404 [4] NCCL INFO Channel 01/0 : 10[4] -> 11[2] [send] via NET/IB/1
g28n02:792075:792404 [4] NCCL INFO Connected all rings
g28n02:792075:792404 [4] NCCL INFO Channel 00/0 : 8[2] -> 10[4] [receive] via NET/IB/0
g28n02:792075:792404 [4] NCCL INFO Channel 00/0 : 10[4] -> 8[2] [send] via NET/IB/0
g28n02:792075:792404 [4] NCCL INFO Channel 00/0 : 10[4] -> 9[0] via P2P/IPC
g28n02:792075:792404 [4] NCCL INFO Channel 01/0 : 10[4] -> 9[0] via P2P/IPC
g28n02:792075:792404 [4] NCCL INFO Connected all trees
g28n02:792075:792404 [4] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g28n02:792075:792404 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n02:792075:792404 [4] NCCL INFO comm 0x12db22630 rank 10 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x864be1a757d9f276 - Init COMPLETE
g28n02:792075:792419 [4] NCCL INFO Using network IB
g28n02:792075:792419 [4] NCCL INFO comm 0x12eafa2f0 rank 5 nranks 12 cudaDevmId 0x15c93a2ee1bf52db - Init COMPLETE
g28n02:792072:792421 [3] NCCL INFO Using network IB
g28n02:792072:792421 [3] NCCL INFO comm 0x15ff9b670 rank 4 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x6876d4a326688ade - Init START
g28n02:792072:792421 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g28n02:792072:792421 [3] NCCL INFO Trees [0] 2/6/-1->4->8 [1] -1/-1/-1->4->5
g28n02:792072:792421 [3] NCCL INFO P2P Chunksize set to 131072
g28n02:792072:792421 [3] NCCL INFO Channel 00/0 : 3[1] -> 4[3] [receive] via NET/IB/3
g28n02:792072:792421 [3] NCCL INFO Channel 01/0 : 3[1] -> 4[3] [receive] via NET/IB/3
g28n02:792072:792421 [3] NCCL INFO Channel 00/0 : 4[3] -> 5[5] [send] via NET/IB/3
g28n02:792072:792421 [3] NCCL INFO Channel 01/0 : 4[3] -> 5[5] [send] via NET/IB/3
g28n02:792072:792421 [3] NCCL INFO Connected all rings
g28n02:792072:792421 [3] NCCL INFO Channel 00/0 : 2[5] -> 4[3] [receive] via NET/IB/3
g28n02:792072:792421 [3] NCCL INFO Channel 00/0 : 4[3] -> 6[1] [send] via NET/IB/3
g28n02:792072:792421 [3] NCCL INFO Channel 00/0 : 4[3] -> 8[5] [send] via NET/IB/3
g28n02:792072:792421 [3] NCCL INFO Channel 00/0 : 8[5] -> 4[3] [receive] via NET/IB/3
g28n02:792072:792421 [3] NCCL INFO Channel 00/0 : 6[1] -> 4[3] [receive] via NET/IB/3
g28n02:792072:792421 [3] NCCL INFO Channel 00/0 : 4[3] -> 2[5] [send] via NET/IB/3
g28n02:792072:792421 [3] NCCL INFO Channel 01/0 : 5[5] -> 4[3] [receive] via NET/IB/3
g28n02:792072:792421 [3] NCCL INFO Connected all trees
g28n02:792072:792421 [3] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g28n02:792072:792421 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n02:792072:792421 [3] NCCL INFO comm 0x15ff9b670 rank 4 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x6876d4a326688ade - Init COMPLETE
 4 nvmlDev 4 busId 3504000 commId 0x76a32220a008087c - Init START
g28n02:792075:792419 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g28n02:792075:792419 [4] NCCL INFO Trees [0] -1/-1/-1->5->6 [1] 6/4/-1->5->7
g28n02:792075:792419 [4] NCCL INFO P2P Chunksize set to 131072
g28n02:792075:792419 [4] NCCL INFO Channel 00/0 : 4[2] -> 5[4] [receive] via NET/IB/1
g28n02:792075:792419 [4] NCCL INFO Channel 01/0 : 4[2] -> 5[4] [receive] via NET/IB/1
g28n02:792075:792419 [4] NCCL INFO Channel 00/0 : 5[4] -> 6[0] [send] via NET/IB/1
g28n02:792075:792419 [4] NCCL INFO Channel 01/0 : 5[4] -> 6[0] [send] via NET/IB/1
g28n02:792075:792419 [4] NCCL INFO Connected all rings
g28n02:792075:792419 [4] NCCL INFO Channel 01/0 : 5[4] -> 7[2] [send] via NET/IB/1
g28n02:792075:792419 [4] NCCL INFO Channel 01/0 : 7[2] -> 5[4] [receive] via NET/IB/1
g28n02:792075:792419 [4] NCCL INFO Channel 00/0 : 6[0] -> 5[4] [receive] via NET/IB/1
g28n02:792075:792419 [4] NCCL INFO Channel 01/0 : 6[0] -> 5[4] [receive] via NET/IB/1
g28n02:792075:792419 [4] NCCL INFO Channel 01/0 : 5[4] -> 4[2] [send] via NET/IB/1
g28n02:792075:792419 [4] NCCL INFO Connected all trees
g28n02:792075:792419 [4] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g28n02:792075:792419 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n02:792075:792419 [4] NCCL INFO comm 0x12eafa2f0 rank 5 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x76a32220a008087c - Init COMPLETE
g28n02:792076:792076 [5] NCCL INFO cudaDriverVersion 12020
g28n02:792076:792076 [5] NCCL INFO Bootstrap : Using ib0:10.41.17.175<0>
g28n02:792076:792076 [5] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g28n02:792076:792076 [5] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g28n02:792076:792308 [5] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.175<0>
g28n02:792076:792308 [5] NCCL INFO Using network IB
g28n02:792076:792308 [5] NCCL INFO comm 0x16da23ae0 rank 41 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x28395441218594c5 - Init START
g28n02:792076:792308 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g28n02:792076:792308 [5] NCCL INFO Trees [0] -1/-1/-1->41->40 [1] 37/-1/-1->41->39 [2] -1/-1/-1->41->40 [3] 37/-1/-1->41->39
g28n02:792076:792308 [5] NCCL INFO P2P Chunksize set to 131072
g28n02:792076:792308 [5] NCCL INFO Channel 00/0 : 41[5] -> 42[0] [send] via NET/IB/1
g28n02:792076:792308 [5] NCCL INFO Channel 02/0 : 41[5] -> 42[0] [send] via NET/IB/1
g28n02:792076:792308 [5] NCCL INFO Channel 01/0 : 41[5] -> 38[2] via P2P/IPC
g28n02:792076:792308 [5] NCCL INFO Channel 03/0 : 41[5] -> 38[2] via P2P/IPC
g28n02:792076:792308 [5] NCCL INFO Connected all rings
g28n02:792076:792308 [5] NCCL INFO Channel 01/0 : 41[5] -> 37[1] via P2P/IPC
g28n02:792076:792308 [5] NCCL INFO Channel 03/0 : 41[5] -> 37[1] via P2P/IPC
g28n02:792076:792308 [5] NCCL INFO Channel 01/0 : 41[5] -> 39[3] via P2P/IPC
g28n02:792076:792308 [5] NCCL INFO Channel 03/0 : 41[5] -> 39[3] via P2P/IPC
g28n02:792076:792308 [5] NCCL INFO Channel 00/0 : 41[5] -> 40[4] via P2P/IPC
g28n02:792076:792308 [5] NCCL INFO Channel 02/0 : 41[5] -> 40[4] via P2P/IPC
g28n02:792076:792308 [5] NCCL INFO Connected all trees
g28n02:792076:792308 [5] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g28n02:792076:792308 [5] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g28n02:792076:792308 [5] NCCL INFO comm 0x16da23ae0 rank 41 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x28395441218594c5 - Init COMPLETE
g28n02:792076:792403 [5] NCCL INFO Using network IB
g28n02:792076:792403 [5] NCCL INFO comm 0x1707cca20 rank 10 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xeee4c143812ce6e4 - Init START
g28n02:792076:792403 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g28n02:792076:792403 [5] NCCL INFO Trees [0] 8/-1/-1->10->9 [1] -1/-1/-1->10->9
g28n02:792076:792403 [5] NCCL INFO P2P Chunksize set to 131072
g28n02:792076:792403 [5] NCCL INFO Channel 00/0 : 10[5] -> 11[3] [send] via NET/IB/3
g28n02:792076:792403 [5] NCCL INFO Channel 01/0 : 10[5] -> 11[3] [send] via NET/IB/3
g28n02:792076:792403 [5] NCCL INFO Connected all rings
g28n02:792076:792403 [5] NCCL INFO Channel 00/0 : 8[3] -> 10[5] [receive] via NET/IB/2
g28n02:792076:792403 [5] NCCL INFO Channel 00/0 : 10[5] -> 8[3] [send] via NET/IB/2
g28n02:792076:792403 [5] NCCL INFO Channel 00/0 : 10[5] -> 9[1] via P2P/IPC
g28n02:792076:792403 [5] NCCL INFO Channel 01/0 : 10[5] -> 9[1] via P2P/IPC
g28n02:792076:792403 [5] NCCL INFO Connected all trees
g28n02:792076:792403 [5] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g28n02:792076:792403 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n02:792076:792403 [5] NCCL INFO comm 0x1707cca20 rank 10 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xeee4c143812ce6e4 - Init COMPLETE
g28n02:792076:792423 [5] NCCL INFO Using network IB
g28n02:792076:792423 [5] NCCL INFO comm 0x16f6be2b0 rank 5 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x17fc85f125de53ec - Init START
g28n02:792076:792423 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g28n02:792076:792423 [5] NCCL INFO Trees [0] -1/-1/-1->5->6 [1] 6/4/-1->5->7
g28n02:792076:792423 [5] NCCL INFO P2P Chunksize set to 131072
g28n02:792076:792g28n02:792071:792071 [2] NCCL INFO cudaDriverVersion 12020
g28n02:792071:792071 [2] NCCL INFO Bootstrap : Using ib0:10.41.17.175<0>
g28n02:792071:792071 [2] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g28n02:792071:792071 [2] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g28n02:792071:792306 [2] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.175<0>
g28n02:792071:792306 [2] NCCL INFO Using network IB
g28n02:792071:792306 [2] NCCL INFO comm 0x1572a4250 rank 38 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x28395441218594c5 - Init START
g28n02:792071:792306 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g28n02:792071:792306 [2] NCCL INFO Trees [0] 39/-1/-1->38->37 [1] -1/-1/-1->38->36 [2] 39/-1/-1->38->37 [3] -1/-1/-1->38->36
g28n02:792071:792306 [2] NCCL INFO P2P Chunksize set to 131072
g28n02:792071:792306 [2] NCCL INFO Channel 00/0 : 38[2] -> 39[3] via P2P/IPC
g28n02:792071:792306 [2] NCCL INFO Channel 02/0 : 38[2] -> 39[3] via P2P/IPC
g28n02:792071:792306 [2] NCCL INFO Channel 01/0 : 38[2] -> 37[1] via P2P/IPC
g28n02:792071:792306 [2] NCCL INFO Channel 03/0 : 38[2] -> 37[1] via P2P/IPC
g28n02:792071:792306 [2] NCCL INFO Connected all rings
g28n02:792071:792306 [2] NCCL INFO Channel 01/0 : 38[2] -> 36[0] via P2P/IPC
g28n02:792071:792306 [2] NCCL INFO Channel 03/0 : 38[2] -> 36[0] via P2P/IPC
g28n02:792071:792306 [2] NCCL INFO Channel 00/0 : 38[2] -> 37[1] via P2P/IPC
g28n02:792071:792306 [2] NCCL INFO Channel 02/0 : 38[2] -> 37[1] via P2P/IPC
g28n02:792071:792306 [2] NCCL INFO Connected all trees
g28n02:792071:792306 [2] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g28n02:792071:792306 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g28n02:792071:792306 [2] NCCL INFO comm 0x1572a4250 rank 38 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x28395441218594c5 - Init COMPLETE
g28n02:792071:792401 [2] NCCL INFO Using network IB
g28n02:792071:792401 [2] NCCL INFO comm 0x159b8c0b0 rank 9 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0xe45164e09488020a - Init START
g28n02:792071:792401 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g28n02:792071:792401 [2] NCCL INFO Trees [0] 7/10/-1->9->6 [1] -1/-1/-1->9->8
g28n02:792071:792401 [2] NCCL INFO P2P Chunksize set to 131072
g28n02:792071:792401 [2] NCCL INFO Channel 00/0 : 8[4] -> 9[2] [receive] via NET/IB/0
g28n02:792071:792401 [2] NCCL INFO Channel 01/0 : 8[4] -> 9[2] [receive] via NET/IB/0
g28n02:792071:792401 [2] NCCL INFO Channel 00/0 : 9[2] -> 10[0] [send] via NET/IB/0
g28n02:792071:792401 [2] NCCL INFO Channel 01/0 : 9[2] -> 10[0] [send] via NET/IB/0
g28n02:792071:792401 [2] NCCL INFO Connected all rings
g28n02:792071:792401 [2] NCCL INFO Channel 00/0 : 7[0] -> 9[2] [receive] via NET/IB/0
g28n02:792071:792401 [2] NCCL INFO Channel 00/0 : 6[2] -> 9[2] [receive] via NET/IB/0
g28n02:792071:792401 [2] NCCL INFO Channel 00/0 : 9[2] -> 6[2] [send] via NET/IB/0
g28n02:792071:792401 [2] NCCL INFO Channel 00/0 : 9[2] -> 7[0] [send] via NET/IB/0
g28n02:792071:792401 [2] NCCL INFO Channel 00/0 : 10[0] -> 9[2] [receive] via NET/IB/0
g28n02:792071:792401 [2] NCCL INFO Channel 01/0 : 9[2] -> 8[4] [send] via NET/IB/0
g28n02:792071:792401 [2] NCCL INFO Connected all trees
g28n02:792071:792401 [2] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g28n02:792071:792401 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n02:792071:792401 [2] NCCL INFO comm 0x159b8c0b0 rank 9 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0xe45164e09488020a - Init COMPLETE
g28n02:792071:792420 [2] NCCL INFO Using network IB
g28n02:792071:792420 [2] NCCL INFO comm 0x15a1981a0 rank 4 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0x8442a1c07b0a8edb - Init START
g28n02:792071:792420 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g28n02:792071:792420 [2] NCCL INFO Trees [0] 2/6/-1->4->423 [5] NCCL INFO Channel 00/0 : 4[3] -> 5[5] [receive] via NET/IB/3
g28n02:792076:792423 [5] NCCL INFO Channel 01/0 : 4[3] -> 5[5] [receive] via NET/IB/3
g28n02:792076:792423 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[1] [send] via NET/IB/3
g28n02:792076:792423 [5] NCCL INFO Channel 01/0 : 5[5] -> 6[1] [send] via NET/IB/3
g28n02:792076:792423 [5] NCCL INFO Connected all rings
g28n02:792076:792423 [5] NCCL INFO Channel 01/0 : 5[5] -> 7[3] [send] via NET/IB/3
g28n02:792076:792423 [5] NCCL INFO Channel 01/0 : 7[3] -> 5[5] [receive] via NET/IB/3
g28n02:792076:792423 [5] NCCL INFO Channel 00/0 : 6[1] -> 5[5] [receive] via NET/IB/3
g28n02:792076:792423 [5] NCCL INFO Channel 01/0 : 6[1] -> 5[5] [receive] via NET/IB/3
g28n02:792076:792423 [5] NCCL INFO Channel 01/0 : 5[5] -> 4[3] [send] via NET/IB/3
g28n02:792076:792423 [5] NCCL INFO Connected all trees
g28n02:792076:792423 [5] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g28n02:792076:792423 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n02:792076:792423 [5] NCCL INFO comm 0x16f6be2b0 rank 5 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x17fc85f125de53ec - Init COMPLETE
g28n02:792068:792068 [0] NCCL INFO cudaDriverVersion 12020
g28n02:792068:792068 [0] NCCL INFO Bootstrap : Using ib0:10.41.17.175<0>
g28n02:792068:792068 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g28n02:792068:792068 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g28n02:792068:792304 [0] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.175<0>
g28n02:792068:792304 [0] NCCL INFO Using network IB
g28n02:792068:792304 [0] NCCL INFO comm 0x1413d4330 rank 36 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x28395441218594c5 - Init START
g28n02:792068:792304 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g28n02:792068:792304 [0] NCCL INFO Trees [0] 37/42/-1->36->24 [1] 38/-1/-1->36->37 [2] 37/-1/-1->36->31 [3] 38/-1/-1->36->37
g28n02:792068:792304 [0] NCCL INFO P2P Chunksize set to 131072
g28n02:792068:792304 [0] NCCL INFO Channel 00/0 : 35[5] -> 36[0] [receive] via NET/IB/0
g28n02:792068:792304 [0] NCCL INFO Channel 02/0 : 35[5] -> 36[0] [receive] via NET/IB/0
g28n02:792068:792304 [0] NCCL INFO Channel 00/0 : 36[0] -> 37[1] via P2P/IPC
g28n02:792068:792304 [0] NCCL INFO Channel 02/0 : 36[0] -> 37[1] via P2P/IPC
g28n02:792068:792304 [0] NCCL INFO Channel 01/0 : 36[0] -> 45[3] [send] via NET/IB/2
g28n02:792068:792304 [0] NCCL INFO Channel 03/0 : 36[0] -> 45[3] [send] via NET/IB/2
g28n02:792068:792304 [0] NCCL INFO Connected all rings
g28n02:792068:792304 [0] NCCL INFO Channel 01/0 : 36[0] -> 37[1] via P2P/IPC
g28n02:792068:792304 [0] NCCL INFO Channel 03/0 : 36[0] -> 37[1] via P2P/IPC
g28n02:792068:792304 [0] NCCL INFO Channel 01/0 : 36[0] -> 38[2] via P2P/IPC
g28n02:792068:792304 [0] NCCL INFO Channel 03/0 : 36[0] -> 38[2] via P2P/IPC
g28n02:792068:792304 [0] NCCL INFO Channel 02/0 : 31[1] -> 36[0] [receive] via NET/IB/0
g28n02:792068:792304 [0] NCCL INFO Channel 00/0 : 36[0] -> 42[0] [send] via NET/IB/0
g28n02:792068:792304 [0] NCCL INFO Channel 00/0 : 24[0] -> 36[0] [receive] via NET/IB/0
g28n02:792068:792304 [0] NCCL INFO Channel 00/0 : 36[0] -> 24[0] [send] via NET/IB/0
g28n02:792068:792304 [0] NCCL INFO Channel 00/0 : 42[0] -> 36[0] [receive] via NET/IB/0
g28n02:792068:792304 [0] NCCL INFO Channel 02/0 : 36[0] -> 31[1] [send] via NET/IB/0
g28n02:792068:792304 [0] NCCL INFO Connected all trees
g28n02:792068:792304 [0] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g28n02:792068:792304 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g28n02:792068:792304 [0] NCCL INFO comm 0x1413d4330 rank 36 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x28395441218594c5 - Init COMPLETE
g28n02:792068:792405 [0] NCCL INFO Using network IB
g28n02:792068:792405 [0] NCCL INFO comm 0x143d203e0 rank 9 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0x864be1a757d9f276 - Init START
g28n02:792068:792405 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g28n02:792068:792405 [0] NCCL INFO Trees [0] 10/11/-1->9->6 [1] 10/-1/-1->9->8
g28n02:792068:792405 [0] NCCL INFO P2P Chunksize set to 131072
g28n02:792068:792405 [0] NCCL INFO Channel 00/0 : 8[2] -> 9[0] [receive] via NET/IB/0
g28n02:792068:792405 [0] NCCL INFO Channel 01/0 : 8[2] -> 9[0] [receive] via NET/IB/0
g28n02:792068:792405 [0] NCCL INFO Channel 00/0 : 9[0] -> 10[4] via P2P/IPC
g28n02:792068:792405 [0] NCCL INFO Channel 01/0 : 9[0] -> 10[4] via P2P/IPC
g28n02:792068:792405 [0] NCCL INFO Connected all rings
g28n02:792068:792405 [0] NCCL INFO Channel 00/0 : 9[0] -> 11[2] [send] via NET/IB/0
g28n02:792068:792405 [0] NCCL INFO Channel 00/0 : 6[0] -> 9[0] [receive] via NET/IB/0
g28n02:792068:792405 [0] NCCL INFO Channel 00/0 : 9[0] -> 6[0] [send] via NET/IB/0
g28n02:792068:792405 [0] NCCL INFO Channel 00/0 : 11[2] -> 9[0] [receive] via NET/IB/0
g28n02:792068:792405 [0] NCCL INFO Channel 01/0 : 9[0] -> 8[2] [send] via NET/IB/0
g28n02:792068:792405 [0] NCCL INFO Connected all trees
g28n02:792068:792405 [0] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g28n02:792068:792405 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n02:792068:792405 [0] NCCL INFO comm 0x143d203e0 rank 9 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0x864be1a757d9f276 - Init COMPLETE
g28n02:792068:792418 [0] NCCL INFO Using network IB
g28n02:792068:792418 [0] NCCL INFO comm 0x14404aa70 rank 4 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x51bdf7b6b8c3aecb - Init START
g28n02:792068:792418 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g28n02:792068:792418 [0] NCCL INFO Trees [0] 2/6/-1->4->8 [1] -1/-1/-1->4->5
g28n02:792068:792418 [0] NCCL INFO P2P Chunksize set to 131072
g28n02:792068:792418 [0] NCCL INFO Channel 00/0 : 3[4] -> 4[0] [receive] via NET/IB/0
g28n02:792068:792418 [0] NCCL INFO Channel 01/0 : 3[4] -> 4[0] [receive] via NET/IB/0
g28n02:792068:792418 [0] NCCL INFO Channel 00/0 : 4[0] -> 5[2] [send] via NET/IB/0
g28n02:792068:792418 [0] NCCL INFO Channel 01/0 : 4[0] -> 5[2] [send] via NET/IB/0
g28n02:792068:792418 [0] NCCL INFO Connected all rings
g28n02:792068:792418 [0] NCCL INFO Channel 00/0 : 2[2] -> 4[0] [receive] via NET/IB/0
g28n02:792068:792418 [0] NCCL INFO Channel 00/0 : 4[0] -> 6[4] [send] via NET/IB/0
g28n02:792068:792418 [0] NCCL INFO Channel 00/0 : 4[0] -> 8[2] [send] via NET/IB/0
g28n02:792068:792418 [0] NCCL INFO Channel 00/0 : 8[2] -> 4[0] [receive] via NET/IB/0
g28n02:792068:792418 [0] NCCL INFO Channel 00/0 : 6[4] -> 4[0] [receive] via NET/IB/0
g28n02:792068:792418 [0] NCCL INFO Channel 00/0 : 4[0] -> 2[2] [send] via NET/IB/0
g28n02:792068:792418 [0] NCCL INFO Channel 01/0 : 5[2] -> 4[0] [receive] via NET/IB/0
g28n02:792068:792418 [0] NCCL INFO Connected all trees
g28n02:792068:792418 [0] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g28n02:792068:792418 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n02:792068:792418 [0] NCCL INFO comm 0x14404aa70 rank 4 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x51bdf7b6b8c3aecb - Init COMPLETE
8 [1] -1/-1/-1->4->5
g28n02:792071:792420 [2] NCCL INFO P2P Chunksize set to 131072
g28n02:792071:792420 [2] NCCL INFO Channel 00/0 : 3[0] -> 4[2] [receive] via NET/IB/0
g28n02:792071:792420 [2] NCCL INFO Channel 01/0 : 3[0] -> 4[2] [receive] via NET/IB/0
g28n02:792071:792420 [2] NCCL INFO Channel 00/0 : 4[2] -> 5[4] [send] via NET/IB/0
g28n02:792071:792420 [2] NCCL INFO Channel 01/0 : 4[2] -> 5[4] [send] via NET/IB/0
g28n02:792071:792420 [2] NCCL INFO Connected all rings
g28n02:792071:792420 [2] NCCL INFO Channel 00/0 : 2[4] -> 4[2] [receive] via NET/IB/0
g28n02:792071:792420 [2] NCCL INFO Channel 00/0 : 4[2] -> 6[0] [send] via NET/IB/0
g28n02:792071:792420 [2] NCCL INFO Channel 00/0 : 4[2] -> 8[4] [send] via NET/IB/0
g28n02:792071:792420 [2] NCCL INFO Channel 00/0 : 8[4] -> 4[2] [receive] via NET/IB/0
g28n02:792071:792420 [2] NCCL INFO Channel 00/0 : 6[0] -> 4[2] [receive] via NET/IB/0
g28n02:792071:792420 [2] NCCL INFO Channel 00/0 : 4[2] -> 2[4] [send] via NET/IB/0
g28n02:792071:792420 [2] NCCL INFO Channel 01/0 : 5[4] -> 4[2] [receive] via NET/IB/0
g28n02:792071:792420 [2] NCCL INFO Connected all trees
g28n02:792071:792420 [2] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g28n02:792071:792420 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n02:792071:792420 [2] NCCL INFO comm 0x15a1981a0 rank 4 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0x8442a1c07b0a8edb - Init COMPLETE
g28n02:792069:792069 [1] NCCL INFO cudaDriverVersion 12020
g28n02:792069:792069 [1] NCCL INFO Bootstrap : Using ib0:10.41.17.175<0>
g28n02:792069:792069 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g28n02:792069:792069 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g28n02:792069:792305 [1] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.17.175<0>
g28n02:792069:792305 [1] NCCL INFO Using network IB
g28n02:792069:792305 [1] NCCL INFO comm 0x16ece4690 rank 37 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x28395441218594c5 - Init START
g28n02:792069:792305 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g28n02:792069:792305 [1] NCCL INFO Trees [0] 38/30/-1->37->36 [1] 36/-1/-1->37->41 [2] 38/-1/-1->37->36 [3] 36/-1/-1->37->41
g28n02:792069:792305 [1] NCCL INFO P2P Chunksize set to 131072
g28n02:792069:792305 [1] NCCL INFO Channel 00/0 : 37[1] -> 38[2] via P2P/IPC
g28n02:792069:792305 [1] NCCL INFO Channel 02/0 : 37[1] -> 38[2] via P2P/IPC
g28n02:792069:792305 [1] NCCL INFO Channel 01/0 : 37[1] -> 36[0] via P2P/IPC
g28n02:792069:792305 [1] NCCL INFO Channel 03/0 : 37[1] -> 36[0] via P2P/IPC
g28n02:792069:792305 [1] NCCL INFO Connected all rings
g28n02:792069:792305 [1] NCCL INFO Channel 01/0 : 37[1] -> 41[5] via P2P/IPC
g28n02:792069:792305 [1] NCCL INFO Channel 03/0 : 37[1] -> 41[5] via P2P/IPC
g28n02:792069:792305 [1] NCCL INFO Channel 00/0 : 30[0] -> 37[1] [receive] via NET/IB/0
g28n02:792069:792305 [1] NCCL INFO Channel 00/0 : 37[1] -> 30[0] [send] via NET/IB/0
g28n02:792069:792305 [1] NCCL INFO Channel 00/0 : 37[1] -> 36[0] via P2P/IPC
g28n02:792069:792305 [1] NCCL INFO Channel 02/0 : 37[1] -> 36[0] via P2P/IPC
g28n02:792069:792305 [1] NCCL INFO Connected all trees
g28n02:792069:792305 [1] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g28n02:792069:792305 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g28n02:792069:792305 [1] NCCL INFO comm 0x16ece4690 rank 37 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x28395441218594c5 - Init COMPLETE
g28n02:792069:792400 [1] NCCL INFO Using network IB
g28n02:792069:792400 [1] NCCL INFO comm 0x172a06990 rank 9 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0xeee4c143812ce6e4 - Init START
g28n02:792069:792400 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g28n02:792069:792400 [1] NCCL INFO Trees [0] 10/11/-1->9->6 [1] 10/-1/-1->9->8
g28n02:792069:792400 [1] NCCL INFO P2P Chunksize set to 131072
g28n02:792069:792400 [1] NCCL INFO Channel 00/0 : 8[3] -> 9[1] [receive] via NET/IB/2
g28n02:792069:792400 [1] NCCL INFO Channel 01/0 : 8[3] -> 9[1] [receive] via NET/IB/2
g28n02:792069:792400 [1] NCCL INFO Channel 00/0 : 9[1] -> 10[5] via P2P/IPC
g28n02:792069:792400 [1] NCCL INFO Channel 01/0 : 9[1] -> 10[5] via P2P/IPC
g28n02:792069:792400 [1] NCCL INFO Connected all rings
g28n02:792069:792400 [1] NCCL INFO Channel 00/0 : 9[1] -> 11[3] [send] via NET/IB/2
g28n02:792069:792400 [1] NCCL INFO Channel 00/0 : 6[1] -> 9[1] [receive] via NET/IB/2
g28n02:792069:792400 [1] NCCL INFO Channel 00/0 : 9[1] -> 6[1] [send] via NET/IB/2
g28n02:792069:792400 [1] NCCL INFO Channel 00/0 : 11[3] -> 9[1] [receive] via NET/IB/2
g28n02:792069:792400 [1] NCCL INFO Channel 01/0 : 9[1] -> 8[3] [send] via NET/IB/2
g28n02:792069:792400 [1] NCCL INFO Connected all trees
g28n02:792069:792400 [1] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g28n02:792069:792400 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n02:792069:792400 [1] NCCL INFO comm 0x172a06990 rank 9 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0xeee4c143812ce6e4 - Init COMPLETE
g28n02:792069:792422 [1] NCCL INFO Using network IB
g28n02:792069:792422 [1] NCCL INFO comm 0x1719db680 rank 4 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0xad5995bf820e7340 - Init START
g28n02:792069:792422 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g28n02:792069:792422 [1] NCCL INFO Trees [0] 2/6/-1->4->8 [1] -1/-1/-1->4->5
g28n02:792069:792422 [1] NCCL INFO P2P Chunksize set to 131072
g28n02:792069:792422 [1] NCCL INFO Channel 00/0 : 3[5] -> 4[1] [receive] via NET/IB/2
g28n02:792069:792422 [1] NCCL INFO Channel 01/0 : 3[5] -> 4[1] [receive] via NET/IB/2
g28n02:792069:792422 [1] NCCL INFO Channel 00/0 : 4[1] -> 5[3] [send] via NET/IB/2
g28n02:792069:792422 [1] NCCL INFO Channel 01/0 : 4[1] -> 5[3] [send] via NET/IB/2
g28n02:792069:792422 [1] NCCL INFO Connected all rings
g28n02:792069:792422 [1] NCCL INFO Channel 00/0 : 2[3] -> 4[1] [receive] via NET/IB/2
g28n02:792069:792422 [1] NCCL INFO Channel 00/0 : 4[1] -> 6[5] [send] via NET/IB/2
g28n02:792069:792422 [1] NCCL INFO Channel 00/0 : 4[1] -> 8[3] [send] via NET/IB/2
g28n02:792069:792422 [1] NCCL INFO Channel 00/0 : 8[3] -> 4[1] [receive] via NET/IB/2
g28n02:792069:792422 [1] NCCL INFO Channel 00/0 : 6[5] -> 4[1] [receive] via NET/IB/2
g28n02:792069:792422 [1] NCCL INFO Channel 00/0 : 4[1] -> 2[3] [send] via NET/IB/2
g28n02:792069:792422 [1] NCCL INFO Channel 01/0 : 5[3] -> 4[1] [receive] via NET/IB/2
g28n02:792069:792422 [1] NCCL INFO Connected all trees
g28n02:792069:792422 [1] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g28n02:792069:792422 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n02:792069:792422 [1] NCCL INFO comm 0x1719db680 rank 4 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0xad5995bf820e7340 - Init COMPLETE
Rank: 0 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:11,608] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2024-03-04 12:19:11,609] [INFO] [utils.py:786:see_memory_usage] MA 6.91 GB         Max_MA 6.91 GB         CA 6.98 GB         Max_CA 7 GB 
[2024-03-04 12:19:11,609] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 102.49 GB, percent = 17.2%
[2024-03-04 12:19:11,934] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2024-03-04 12:19:11,935] [INFO] [utils.py:786:see_memory_usage] MA 8.58 GB         Max_MA 9.41 GB         CA 9.57 GB         Max_CA 10 GB 
[2024-03-04 12:19:11,935] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 114.08 GB, percent = 19.1%
[2024-03-04 12:19:11,935] [INFO] [stage_1_and_2.py:493:__init__] optimizer state initialized
[2024-03-04 12:19:12,012] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2024-03-04 12:19:12,013] [INFO] [utils.py:786:see_memory_usage] MA 8.58 GB         Max_MA 8.58 GB         CA 9.57 GB         Max_CA 10 GB 
[2024-03-04 12:19:12,013] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 114.1 GB, percent = 19.1%
[2024-03-04 12:19:12,016] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
[2024-03-04 12:19:12,016] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-03-04 12:19:12,016] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.optimizer_param_scheduler.OptimizerParamScheduler object at 0x200127164290>
[2024-03-04 12:19:12,017] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[2024-03-04 12:19:12,018] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2024-03-04 12:19:12,018] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-03-04 12:19:12,018] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-03-04 12:19:12,018] [INFO] [config.py:964:print]   amp_enabled .................. False
[2024-03-04 12:19:12,019] [INFO] [config.py:964:print]   amp_params ................... False
[2024-03-04 12:19:12,019] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-03-04 12:19:12,019] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[2024-03-04 12:19:12,019] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2024-03-04 12:19:12,019] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2024-03-04 12:19:12,019] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2024-03-04 12:19:12,019] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x200127165710>
[2024-03-04 12:19:12,019] [INFO] [config.py:964:print]   communication_data_type ...... None
[2024-03-04 12:19:12,019] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-03-04 12:19:12,019] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2024-03-04 12:19:12,019] [INFO] [config.py:964:print]   curriculum_params_legacy ..... {'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}
[2024-03-04 12:19:12,019] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-03-04 12:19:12,019] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2024-03-04 12:19:12,019] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2024-03-04 12:19:12,019] [INFO] [config.py:964:print]   disable_allgather ............ False
[2024-03-04 12:19:12,019] [INFO] [config.py:964:print]   dump_state ................... False
[2024-03-04 12:19:12,020] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 500, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2024-03-04 12:19:12,020] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2024-03-04 12:19:12,020] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2024-03-04 12:19:12,020] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-03-04 12:19:12,020] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2024-03-04 12:19:12,020] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2024-03-04 12:19:12,020] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2024-03-04 12:19:12,020] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2024-03-04 12:19:12,020] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2024-03-04 12:19:12,020] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2024-03-04 12:19:12,020] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-03-04 12:19:12,020] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[2024-03-04 12:19:12,020] [INFO] [config.py:964:print]   fp16_enabled ................. True
[2024-03-04 12:19:12,020] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2024-03-04 12:19:12,020] [INFO] [config.py:964:print]   global_rank .................. 0
[2024-03-04 12:19:12,020] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2024-03-04 12:19:12,020] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2024-03-04 12:19:12,020] [INFO] [config.py:964:print]   gradient_clipping ............ 1.0
[2024-03-04 12:19:12,020] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2024-03-04 12:19:12,020] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-03-04 12:19:12,020] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[2024-03-04 12:19:12,020] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2024-03-04 12:19:12,020] [INFO] [config.py:964:print]   loss_scale ................... 0
[2024-03-04 12:19:12,020] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2024-03-04 12:19:12,020] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2024-03-04 12:19:12,021] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2024-03-04 12:19:12,021] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-03-04 12:19:12,021] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-03-04 12:19:12,021] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2024-03-04 12:19:12,021] [INFO] [config.py:964:print]   optimizer_name ............... None
[2024-03-04 12:19:12,021] [INFO] [config.py:964:print]   optimizer_params ............. None
[2024-03-04 12:19:12,021] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2024-03-04 12:19:12,021] [INFO] [config.py:964:print]   pld_enabled .................. False
[2024-03-04 12:19:12,021] [INFO] [config.py:964:print]   pld_params ................... False
[2024-03-04 12:19:12,021] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2024-03-04 12:19:12,021] [INFO] [config.py:964:print]   scheduler_name ............... None
[2024-03-04 12:19:12,021] [INFO] [config.py:964:print]   scheduler_params ............. None
[2024-03-04 12:19:12,021] [INFO] [config.py:964:print]   sparse_attention ............. None
[2024-03-04 12:19:12,021] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2024-03-04 12:19:12,021] [INFO] [config.py:964:print]   steps_per_print .............. 10
[2024-03-04 12:19:12,021] [INFO] [config.py:964:print]   train_batch_size ............. 24
[2024-03-04 12:19:12,021] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2024-03-04 12:19:12,021] [INFO] [config.py:964:print]   use_node_local_storage ....... False
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in[2024-03-04 12:19:12,021] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_inArgs= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in[2024-03-04 12:19:12,021] [INFO] [config.py:964:print]   world_size ................... 24
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_inArgs= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_inArgs= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in[2024-03-04 12:19:12,021] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  False
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_inArgs= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in[2024-03-04 12:19:12,022] [INFO] [config.py:964:print]   zero_config .................. stage=1 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2024-03-04 12:19:12,022] [INFO] [config.py:964:print]   zero_enabled ................. True
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_inArgs= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_inArgs= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_inArgs= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in[2024-03-04 12:19:12,022] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2024-03-04 12:19:12,022] [INFO] [config.py:964:print]   zero_optimization_stage ...... 1
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_inArgs= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_inArgs= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in[2024-03-04 12:19:12,022] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 24, 
    "train_micro_batch_size_per_gpu": 1, 
    "steps_per_print": 10, 
    "zero_optimization": {
        "stage": 1
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 500, 
        "hysteresis": 2, 
        "min_loss_scale": 1, 
        "initial_scale_power": 11
    }, 
    "bf16": {
        "enabled": false
    }, 
    "curriculum_learning": {
        "enabled": false, 
        "curriculum_type": "seqlen", 
        "min_difficulty": 80, 
        "max_difficulty": 2.048000e+03, 
        "schedule_type": "fixed_linear", 
        "schedule_config": {
            "total_curriculum_step": 2.349624e+06, 
            "difficulty_step": 8
        }
    }, 
    "wall_clock_breakdown": false
}
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_inArgs= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=42, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=18, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=47, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_inArgs= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=19, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_inArgs= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=45, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=21, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=30, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel__pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=43, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=20, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=32, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel__pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=46, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=22, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=31, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel__pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=44, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel__pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=90, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_insize=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 18 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,022] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 42 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,022] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
_pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=93, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_insize=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 19 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,022] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
_pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=34, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 44 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,022] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
_pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=92, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 21 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,022] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_inArgs= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_insize=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 47 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,022] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
_pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=91, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel__pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=23, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_inArgs= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_insize=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 45 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,022] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
_pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=94, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 20 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,022] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
_pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=35, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_inArgs= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_insize=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 46 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,022] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
_pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=95, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 22 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,022] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_insize=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 43 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,022] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 91 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,023] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_insize=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 23 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,022] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
_pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=33, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_insize=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 93 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,023] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_insize=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 95 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,023] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_inArgs= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_insize=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 30 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,022] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_insize=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 90 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,023] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[2024-03-04 12:19:12,023] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
WARNING: could not find the metadata file /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true 
    will not load any checkpoints and will start from random
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_inArgs= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_insize=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 94 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,023] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_inArgs= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_insize=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 92 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,023] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_insize=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 32 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,022] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=48, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_inArgs= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=24, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_No existing process group found, creating a new group named: ep_size_8
Rank: 1 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,023] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
_pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=68, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=71, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 35 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,022] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=66, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_inArgs= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=49, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 66 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,024] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=25, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=67, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 34 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,022] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 71 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,024] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=6, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_s_pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=69, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_inArgs= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=50, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 67 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,024] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 33 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,022] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
_pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=29, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_No existing process group found, creating a new group named: ep_size_8
Rank: 3 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,023] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 68 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,024] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
_pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=9, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_s_pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=70, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_insize=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 69 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,024] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 31 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,022] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=51, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 70 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,024] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
_pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=8, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_s_pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=27, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=56, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=11, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel__pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=36, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel__pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=28, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel__pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=55, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_No existing process group found, creating a new group named: ep_size_8
Rank: 2 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,023] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=52, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel__pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=10, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 36 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,022] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=54, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel__pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=26, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=5, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 6 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,023] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 49 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,022] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 54 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,023] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
_pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=37, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_insize=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 24 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,022] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
_pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=7, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_s_pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=4, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_ssize=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 55 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,023] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 48 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,022] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_insize=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 11 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,023] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 25 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,022] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 56 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,023] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
ize=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 4 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,023] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_inArgs= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_insize=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 50 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,022] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
ize=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 8 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,023] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
_pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=41, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel__pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=57, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 29 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,022] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
_pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=79, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_ize=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 5 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,023] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
ize=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 9 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,023] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 52 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,022] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
_pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=59, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel__pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=82, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 27 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,022] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_insize=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 10 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,023] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 51 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,022] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
_pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=58, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel__pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=39, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel__pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=80, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 26 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,022] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
ize=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 7 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,023] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
_pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=53, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 57 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,023] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
_pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=83, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 28 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,022] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_insize=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 58 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,023] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 53 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,022] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=78, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel__pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=40, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_insize=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 59 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,023] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
_pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=81, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 79 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,024] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_inArgs= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_insize=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 80 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,024] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
_pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=38, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 82 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,024] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_insize=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 78 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,024] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 81 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,024] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_insize=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 83 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,024] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_inArgs= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_insize=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 37 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,022] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_inArgs= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=72, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_insize=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 38 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,022] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
_pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=74, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel__pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=73, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel__pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=12, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel__pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=76, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 39 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,022] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=75, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel__pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=77, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel__pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=17, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 73 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,024] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 40 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,022] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_insize=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 74 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,024] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 72 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,024] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
_pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=15, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 76 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,024] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 41 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,022] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_insize=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 75 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,024] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 77 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,024] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 12 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,023] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_inArgs= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=13, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=60, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel__pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=14, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=65, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 15 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,023] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=62, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel__pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=16, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=63, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 17 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,023] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_inArgs= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[8], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4080, ffn_hidden_size=16320, num_attention_heads=24, num_key_value_heads=24, kv_channels=170, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true_batch1_2024.03.04-12.18.33', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=4, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=8, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_insize=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 13 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,023] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
_pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=86, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 60 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,024] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 16 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,023] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=84, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 62 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,024] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 14 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,023] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
_pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=85, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 63 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,024] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
_pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=87, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel__pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=61, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel__pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=88, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 65 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,024] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 84 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,024] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
_pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=64, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 86 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,024] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 61 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,024] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
_pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=89, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 64 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,024] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 87 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,024] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 88 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,024] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 85 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,024] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
size=24, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=50688, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 1}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_8
Rank: 89 partition count [24, 24, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12] and sizes[(46914560, False), (37060, False), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (11097600, True), (27200, True)] 
[2024-03-04 12:19:12,024] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-4-pp-1-ep-8-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
(min, max) time across ranks (ms):
    load-checkpoint ................................: (1.78, 1.83)
[after model, optimizer, and learning rate scheduler are built] datetime: 2024-03-04 12:19:12 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      439453104
    validation: 43945440
    test:       240
> building train, validation, and test datasets for GPT ...
Single data path provided for train, valid & test
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.003570 seconds
    number of documents: 27933
 > dataset split:
    train:
     document indices in [0, 27374) total of 27374 documents
    validation:
     document indices in [27374, 27933) total of 559 documents
    test:
     document indices in [27933, 27933) total of 0 documents
NCCL version 2.18.3+cuda11.8
NCCL version 2.18.3+cuda11.8
NCCL version 2.18.3+cuda11.8
NCCL version 2.18.3+cuda11.8
NCCL version 2.18.3+cuda11.8
NCCL version 2.18.3+cuda11.8
NCCL version 2.18.3+cuda11.8
NCCL version 2.18.3+cuda11.8
NCCL version 2.18.3+cuda11.8
NCCL version 2.18.3+cuda11.8
NCCL version 2.18.3+cuda11.8
NCCL version 2.18.3+cuda11.8
NCCL version 2.18.3+cuda11.8
NCCL version 2.18.3+cuda11.8
NCCL version 2.18.3+cuda11.8
NCCL version 2.18.3+cuda11.8
NCCL version 2.18.3+cuda11.8
NCCL version 2.18.3+cuda11.8
NCCL version 2.18.3+cuda11.8
NCCL version 2.18.3+cuda11.8
NCCL version 2.18.3+cuda11.8
g28n04:841287:841638 [4] NCCL INFO Using network IB
g28n04:841287:841638 [4] NCCL INFO comm 0x1c1ba8f90 rank 0 nranks 1 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xe3955d9500faea7e - Init START
g28n04:841280:841637 [0] NCCL INFO Using network IB
g28n04:841280:841637 [0] NCCL INFO comm 0x1b87f96e0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 404000 commId 0xe7a40c1b3eea6f67 - Init START
g32n03:753098:753448 [2] NCCL INFO Using network IB
g31n02:736467:736817 [4] NCCL INFO Using network IB
g26n02:851589:851944 [0] NCCL INFO Using network IB
g26n02:851589:851944 [0] NCCL INFO comm 0x1db2708d0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 404000 commId 0xea914bd3a0154e23 - Init START
g26n01:811333:811682 [2] NCCL INFO Using network IB
g26n01:811333:811682 [2] NCCL INFO comm 0x1a5578320 rank 0 nranks 1 cudaDev 2 nvmlDev 2 busId 406000 commId 0x6c30024398fc3b4 - Init START
g31n04:688228:688579 [4] NCCL INFO Using network IB
g31n04:688228:688579 [4] NCCL INFO comm 0x1565ab780 rank 0 nranks 1 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x89ebba3b90470397 - Init START
g31n04:688224:688580 [0] NCCL INFO Using network IB
g31n04:688224:688580 [0] NCCL INFO comm 0x1a3fa80c0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 404000 commId 0xea310ea682cda056 - Init START
g26n02:851593:851943 [4] NCCL INFO Using network IB
g26n02:851593:851943 [4] NCCL INFO comm 0x1c3373480 rank 0 nranks 1 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xcaea063a0efa5309 - Init START
g32n02:753683:754038 [0] NCCL INFO Using network IB
g32n02:753683:754038 [0] NCCL INFO comm 0x19a823d30 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 404000 commId 0x7f373c3911bea3a7 - Init START
g31n01:617758:618115 [2] NCCL INFO Using network IB
g31n01:617758:618115 [2] NCCL INFO comm 0x1d6f4eda0 rank 0 nranks 1 cudaDev 2 nvmlDev 2 busId 406000 commId 0x14caf5ccbfada61b - Init START
g28n01:770095:770447 [2] NCCL INFO Using network IB
g28n01:770095:770447 [2] NCCL INFO comm 0x1c1764cc0 rank 0 nranks 1 cudaDev 2 nvmlDev 2 busId 406000 commId 0x491b4463cc8a48aa - Init START
g28n03:859956:860309 [2] NCCL INFO Using network IB
g28n03:859956:860309 [2] NCCL INFO comm 0x1a7e1ed10 rank 0 nranks 1 cudaDev 2 nvmlDev 2 busId 406000 commId 0x4596729b83108ae7 - Init START
g27n18:839782:840137 [0] NCCL INFO Using network IB
g27n18:839782:840137 [0] NCCL INFO comm 0x1f0e97ea0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 404000 commId 0xa7ff78483da4760c - Init START
g28n02:792075:792438 [4] NCCL INFO Using network IB
g28n02:792075:792438 [4] NCCL INFO comm 0x1a15e7a80 rank 0 nranks 1 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x395029e0519dc04a - Init START
g28n02:792068:792439 [0] NCCL INFO Using network IB
g28n02:792068:792439 [0] NCCL INFO comm 0x19eca2460 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 404000 commId 0x69f353dd957778b1 - Init START
g27n18:839789:840136 [4] NCCL INFO Using network IB
g27n18:839789:840136 [4] NCCL INFO comm 0x1ab86d940 rank 0 nranks 1 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x2aa8f33bde08819b - Init START
g28n03:859956:860309 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g28n03:859956:860309 [2] NCCL INFO Channel 00/32 :    0
g28n03:859956:860309 [2] NCCL INFO Channel 01/32 :    0
g28n03:859956:860309 [2] NCCL INFO Channel 02/32 :    0
g28n03:859956:860309 [2] NCCL INFO Channel 03/32 :    0
g28n03:859956:860309 [2] NCCL INFO Channel 04/32 :    0
g28n03:859956:860309 [2] NCCL INFO Channel 05/32 :    0
g28n03:859956:860309 [2] NCCL INFO Channel 06/32 :    0
g28n03:859956:860309 [2] NCCL INFO Channel 07/32 :    0
g28n03:859956:860309 [2] NCCL INFO Channel 08/32 :    0
g28n03:859956:860309 [2] NCCL INFO Channel 09/32 :    0
g28n03:859956:860309 [2] NCCL INFO Channel 10/32 :    0
g28n03:859956:860309 [2] NCCL INFO Channel 11/32 :    0
g28n03:859956:860309 [2] NCCL INFO Channel 12/32 :    0
g28n03:859956:860309 [2] NCCL INFO Channel 13/32 :    0
g28n03:859956:860309 [2] NCCL INFO Channel 14/32 :    0
g28n03:859956:860309 [2] NCCL INFO Channel 15/32 :    0
g28n03:859956:860309 [2] NCCL INFO Channel 16/32 :    0
g28n03:859956:860309 [2] NCCL INFO Channel 17/32 :    0
g28n03:859956:860309 [2] NCCL INFO Channel 18/32 :    0
g28n03:859956:860309 [2] NCCL INFO Channel 19/32 :    0
g28n03:859956:860309 [2] NCCL INFO Channel 20/32 :    0
g28n03:859956:860309 [2] NCCL INFO Channel 21/32 :    0
g28n03:859956:860309 [2] NCCL INFO Channel 22/32 :    0
g28n03:859956:860309 [2] NCCL INFO Channel 23/32 :    0
g28n03:859956:860309 [2] NCCL INFO Channel 24/32 :    0
g28n03:859956:860309 [2] NCCL INFO Channel 25/32 :    0
g28n03:859956:860309 [2] NCCL INFO Channel 26/32 :    0
g28n03:859956:860309 [2] NCCL INFO Channel 27/32 :    0
g28n03:859956:860309 [2] NCCL INFO Channel 28/32 :    0
g28n03:859956:860309 [2] NCCL INFO Channel 29/32 :    0
g28n03:859956:860309 [2] NCCL INFO Channel 30/32 :    0
g28n03:859956:860309 [2] NCCL INFO Channel 31/32 :    0
g28n03:859956:860309 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
g28n03:859956:860309 [2] NCCL INFO P2P Chunksize set to 131072
g28n03:859956:860309 [2] NCCL INFO Connected all rings
g28n03:859956:860309 [2] NCCL INFO Connected all trees
g28n03:859956:860309 [2] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
g27n17:842063:842419 [2] NCCL INFO Using network IB
g27n17:842063:842419 [2] NCCL INFO comm 0x19ace8b40 rank 0 nranks 1 cudaDev 2 nvmlDev 2 busId 406000 commId 0x534b2b22bc310117 - Init START
g27n17:842063:842419 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g27n17:842063:842419 [2] NCCL INFO Channel 00/32 :    0
g27n17:842063:842419 [2] NCCL INFO Channel 01/32 :    0
g27n17:842063:842419 [2] NCCL INFO Channel 02/32 :    0
g27n17:842063:842419 [2] NCCL INFO Channel 03/32 :    0
g27n17:842063:842419 [2] NCCL INFO Channel 04/32 :    0
g27n17:842063:842419 [2] NCCL INFO Channel 05/32 :    0
g27n17:842063:842419 [2] NCCL INFO Channel 06/32 :    0
g27n17:842063:842419 [2] NCCL INFO Channel 07/32 :    0
g27n17:842063:842419 [2] NCCL INFO Channel 08/32 :    0
g27n17:842063:842419 [2] NCCL INFO Channel 09/32 :    0
g27n17:842063:842419 [2] NCCL INFO Channel 10/32 :    0
g27n17:842063:842419 [2] NCCL INFO Channel 11/32 :    0
g27n17:842063:842419 [2] NCCL INFO Channel 12/32 :    0
g27n17:842063:842419 [2] NCCL INFO Channel 13/32 :    0
g27n17:842063:842419 [2] NCCL INFO Channel 14/32 :    0
g27n17:842063:842419 [2] NCCL INFO Channel 15/32 :    0
g27n17:842063:842419 [2] NCCL INFO Channel 16/32 :    0
g27n17:842063:842419 [2] NCCL INFO Channel 17/32 :    0
g27n17:842063:842419 [2] NCCL INFO Channel 18/32 :    0
g27n17:842063:842419 [2] NCCL INFO Channel 19/32 :    0
g27n17:842063:842419 [2] NCCL INFO Channel 20/32 :    0
g27n17:842063:842419 [2] NCCL INFO Channel 21/32 :    0
g27n17:842063:842419 [2] NCCL INFO Channel 22/32 :    0
g27n17:842063:842419 [2] NCCL INFO Channel 23/32 :    0
g27n17:842063:842419 [2] NCCL INFO Channel 24/32 :    0
g27n17:842063:842419 [2] NCCL INFO Channel 25/32 :    0
g27n17:842063:842419 [2] NCCL INFO Channel 26/32 :    0
g27n17:842063:842419 [2] NCCL INFO Channel 27/32 :    0
g27n17:842063:842419 [2] NCCL INFO Channel 28/32 :    0
g27n17:842063:842419 [2] NCCL INFO Channel 29/32 :    0
g27n17:842063:842419 [2] NCCL INFO Channel 30/32 :    0
g27n17:842063:842419 [2] NCCL INFO Channel 31/32 :    0
g27n17:842063:842419 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
g27n17:842063:842419 [2] NCCL INFO P2P Chunksize set to 131072
g27n17:842063:842419 [2] NCCL INFO Connected all rings
g27n17:842063:842419 [2] NCCL INFO Connected all trees
g27n17:842063:842419 [2] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
g28n01:770095:770447 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g28n01:770095:770447 [2] NCCL INFO Channel 00/32 :    0
g28n01:770095:770447 [2] NCCL INFO Channel 01/32 :    0
g28n01:770095:770447 [2] NCCL INFO Channel 02/32 :    0
g28n01:770095:770447 [2] NCCL INFO Channel 03/32 :    0
g28n01:770095:770447 [2] NCCL INFO Channel 04/32 :    0
g28n01:770095:770447 [2] NCCL INFO Channel 05/32 :    0
g28n01:770095:770447 [2] NCCL INFO Channel 06/32 :    0
g28n01:770095:770447 [2] NCCL INFO Channel 07/32 :    0
g28n01:770095:770447 [2] NCCL INFO Channel 08/32 :    0
g28n01:770095:770447 [2] NCCL INFO Channel 09/32 :    0
g28n01:770095:770447 [2] NCCL INFO Channel 10/32 :    0
g28n01:770095:770447 [2] NCCL INFO Channel 11/32 :    0
g28n01:770095:770447 [2] NCCL INFO Channel 12/32 :    0
g28n01:770095:770447 [2] NCCL INFO Channel 13/32 :    0
g28n01:770095:770447 [2] NCCL INFO Channel 14/32 :    0
g28n01:770095:770447 [2] NCCL INFO Channel 15/32 :    0
g28n01:770095:770447 [2] NCCL INFO Channel 16/32 :    0
g28n01:770095:770447 [2] NCCL INFO Channel 17/32 :    0
g28n01:770095:770447 [2] NCCL INFO Channel 18/32 :    0
g28n01:770095:770447 [2] NCCL INFO Channel 19/32 :    0
g28n01:770095:770447 [2] NCCL INFO Channel 20/32 :    0
g28n01:770095:770447 [2] NCCL INFO Channel 21/32 :    0
g28n01:770095:770447 [2] NCCL INFO Channel 22/32 :    0
g28n01:770095:770447 [2] NCCL INFO Channel 23/32 :    0
g28n01:770095:770447 [2] NCCL INFO Channel 24/32 :    0
g28n01:770095:770447 [2] NCCL INFO Channel 25/32 :    0
g28n01:770095:770447 [2] NCCL INFO Channel 26/32 :    0
g28n01:770095:770447 [2] NCCL INFO Channel 27/32 :    0
g28n01:770095:770447 [2] NCCL INFO Channel 28/32 :    0
g28n01:770095:770447 [2] NCCL INFO Channel 29/32 :    0
g28n01:770095:770447 [2] NCCL INFO Channel 30/32 :    0
g28n01:770095:770447 [2] NCCL INFO Channel 31/32 :    0
g28n01:770095:770447 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
g28n01:770095:770447 [2] NCCL INFO P2P Chunksize set to 131072
g28n01:770095:770447 [2] NCCL INFO Connected all rings
g28n01:770095:770447 [2] NCCL INFO Connected all trees
g28n01:770095:770447 [2] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
g31n03:688511:688862 [2] NCCL INFO Using network IB
g31n03:688511:688862 [2] NCCL INFO comm 0x13bead200 rank 0 nranks 1 cudaDev 2 nvmlDev 2 busId 406000 commId 0x2c26565dd289e29f - Init START
g31n03:688511:688862 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g31n03:688511:688862 [2] NCCL INFO Channel 00/32 :    0
g31n03:688511:688862 [2] NCCL INFO Channel 01/32 :    0
g31n03:688511:688862 [2] NCCL INFO Channel 02/32 :    0
g31n03:688511:688862 [2] NCCL INFO Channel 03/32 :    0
g31n03:688511:688862 [2] NCCL INFO Channel 04/32 :    0
g31n03:688511:688862 [2] NCCL INFO Channel 05/32 :    0
g31n03:688511:688862 [2] NCCL INFO Channel 06/32 :    0
g31n03:688511:688862 [2] NCCL INFO Channel 07/32 :    0
g31n03:688511:688862 [2] NCCL INFO Channel 08/32 :    0
g31n03:688511:688862 [2] NCCL INFO Channel 09/32 :    0
g31n03:688511:688862 [2] NCCL INFO Channel 10/32 :    0
g31n03:688511:688862 [2] NCCL INFO Channel 11/32 :    0
g31n03:688511:688862 [2] NCCL INFO Channel 12/32 :    0
g31n03:688511:688862 [2] NCCL INFO Channel 13/32 :    0
g31n03:688511:688862 [2] NCCL INFO Channel 14/32 :    0
g31n03:688511:688862 [2] NCCL INFO Channel 15/32 :    0
g31n03:688511:688862 [2] NCCL INFO Channel 16/32 :    0
g31n03:688511:688862 [2] NCCL INFO Channel 17/32 :    0
g31n03:688511:688862 [2] NCCL INFO Channel 18/32 :    0
g31n03:688511:688862 [2] NCCL INFO Channel 19/32 :    0
g31n03:688511:688862 [2] NCCL INFO Channel 20/32 :    0
g31n03:688511:688862 [2] NCCL INFO Channel 21/32 :    0
g31n03:688511:688862 [2] NCCL INFO Channel 22/32 :    0
g31n03:688511:688862 [2] NCCL INFO Channel 23/32 :    0
g31n03:688511:688862 [2] NCCL INFO Channel 24/32 :    0
g31n03:688511:688862 [2] NCCL INFO Channel 25/32 :    0
g31n03:688511:688862 [2] NCCL INFO Channel 26/32 :    0
g31n03:688511:688862 [2] NCCL INFO Channel 27/32 :    0
g31n03:688511:688862 [2] NCCL INFO Channel 28/32 :    0
g31n03:688511:688862 [2] NCCL INFO Channel 29/32 :    0
g31n03:688511:688862 [2] NCCL INFO Channel 30/32 :    0
g31n03:688511:688862 [2] NCCL INFO Channel 31/32 :    0
g31n03:688511:688862 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
g31n03:688511:688862 [2] NCCL INFO P2P Chunksize set to 131072
g31n03:688511:688862 [2] NCCL INFO Connected all rings
g31n03:688511:688862 [2] NCCL INFO Connected all trees
g31n03:688511:688862 [2] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
g31n01:617758:618115 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g31n01:617758:618115 [2] NCCL INFO Channel 00/32 :    0
g31n01:617758:618115 [2] NCCL INFO Channel 01/32 :    0
g31n01:617758:618115 [2] NCCL INFO Channel 02/32 :    0
g31n01:617758:618115 [2] NCCL INFO Channel 03/32 :    0
g31n01:617758:618115 [2] NCCL INFO Channel 04/32 :    0
g31n01:617758:618115 [2] NCCL INFO Channel 05/32 :    0
g31n01:617758:618115 [2] NCCL INFO Channel 06/32 :    0
g31n01:617758:618115 [2] NCCL INFO Channel 07/32 :    0
g31n01:617758:618115 [2] NCCL INFO Channel 08/32 :    0
g31n01:617758:618115 [2] NCCL INFO Channel 09/32 :    0
g31n01:617758:618115 [2] NCCL INFO Channel 10/32 :    0
g31n01:617758:618115 [2] NCCL INFO Channel 11/32 :    0
g31n01:617758:618115 [2] NCCL INFO Channel 12/32 :    0
g31n01:617758:618115 [2] NCCL INFO Channel 13/32 :    0
g31n01:617758:618115 [2] NCCL INFO Channel 14/32 :    0
g31n01:617758:618115 [2] NCCL INFO Channel 15/32 :    0
g31n01:617758:618115 [2] NCCL INFO Channel 16/32 :    0
g31n01:617758:618115 [2] NCCL INFO Channel 17/32 :    0
g31n01:617758:618115 [2] NCCL INFO Channel 18/32 :    0
g31n01:617758:618115 [2] NCCL INFO Channel 19/32 :    0
g31n01:617758:618115 [2] NCCL INFO Channel 20/32 :    0
g31n01:617758:618115 [2] NCCL INFO Channel 21/32 :    0
g31n01:617758:618115 [2] NCCL INFO Channel 22/32 :    0
g31n01:617758:618115 [2] NCCL INFO Channel 23/32 :    0
g31n01:617758:618115 [2] NCCL INFO Channel 24/32 :    0
g31n01:617758:618115 [2] NCCL INFO Channel 25/32 :    0
g31n01:617758:618115 [2] NCCL INFO Channel 26/32 :    0
g31n01:617758:618115 [2] NCCL INFO Channel 27/32 :    0
g31n01:617758:618115 [2] NCCL INFO Channel 28/32 :    0
g31n01:617758:618115 [2] NCCL INFO Channel 29/32 :    0
g31n01:617758:618115 [2] NCCL INFO Channel 30/32 :    0
g31n01:617758:618115 [2] NCCL INFO Channel 31/32 :    0
g31n01:617758:618115 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
g31n01:617758:618115 [2] NCCL INFO P2P Chunksize set to 131072
g31n01:617758:618115 [2] NCCL INFO Connected all rings
g31n01:617758:618115 [2] NCCL INFO Connected all trees
g31n01:617758:618115 [2] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
g26n01:811333:811682 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g26n01:811333:811682 [2] NCCL INFO Channel 00/32 :    0
g26n01:811333:811682 [2] NCCL INFO Channel 01/32 :    0
g26n01:811333:811682 [2] NCCL INFO Channel 02/32 :    0
g26n01:811333:811682 [2] NCCL INFO Channel 03/32 :    0
g26n01:811333:811682 [2] NCCL INFO Channel 04/32 :    0
g26n01:811333:811682 [2] NCCL INFO Channel 05/32 :    0
g26n01:811333:811682 [2] NCCL INFO Channel 06/32 :    0
g26n01:811333:811682 [2] NCCL INFO Channel 07/32 :    0
g26n01:811333:811682 [2] NCCL INFO Channel 08/32 :    0
g26n01:811333:811682 [2] NCCL INFO Channel 09/32 :    0
g26n01:811333:811682 [2] NCCL INFO Channel 10/32 :    0
g26n01:811333:811682 [2] NCCL INFO Channel 11/32 :    0
g26n01:811333:811682 [2] NCCL INFO Channel 12/32 :    0
g26n01:811333:811682 [2] NCCL INFO Channel 13/32 :    0
g26n01:811333:811682 [2] NCCL INFO Channel 14/32 :    0
g26n01:811333:811682 [2] NCCL INFO Channel 15/32 :    0
g26n01:811333:811682 [2] NCCL INFO Channel 16/32 :    0
g26n01:811333:811682 [2] NCCL INFO Channel 17/32 :    0
g26n01:811333:811682 [2] NCCL INFO Channel 18/32 :    0
g26n01:811333:811682 [2] NCCL INFO Channel 19/32 :    0
g26n01:811333:811682 [2] NCCL INFO Channel 20/32 :    0
g26n01:811333:811682 [2] NCCL INFO Channel 21/32 :    0
g26n01:811333:811682 [2] NCCL INFO Channel 22/32 :    0
g26n01:811333:811682 [2] NCCL INFO Channel 23/32 :    0
g26n01:811333:811682 [2] NCCL INFO Channel 24/32 :    0
g26n01:811333:811682 [2] NCCL INFO Channel 25/32 :    0
g26n01:811333:811682 [2] NCCL INFO Channel 26/32 :    0
g26n01:811333:811682 [2] NCCL INFO Channel 27/32 :    0
g26n01:811333:811682 [2] NCCL INFO Channel 28/32 :    0
g26n01:811333:811682 [2] NCCL INFO Channel 29/32 :    0
g26n01:811333:811682 [2] NCCL INFO Channel 30/32 :    0
g26n01:811333:811682 [2] NCCL INFO Channel 31/32 :    0
g26n01:811333:811682 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
g26n01:811333:811682 [2] NCCL INFO P2P Chunksize set to 131072
g26n01:811333:811682 [2] NCCL INFO Connected all rings
g26n01:811333:811682 [2] NCCL INFO Connected all trees
g26n01:811333:811682 [2] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
g32n02:753683:754038 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g32n02:753683:754038 [0] NCCL INFO Channel 00/32 :    0
g32n02:753683:754038 [0] NCCL INFO Channel 01/32 :    0
g32n02:753683:754038 [0] NCCL INFO Channel 02/32 :    0
g32n02:753683:754038 [0] NCCL INFO Channel 03/32 :    0
g32n02:753683:754038 [0] NCCL INFO Channel 04/32 :    0
g32n02:753683:754038 [0] NCCL INFO Channel 05/32 :    0
g32n02:753683:754038 [0] NCCL INFO Channel 06/32 :    0
g32n02:753683:754038 [0] NCCL INFO Channel 07/32 :    0
g32n02:753683:754038 [0] NCCL INFO Channel 08/32 :    0
g32n02:753683:754038 [0] NCCL INFO Channel 09/32 :    0
g32n02:753683:754038 [0] NCCL INFO Channel 10/32 :    0
g32n02:753683:754038 [0] NCCL INFO Channel 11/32 :    0
g32n02:753683:754038 [0] NCCL INFO Channel 12/32 :    0
g32n02:753683:754038 [0] NCCL INFO Channel 13/32 :    0
g32n02:753683:754038 [0] NCCL INFO Channel 14/32 :    0
g32n02:753683:754038 [0] NCCL INFO Channel 15/32 :    0
g32n02:753683:754038 [0] NCCL INFO Channel 16/32 :    0
g32n02:753683:754038 [0] NCCL INFO Channel 17/32 :    0
g32n02:753683:754038 [0] NCCL INFO Channel 18/32 :    0
g32n02:753683:754038 [0] NCCL INFO Channel 19/32 :    0
g32n02:753683:754038 [0] NCCL INFO Channel 20/32 :    0
g32n02:753683:754038 [0] NCCL INFO Channel 21/32 :    0
g32n02:753683:754038 [0] NCCL INFO Channel 22/32 :    0
g32n02:753683:754038 [0] NCCL INFO Channel 23/32 :    0
g32n02:753683:754038 [0] NCCL INFO Channel 24/32 :    0
g32n02:753683:754038 [0] NCCL INFO Channel 25/32 :    0
g32n02:753683:754038 [0] NCCL INFO Channel 26/32 :    0
g32n02:753683:754038 [0] NCCL INFO Channel 27/32 :    0
g32n02:753683:754038 [0] NCCL INFO Channel 28/32 :    0
g32n02:753683:754038 [0] NCCL INFO Channel 29/32 :    0
g32n02:753683:754038 [0] NCCL INFO Channel 30/32 :    0
g32n02:753683:754038 [0] NCCL INFO Channel 31/32 :    0
g32n02:753683:754038 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
g32n02:753683:754038 [0] NCCL INFO P2P Chunksize set to 131072
g32n02:753683:754038 [0] NCCL INFO Connected all rings
g32n02:753683:754038 [0] NCCL INFO Connected all trees
g32n02:753683:754038 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
g25n18:898856:899306 [4] NCCL INFO Using network IB
g25n18:898852:899308 [0] NCCL INFO Using network IB
g25n18:898852:899308 [0] NCCL INFO comm 0x13c1124c0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 404000 commId 0x869de5d9b36805c2 - Init START
g25n18:898852:899308 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g25n18:898852:899308 [0] NCCL INFO Channel 00/32 :    0
g25n18:898852:899308 [0] NCCL INFO Channel 01/32 :    0
g25n18:898852:899308 [0] NCCL INFO Channel 02/32 :    0
g25n18:898852:899308 [0] NCCL INFO Channel 03/32 :    0
g25n18:898852:899308 [0] NCCL INFO Channel 04/32 :    0
g25n18:898852:899308 [0] NCCL INFO Channel 05/32 :    0
g25n18:898852:899308 [0] NCCL INFO Channel 06/32 :    0
g25n18:898852:899308 [0] NCCL INFO Channel 07/32 :    0
g25n18:898852:899308 [0] NCCL INFO Channel 08/32 :    0
g25n18:898852:899308 [0] NCCL INFO Channel 09/32 :    0
g25n18:898852:899308 [0] NCCL INFO Channel 10/32 :    0
g25n18:898852:899308 [0] NCCL INFO Channel 11/32 :    0
g25n18:898852:899308 [0] NCCL INFO Channel 12/32 :    0
g25n18:898852:899308 [0] NCCL INFO Channel 13/32 :    0
g25n18:898852:899308 [0] NCCL INFO Channel 14/32 :    0
g25n18:898852:899308 [0] NCCL INFO Channel 15/32 :    0
g25n18:898852:899308 [0] NCCL INFO Channel 16/32 :    0
g25n18:898852:899308 [0] NCCL INFO Channel 17/32 :    0
g25n18:898852:899308 [0] NCCL INFO Channel 18/32 :    0
g25n18:898852:899308 [0] NCCL INFO Channel 19/32 :    0
g25n18:898852:899308 [0] NCCL INFO Channel 20/32 :    0
g25n18:898852:899308 [0] NCCL INFO Channel 21/32 :    0
g25n18:898852:899308 [0] NCCL INFO Channel 22/32 :    0
g25n18:898852:899308 [0] NCCL INFO Channel 23/32 :    0
g25n18:898852:899308 [0] NCCL INFO Channel 24/32 :    0
g25n18:898852:899308 [0] NCCL INFO Channel 25/32 :    0
g25n18:898852:899308 [0] NCCL INFO Channel 26/32 :    0
g25n18:898852:899308 [0] NCCL INFO Channel 27/32 :    0
g25n18:898852:899308 [0] NCCL INFO Channel 28/32 :    0
g25n18:898852:899308 [0] NCCL INFO Channel 29/32 :    0
g25n18:898852:899308 [0] NCCL INFO Channel 30/32 :    0
g25n18:898852:899308 [0] NCCL INFO Channel 31/32 :    0
g25n18:898852:899308 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
g25n18:898852:899308 [0] NCCL INFO P2P Chunksize set to 131072
g25n18:898852:899308 [0] NCCL INFO Connected all rings
g25n18:898852:899308 [0] NCCL INFO Connected all trees
g25n18:898852:899308 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
NCCL version 2.18.3+cuda11.8
g31n05:704346:704697 [2] NCCL INFO Using network IB
g31n05:704346:704697 [2] NCCL INFO comm 0x1c5c1b560 rank 0 nranks 1 cudaDev 2 nvmlDev 2 busId 406000 commId 0xfaa707b9e0ba3fba - Init START
g31n05:704346:704697 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g31n05:704346:704697 [2] NCCL INFO Channel 00/32 :    0
g31n05:704346:704697 [2] NCCL INFO Channel 01/32 :    0
g31n05:704346:704697 [2] NCCL INFO Channel 02/32 :    0
g31n05:704346:704697 [2] NCCL INFO Channel 03/32 :    0
g31n05:704346:704697 [2] NCCL INFO Channel 04/32 :    0
g31n05:704346:704697 [2] NCCL INFO Channel 05/32 :    0
g31n05:704346:704697 [2] NCCL INFO Channel 06/32 :    0
g31n05:704346:704697 [2] NCCL INFO Channel 07/32 :    0
g31n05:704346:704697 [2] NCCL INFO Channel 08/32 :    0
g31n05:704346:704697 [2] NCCL INFO Channel 09/32 :    0
g31n05:704346:704697 [2] NCCL INFO Channel 10/32 :    0
g31n05:704346:704697 [2] NCCL INFO Channel 11/32 :    0
g31n05:704346:704697 [2] NCCL INFO Channel 12/32 :    0
g31n05:704346:704697 [2] NCCL INFO Channel 13/32 :    0
g31n05:704346:704697 [2] NCCL INFO Channel 14/32 :    0
g31n05:704346:704697 [2] NCCL INFO Channel 15/32 :    0
g31n05:704346:704697 [2] NCCL INFO Channel 16/32 :    0
g31n05:704346:704697 [2] NCCL INFO Channel 17/32 :    0
g31n05:704346:704697 [2] NCCL INFO Channel 18/32 :    0
g31n05:704346:704697 [2] NCCL INFO Channel 19/32 :    0
g31n05:704346:704697 [2] NCCL INFO Channel 20/32 :    0
g31n05:704346:704697 [2] NCCL INFO Channel 21/32 :    0
g31n05:704346:704697 [2] NCCL INFO Channel 22/32 :    0
g31n05:704346:704697 [2] NCCL INFO Channel 23/32 :    0
g31n05:704346:704697 [2] NCCL INFO Channel 24/32 :    0
g31n05:704346:704697 [2] NCCL INFO Channel 25/32 :    0
g31n05:704346:704697 [2] NCCL INFO Channel 26/32 :    0
g31n05:704346:704697 [2] NCCL INFO Channel 27/32 :    0
g31n05:704346:704697 [2] NCCL INFO Channel 28/32 :    0
g31n05:704346:704697 [2] NCCL INFO Channel 29/32 :    0
g31n05:704346:704697 [2] NCCL INFO Channel 30/32 :    0
g31n05:704346:704697 [2] NCCL INFO Channel 31/32 :    0
g31n05:704346:704697 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
g31n05:704346:704697 [2] NCCL INFO P2P Chunksize set to 131072
g31n05:704346:704697 [2] NCCL INFO Connected all rings
g31n05:704346:704697 [2] NCCL INFO Connected all trees
g31n05:704346:704697 [2] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
g27n17:842063:842419 [2] NCCL INFO comm 0x19ace8b40 rank 0 nranks 1 cudaDev 2 nvmlDev 2 busId 406000 commId 0x534b2b22bc310117 - Init COMPLETE
g28n03:859956:860309 [2] NCCL INFO comm 0x1a7e1ed10 rank 0 nranks 1 cudaDev 2 nvmlDev 2 busId 406000 commId 0x4596729b83108ae7 - Init COMPLETE
g28n01:770095:770447 [2] NCCL INFO comm 0x1c1764cc0 rank 0 nranks 1 cudaDev 2 nvmlDev 2 busId 406000 commId 0x491b4463cc8a48aa - Init COMPLETE
g31n03:688511:688862 [2] NCCL INFO comm 0x13bead200 rank 0 nranks 1 cudaDev 2 nvmlDev 2 busId 406000 commId 0x2c26565dd289e29f - Init COMPLETE
g25n18:898852:899308 [0] NCCL INFO comm 0x13c1124c0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 404000 commId 0x869de5d9b36805c2 - Init COMPLETE
g31n01:617758:618115 [2] NCCL INFO comm 0x1d6f4eda0 rank 0 nranks 1 cudaDev 2 nvmlDev 2 busId 406000 commId 0x14caf5ccbfada61b - Init COMPLETE
 > loading doc-idx mapping from /gpfs/alpine2/stf218/world-shared/sajal/gptdata/index-cache/b03a1c5153aea8a63ec7eb8e74212aa9_doc_idx.npy
g26n01:811333:811682 [2] NCCL INFO comm 0x1a5578320 rank 0 nranks 1 cudaDev 2 nvmlDev 2 busId 406000 commId 0x6c30024398fc3b4 - Init COMPLETE
g27n18:839782:840137 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g27n18:839782:840137 [0] NCCL INFO Channel 00/32 :    0
g27n18:839782:840137 [0] NCCL INFO Channel 01/32 :    0
g27n18:839782:840137 [0] NCCL INFO Channel 02/32 :    0
g27n18:839782:840137 [0] NCCL INFO Channel 03/32 :    0
g27n18:839782:840137 [0] NCCL INFO Channel 04/32 :    0
g27n18:839782:840137 [0] NCCL INFO Channel 05/32 :    0
g27n18:839782:840137 [0] NCCL INFO Channel 06/32 :    0
g27n18:839782:840137 [0] NCCL INFO Channel 07/32 :    0
g27n18:839782:840137 [0] NCCL INFO Channel 08/32 :    0
g27n18:839782:840137 [0] NCCL INFO Channel 09/32 :    0
g27n18:839782:840137 [0] NCCL INFO Channel 10/32 :    0
g27n18:839782:840137 [0] NCCL INFO Channel 11/32 :    0
g27n18:839782:840137 [0] NCCL INFO Channel 12/32 :    0
g27n18:839782:840137 [0] NCCL INFO Channel 13/32 :    0
g27n18:839782:840137 [0] NCCL INFO Channel 14/32 :    0
g27n18:839782:840137 [0] NCCL INFO Channel 15/32 :    0
g27n18:839782:840137 [0] NCCL INFO Channel 16/32 :    0
g27n18:839782:840137 [0] NCCL INFO Channel 17/32 :    0
g27n18:839782:840137 [0] NCCL INFO Channel 18/32 :    0
g27n18:839782:840137 [0] NCCL INFO Channel 19/32 :    0
g27n18:839782:840137 [0] NCCL INFO Channel 20/32 :    0
g27n18:839782:840137 [0] NCCL INFO Channel 21/32 :    0
g27n18:839782:840137 [0] NCCL INFO Channel 22/32 :    0
g27n18:839782:840137 [0] NCCL INFO Channel 23/32 :    0
g27n18:839782:840137 [0] NCCL INFO Channel 24/32 :    0
g27n18:839782:840137 [0] NCCL INFO Channel 25/32 :    0
g27n18:839782:840137 [0] NCCL INFO Channel 26/32 :    0
g27n18:839782:840137 [0] NCCL INFO Channel 27/32 :    0
g27n18:839782:840137 [0] NCCL INFO Channel 28/32 :    0
g27n18:839782:840137 [0] NCCL INFO Channel 29/32 :    0
g27n18:839782:840137 [0] NCCL INFO Channel 30/32 :    0
g27n18:839782:840137 [0] NCCL INFO Channel 31/32 :    0
g27n18:839782:840137 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
g27n18:839782:840137 [0] NCCL INFO P2P Chunksize set to 131072
g27n18:839782:840137 [0] NCCL INFO Connected all rings
g27n18:839782:840137 [0] NCCL INFO Connected all trees
g27n18:839782:840137 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
g28n04:841280:841637 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g28n04:841280:841637 [0] NCCL INFO Channel 00/32 :    0
g28n04:841280:841637 [0] NCCL INFO Channel 01/32 :    0
g28n04:841280:841637 [0] NCCL INFO Channel 02/32 :    0
g28n04:841280:841637 [0] NCCL INFO Channel 03/32 :    0
g28n04:841280:841637 [0] NCCL INFO Channel 04/32 :    0
g28n04:841280:841637 [0] NCCL INFO Channel 05/32 :    0
g28n04:841280:841637 [0] NCCL INFO Channel 06/32 :    0
g28n04:841280:841637 [0] NCCL INFO Channel 07/32 :    0
g28n04:841280:841637 [0] NCCL INFO Channel 08/32 :    0
g28n04:841280:841637 [0] NCCL INFO Channel 09/32 :    0
g28n04:841280:841637 [0] NCCL INFO Channel 10/32 :    0
g28n04:841280:841637 [0] NCCL INFO Channel 11/32 :    0
g28n04:841280:841637 [0] NCCL INFO Channel 12/32 :    0
g28n04:841280:841637 [0] NCCL INFO Channel 13/32 :    0
g28n04:841280:841637 [0] NCCL INFO Channel 14/32 :    0
g28n04:841280:841637 [0] NCCL INFO Channel 15/32 :    0
g28n04:841280:841637 [0] NCCL INFO Channel 16/32 :    0
g28n04:841280:841637 [0] NCCL INFO Channel 17/32 :    0
g28n04:841280:841637 [0] NCCL INFO Channel 18/32 :    0
g28n04:841280:841637 [0] NCCL INFO Channel 19/32 :    0
g28n04:841280:841637 [0] NCCL INFO Channel 20/32 :    0
g28n04:841280:841637 [0] NCCL INFO Channel 21/32 :    0
g28n04:841280:841637 [0] NCCL INFO Channel 22/32 :    0
g28n04:841280:841637 [0] NCCL INFO Channel 23/32 :    0
g28n04:841280:841637 [0] NCCL INFO Channel 24/32 :    0
g28n04:841280:841637 [0] NCCL INFO Channel 25/32 :    0
g28n04:841280:841637 [0] NCCL INFO Channel 26/32 :    0
g28n04:841280:841637 [0] NCCL INFO Channel 27/32 :    0
g28n04:841280:841637 [0] NCCL INFO Channel 28/32 :    0
g28n04:841280:841637 [0] NCCL INFO Channel 29/32 :    0
g28n04:841280:841637 [0] NCCL INFO Channel 30/32 :    0
g28n04:841280:841637 [0] NCCL INFO Channel 31/32 :    0
g28n04:841280:841637 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
g28n04:841280:841637 [0] NCCL INFO P2P Chunksize set to 131072
g28n04:841280:841637 [0] NCCL INFO Connected all rings
g28n04:841280:841637 [0] NCCL INFO Connected all trees
g28n04:841280:841637 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
g27n18:839789:840136 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g27n18:839789:840136 [4] NCCL INFO Channel 00/32 :    0
g27n18:839789:840136 [4] NCCL INFO Channel 01/32 :    0
g27n18:839789:840136 [4] NCCL INFO Channel 02/32 :    0
g27n18:839789:840136 [4] NCCL INFO Channel 03/32 :    0
g27n18:839789:840136 [4] NCCL INFO Channel 04/32 :    0
g27n18:839789:840136 [4] NCCL INFO Channel 05/32 :    0
g27n18:839789:840136 [4] NCCL INFO Channel 06/32 :    0
g27n18:839789:840136 [4] NCCL INFO Channel 07/32 :    0
g27n18:839789:840136 [4] NCCL INFO Channel 08/32 :    0
g27n18:839789:840136 [4] NCCL INFO Channel 09/32 :    0
g27n18:839789:840136 [4] NCCL INFO Channel 10/32 :    0
g27n18:839789:840136 [4] NCCL INFO Channel 11/32 :    0
g27n18:839789:840136 [4] NCCL INFO Channel 12/32 :    0
g27n18:839789:840136 [4] NCCL INFO Channel 13/32 :    0
g27n18:839789:840136 [4] NCCL INFO Channel 14/32 :    0
g27n18:839789:840136 [4] NCCL INFO Channel 15/32 :    0
g27n18:839789:840136 [4] NCCL INFO Channel 16/32 :    0
g27n18:839789:840136 [4] NCCL INFO Channel 17/32 :    0
g27n18:839789:840136 [4] NCCL INFO Channel 18/32 :    0
g27n18:839789:840136 [4] NCCL INFO Channel 19/32 :    0
g27n18:839789:840136 [4] NCCL INFO Channel 20/32 :    0
g27n18:839789:840136 [4] NCCL INFO Channel 21/32 :    0
g27n18:839789:840136 [4] NCCL INFO Channel 22/32 :    0
g27n18:839789:840136 [4] NCCL INFO Channel 23/32 :    0
g27n18:839789:840136 [4] NCCL INFO Channel 24/32 :    0
g27n18:839789:840136 [4] NCCL INFO Channel 25/32 :    0
g27n18:839789:840136 [4] NCCL INFO Channel 26/32 :    0
g27n18:839789:840136 [4] NCCL INFO Channel 27/32 :    0
g27n18:839789:840136 [4] NCCL INFO Channel 28/32 :    0
g27n18:839789:840136 [4] NCCL INFO Channel 29/32 :    0
g27n18:839789:840136 [4] NCCL INFO Channel 30/32 :    0
g27n18:839789:840136 [4] NCCL INFO Channel 31/32 :    0
g27n18:839789:840136 [4] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
g27n18:839789:840136 [4] NCCL INFO P2P Chunksize set to 131072
g27n18:839789:840136 [4] NCCL INFO Connected all rings
g27n18:839789:840136 [4] NCCL INFO Connected all trees
g27n18:839789:840136 [4] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
g28n04:841287:841638 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g28n04:841287:841638 [4] NCCL INFO Channel 00/32 :    0
g28n04:841287:841638 [4] NCCL INFO Channel 01/32 :    0
g28n04:841287:841638 [4] NCCL INFO Channel 02/32 :    0
g28n04:841287:841638 [4] NCCL INFO Channel 03/32 :    0
g28n04:841287:841638 [4] NCCL INFO Channel 04/32 :    0
g28n04:841287:841638 [4] NCCL INFO Channel 05/32 :    0
g28n04:841287:841638 [4] NCCL INFO Channel 06/32 :    0
g28n04:841287:841638 [4] NCCL INFO Channel 07/32 :    0
g28n04:841287:841638 [4] NCCL INFO Channel 08/32 :    0
g28n04:841287:841638 [4] NCCL INFO Channel 09/32 :    0
g28n04:841287:841638 [4] NCCL INFO Channel 10/32 :    0
g28n04:841287:841638 [4] NCCL INFO Channel 11/32 :    0
g28n04:841287:841638 [4] NCCL INFO Channel 12/32 :    0
g28n04:841287:841638 [4] NCCL INFO Channel 13/32 :    0
g28n04:841287:841638 [4] NCCL INFO Channel 14/32 :    0
g28n04:841287:841638 [4] NCCL INFO Channel 15/32 :    0
g28n04:841287:841638 [4] NCCL INFO Channel 16/32 :    0
g28n04:841287:841638 [4] NCCL INFO Channel 17/32 :    0
g28n04:841287:841638 [4] NCCL INFO Channel 18/32 :    0
g28n04:841287:841638 [4] NCCL INFO Channel 19/32 :    0
g28n04:841287:841638 [4] NCCL INFO Channel 20/32 :    0
g28n04:841287:841638 [4] NCCL INFO Channel 21/32 :    0
g28n04:841287:841638 [4] NCCL INFO Channel 22/32 :    0
g28n04:841287:841638 [4] NCCL INFO Channel 23/32 :    0
g28n04:841287:841638 [4] NCCL INFO Channel 24/32 :    0
g28n04:841287:841638 [4] NCCL INFO Channel 25/32 :    0
g28n04:841287:841638 [4] NCCL INFO Channel 26/32 :    0
g28n04:841287:841638 [4] NCCL INFO Channel 27/32 :    0
g28n04:841287:841638 [4] NCCL INFO Channel 28/32 :    0
g28n04:841287:841638 [4] NCCL INFO Channel 29/32 :    0
g28n04:841287:841638 [4] NCCL INFO Channel 30/32 :    0
g28n04:841287:841638 [4] NCCL INFO Channel 31/32 :    0
g28n04:841287:841638 [4] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
g28n04:841287:841638 [4] NCCL INFO P2P Chunksize set to 131072
g28n04:841287:841638 [4] NCCL INFO Connected all rings
g28n04:841287:841638 [4] NCCL INFO Connected all trees
g28n04:841287:841638 [4] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
g32n02:753683:754038 [0] NCCL INFO comm 0x19a823d30 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 404000 commId 0x7f373c3911bea3a7 - Init COMPLETE
g28n02:792068:792439 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g28n02:792068:792439 [0] NCCL INFO Channel 00/32 :    0
g28n02:792068:792439 [0] NCCL INFO Channel 01/32 :    0
g28n02:792068:792439 [0] NCCL INFO Channel 02/32 :    0
g28n02:792068:792439 [0] NCCL INFO Channel 03/32 :    0
g28n02:792068:792439 [0] NCCL INFO Channel 04/32 :    0
g28n02:792068:792439 [0] NCCL INFO Channel 05/32 :    0
g28n02:792068:792439 [0] NCCL INFO Channel 06/32 :    0
g28n02:792068:792439 [0] NCCL INFO Channel 07/32 :    0
g28n02:792068:792439 [0] NCCL INFO Channel 08/32 :    0
g28n02:792068:792439 [0] NCCL INFO Channel 09/32 :    0
g28n02:792068:792439 [0] NCCL INFO Channel 10/32 :    0
g28n02:792068:792439 [0] NCCL INFO Channel 11/32 :    0
g28n02:792068:792439 [0] NCCL INFO Channel 12/32 :    0
g28n02:792068:792439 [0] NCCL INFO Channel 13/32 :    0
g28n02:792068:792439 [0] NCCL INFO Channel 14/32 :    0
g28n02:792068:792439 [0] NCCL INFO Channel 15/32 :    0
g28n02:792068:792439 [0] NCCL INFO Channel 16/32 :    0
g28n02:792068:792439 [0] NCCL INFO Channel 17/32 :    0
g28n02:792068:792439 [0] NCCL INFO Channel 18/32 :    0
g28n02:792068:792439 [0] NCCL INFO Channel 19/32 :    0
g28n02:792068:792439 [0] NCCL INFO Channel 20/32 :    0
g28n02:792068:792439 [0] NCCL INFO Channel 21/32 :    0
g28n02:792068:792439 [0] NCCL INFO Channel 22/32 :    0
g28n02:792068:792439 [0] NCCL INFO Channel 23/32 :    0
g28n02:792068:792439 [0] NCCL INFO Channel 24/32 :    0
g28n02:792068:792439 [0] NCCL INFO Channel 25/32 :    0
g28n02:792068:792439 [0] NCCL INFO Channel 26/32 :    0
g28n02:792068:792439 [0] NCCL INFO Channel 27/32 :    0
g28n02:792068:792439 [0] NCCL INFO Channel 28/32 :    0
g28n02:792068:792439 [0] NCCL INFO Channel 29/32 :    0
g28n02:792068:792439 [0] NCCL INFO Channel 30/32 :    0
g28n02:792068:792439 [0] NCCL INFO Channel 31/32 :    0
g28n02:792068:792439 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
g28n02:792068:792439 [0] NCCL INFO P2P Chunksize set to 131072
g28n02:792068:792439 [0] NCCL INFO Connected all rings
g28n02:792068:792439 [0] NCCL INFO Connected all trees
g28n02:792068:792439 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
g28n02:792075:792438 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g28n02:792075:792438 [4] NCCL INFO Channel 00/32 :    0
g28n02:792075:792438 [4] NCCL INFO Channel 01/32 :    0
g28n02:792075:792438 [4] NCCL INFO Channel 02/32 :    0
g28n02:792075:792438 [4] NCCL INFO Channel 03/32 :    0
g28n02:792075:792438 [4] NCCL INFO Channel 04/32 :    0
g28n02:792075:792438 [4] NCCL INFO Channel 05/32 :    0
g28n02:792075:792438 [4] NCCL INFO Channel 06/32 :    0
g28n02:792075:792438 [4] NCCL INFO Channel 07/32 :    0
g28n02:792075:792438 [4] NCCL INFO Channel 08/32 :    0
g28n02:792075:792438 [4] NCCL INFO Channel 09/32 :    0
g28n02:792075:792438 [4] NCCL INFO Channel 10/32 :    0
g28n02:792075:792438 [4] NCCL INFO Channel 11/32 :    0
g28n02:792075:792438 [4] NCCL INFO Channel 12/32 :    0
g28n02:792075:792438 [4] NCCL INFO Channel 13/32 :    0
g28n02:792075:792438 [4] NCCL INFO Channel 14/32 :    0
g28n02:792075:792438 [4] NCCL INFO Channel 15/32 :    0
g28n02:792075:792438 [4] NCCL INFO Channel 16/32 :    0
g28n02:792075:792438 [4] NCCL INFO Channel 17/32 :    0
g28n02:792075:792438 [4] NCCL INFO Channel 18/32 :    0
g28n02:792075:792438 [4] NCCL INFO Channel 19/32 :    0
g28n02:792075:792438 [4] NCCL INFO Channel 20/32 :    0
g28n02:792075:792438 [4] NCCL INFO Channel 21/32 :    0
g28n02:792075:792438 [4] NCCL INFO Channel 22/32 :    0
g28n02:792075:792438 [4] NCCL INFO Channel 23/32 :    0
g28n02:792075:792438 [4] NCCL INFO Channel 24/32 :    0
g28n02:792075:792438 [4] NCCL INFO Channel 25/32 :    0
g28n02:792075:792438 [4] NCCL INFO Channel 26/32 :    0
g28n02:792075:792438 [4] NCCL INFO Channel 27/32 :    0
g28n02:792075:792438 [4] NCCL INFO Channel 28/32 :    0
g28n02:792075:792438 [4] NCCL INFO Channel 29/32 :    0
g28n02:792075:792438 [4] NCCL INFO Channel 30/32 :    0
g28n02:792075:792438 [4] NCCL INFO Channel 31/32 :    0
g28n02:792075:792438 [4] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
g28n02:792075:792438 [4] NCCL INFO P2P Chunksize set to 131072
g28n02:792075:792438 [4] NCCL INFO Connected all rings
g28n02:792075:792438 [4] NCCL INFO Connected all trees
g28n02:792075:792438 [4] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
g31n04:688224:688580 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g31n04:688224:688580 [0] NCCL INFO Channel 00/32 :    0
g31n04:688224:688580 [0] NCCL INFO Channel 01/32 :    0
g31n04:688224:688580 [0] NCCL INFO Channel 02/32 :    0
g31n04:688224:688580 [0] NCCL INFO Channel 03/32 :    0
g31n04:688224:688580 [0] NCCL INFO Channel 04/32 :    0
g31n04:688224:688580 [0] NCCL INFO Channel 05/32 :    0
g31n04:688224:688580 [0] NCCL INFO Channel 06/32 :    0
g31n04:688224:688580 [0] NCCL INFO Channel 07/32 :    0
g31n04:688224:688580 [0] NCCL INFO Channel 08/32 :    0
g31n04:688224:688580 [0] NCCL INFO Channel 09/32 :    0
g31n04:688224:688580 [0] NCCL INFO Channel 10/32 :    0
g31n04:688224:688580 [0] NCCL INFO Channel 11/32 :    0
g31n04:688224:688580 [0] NCCL INFO Channel 12/32 :    0
g31n04:688224:688580 [0] NCCL INFO Channel 13/32 :    0
g31n04:688224:688580 [0] NCCL INFO Channel 14/32 :    0
g31n04:688224:688580 [0] NCCL INFO Channel 15/32 :    0
g31n04:688224:688580 [0] NCCL INFO Channel 16/32 :    0
g31n04:688224:688580 [0] NCCL INFO Channel 17/32 :    0
g31n04:688224:688580 [0] NCCL INFO Channel 18/32 :    0
g31n04:688224:688580 [0] NCCL INFO Channel 19/32 :    0
g31n04:688224:688580 [0] NCCL INFO Channel 20/32 :    0
g31n04:688224:688580 [0] NCCL INFO Channel 21/32 :    0
g31n04:688224:688580 [0] NCCL INFO Channel 22/32 :    0
g31n04:688224:688580 [0] NCCL INFO Channel 23/32 :    0
g31n04:688224:688580 [0] NCCL INFO Channel 24/32 :    0
g31n04:688224:688580 [0] NCCL INFO Channel 25/32 :    0
g31n04:688224:688580 [0] NCCL INFO Channel 26/32 :    0
g31n04:688224:688580 [0] NCCL INFO Channel 27/32 :    0
g31n04:688224:688580 [0] NCCL INFO Channel 28/32 :    0
g31n04:688224:688580 [0] NCCL INFO Channel 29/32 :    0
g31n04:688224:688580 [0] NCCL INFO Channel 30/32 :    0
g31n04:688224:688580 [0] NCCL INFO Channel 31/32 :    0
g31n04:688224:688580 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
g31n04:688224:688580 [0] NCCL INFO P2P Chunksize set to 131072
g31n04:688224:688580 [0] NCCL INFO Connected all rings
g31n04:688224:688580 [0] NCCL INFO Connected all trees
g31n04:688224:688580 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
g31n04:688228:688579 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g31n04:688228:688579 [4] NCCL INFO Channel 00/32 :    0
g31n04:688228:688579 [4] NCCL INFO Channel 01/32 :    0
g31n04:688228:688579 [4] NCCL INFO Channel 02/32 :    0
g31n04:688228:688579 [4] NCCL INFO Channel 03/32 :    0
g31n04:688228:688579 [4] NCCL INFO Channel 04/32 :    0
g31n04:688228:688579 [4] NCCL INFO Channel 05/32 :    0
g31n04:688228:688579 [4] NCCL INFO Channel 06/32 :    0
g31n04:688228:688579 [4] NCCL INFO Channel 07/32 :    0
g31n04:688228:688579 [4] NCCL INFO Channel 08/32 :    0
g31n04:688228:688579 [4] NCCL INFO Channel 09/32 :    0
g31n04:688228:688579 [4] NCCL INFO Channel 10/32 :    0
g31n04:688228:688579 [4] NCCL INFO Channel 11/32 :    0
g31n04:688228:688579 [4] NCCL INFO Channel 12/32 :    0
g31n04:688228:688579 [4] NCCL INFO Channel 13/32 :    0
g31n04:688228:688579 [4] NCCL INFO Channel 14/32 :    0
g31n04:688228:688579 [4] NCCL INFO Channel 15/32 :    0
g31n04:688228:688579 [4] NCCL INFO Channel 16/32 :    0
g31n04:688228:688579 [4] NCCL INFO Channel 17/32 :    0
g31n04:688228:688579 [4] NCCL INFO Channel 18/32 :    0
g31n04:688228:688579 [4] NCCL INFO Channel 19/32 :    0
g31n04:688228:688579 [4] NCCL INFO Channel 20/32 :    0
g31n04:688228:688579 [4] NCCL INFO Channel 21/32 :    0
g31n04:688228:688579 [4] NCCL INFO Channel 22/32 :    0
g31n04:688228:688579 [4] NCCL INFO Channel 23/32 :    0
g31n04:688228:688579 [4] NCCL INFO Channel 24/32 :    0
g31n04:688228:688579 [4] NCCL INFO Channel 25/32 :    0
g31n04:688228:688579 [4] NCCL INFO Channel 26/32 :    0
g31n04:688228:688579 [4] NCCL INFO Channel 27/32 :    0
g31n04:688228:688579 [4] NCCL INFO Channel 28/32 :    0
g31n04:688228:688579 [4] NCCL INFO Channel 29/32 :    0
g31n04:688228:688579 [4] NCCL INFO Channel 30/32 :    0
g31n04:688228:688579 [4] NCCL INFO Channel 31/32 :    0
g31n04:688228:688579 [4] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
g31n04:688228:688579 [4] NCCL INFO P2P Chunksize set to 131072
g31n04:688228:688579 [4] NCCL INFO Connected all rings
g31n04:688228:688579 [4] NCCL INFO Connected all trees
g31n04:688228:688579 [4] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
g31n02:736467:736817 [4] NCCL INFO comm 0x1b452b180 rank 0 nranks 1 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xd9d247396f8e0da4 - Init START
g31n02:736467:736817 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g31n02:736467:736817 [4] NCCL INFO Channel 00/32 :    0
g31n02:736467:736817 [4] NCCL INFO Channel 01/32 :    0
g31n02:736467:736817 [4] NCCL INFO Channel 02/32 :    0
g31n02:736467:736817 [4] NCCL INFO Channel 03/32 :    0
g31n02:736467:736817 [4] NCCL INFO Channel 04/32 :    0
g31n02:736467:736817 [4] NCCL INFO Channel 05/32 :    0
g31n02:736467:736817 [4] NCCL INFO Channel 06/32 :    0
g31n02:736467:736817 [4] NCCL INFO Channel 07/32 :    0
g31n02:736467:736817 [4] NCCL INFO Channel 08/32 :    0
g31n02:736467:736817 [4] NCCL INFO Channel 09/32 :    0
g31n02:736467:736817 [4] NCCL INFO Channel 10/32 :    0
g31n02:736467:736817 [4] NCCL INFO Channel 11/32 :    0
g31n02:736467:736817 [4] NCCL INFO Channel 12/32 :    0
g31n02:736467:736817 [4] NCCL INFO Channel 13/32 :    0
g31n02:736467:736817 [4] NCCL INFO Channel 14/32 :    0
g31n02:736467:736817 [4] NCCL INFO Channel 15/32 :    0
g31n02:736467:736817 [4] NCCL INFO Channel 16/32 :    0
g31n02:736467:736817 [4] NCCL INFO Channel 17/32 :    0
g31n02:736467:736817 [4] NCCL INFO Channel 18/32 :    0
g31n02:736467:736817 [4] NCCL INFO Channel 19/32 :    0
g31n02:736467:736817 [4] NCCL INFO Channel 20/32 :    0
g31n02:736467:736817 [4] NCCL INFO Channel 21/32 :    0
g31n02:736467:736817 [4] NCCL INFO Channel 22/32 :    0
g31n02:736467:736817 [4] NCCL INFO Channel 23/32 :    0
g31n02:736467:736817 [4] NCCL INFO Channel 24/32 :    0
g31n02:736467:736817 [4] NCCL INFO Channel 25/32 :    0
g31n02:736467:736817 [4] NCCL INFO Channel 26/32 :    0
g31n02:736467:736817 [4] NCCL INFO Channel 27/32 :    0
g31n02:736467:736817 [4] NCCL INFO Channel 28/32 :    0
g31n02:736467:736817 [4] NCCL INFO Channel 29/32 :    0
g31n02:736467:736817 [4] NCCL INFO Channel 30/32 :    0
g31n02:736467:736817 [4] NCCL INFO Channel 31/32 :    0
g31n02:736467:736817 [4] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
g31n02:736467:736817 [4] NCCL INFO P2P Chunksize set to 131072
g31n02:736467:736817 [4] NCCL INFO Connected all rings
g31n02:736467:736817 [4] NCCL INFO Connected all trees
g31n02:736467:736817 [4] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
g26n02:851589:851944 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g26n02:851589:851944 [0] NCCL INFO Channel 00/32 :    0
g26n02:851589:851944 [0] NCCL INFO Channel 01/32 :    0
g26n02:851589:851944 [0] NCCL INFO Channel 02/32 :    0
g26n02:851589:851944 [0] NCCL INFO Channel 03/32 :    0
g26n02:851589:851944 [0] NCCL INFO Channel 04/32 :    0
g26n02:851589:851944 [0] NCCL INFO Channel 05/32 :    0
g26n02:851589:851944 [0] NCCL INFO Channel 06/32 :    0
g26n02:851589:851944 [0] NCCL INFO Channel 07/32 :    0
g26n02:851589:851944 [0] NCCL INFO Channel 08/32 :    0
g26n02:851589:851944 [0] NCCL INFO Channel 09/32 :    0
g26n02:851589:851944 [0] NCCL INFO Channel 10/32 :    0
g26n02:851589:851944 [0] NCCL INFO Channel 11/32 :    0
g26n02:851589:851944 [0] NCCL INFO Channel 12/32 :    0
g26n02:851589:851944 [0] NCCL INFO Channel 13/32 :    0
g26n02:851589:851944 [0] NCCL INFO Channel 14/32 :    0
g26n02:851589:851944 [0] NCCL INFO Channel 15/32 :    0
g26n02:851589:851944 [0] NCCL INFO Channel 16/32 :    0
g26n02:851589:851944 [0] NCCL INFO Channel 17/32 :    0
g26n02:851589:851944 [0] NCCL INFO Channel 18/32 :    0
g26n02:851589:851944 [0] NCCL INFO Channel 19/32 :    0
g26n02:851589:851944 [0] NCCL INFO Channel 20/32 :    0
g26n02:851589:851944 [0] NCCL INFO Channel 21/32 :    0
g26n02:851589:851944 [0] NCCL INFO Channel 22/32 :    0
g26n02:851589:851944 [0] NCCL INFO Channel 23/32 :    0
g26n02:851589:851944 [0] NCCL INFO Channel 24/32 :    0
g26n02:851589:851944 [0] NCCL INFO Channel 25/32 :    0
g26n02:851589:851944 [0] NCCL INFO Channel 26/32 :    0
g26n02:851589:851944 [0] NCCL INFO Channel 27/32 :    0
g26n02:851589:851944 [0] NCCL INFO Channel 28/32 :    0
g26n02:851589:851944 [0] NCCL INFO Channel 29/32 :    0
g26n02:851589:851944 [0] NCCL INFO Channel 30/32 :    0
g26n02:851589:851944 [0] NCCL INFO Channel 31/32 :    0
g26n02:851589:851944 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
g26n02:851589:851944 [0] NCCL INFO P2P Chunksize set to 131072
g26n02:851589:851944 [0] NCCL INFO Connected all rings
g26n02:851589:851944 [0] NCCL INFO Connected all trees
g26n02:851589:851944 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
g31n02:736463:736818 [0] NCCL INFO Using network IB
g31n02:736463:736818 [0] NCCL INFO comm 0x1149e02c0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 404000 commId 0x4f732be917f56b40 - Init START
g31n02:736463:736818 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g31n02:736463:736818 [0] NCCL INFO Channel 00/32 :    0
g31n02:736463:736818 [0] NCCL INFO Channel 01/32 :    0
g31n02:736463:736818 [0] NCCL INFO Channel 02/32 :    0
g31n02:736463:736818 [0] NCCL INFO Channel 03/32 :    0
g31n02:736463:736818 [0] NCCL INFO Channel 04/32 :    0
g31n02:736463:736818 [0] NCCL INFO Channel 05/32 :    0
g31n02:736463:736818 [0] NCCL INFO Channel 06/32 :    0
g31n02:736463:736818 [0] NCCL INFO Channel 07/32 :    0
g31n02:736463:736818 [0] NCCL INFO Channel 08/32 :    0
g31n02:736463:736818 [0] NCCL INFO Channel 09/32 :    0
g31n02:736463:736818 [0] NCCL INFO Channel 10/32 :    0
g31n02:736463:736818 [0] NCCL INFO Channel 11/32 :    0
g31n02:736463:736818 [0] NCCL INFO Channel 12/32 :    0
g31n02:736463:736818 [0] NCCL INFO Channel 13/32 :    0
g31n02:736463:736818 [0] NCCL INFO Channel 14/32 :    0
g31n02:736463:736818 [0] NCCL INFO Channel 15/32 :    0
g31n02:736463:736818 [0] NCCL INFO Channel 16/32 :    0
g31n02:736463:736818 [0] NCCL INFO Channel 17/32 :    0
g31n02:736463:736818 [0] NCCL INFO Channel 18/32 :    0
g31n02:736463:736818 [0] NCCL INFO Channel 19/32 :    0
g31n02:736463:736818 [0] NCCL INFO Channel 20/32 :    0
g31n02:736463:736818 [0] NCCL INFO Channel 21/32 :    0
g31n02:736463:736818 [0] NCCL INFO Channel 22/32 :    0
g31n02:736463:736818 [0] NCCL INFO Channel 23/32 :    0
g31n02:736463:736818 [0] NCCL INFO Channel 24/32 :    0
g31n02:736463:736818 [0] NCCL INFO Channel 25/32 :    0
g31n02:736463:736818 [0] NCCL INFO Channel 26/32 :    0
g31n02:736463:736818 [0] NCCL INFO Channel 27/32 :    0
g31n02:736463:736818 [0] NCCL INFO Channel 28/32 :    0
g31n02:736463:736818 [0] NCCL INFO Channel 29/32 :    0
g31n02:736463:736818 [0] NCCL INFO Channel 30/32 :    0
g31n02:736463:736818 [0] NCCL INFO Channel 31/32 :    0
g31n02:736463:736818 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
g31n02:736463:736818 [0] NCCL INFO P2P Chunksize set to 131072
g31n02:736463:736818 [0] NCCL INFO Connected all rings
g31n02:736463:736818 [0] NCCL INFO Connected all trees
g31n02:736463:736818 [0] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
g26n02:851593:851943 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g26n02:851593:851943 [4] NCCL INFO Channel 00/32 :    0
g26n02:851593:851943 [4] NCCL INFO Channel 01/32 :    0
g26n02:851593:851943 [4] NCCL INFO Channel 02/32 :    0
g26n02:851593:851943 [4] NCCL INFO Channel 03/32 :    0
g26n02:851593:851943 [4] NCCL INFO Channel 04/32 :    0
g26n02:851593:851943 [4] NCCL INFO Channel 05/32 :    0
g26n02:851593:851943 [4] NCCL INFO Channel 06/32 :    0
g26n02:851593:851943 [4] NCCL INFO Channel 07/32 :    0
g26n02:851593:851943 [4] NCCL INFO Channel 08/32 :    0
g26n02:851593:851943 [4] NCCL INFO Channel 09/32 :    0
g26n02:851593:851943 [4] NCCL INFO Channel 10/32 :    0
g26n02:851593:851943 [4] NCCL INFO Channel 11/32 :    0
g26n02:851593:851943 [4] NCCL INFO Channel 12/32 :    0
g26n02:851593:851943 [4] NCCL INFO Channel 13/32 :    0
g26n02:851593:851943 [4] NCCL INFO Channel 14/32 :    0
g26n02:851593:851943 [4] NCCL INFO Channel 15/32 :    0
g26n02:851593:851943 [4] NCCL INFO Channel 16/32 :    0
g26n02:851593:851943 [4] NCCL INFO Channel 17/32 :    0
g26n02:851593:851943 [4] NCCL INFO Channel 18/32 :    0
g26n02:851593:851943 [4] NCCL INFO Channel 19/32 :    0
g26n02:851593:851943 [4] NCCL INFO Channel 20/32 :    0
g26n02:851593:851943 [4] NCCL INFO Channel 21/32 :    0
g26n02:851593:851943 [4] NCCL INFO Channel 22/32 :    0
g26n02:851593:851943 [4] NCCL INFO Channel 23/32 :    0
g26n02:851593:851943 [4] NCCL INFO Channel 24/32 :    0
g26n02:851593:851943 [4] NCCL INFO Channel 25/32 :    0
g26n02:851593:851943 [4] NCCL INFO Channel 26/32 :    0
g26n02:851593:851943 [4] NCCL INFO Channel 27/32 :    0
g26n02:851593:851943 [4] NCCL INFO Channel 28/32 :    0
g26n02:851593:851943 [4] NCCL INFO Channel 29/32 :    0
g26n02:851593:851943 [4] NCCL INFO Channel 30/32 :    0
g26n02:851593:851943 [4] NCCL INFO Channel 31/32 :    0
g26n02:851593:851943 [4] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
g26n02:851593:851943 [4] NCCL INFO P2P Chunksize set to 131072
g26n02:851593:851943 [4] NCCL INFO Connected all rings
g26n02:851593:851943 [4] NCCL INFO Connected all trees
g26n02:851593:851943 [4] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
g31n05:704346:704697 [2] NCCL INFO comm 0x1c5c1b560 rank 0 nranks 1 cudaDev 2 nvmlDev 2 busId 406000 commId 0xfaa707b9e0ba3fba - Init COMPLETE
g28n04:841287:841638 [4] NCCL INFO comm 0x1c1ba8f90 rank 0 nranks 1 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xe3955d9500faea7e - Init COMPLETE
g27n18:839782:840137 [0] NCCL INFO comm 0x1f0e97ea0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 404000 commId 0xa7ff78483da4760c - Init COMPLETE
g28n04:841280:841637 [0] NCCL INFO comm 0x1b87f96e0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 404000 commId 0xe7a40c1b3eea6f67 - Init COMPLETE
g28n02:792068:792439 [0] NCCL INFO comm 0x19eca2460 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 404000 commId 0x69f353dd957778b1 - Init COMPLETE
g27n18:839789:840136 [4] NCCL INFO comm 0x1ab86d940 rank 0 nranks 1 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x2aa8f33bde08819b - Init COMPLETE
g31n04:688224:688580 [0] NCCL INFO comm 0x1a3fa80c0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 404000 commId 0xea310ea682cda056 - Init COMPLETE
g28n02:792075:792438 [4] NCCL INFO comm 0x1a15e7a80 rank 0 nranks 1 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x395029e0519dc04a - Init COMPLETE
g31n04:688228:688579 [4] NCCL INFO comm 0x1565ab780 rank 0 nranks 1 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x89ebba3b90470397 - Init COMPLETE
g31n02:736467:736817 [4] NCCL INFO comm 0x1b452b180 rank 0 nranks 1 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xd9d247396f8e0da4 - Init COMPLETE
g26n02:851589:851944 [0] NCCL INFO comm 0x1db2708d0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 404000 commId 0xea914bd3a0154e23 - Init COMPLETE
g26n02:851593:851943 [4] NCCL INFO comm 0x1c3373480 rank 0 nranks 1 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xcaea063a0efa5309 - Init COMPLETE
g31n02:736463:736818 [0] NCCL INFO comm 0x1149e02c0 rank 0 nranks 1 cudaDev 0 nvmlDev 0 busId 404000 commId 0x4f732be917f56b40 - Init COMPLETE
g32n02:753687:754042 [4] NCCL INFO Using network IB
g32n02:753687:754042 [4] NCCL INFO comm 0x1d1bdeca0 rank 0 nranks 1 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xbfccb669968bff8c - Init START
g32n02:753687:754042 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g32n02:753687:754042 [4] NCCL INFO Channel 00/32 :    0
g32n02:753687:754042 [4] NCCL INFO Channel 01/32 :    0
g32n02:753687:754042 [4] NCCL INFO Channel 02/32 :    0
g32n02:753687:754042 [4] NCCL INFO Channel 03/32 :    0
g32n02:753687:754042 [4] NCCL INFO Channel 04/32 :    0
g32n02:753687:754042 [4] NCCL INFO Channel 05/32 :    0
g32n02:753687:754042 [4] NCCL INFO Channel 06/32 :    0
g32n02:753687:754042 [4] NCCL INFO Channel 07/32 :    0
g32n02:753687:754042 [4] NCCL INFO Channel 08/32 :    0
g32n02:753687:754042 [4] NCCL INFO Channel 09/32 :    0
g32n02:753687:754042 [4] NCCL INFO Channel 10/32 :    0
g32n02:753687:754042 [4] NCCL INFO Channel 11/32 :    0
g32n02:753687:754042 [4] NCCL INFO Channel 12/32 :    0
g32n02:753687:754042 [4] NCCL INFO Channel 13/32 :    0
g32n02:753687:754042 [4] NCCL INFO Channel 14/32 :    0
g32n02:753687:754042 [4] NCCL INFO Channel 15/32 :    0
g32n02:753687:754042 [4] NCCL INFO Channel 16/32 :    0
g32n02:753687:754042 [4] NCCL INFO Channel 17/32 :    0
g32n02:753687:754042 [4] NCCL INFO Channel 18/32 :    0
g32n02:753687:754042 [4] NCCL INFO Channel 19/32 :    0
g32n02:753687:754042 [4] NCCL INFO Channel 20/32 :    0
g32n02:753687:754042 [4] NCCL INFO Channel 21/32 :    0
g32n02:753687:754042 [4] NCCL INFO Channel 22/32 :    0
g32n02:753687:754042 [4] NCCL INFO Channel 23/32 :    0
g32n02:753687:754042 [4] NCCL INFO Channel 24/32 :    0
g32n02:753687:754042 [4] NCCL INFO Channel 25/32 :    0
g32n02:753687:754042 [4] NCCL INFO Channel 26/32 :    0
g32n02:753687:754042 [4] NCCL INFO Channel 27/32 :    0
g32n02:753687:754042 [4] NCCL INFO Channel 28/32 :    0
g32n02:753687:754042 [4] NCCL INFO Channel 29/32 :    0
g32n02:753687:754042 [4] NCCL INFO Channel 30/32 :    0
g32n02:753687:754042 [4] NCCL INFO Channel 31/32 :    0
g32n02:753687:754042 [4] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
g32n02:753687:754042 [4] NCCL INFO P2P Chunksize set to 131072
g32n02:753687:754042 [4] NCCL INFO Connected all rings
g32n02:753687:754042 [4] NCCL INFO Connected all trees
g32n02:753687:754042 [4] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
g32n02:753687:754042 [4] NCCL INFO comm 0x1d1bdeca0 rank 0 nranks 1 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xbfccb669968bff8c - Init COMPLETE
 > loading sample-idx mapping from /gpfs/alpine2/stf218/world-shared/sajal/gptdata/index-cache/b03a1c5153aea8a63ec7eb8e74212aa9_sample_idx.npy
g32n03:753098:753448 [2] NCCL INFO comm 0x1adb634d0 rank 0 nranks 1 cudaDev 2 nvmlDev 2 busId 406000 commId 0x260e60b1770bb3ec - Init START
g32n03:753098:753448 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g32n03:753098:753448 [2] NCCL INFO Channel 00/32 :    0
g32n03:753098:753448 [2] NCCL INFO Channel 01/32 :    0
g32n03:753098:753448 [2] NCCL INFO Channel 02/32 :    0
g32n03:753098:753448 [2] NCCL INFO Channel 03/32 :    0
g32n03:753098:753448 [2] NCCL INFO Channel 04/32 :    0
g32n03:753098:753448 [2] NCCL INFO Channel 05/32 :    0
g32n03:753098:753448 [2] NCCL INFO Channel 06/32 :    0
g32n03:753098:753448 [2] NCCL INFO Channel 07/32 :    0
g32n03:753098:753448 [2] NCCL INFO Channel 08/32 :    0
g32n03:753098:753448 [2] NCCL INFO Channel 09/32 :    0
g32n03:753098:753448 [2] NCCL INFO Channel 10/32 :    0
g32n03:753098:753448 [2] NCCL INFO Channel 11/32 :    0
g32n03:753098:753448 [2] NCCL INFO Channel 12/32 :    0
g32n03:753098:753448 [2] NCCL INFO Channel 13/32 :    0
g32n03:753098:753448 [2] NCCL INFO Channel 14/32 :    0
g32n03:753098:753448 [2] NCCL INFO Channel 15/32 :    0
g32n03:753098:753448 [2] NCCL INFO Channel 16/32 :    0
g32n03:753098:753448 [2] NCCL INFO Channel 17/32 :    0
g32n03:753098:753448 [2] NCCL INFO Channel 18/32 :    0
g32n03:753098:753448 [2] NCCL INFO Channel 19/32 :    0
g32n03:753098:753448 [2] NCCL INFO Channel 20/32 :    0
g32n03:753098:753448 [2] NCCL INFO Channel 21/32 :    0
g32n03:753098:753448 [2] NCCL INFO Channel 22/32 :    0
g32n03:753098:753448 [2] NCCL INFO Channel 23/32 :    0
g32n03:753098:753448 [2] NCCL INFO Channel 24/32 :    0
g32n03:753098:753448 [2] NCCL INFO Channel 25/32 :    0
g32n03:753098:753448 [2] NCCL INFO Channel 26/32 :    0
g32n03:753098:753448 [2] NCCL INFO Channel 27/32 :    0
g32n03:753098:753448 [2] NCCL INFO Channel 28/32 :    0
g32n03:753098:753448 [2] NCCL INFO Channel 29/32 :    0
g32n03:753098:753448 [2] NCCL INFO Channel 30/32 :    0
g32n03:753098:753448 [2] NCCL INFO Channel 31/32 :    0
g32n03:753098:753448 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
g32n03:753098:753448 [2] NCCL INFO P2P Chunksize set to 131072
g32n03:753098:753448 [2] NCCL INFO Connected all rings
g32n03:753098:753448 [2] NCCL INFO Connected all trees
g32n03:753098:753448 [2] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
 > loading shuffle-idx mapping from /gpfs/alpine2/stf218/world-shared/sajal/gptdata/index-cache/b03a1c5153aea8a63ec7eb8e74212aa9_shuffle_idx.npy
g32n03:753098:753448 [2] NCCL INFO comm 0x1adb634d0 rank 0 nranks 1 cudaDev 2 nvmlDev 2 busId 406000 commId 0x260e60b1770bb3ec - Init COMPLETE
g25n18:898856:899306 [4] NCCL INFO comm 0x14d7b6a10 rank 0 nranks 1 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x732518713f99b80b - Init START
g25n18:898856:899306 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g25n18:898856:899306 [4] NCCL INFO Channel 00/32 :    0
g25n18:898856:899306 [4] NCCL INFO Channel 01/32 :    0
g25n18:898856:899306 [4] NCCL INFO Channel 02/32 :    0
g25n18:898856:899306 [4] NCCL INFO Channel 03/32 :    0
g25n18:898856:899306 [4] NCCL INFO Channel 04/32 :    0
g25n18:898856:899306 [4] NCCL INFO Channel 05/32 :    0
g25n18:898856:899306 [4] NCCL INFO Channel 06/32 :    0
g25n18:898856:899306 [4] NCCL INFO Channel 07/32 :    0
g25n18:898856:899306 [4] NCCL INFO Channel 08/32 :    0
g25n18:898856:899306 [4] NCCL INFO Channel 09/32 :    0
g25n18:898856:899306 [4] NCCL INFO Channel 10/32 :    0
g25n18:898856:899306 [4] NCCL INFO Channel 11/32 :    0
g25n18:898856:899306 [4] NCCL INFO Channel 12/32 :    0
g25n18:898856:899306 [4] NCCL INFO Channel 13/32 :    0
g25n18:898856:899306 [4] NCCL INFO Channel 14/32 :    0
g25n18:898856:899306 [4] NCCL INFO Channel 15/32 :    0
g25n18:898856:899306 [4] NCCL INFO Channel 16/32 :    0
g25n18:898856:899306 [4] NCCL INFO Channel 17/32 :    0
g25n18:898856:899306 [4] NCCL INFO Channel 18/32 :    0
g25n18:898856:899306 [4] NCCL INFO Channel 19/32 :    0
g25n18:898856:899306 [4] NCCL INFO Channel 20/32 :    0
g25n18:898856:899306 [4] NCCL INFO Channel 21/32 :    0
g25n18:898856:899306 [4] NCCL INFO Channel 22/32 :    0
g25n18:898856:899306 [4] NCCL INFO Channel 23/32 :    0
g25n18:898856:899306 [4] NCCL INFO Channel 24/32 :    0
g25n18:898856:899306 [4] NCCL INFO Channel 25/32 :    0
g25n18:898856:899306 [4] NCCL INFO Channel 26/32 :    0
g25n18:898856:899306 [4] NCCL INFO Channel 27/32 :    0
g25n18:898856:899306 [4] NCCL INFO Channel 28/32 :    0
g25n18:898856:899306 [4] NCCL INFO Channel 29/32 :    0
g25n18:898856:899306 [4] NCCL INFO Channel 30/32 :    0
g25n18:898856:899306 [4] NCCL INFO Channel 31/32 :    0
g25n18:898856:899306 [4] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
g25n18:898856:899306 [4] NCCL INFO P2P Chunksize set to 131072
g25n18:898856:899306 [4] NCCL INFO Connected all rings
g25n18:898856:899306 [4] NCCL INFO Connected all trees
g25n18:898856:899306 [4] NCCL INFO 32 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
g25n18:898856:899306 [4] NCCL INFO comm 0x14d7b6a10 rank 0 nranks 1 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x732518713f99b80b - Init COMPLETE
    loaded indexed file in 0.093 seconds
    total number of samples: 439455219
    total number of epochs: 93214
 > loading doc-idx mapping from /gpfs/alpine2/stf218/world-shared/sajal/gptdata/index-cache/a718afb52de469266fb9bff4c0883da9_doc_idx.npy
 > loading sample-idx mapping from /gpfs/alpine2/stf218/world-shared/sajal/gptdata/index-cache/a718afb52de469266fb9bff4c0883da9_sample_idx.npy
 > loading shuffle-idx mapping from /gpfs/alpine2/stf218/world-shared/sajal/gptdata/index-cache/a718afb52de469266fb9bff4c0883da9_shuffle_idx.npy
    loaded indexed file in 0.079 seconds
    total number of samples: 43945467
    total number of epochs: 534358
> finished creating GPT datasets ...
g31n05:704345:704706 [1] NCCL INFO Using network IB
g31n05:704345:704706 [1] NCCL INFO comm 0x1a17ecfd0 rank 3 nranks 4 cudaDev 1 nvmlDev 1 busId 405000 commId 0xcc471a684c90aea8 - Init START
g27n18:839783:840146 [1] NCCL INFO Using network IB
g27n18:839783:840146 [1] NCCL INFO comm 0x172d0cf10 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 405000 commId 0x73978e8b82b8f783 - Init START
g27n18:839790:840149 [5] NCCL INFO Using network IB
g25n18:898856:899314 [4] NCCL INFO Using network IB
g25n18:898852:899316 [0] NCCL INFO Using network IB
g32n03:753097:753452 [1] NCCL INFO Using network IB
g32n03:753097:753452 [1] NCCL INFO comm 0x1ef652810 rank 3 nranks 4 cudaDev 1 nvmlDev 1 busId 405000 commId 0xf8329a6eddb14cdd - Init START
g32n03:753097:753452 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g32n03:753097:753452 [1] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
g32n03:753097:753452 [1] NCCL INFO P2P Chunksize set to 131072
g32n03:753096:753453 [0] NCCL INFO Using network IB
g32n03:753096:753453 [0] NCCL INFO comm 0x1ae46b590 rank 2 nranks 4 cudaDev 0 nvmlDev 0 busId 404000 commId 0xf8329a6eddb14cdd - Init START
g32n03:753096:753453 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g32n03:753096:753453 [0] NCCL INFO Trees [0] 3/-1/-1->2->0 [1] 3/0/-1->2->-1
g32n03:753096:753453 [0] NCCL INFO P2P Chunksize set to 131072
g32n03:753096:753453 [0] NCCL INFO Channel 00/0 : 1[5] -> 2[0] [receive] via NET/IB/0
g32n03:753096:753453 [0] NCCL INFO Channel 01/0 : 1[5] -> 2[0] [receive] via NET/IB/0
g32n03:753096:753453 [0] NCCL INFO Channel 00/0 : 2[0] -> 3[1] via P2P/IPC
g32n02:753687:754047 [4] NCCL INFO Using network IB
g32n02:753687:754047 [4] NCCL INFO comm 0x1d1bc47b0 rank 0 nranks 4 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xf8329a6eddb14cdd - Init START
g32n02:753687:754047 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g32n02:753687:754047 [4] NCCL INFO Channel 00/02 :    0   1   2   3
g32n02:753687:754047 [4] NCCL INFO Channel 01/02 :    0   1   2   3
g32n02:753687:754047 [4] NCCL INFO Trees [0] 1/2/-1->0->-1 [1] 1/-1/-1->0->2
g32n02:753687:754047 [4] NCCL INFO P2P Chunksize set to 131072
g32n02:753687:754047 [4] NCCL INFO Channel 00/0 : 3[1] -> 0[4] [receive] via NET/IB/1
g32n02:753687:754047 [4] NCCL INFO Channel 01/0 : 3[1] -> 0[4] [receive] via NET/IB/1
g32n02:753687:754047 [4] NCCL INFO Channel 00/0 : 0[4] -> 1[5] via P2P/IPC
g32n02:753687:754047 [4] NCCL INFO Channel 01/0 : 0[4] -> 1[5] via P2P/IPC
g32n03:753096:753453 [0] NCCL INFO Channel 01/0 : 2[0] -> 3[1] via P2P/IPC
g27n17:842060:842428 [0] NCCL INFO Using network IB
g27n17:842060:842428 [0] NCCL INFO comm 0x1bda86a80 rank 2 nranks 4 cudaDev 0 nvmlDev 0 busId 404000 commId 0xdde4cc035ecac97b - Init START
g27n17:842061:842427 [1] NCCL INFO Using network IB
g27n17:842061:842427 [1] NCCL INFO comm 0x1a8ffd220 rank 3 nranks 4 cudaDev 1 nvmlDev 1 busId 405000 commId 0xdde4cc035ecac97b - Init START
g27n17:842063:842423 [2] NCCL INFO Using network IB
g27n17:842063:842423 [2] NCCL INFO comm 0x19acce640 rank 0 nranks 4 cudaDev 2 nvmlDev 2 busId 406000 commId 0x943bcf09c0d96019 - Init START
g31n03:688512:688868 [3] NCCL INFO Using network IB
g31n03:688512:688868 [3] NCCL INFO comm 0x1d6615380 rank 1 nranks 4 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xc44305de50f5a865 - Init START
g31n03:688513:688867 [4] NCCL INFO Using network IB
g31n03:688513:688867 [4] NCCL INFO comm 0x1ddc15580 rank 2 nranks 4 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xc44305de50f5a865 - Init START
g27n17:842064:842424 [3] NCCL INFO Using network IB
g27n17:842064:842424 [3] NCCL INFO comm 0x1c9f9a2c0 rank 1 nranks 4 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x943bcf09c0d96019 - Init START
g31n03:688514:688869 [5] NCCL INFO Using network IB
g31n03:688514:688869 [5] NCCL INFO comm 0x1e41ff050 rank 3 nranks 4 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xc44305de50f5a865 - Init START
g27n17:842068:842425 [5] NCCL INFO Using network IB
g27n17:842068:842425 [5] NCCL INFO comm 0x1c1331490 rank 3 nranks 4 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x943bcf09c0d96019 - Init START
g27n17:842067:842426 [4] NCCL INFO Using network IB
g27n17:842067:842426 [4] NCCL INFO comm 0x18b8774c0 rank 2 nranks 4 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x943bcf09c0d96019 - Init START
g28n04:841284:841648 [3] NCCL INFO Using network IB
g28n04:841284:841648 [3] NCCL INFO comm 0x1e08ad070 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xbe090fc4d31d3749 - Init START
g32n03:753099:753456 [3] NCCL INFO Using network IB
g32n03:753099:753456 [3] NCCL INFO comm 0x15fcd7ca0 rank 1 nranks 4 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xe7e649e6bd56d868 - Init START
g28n04:841287:841646 [4] NCCL INFO Using network IB
g28n04:841287:841646 [4] NCCL INFO comm 0x1c1b8ea30 rank 0 nranks 4 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x24db34cf1c66119e - Init START
g32n03:753100:753455 [4] NCCL INFO Using network IB
g32n03:753100:753455 [4] NCCL INFO comm 0x1a72efe10 rank 2 nranks 4 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xe7e649e6bd56d868 - Init START
g31n03:688509:688870 [0] NCCL INFO Using network IB
g31n03:688509:688870 [0] NCCL INFO comm 0x1c26bd050 rank 2 nranks 4 cudaDev 0 nvmlDev 0 busId 404000 commId 0x2f29f621d04a681f - Init START
g31n03:688509:688870 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g28n04:841281:841649 [1] NCCL INFO Using network IB
g28n04:841281:841649 [1] NCCL INFO comm 0x1831f6510 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 405000 commId 0xbe090fc4d31d3749 - Init START
g31n03:688510:688871 [1] NCCL INFO Using network IB
g31n03:688510:688871 [1] NCCL INFO comm 0x1ada76bf0 rank 3 nranks 4 cudaDev 1 nvmlDev 1 busId 405000 commId 0x2f29f621d04a681f - Init START
g31n03:688510:688871 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g32n03:753098:753454 [2] NCCL INFO Using network IB
g32n03:753098:753454 [2] NCCL INFO comm 0x1adb48f90 rank 0 nranks 4 cudaDev 2 nvmlDev 2 busId 406000 commId 0xe7e649e6bd56d868 - Init START
g28n04:841280:841645 [0] NCCL INFO Using network IB
g28n04:841280:841645 [0] NCCL INFO comm 0x1b87df1f0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 404000 commId 0xbe090fc4d31d3749 - Init START
g31n03:688511:688866 [2] NCCL INFO Using network IB
g31n03:688511:688866 [2] NCCL INFO comm 0x13beb20d0 rank 0 nranks 4 cudaDev 2 nvmlDev 2 busId 406000 commId 0xc44305de50f5a865 - Init START
g32n03:753101:753457 [5] NCCL INFO Using network IB
g32n03:753101:753457 [5] NCCL INFO comm 0x1e4bddc60 rank 3 nranks 4 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xe7e649e6bd56d868 - Init START
g28n04:841283:841647 [2] NCCL INFO Using network IB
g28n04:841283:841647 [2] NCCL INFO comm 0x199f72eb0 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 406000 commId 0xbe090fc4d31d3749 - Init START
g31n05:704348:704702 [4] NCCL INFO Using network IB
g31n05:704348:704702 [4] NCCL INFO comm 0x19a8819a0 rank 2 nranks 4 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x91e4d8ed552aed23 - Init START
g31n05:704349:704703 [5] NCCL INFO Using network IB
g31n05:704349:704703 [5] NCCL INFO comm 0x1b57dc530 rank 3 nranks 4 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x91e4d8ed552aed23 - Init START
g28n04:841288:841650 [5] NCCL INFO Using network IB
g28n04:841288:841650 [5] NCCL INFO comm 0x1db0038c0 rank 1 nranks 4 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x24db34cf1c66119e - Init START
g31n02:736464:736827 [1] NCCL INFO Using network IB
g31n02:736464:736827 [1] NCCL INFO comm 0x188c20060 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 405000 commId 0xc1f28ebe8d1bf8e6 - Init START
g31n05:704346:704701 [2] NCCL INFO Using network IB
g31n05:704346:704701 [2] NCCL INFO comm 0x1c5c00e80 rank 0 nranks 4 cudaDev 2 nvmlDev 2 busId 406000 commId 0x91e4d8ed552aed23 - Init START
g31n05:704344:704705 [0] NCCL INFO Using network IB
g31n05:704344:704705 [0] NCCL INFO comm 0x1e8507d10 rank 2 nranks 4 cudaDev 0 nvmlDev 0 busId 404000 commId 0xcc471a684c90aea8 - Init START
g31n02:736465:736828 [2] NCCL INFO Using network IB
g31n02:736465:736828 [2] NCCL INFO comm 0x1c42b31e0 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 406000 commId 0xc1f28ebe8d1bf8e6 - Init START
g26n02:851592:851955 [3] NCCL INFO Using network IB
g26n02:851592:851955 [3] NCCL INFO comm 0x1b2e847f0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xf34d505a1105311f - Init START
g31n05:704347:704704 [3] NCCL INFO Using network IB
g31n05:704347:704704 [3] NCCL INFO comm 0x1e03c8b10 rank 1 nranks 4 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x91e4d8ed552aed23 - Init START
g31n02:736463:736825 [0] NCCL INFO Using network IB
g31n02:736463:736825 [0] NCCL INFO comm 0x1149c5dc0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 404000 commId 0xc1f28ebe8d1bf8e6 - Init START
g31n02:736466:736829 [3] NCCL INFO Using network IB
g31n02:736466:736829 [3] NCCL INFO comm 0x1c192b010 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xc1f28ebe8d1bf8e6 - Init START
g31n02:736468:736830 [5] NCCL INFO Using network IB
g31n02:736468:736830 [5] NCCL INFO comm 0x1c2c763e0 rank 1 nranks 4 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x2f29f621d04a681f - Init START
g26n01:811332:811688 [1] NCCL INFO Using network IB
g26n01:811332:811688 [1] NCCL INFO comm 0x1b327bab0 rank 3 nranks 4 cudaDev 1 nvmlDev 1 busId 405000 commId 0x960bdb9dee6b6c07 - Init START
g31n02:736467:736826 [4] NCCL INFO Using network IB
g31n02:736467:736826 [4] NCCL INFO comm 0x1b4510c80 rank 0 nranks 4 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x2f29f621d04a681f - Init START
g26n01:811331:811687 [0] NCCL INFO Using network IB
g26n01:811331:811687 [0] NCCL INFO comm 0x1ad514000 rank 2 nranks 4 cudaDev 0 nvmlDev 0 busId 404000 commId 0x960bdb9dee6b6c07 - Init START
g26n02:851589:851951 [0] NCCL INFO Using network IB
g26n02:851589:851951 [0] NCCL INFO comm 0x1db256390 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 404000 commId 0xf34d505a1105311f - Init START
g26n01:811333:811686 [2] NCCL INFO Using network IB
g26n01:811333:811686 [2] NCCL INFO comm 0x1a555de30 rank 0 nranks 4 cudaDev 2 nvmlDev 2 busId 406000 commId 0x34c28deeb8633891 - Init START
g25n18:898855:899320 [3] NCCL INFO Using network IB
g25n18:898855:899320 [3] NCCL INFO comm 0x1c3d15ad0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x9a30262117f4b78d - Init START
g26n01:811336:811690 [5] NCCL INFO Using network IB
g26n01:811336:811690 [5] NCCL INFO comm 0x1bb0983c0 rank 3 nranks 4 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x34c28deeb8633891 - Init START
g25n18:898857:899317 [5] NCCL INFO Using network IB
g25n18:898857:899317 [5] NCCL INFO comm 0x193769450 rank 1 nranks 4 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x960bdb9dee6b6c07 - Init START
g26n01:811334:811689 [3] NCCL INFO Using network IB
g26n01:811334:811689 [3] NCCL INFO comm 0x1b20bc7d0 rank 1 nranks 4 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x34c28deeb8633891 - Init START
g26n02:851591:851954 [2] NCCL INFO Using network IB
g26n02:851591:851954 [2] NCCL INFO comm 0x1c1236480 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 406000 commId 0xf34d505a1105311f - Init START
g26n01:811335:811691 [4] NCCL INFO Using network IB
g26n01:811335:811691 [4] NCCL INFO comm 0x186131c30 rank 2 nranks 4 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x34c28deeb8633891 - Init START
g26n02:851590:851953 [1] NCCL INFO Using network IB
g26n02:851590:851953 [1] NCCL INFO comm 0x1be2792e0 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 405000 commId 0xf34d505a1105311f - Init START
g31n04:688229:688589 [5] NCCL INFO Using network IB
g31n04:688229:688589 [5] NCCL INFO comm 0x19bc4ea10 rank 1 nranks 4 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xcc471a684c90aea8 - Init START
g31n04:688228:688586 [4] NCCL INFO Using network IB
g31n04:688228:688586 [4] NCCL INFO comm 0x1565aef70 rank 0 nranks 4 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xcc471a684c90aea8 - Init START
g31n04:688227:688592 [3] NCCL INFO Using network IB
g31n04:688227:688592 [3] NCCL INFO comm 0x1a9395f60 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x51f76f63babe37f6 - Init START
g31n04:688226:688590 [2] NCCL INFO Using network IB
g31n04:688226:688590 [2] NCCL INFO comm 0x1f177ecf0 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 406000 commId 0x51f76f63babe37f6 - Init START
g25n18:898853:899318 [1] NCCL INFO Using network IB
g25n18:898853:899318 [1] NCCL INFO comm 0x1bc6392e0 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 405000 commId 0x9a30262117f4b78d - Init START
g31n04:688225:688591 [1] NCCL INFO Using network IB
g31n04:688225:688591 [1] NCCL INFO comm 0x1c1758ac0 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 405000 commId 0x51f76f63babe37f6 - Init START
g31n04:688224:688588 [0] NCCL INFO Using network IB
g31n04:688224:688588 [0] NCCL INFO comm 0x1a3f8dbc0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 404000 commId 0x51f76f63babe37f6 - Init START
g26n02:851594:851956 [5] NCCL INFO Using network IB
g26n02:851594:851956 [5] NCCL INFO comm 0x1e5c2c0d0 rank 1 nranks 4 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xdde4cc035ecac97b - Init START
g26n02:851593:851952 [4] NCCL INFO Using network IB
g26n02:851593:851952 [4] NCCL INFO comm 0x1c3358f80 rank 0 nranks 4 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xdde4cc035ecac97b - Init START
g32n02:753684:754052 [1] NCCL INFO Using network IB
g32n02:753684:754052 [1] NCCL INFO comm 0x1bb2d1710 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 405000 commId 0x6b4cabcca34779f8 - Init START
g32n02:753683:754048 [0] NCCL INFO Using network IB
g32n02:753683:754048 [0] NCCL INFO comm 0x19a809900 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 404000 commId 0x6b4cabcca34779f8 - Init START
g31n01:617757:618124 [1] NCCL INFO Using network IB
g31n01:617757:618124 [1] NCCL INFO comm 0x1a2ccf8f0 rank 3 nranks 4 cudaDev 1 nvmlDev 1 busId 405000 commId 0x24db34cf1c66119e - Init START
g31n01:617758:618119 [2] NCCL INFO Using network IB
g31n01:617756:618123 [0] NCCL INFO Using network IB
g31n01:617756:618123 [0] NCCL INFO comm 0x1e436b870 rank 2 nranks 4 cudaDev 0 nvmlDev 0 busId 404000 commId 0x24db34cf1c66119e - Init START
g28n01:770099:770454 [4] NCCL INFO Using network IB
g28n01:770099:770454 [4] NCCL INFO comm 0x1b200cec0 rank 2 nranks 4 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x8bd3134fb91d287 - Init START
g28n01:770100:770452 [5] NCCL INFO Using network IB
g28n01:770100:770452 [5] NCCL INFO comm 0x1aecc7130 rank 3 nranks 4 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x8bd3134fb91d287 - Init START
g28n01:770096:770453 [3] NCCL INFO Using network IB
g28n01:770096:770453 [3] NCCL INFO comm 0x1e55f3610 rank 1 nranks 4 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x8bd3134fb91d287 - Init START
g31n01:617759:618120 [3] NCCL INFO Using network IB
g31n01:617761:618121 [5] NCCL INFO Using network IB
g25n18:898854:899319 [2] NCCL INFO Using network IB
g25n18:898854:899319 [2] NCCL INFO comm 0x1bb59ed90 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 406000 commId 0x9a30262117f4b78d - Init START
g32n02:753686:754051 [3] NCCL INFO Using network IB
g32n02:753686:754051 [3] NCCL INFO comm 0x18678d940 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x6b4cabcca34779f8 - Init START
g28n01:770092:770456 [0] NCCL INFO Using network IB
g32n02:753688:754049 [5] NCCL INFO Using network IB
g32n02:753688:754049 [5] NCCL INFO comm 0x1bd139a50 rank 1 nranks 4 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xf8329a6eddb14cdd - Init START
g32n02:753688:754049 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g32n02:753688:754049 [5] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
g32n02:753688:754049 [5] NCCL INFO P2P Chunksize set to 131072
g28n01:770093:770455 [1] NCCL INFO Using network IB
g28n01:770095:770451 [2] NCCL INFO Using network IB
g28n01:770095:770451 [2] NCCL INFO comm 0x1c174a7c0 rank 0 nranks 4 cudaDev 2 nvmlDev 2 busId 406000 commId 0x8bd3134fb91d287 - Init START
g28n03:859958:860318 [3] NCCL INFO Using network IB
g28n03:859958:860318 [3] NCCL INFO comm 0x1cdb076e0 rank 1 nranks 4 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x6617c45e414945a1 - Init START
g28n03:859960:860316 [4] NCCL INFO Using network IB
g28n03:859960:860316 [4] NCCL INFO comm 0x1cb689e00 rank 2 nranks 4 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x6617c45e414945a1 - Init START
g28n03:859961:860317 [5] NCCL INFO Using network IB
g28n03:859961:860317 [5] NCCL INFO comm 0x1cda5ebf0 rank 3 nranks 4 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x6617c45e414945a1 - Init START
g28n03:859953:860315 [0] NCCL INFO Using network IB
g28n03:859953:860315 [0] NCCL INFO comm 0x1e469cd80 rank 2 nranks 4 cudaDev 0 nvmlDev 0 busId 404000 commId 0xef632f2039341538 - Init START
g28n03:859954:860314 [1] NCCL INFO Using network IB
g28n03:859954:860314 [1] NCCL INFO comm 0x1483f6020 rank 3 nranks 4 cudaDev 1 nvmlDev 1 busId 405000 commId 0xef632f2039341538 - Init START
g27n18:839785:840147 [2] NCCL INFO Using network IB
g27n18:839785:840147 [2] NCCL INFO comm 0x1d5635800 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 406000 commId 0x73978e8b82b8f783 - Init START
g28n03:859956:860313 [2] NCCL INFO Using network IB
g28n03:859956:860313 [2] NCCL INFO comm 0x1a7e047b0 rank 0 nranks 4 cudaDev 2 nvmlDev 2 busId 406000 commId 0x6617c45e414945a1 - Init START
g31n01:617760:618122 [4] NCCL INFO Using network IB
g27n18:839782:840144 [0] NCCL INFO Using network IB
g27n18:839782:840144 [0] NCCL INFO comm 0x1f0e7d9b0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 404000 commId 0x73978e8b82b8f783 - Init START
g32n02:753685:754050 [2] NCCL INFO Using network IB
g32n02:753685:754050 [2] NCCL INFO comm 0x1b4859150 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 406000 commId 0x6b4cabcca34779f8 - Init START
g28n02:792072:792450 [3] NCCL INFO Using network IB
g28n02:792072:792450 [3] NCCL INFO comm 0x1cac2f0b0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x9a4d1656c0da603a - Init START
g28n02:792075:792446 [4] NCCL INFO Using network IB
g28n02:792075:792446 [4] NCCL INFO comm 0x1a15cd580 rank 0 nranks 4 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xef632f2039341538 - Init START
g28n02:792071:792451 [2] NCCL INFO Using network IB
g28n02:792071:792451 [2] NCCL INFO comm 0x1c4989c80 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 406000 commId 0x9a4d1656c0da603a - Init START
g27n18:839787:840148 [3] NCCL INFO Using network IB
g27n18:839787:840148 [3] NCCL INFO comm 0x1c5fc4570 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x73978e8b82b8f783 - Init START
g27n18:839789:840145 [4] NCCL INFO Using network IB
g27n18:839789:840145 [4] NCCL INFO comm 0x1b1aa78a0 rank 0 nranks 4 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x4e7980dedb654e20 - Init START
g28n02:792068:792447 [0] NCCL INFO Using network IB
g28n02:792068:792447 [0] NCCL INFO comm 0x19ec87f70 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 404000 commId 0x9a4d1656c0da603a - Init START
g28n02:792076:792448 [5] NCCL INFO Using network IB
g28n02:792076:792448 [5] NCCL INFO comm 0x1e913d960 rank 1 nranks 4 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xef632f2039341538 - Init START
g28n02:792069:792449 [1] NCCL INFO Using network IB
g28n02:792069:792449 [1] NCCL INFO comm 0x1ca612370 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 405000 commId 0x9a4d1656c0da603a - Init START
g32n03:753097:753452 [1] NCCL INFO Channel 00/0 : 3[1] -> 0[4] [send] via NET/IB/0
g32n03:753097:753452 [1] NCCL INFO Channel 01/0 : 3[1] -> 0[4] [send] via NET/IB/0
g32n02:753688:754049 [5] NCCL INFO Channel 00/0 : 1[5] -> 2[0] [send] via NET/IB/1
g32n02:753688:754049 [5] NCCL INFO Channel 01/0 : 1[5] -> 2[0] [send] via NET/IB/1
g27n18:839790:840149 [5] NCCL INFO comm 0x166dcf1e0 rank 1 nranks 4 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x4e7980dedb654e20 - Init START
g25n18:898856:899314 [4] NCCL INFO comm 0x14d79c2c0 rank 0 nranks 4 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x960bdb9dee6b6c07 - Init START
g32n02:753688:754049 [5] NCCL INFO Connected all rings
g32n02:753688:754049 [5] NCCL INFO Channel 00/0 : 1[5] -> 0[4] via P2P/IPC
g32n03:753097:753452 [1] NCCL INFO Connected all rings
g32n03:753097:753452 [1] NCCL INFO Channel 00/0 : 3[1] -> 2[0] via P2P/IPC
g32n02:753688:754049 [5] NCCL INFO Channel 01/0 : 1[5] -> 0[4] via P2P/IPC
g32n03:753097:753452 [1] NCCL INFO Channel 01/0 : 3[1] -> 2[0] via P2P/IPC
g32n03:753096:753453 [0] NCCL INFO Connected all rings
g32n03:753096:753453 [0] NCCL INFO Channel 00/0 : 0[4] -> 2[0] [receive] via NET/IB/0
g32n03:753096:753453 [0] NCCL INFO Channel 01/0 : 0[4] -> 2[0] [receive] via NET/IB/0
g32n03:753096:753453 [0] NCCL INFO Channel 00/0 : 2[0] -> 0[4] [send] via NET/IB/0
g32n03:753096:753453 [0] NCCL INFO Channel 01/0 : 2[0] -> 0[4] [send] via NET/IB/0
g32n02:753687:754047 [4] NCCL INFO Connected all rings
g32n02:753687:754047 [4] NCCL INFO Channel 00/0 : 2[0] -> 0[4] [receive] via NET/IB/1
g32n02:753687:754047 [4] NCCL INFO Channel 01/0 : 2[0] -> 0[4] [receive] via NET/IB/1
g32n02:753687:754047 [4] NCCL INFO Channel 00/0 : 0[4] -> 2[0] [send] via NET/IB/1
g32n02:753687:754047 [4] NCCL INFO Channel 01/0 : 0[4] -> 2[0] [send] via NET/IB/1
g31n03:688510:688871 [1] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
g31n03:688510:688871 [1] NCCL INFO P2P Chunksize set to 131072
g31n03:688509:688870 [0] NCCL INFO Trees [0] 3/-1/-1->2->0 [1] 3/0/-1->2->-1
g31n03:688509:688870 [0] NCCL INFO P2P Chunksize set to 131072
g31n03:688509:688870 [0] NCCL INFO Channel 00/0 : 1[5] -> 2[0] [receive] via NET/IB/0
g31n03:688509:688870 [0] NCCL INFO Channel 01/0 : 1[5] -> 2[0] [receive] via NET/IB/0
g31n03:688509:688870 [0] NCCL INFO Channel 00/0 : 2[0] -> 3[1] via P2P/IPC
g31n02:736468:736830 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g31n02:736468:736830 [5] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
g31n02:736468:736830 [5] NCCL INFO P2P Chunksize set to 131072
g25n18:898852:899316 [0] NCCL INFO comm 0x13b80adf0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 404000 commId 0x9a30262117f4b78d - Init START
g25n18:898856:899314 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g25n18:898856:899314 [4] NCCL INFO Channel 00/02 :    0   1   2   3
g25n18:898856:899314 [4] NCCL INFO Channel 01/02 :    0   1   2   3
g25n18:898856:899314 [4] NCCL INFO Trees [0] 1/2/-1->0->-1 [1] 1/-1/-1->0->2
g25n18:898856:899314 [4] NCCL INFO P2P Chunksize set to 131072
g25n18:898856:899314 [4] NCCL INFO Channel 00/0 : 3[1] -> 0[4] [receive] via NET/IB/1
g25n18:898856:899314 [4] NCCL INFO Channel 01/0 : 3[1] -> 0[4] [receive] via NET/IB/1
g25n18:898856:899314 [4] NCCL INFO Channel 00/0 : 0[4] -> 1[5] via P2P/IPC
g25n18:898857:899317 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g25n18:898857:899317 [5] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
g25n18:898857:899317 [5] NCCL INFO P2P Chunksize set to 131072
g31n02:736467:736826 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g31n02:736467:736826 [4] NCCL INFO Channel 00/02 :    0   1   2   3
g31n02:736467:736826 [4] NCCL INFO Channel 01/02 :    0   1   2   3
g31n02:736467:736826 [4] NCCL INFO Trees [0] 1/2/-1->0->-1 [1] 1/-1/-1->0->2
g31n02:736467:736826 [4] NCCL INFO P2P Chunksize set to 131072
g31n02:736467:736826 [4] NCCL INFO Channel 00/0 : 3[1] -> 0[4] [receive] via NET/IB/1
g31n02:736467:736826 [4] NCCL INFO Channel 01/0 : 3[1] -> 0[4] [receive] via NET/IB/1
g31n02:736467:736826 [4] NCCL INFO Channel 00/0 : 0[4] -> 1[5] via P2P/IPC
g31n03:688509:688870 [0] NCCL INFO Channel 01/0 : 2[0] -> 3[1] via P2P/IPC
g26n01:811332:811688 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g26n01:811332:811688 [1] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
g26n01:811332:811688 [1] NCCL INFO P2P Chunksize set to 131072
g25n18:898856:899314 [4] NCCL INFO Channel 01/0 : 0[4] -> 1[5] via P2P/IPC
g26n01:811331:811687 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g26n01:811331:811687 [0] NCCL INFO Trees [0] 3/-1/-1->2->0 [1] 3/0/-1->2->-1
g26n01:811331:811687 [0] NCCL INFO P2P Chunksize set to 131072
g26n01:811331:811687 [0] NCCL INFO Channel 00/0 : 1[5] -> 2[0] [receive] via NET/IB/0
g26n01:811331:811687 [0] NCCL INFO Channel 01/0 : 1[5] -> 2[0] [receive] via NET/IB/0
g26n01:811331:811687 [0] NCCL INFO Channel 00/0 : 2[0] -> 3[1] via P2P/IPC
g31n02:736467:736826 [4] NCCL INFO Channel 01/0 : 0[4] -> 1[5] via P2P/IPC
g31n03:688510:688871 [1] NCCL INFO Channel 00/0 : 3[1] -> 0[4] [send] via NET/IB/0
g31n03:688510:688871 [1] NCCL INFO Channel 01/0 : 3[1] -> 0[4] [send] via NET/IB/0
g31n02:736468:736830 [5] NCCL INFO Channel 00/0 : 1[5] -> 2[0] [send] via NET/IB/1
g31n02:736468:736830 [5] NCCL INFO Channel 01/0 : 1[5] -> 2[0] [send] via NET/IB/1
g26n01:811331:811687 [0] NCCL INFO Channel 01/0 : 2[0] -> 3[1] via P2P/IPC
g32n03:753097:753452 [1] NCCL INFO Connected all trees
g32n03:753097:753452 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g32n03:753097:753452 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g26n01:811332:811688 [1] NCCL INFO Channel 00/0 : 3[1] -> 0[4] [send] via NET/IB/0
g26n01:811332:811688 [1] NCCL INFO Channel 01/0 : 3[1] -> 0[4] [send] via NET/IB/0
g32n02:753688:754049 [5] NCCL INFO Connected all trees
g32n02:753688:754049 [5] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g32n02:753688:754049 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n05:704346:704701 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g31n05:704346:704701 [2] NCCL INFO Channel 00/02 :    0   1   2   3
g31n05:704346:704701 [2] NCCL INFO Channel 01/02 :    0   1   2   3
g31n05:704346:704701 [2] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
g31n05:704346:704701 [2] NCCL INFO P2P Chunksize set to 524288
g31n05:704347:704704 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g31n05:704347:704704 [3] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
g31n05:704347:704704 [3] NCCL INFO P2P Chunksize set to 524288
g31n05:704348:704702 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g31n05:704348:704702 [4] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
g31n05:704348:704702 [4] NCCL INFO P2P Chunksize set to 524288
g31n04:688224:688588 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g31n04:688224:688588 [0] NCCL INFO Channel 00/02 :    0   1   2   3
g31n04:688224:688588 [0] NCCL INFO Channel 01/02 :    0   1   2   3
g31n04:688224:688588 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
g31n04:688224:688588 [0] NCCL INFO P2P Chunksize set to 524288
g31n04:688227:688592 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g31n04:688227:688592 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
g31n04:688227:688592 [3] NCCL INFO P2P Chunksize set to 524288
g31n04:688226:688590 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g31n04:688226:688590 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
g31n04:688226:688590 [2] NCCL INFO P2P Chunksize set to 524288
g31n04:688225:688591 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g31n04:688225:688591 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
g31n04:688225:688591 [1] NCCL INFO P2P Chunksize set to 524288
g31n05:704346:704701 [2] NCCL INFO Channel 00/0 : 0[2] -> 1[3] via P2P/IPC
g31n05:704348:704702 [4] NCCL INFO Channel 00/0 : 2[4] -> 3[5] via P2P/IPC
g31n04:688224:688588 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/IPC
g31n04:688226:688590 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/IPC
g31n05:704348:704702 [4] NCCL INFO Channel 01/0 : 2[4] -> 3[5] via P2P/IPC
g31n05:704346:704701 [2] NCCL INFO Channel 01/0 : 0[2] -> 1[3] via P2P/IPC
g27n18:839782:840144 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g27n18:839782:840144 [0] NCCL INFO Channel 00/02 :    0   1   2   3
g27n18:839782:840144 [0] NCCL INFO Channel 01/02 :    0   1   2   3
g27n18:839782:840144 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
g27n18:839782:840144 [0] NCCL INFO P2P Chunksize set to 524288
g31n05:704347:704704 [3] NCCL INFO Channel 00/0 : 1[3] -> 2[4] via P2P/IPC
g27n18:839783:840146 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g27n18:839783:840146 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
g27n18:839783:840146 [1] NCCL INFO P2P Chunksize set to 524288
g27n18:839785:840147 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g27n18:839785:840147 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
g27n18:839785:840147 [2] NCCL INFO P2P Chunksize set to 524288
g31n04:688225:688591 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/IPC
g31n04:688224:688588 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/IPC
g28n04:841284:841648 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g28n04:841284:841648 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
g28n04:841284:841648 [3] NCCL INFO P2P Chunksize set to 524288
g31n04:688227:688592 [3] NCCL INFO Channel 00/0 : 3[3] -> 0[0] via P2P/IPC
g28n04:841283:841647 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g28n04:841283:841647 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
g28n04:841283:841647 [2] NCCL INFO P2P Chunksize set to 524288
g28n04:841280:841645 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g28n04:841280:841645 [0] NCCL INFO Channel 00/02 :    0   1   2   3
g28n04:841280:841645 [0] NCCL INFO Channel 01/02 :    0   1   2   3
g28n04:841280:841645 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
g28n04:841280:841645 [0] NCCL INFO P2P Chunksize set to 524288
g31n04:688226:688590 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/IPC
g28n04:841281:841649 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g28n04:841281:841649 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
g28n04:841281:841649 [1] NCCL INFO P2P Chunksize set to 524288
g27n18:839787:840148 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g27n18:839787:840148 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
g27n18:839787:840148 [3] NCCL INFO P2P Chunksize set to 524288
g31n04:688225:688591 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/IPC
g31n04:688227:688592 [3] NCCL INFO Channel 01/0 : 3[3] -> 0[0] via P2P/IPC
g31n02:736468:736830 [5] NCCL INFO Connected all rings
g31n02:736468:736830 [5] NCCL INFO Channel 00/0 : 1[5] -> 0[4] via P2P/IPC
g31n05:704347:704704 [3] NCCL INFO Channel 01/0 : 1[3] -> 2[4] via P2P/IPC
g26n02:851592:851955 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g26n02:851592:851955 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
g26n02:851592:851955 [3] NCCL INFO P2P Chunksize set to 524288
g27n18:839783:840146 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/IPC
g26n02:851589:851951 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g26n02:851589:851951 [0] NCCL INFO Channel 00/02 :    0   1   2   3
g26n02:851589:851951 [0] NCCL INFO Channel 01/02 :    0   1   2   3
g26n02:851589:851951 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
g26n02:851589:851951 [0] NCCL INFO P2P Chunksize set to 524288
g26n02:851590:851953 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g26n02:851590:851953 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
g26n02:851590:851953 [1] NCCL INFO P2P Chunksize set to 524288
g26n02:851591:851954 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g26n02:851591:851954 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
g26n02:851591:851954 [2] NCCL INFO P2P Chunksize set to 524288
g27n18:839782:840144 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/IPC
g27n18:839787:840148 [3] NCCL INFO Channel 00/0 : 3[3] -> 0[0] via P2P/IPC
g28n04:841284:841648 [3] NCCL INFO Channel 00/0 : 3[3] -> 0[0] via P2P/IPC
g27n18:839783:840146 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/IPC
g31n03:688510:688871 [1] NCCL INFO Connected all rings
g31n03:688510:688871 [1] NCCL INFO Channel 00/0 : 3[1] -> 2[0] via P2P/IPC
g27n18:839785:840147 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/IPC
g31n02:736468:736830 [5] NCCL INFO Channel 01/0 : 1[5] -> 0[4] via P2P/IPC
g31n02:736467:736826 [4] NCCL INFO Connected all rings
g31n02:736467:736826 [4] NCCL INFO Channel 00/0 : 2[0] -> 0[4] [receive] via NET/IB/1
g31n02:736467:736826 [4] NCCL INFO Channel 01/0 : 2[0] -> 0[4] [receive] via NET/IB/1
g31n02:736467:736826 [4] NCCL INFO Channel 00/0 : 0[4] -> 2[0] [send] via NET/IB/1
g31n02:736467:736826 [4] NCCL INFO Channel 01/0 : 0[4] -> 2[0] [send] via NET/IB/1
g31n03:688509:688870 [0] NCCL INFO Connected all rings
g31n03:688509:688870 [0] NCCL INFO Channel 00/0 : 0[4] -> 2[0] [receive] via NET/IB/0
g31n03:688509:688870 [0] NCCL INFO Channel 01/0 : 0[4] -> 2[0] [receive] via NET/IB/0
g31n03:688509:688870 [0] NCCL INFO Channel 00/0 : 2[0] -> 0[4] [send] via NET/IB/0
g31n03:688509:688870 [0] NCCL INFO Channel 01/0 : 2[0] -> 0[4] [send] via NET/IB/0
g27n18:839782:840144 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/IPC
g27n18:839787:840148 [3] NCCL INFO Channel 01/0 : 3[3] -> 0[0] via P2P/IPC
g28n04:841283:841647 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/IPC
g27n18:839785:840147 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/IPC
g28n04:841281:841649 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/IPC
g28n04:841280:841645 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/IPC
g31n03:688510:688871 [1] NCCL INFO Channel 01/0 : 3[1] -> 2[0] via P2P/IPC
g28n04:841283:841647 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/IPC
g26n02:851592:851955 [3] NCCL INFO Channel 00/0 : 3[3] -> 0[0] via P2P/IPC
g28n04:841281:841649 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/IPC
g26n02:851590:851953 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/IPC
g26n02:851589:851951 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/IPC
g26n02:851591:851954 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/IPC
g28n04:841284:841648 [3] NCCL INFO Channel 01/0 : 3[3] -> 0[0] via P2P/IPC
g28n04:841280:841645 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/IPC
g31n04:688225:688591 [1] NCCL INFO Connected all rings
g26n02:851592:851955 [3] NCCL INFO Channel 01/0 : 3[3] -> 0[0] via P2P/IPC
g26n02:851590:851953 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/IPC
g31n05:704347:704704 [3] NCCL INFO Connected all rings
g31n04:688226:688590 [2] NCCL INFO Connected all rings
g26n02:851589:851951 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/IPC
g26n02:851591:851954 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/IPC
g31n04:688227:688592 [3] NCCL INFO Connected all rings
g31n04:688227:688592 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/IPC
g31n03:688510:688871 [1] NCCL INFO Connected all trees
g31n03:688510:688871 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g31n03:688510:688871 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n02:792069:792449 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g28n02:792069:792449 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
g28n02:792069:792449 [1] NCCL INFO P2P Chunksize set to 524288
g28n02:792068:792447 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g28n02:792068:792447 [0] NCCL INFO Channel 00/02 :    0   1   2   3
g28n02:792068:792447 [0] NCCL INFO Channel 01/02 :    0   1   2   3
g28n02:792068:792447 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
g28n02:792068:792447 [0] NCCL INFO P2P Chunksize set to 524288
g31n05:704347:704704 [3] NCCL INFO Channel 00/0 : 1[3] -> 0[2] via P2P/IPC
g28n02:792071:792451 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g28n02:792071:792451 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
g28n02:792071:792451 [2] NCCL INFO P2P Chunksize set to 524288
g27n18:839782:840144 [0] NCCL INFO Connected all rings
g28n02:792076:792448 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g28n02:792076:792448 [5] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
g28n02:792076:792448 [5] NCCL INFO P2P Chunksize set to 131072
g31n02:736468:736830 [5] NCCL INFO Connected all trees
g31n02:736468:736830 [5] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g31n02:736468:736830 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n02:792072:792450 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g28n02:792072:792450 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
g28n02:792072:792450 [3] NCCL INFO P2P Chunksize set to 524288
g31n05:704347:704704 [3] NCCL INFO Channel 01/0 : 1[3] -> 0[2] via P2P/IPC
g28n02:792075:792446 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g28n02:792075:792446 [4] NCCL INFO Channel 00/02 :    0   1   2   3
g28n02:792075:792446 [4] NCCL INFO Channel 01/02 :    0   1   2   3
g28n02:792075:792446 [4] NCCL INFO Trees [0] 1/2/-1->0->-1 [1] 1/-1/-1->0->2
g28n02:792075:792446 [4] NCCL INFO P2P Chunksize set to 131072
g28n02:792075:792446 [4] NCCL INFO Channel 00/0 : 3[1] -> 0[4] [receive] via NET/IB/1
g28n02:792075:792446 [4] NCCL INFO Channel 01/0 : 3[1] -> 0[4] [receive] via NET/IB/1
g28n02:792075:792446 [4] NCCL INFO Channel 00/0 : 0[4] -> 1[5] via P2P/IPC
g28n03:859953:860315 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g28n03:859953:860315 [0] NCCL INFO Trees [0] 3/-1/-1->2->0 [1] 3/0/-1->2->-1
g28n03:859953:860315 [0] NCCL INFO P2P Chunksize set to 131072
g28n03:859953:860315 [0] NCCL INFO Channel 00/0 : 1[5] -> 2[0] [receive] via NET/IB/0
g28n03:859953:860315 [0] NCCL INFO Channel 01/0 : 1[5] -> 2[0] [receive] via NET/IB/0
g28n03:859953:860315 [0] NCCL INFO Channel 00/0 : 2[0] -> 3[1] via P2P/IPC
g31n04:688227:688592 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/IPC
g28n03:859954:860314 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g28n03:859954:860314 [1] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
g28n03:859954:860314 [1] NCCL INFO P2P Chunksize set to 131072
g32n02:753687:754047 [4] NCCL INFO Connected all trees
g32n02:753687:754047 [4] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g32n02:753687:754047 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g27n18:839783:840146 [1] NCCL INFO Connected all rings
g31n04:688225:688591 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/IPC
g31n04:688224:688588 [0] NCCL INFO Connected all rings
g27n18:839787:840148 [3] NCCL INFO Connected all rings
g27n18:839787:840148 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/IPC
g26n01:811333:811686 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g26n01:811333:811686 [2] NCCL INFO Channel 00/02 :    0   1   2   3
g26n01:811333:811686 [2] NCCL INFO Channel 01/02 :    0   1   2   3
g26n01:811333:811686 [2] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
g26n01:811333:811686 [2] NCCL INFO P2P Chunksize set to 524288
g26n01:811335:811691 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g26n01:811335:811691 [4] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
g26n01:811335:811691 [4] NCCL INFO P2P Chunksize set to 524288
g26n01:811334:811689 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g26n01:811334:811689 [3] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
g26n01:811334:811689 [3] NCCL INFO P2P Chunksize set to 524288
g31n04:688226:688590 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/IPC
g28n02:792075:792446 [4] NCCL INFO Channel 01/0 : 0[4] -> 1[5] via P2P/IPC
g32n03:753096:753453 [0] NCCL INFO Connected all trees
g32n03:753096:753453 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g32n03:753096:753453 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n02:792068:792447 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/IPC
g27n18:839787:840148 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/IPC
g31n04:688225:688591 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/IPC
g26n02:851589:851951 [0] NCCL INFO Connected all rings
g28n04:841283:841647 [2] NCCL INFO Connected all rings
g28n04:841281:841649 [1] NCCL INFO Connected all rings
g28n02:792071:792451 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/IPC
g28n02:792072:792450 [3] NCCL INFO Channel 00/0 : 3[3] -> 0[0] via P2P/IPC
g26n01:811333:811686 [2] NCCL INFO Channel 00/0 : 0[2] -> 1[3] via P2P/IPC
g28n02:792068:792447 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/IPC
g26n01:811335:811691 [4] NCCL INFO Channel 00/0 : 2[4] -> 3[5] via P2P/IPC
g26n01:811334:811689 [3] NCCL INFO Channel 00/0 : 1[3] -> 2[4] via P2P/IPC
g28n02:792069:792449 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/IPC
g31n04:688226:688590 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/IPC
g31n03:688511:688866 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g31n03:688511:688866 [2] NCCL INFO Channel 00/02 :    0   1   2   3
g31n03:688511:688866 [2] NCCL INFO Channel 01/02 :    0   1   2   3
g31n03:688511:688866 [2] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
g31n03:688511:688866 [2] NCCL INFO P2P Chunksize set to 524288
g27n18:839783:840146 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/IPC
g28n03:859953:860315 [0] NCCL INFO Channel 01/0 : 2[0] -> 3[1] via P2P/IPC
g27n18:839785:840147 [2] NCCL INFO Connected all rings
g28n02:792071:792451 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/IPC
g26n01:811333:811686 [2] NCCL INFO Channel 01/0 : 0[2] -> 1[3] via P2P/IPC
g31n03:688514:688869 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g31n03:688514:688869 [5] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
g31n03:688514:688869 [5] NCCL INFO P2P Chunksize set to 524288
g28n02:792072:792450 [3] NCCL INFO Channel 01/0 : 3[3] -> 0[0] via P2P/IPC
g26n01:811335:811691 [4] NCCL INFO Channel 01/0 : 2[4] -> 3[5] via P2P/IPC
g31n03:688512:688868 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g31n03:688512:688868 [3] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
g31n03:688512:688868 [3] NCCL INFO P2P Chunksize set to 524288
g28n02:792069:792449 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/IPC
g26n01:811334:811689 [3] NCCL INFO Channel 01/0 : 1[3] -> 2[4] via P2P/IPC
g31n03:688513:688867 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g31n03:688513:688867 [4] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
g31n03:688513:688867 [4] NCCL INFO P2P Chunksize set to 524288
g26n02:851592:851955 [3] NCCL INFO Connected all rings
g26n02:851592:851955 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/IPC
g31n02:736464:736827 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g31n02:736464:736827 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
g31n02:736464:736827 [1] NCCL INFO P2P Chunksize set to 524288
g31n02:736465:736828 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g31n02:736465:736828 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
g31n02:736465:736828 [2] NCCL INFO P2P Chunksize set to 524288
g31n02:736463:736825 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g31n02:736463:736825 [0] NCCL INFO Channel 00/02 :    0   1   2   3
g31n02:736463:736825 [0] NCCL INFO Channel 01/02 :    0   1   2   3
g31n02:736463:736825 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
g31n02:736463:736825 [0] NCCL INFO P2P Chunksize set to 524288
g31n02:736466:736829 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g31n02:736466:736829 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
g31n02:736466:736829 [3] NCCL INFO P2P Chunksize set to 524288
g27n18:839783:840146 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/IPC
g27n18:839785:840147 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/IPC
g26n02:851591:851954 [2] NCCL INFO Connected all rings
g32n02:753687:754047 [4] NCCL INFO comm 0x1d1bc47b0 rank 0 nranks 4 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xf8329a6eddb14cdd - Init COMPLETE
g32n02:753688:754049 [5] NCCL INFO comm 0x1bd139a50 rank 1 nranks 4 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xf8329a6eddb14cdd - Init COMPLETE
g28n04:841284:841648 [3] NCCL INFO Connected all rings
g28n04:841284:841648 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/IPC
g28n04:841283:841647 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/IPC
g26n02:851592:851955 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/IPC
g26n01:811334:811689 [3] NCCL INFO Connected all rings
g28n04:841281:841649 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/IPC
g28n04:841280:841645 [0] NCCL INFO Connected all rings
g27n18:839785:840147 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/IPC
g28n03:859954:860314 [1] NCCL INFO Channel 00/0 : 3[1] -> 0[4] [send] via NET/IB/0
g28n03:859954:860314 [1] NCCL INFO Channel 01/0 : 3[1] -> 0[4] [send] via NET/IB/0
g28n04:841288:841650 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g28n04:841288:841650 [5] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
g28n04:841288:841650 [5] NCCL INFO P2P Chunksize set to 131072
g31n03:688514:688869 [5] NCCL INFO Channel 00/0 : 3[5] -> 0[2] via P2P/IPC
g27n18:839790:840149 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g27n18:839790:840149 [5] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
g27n18:839790:840149 [5] NCCL INFO P2P Chunksize set to 131072
g31n03:688512:688868 [3] NCCL INFO Channel 00/0 : 1[3] -> 2[4] via P2P/IPC
g28n04:841287:841646 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g28n04:841287:841646 [4] NCCL INFO Channel 00/02 :    0   1   2   3
g28n04:841287:841646 [4] NCCL INFO Channel 01/02 :    0   1   2   3
g28n04:841287:841646 [4] NCCL INFO Trees [0] 1/2/-1->0->-1 [1] 1/-1/-1->0->2
g28n04:841287:841646 [4] NCCL INFO P2P Chunksize set to 131072
g28n04:841287:841646 [4] NCCL INFO Channel 00/0 : 3[1] -> 0[4] [receive] via NET/IB/1
g28n04:841287:841646 [4] NCCL INFO Channel 01/0 : 3[1] -> 0[4] [receive] via NET/IB/1
g28n04:841287:841646 [4] NCCL INFO Channel 00/0 : 0[4] -> 1[5] via P2P/IPC
g26n02:851594:851956 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g26n02:851594:851956 [5] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
g26n02:851594:851956 [5] NCCL INFO P2P Chunksize set to 131072
g26n02:851593:851952 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g26n02:851593:851952 [4] NCCL INFO Channel 00/02 :    0   1   2   3
g26n02:851593:851952 [4] NCCL INFO Channel 01/02 :    0   1   2   3
g26n02:851593:851952 [4] NCCL INFO Trees [0] 1/2/-1->0->-1 [1] 1/-1/-1->0->2
g26n02:851593:851952 [4] NCCL INFO P2P Chunksize set to 131072
g26n02:851593:851952 [4] NCCL INFO Channel 00/0 : 3[1] -> 0[4] [receive] via NET/IB/1
g26n02:851593:851952 [4] NCCL INFO Channel 01/0 : 3[1] -> 0[4] [receive] via NET/IB/1
g26n02:851593:851952 [4] NCCL INFO Channel 00/0 : 0[4] -> 1[5] via P2P/IPC
g27n17:842061:842427 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g27n17:842061:842427 [1] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
g27n17:842061:842427 [1] NCCL INFO P2P Chunksize set to 131072
g26n02:851590:851953 [1] NCCL INFO Connected all rings
g26n01:811334:811689 [3] NCCL INFO Channel 00/0 : 1[3] -> 0[2] via P2P/IPC
g31n03:688511:688866 [2] NCCL INFO Channel 00/0 : 0[2] -> 1[3] via P2P/IPC
g31n01:617757:618124 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g31n01:617757:618124 [1] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
g31n01:617757:618124 [1] NCCL INFO P2P Chunksize set to 131072
g31n03:688513:688867 [4] NCCL INFO Channel 00/0 : 2[4] -> 3[5] via P2P/IPC
g26n01:811334:811689 [3] NCCL INFO Channel 01/0 : 1[3] -> 0[2] via P2P/IPC
g28n02:792076:792448 [5] NCCL INFO Channel 00/0 : 1[5] -> 2[0] [send] via NET/IB/1
g28n02:792076:792448 [5] NCCL INFO Channel 01/0 : 1[5] -> 2[0] [send] via NET/IB/1
g28n04:841284:841648 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/IPC
g31n01:617756:618123 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g31n01:617756:618123 [0] NCCL INFO Trees [0] 3/-1/-1->2->0 [1] 3/0/-1->2->-1
g31n01:617756:618123 [0] NCCL INFO P2P Chunksize set to 131072
g31n01:617756:618123 [0] NCCL INFO Channel 00/0 : 1[5] -> 2[0] [receive] via NET/IB/0
g31n01:617756:618123 [0] NCCL INFO Channel 01/0 : 1[5] -> 2[0] [receive] via NET/IB/0
g31n01:617756:618123 [0] NCCL INFO Channel 00/0 : 2[0] -> 3[1] via P2P/IPC
g31n02:736465:736828 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/IPC
g31n03:688514:688869 [5] NCCL INFO Channel 01/0 : 3[5] -> 0[2] via P2P/IPC
g27n17:842060:842428 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g27n17:842060:842428 [0] NCCL INFO Trees [0] 3/-1/-1->2->0 [1] 3/0/-1->2->-1
g27n17:842060:842428 [0] NCCL INFO P2P Chunksize set to 131072
g27n17:842060:842428 [0] NCCL INFO Channel 00/0 : 1[5] -> 2[0] [receive] via NET/IB/0
g27n17:842060:842428 [0] NCCL INFO Channel 01/0 : 1[5] -> 2[0] [receive] via NET/IB/0
g27n17:842060:842428 [0] NCCL INFO Channel 00/0 : 2[0] -> 3[1] via P2P/IPC
g31n02:736464:736827 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/IPC
g31n03:688512:688868 [3] NCCL INFO Channel 01/0 : 1[3] -> 2[4] via P2P/IPC
g31n02:736463:736825 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/IPC
g28n01:770093:770455 [1] NCCL INFO comm 0x18d46f060 rank 3 nranks 4 cudaDev 1 nvmlDev 1 busId 405000 commId 0x4e7980dedb654e20 - Init START
g28n01:770093:770455 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g28n01:770093:770455 [1] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
g28n01:770093:770455 [1] NCCL INFO P2P Chunksize set to 131072
g31n03:688511:688866 [2] NCCL INFO Channel 01/0 : 0[2] -> 1[3] via P2P/IPC
g31n02:736466:736829 [3] NCCL INFO Channel 00/0 : 3[3] -> 0[0] via P2P/IPC
g28n01:770092:770456 [0] NCCL INFO comm 0x1c169c380 rank 2 nranks 4 cudaDev 0 nvmlDev 0 busId 404000 commId 0x4e7980dedb654e20 - Init START
g28n01:770092:770456 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g28n01:770092:770456 [0] NCCL INFO Trees [0] 3/-1/-1->2->0 [1] 3/0/-1->2->-1
g28n01:770092:770456 [0] NCCL INFO P2P Chunksize set to 131072
g28n01:770092:770456 [0] NCCL INFO Channel 00/0 : 1[5] -> 2[0] [receive] via NET/IB/0
g28n01:770092:770456 [0] NCCL INFO Channel 01/0 : 1[5] -> 2[0] [receive] via NET/IB/0
g28n01:770092:770456 [0] NCCL INFO Channel 00/0 : 2[0] -> 3[1] via P2P/IPC
g31n03:688513:688867 [4] NCCL INFO Channel 01/0 : 2[4] -> 3[5] via P2P/IPC
g28n04:841283:841647 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/IPC
g27n18:839789:840145 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g27n18:839789:840145 [4] NCCL INFO Channel 00/02 :    0   1   2   3
g27n18:839789:840145 [4] NCCL INFO Channel 01/02 :    0   1   2   3
g27n18:839789:840145 [4] NCCL INFO Trees [0] 1/2/-1->0->-1 [1] 1/-1/-1->0->2
g27n18:839789:840145 [4] NCCL INFO P2P Chunksize set to 131072
g27n18:839789:840145 [4] NCCL INFO Channel 00/0 : 3[1] -> 0[4] [receive] via NET/IB/1
g27n18:839789:840145 [4] NCCL INFO Channel 01/0 : 3[1] -> 0[4] [receive] via NET/IB/1
g27n18:839789:840145 [4] NCCL INFO Channel 00/0 : 0[4] -> 1[5] via P2P/IPC
g28n04:841281:841649 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/IPC
g32n03:753097:753452 [1] NCCL INFO comm 0x1ef652810 rank 3 nranks 4 cudaDev 1 nvmlDev 1 busId 405000 commId 0xf8329a6eddb14cdd - Init COMPLETE
g32n03:753096:753453 [0] NCCL INFO comm 0x1ae46b590 rank 2 nranks 4 cudaDev 0 nvmlDev 0 busId 404000 commId 0xf8329a6eddb14cdd - Init COMPLETE
g28n04:841287:841646 [4] NCCL INFO Channel 01/0 : 0[4] -> 1[5] via P2P/IPC
g26n02:851593:851952 [4] NCCL INFO Channel 01/0 : 0[4] -> 1[5] via P2P/IPC
g31n02:736465:736828 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/IPC
g31n02:736463:736825 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/IPC
g26n02:851591:851954 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/IPC
g31n02:736464:736827 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/IPC
g31n02:736466:736829 [3] NCCL INFO Channel 01/0 : 3[3] -> 0[0] via P2P/IPC
g27n18:839782:840144 [0] NCCL INFO Connected all trees
g27n18:839782:840144 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g27n18:839782:840144 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n04:841288:841650 [5] NCCL INFO Channel 00/0 : 1[5] -> 2[0] [send] via NET/IB/1
g28n04:841288:841650 [5] NCCL INFO Channel 01/0 : 1[5] -> 2[0] [send] via NET/IB/1
g26n02:851590:851953 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/IPC
g26n02:851591:851954 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/IPC
g31n05:704349:704703 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g31n05:704349:704703 [5] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
g31n05:704349:704703 [5] NCCL INFO P2P Chunksize set to 524288
g26n02:851590:851953 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/IPC
g31n05:704344:704705 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g27n18:839789:840145 [4] NCCL INFO Channel 01/0 : 0[4] -> 1[5] via P2P/IPC
g31n04:688224:688588 [0] NCCL INFO Connected all trees
g31n04:688224:688588 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g31n04:688224:688588 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n04:688225:688591 [1] NCCL INFO Connected all trees
g31n04:688225:688591 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g31n04:688225:688591 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n04:688227:688592 [3] NCCL INFO Connected all trees
g31n04:688227:688592 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g31n04:688227:688592 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g25n18:898857:899317 [5] NCCL INFO Channel 00/0 : 1[5] -> 2[0] [send] via NET/IB/1
g25n18:898857:899317 [5] NCCL INFO Channel 01/0 : 1[5] -> 2[0] [send] via NET/IB/1
g26n01:811336:811690 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g26n01:811336:811690 [5] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
g26n01:811336:811690 [5] NCCL INFO P2P Chunksize set to 524288
g28n02:792068:792447 [0] NCCL INFO Connected all rings
g28n02:792069:792449 [1] NCCL INFO Connected all rings
g27n18:839787:840148 [3] NCCL INFO Connected all trees
g27n18:839787:840148 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g27n18:839787:840148 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g27n17:842060:842428 [0] NCCL INFO Channel 01/0 : 2[0] -> 3[1] via P2P/IPC
g31n04:688226:688590 [2] NCCL INFO Connected all trees
g31n04:688226:688590 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g31n04:688226:688590 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n01:617756:618123 [0] NCCL INFO Channel 01/0 : 2[0] -> 3[1] via P2P/IPC
g28n01:770092:770456 [0] NCCL INFO Channel 01/0 : 2[0] -> 3[1] via P2P/IPC
g26n02:851594:851956 [5] NCCL INFO Channel 00/0 : 1[5] -> 2[0] [send] via NET/IB/1
g26n02:851594:851956 [5] NCCL INFO Channel 01/0 : 1[5] -> 2[0] [send] via NET/IB/1
g26n02:851592:851955 [3] NCCL INFO Connected all trees
g26n02:851592:851955 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g26n02:851592:851955 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n01:617758:618119 [2] NCCL INFO comm 0x1d6f348b0 rank 0 nranks 4 cudaDev 2 nvmlDev 2 busId 406000 commId 0x6496ef7985dfcdea - Init START
g31n01:617757:618124 [1] NCCL INFO Channel 00/0 : 3[1] -> 0[4] [send] via NET/IB/0
g31n01:617757:618124 [1] NCCL INFO Channel 01/0 : 3[1] -> 0[4] [send] via NET/IB/0
g31n01:617759:618120 [3] NCCL INFO comm 0x1aaef9f10 rank 1 nranks 4 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x6496ef7985dfcdea - Init START
g31n01:617761:618121 [5] NCCL INFO comm 0x1c2e6ad20 rank 3 nranks 4 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x6496ef7985dfcdea - Init START
g27n18:839790:840149 [5] NCCL INFO Channel 00/0 : 1[5] -> 2[0] [send] via NET/IB/1
g27n18:839790:840149 [5] NCCL INFO Channel 01/0 : 1[5] -> 2[0] [send] via NET/IB/1
g28n01:770093:770455 [1] NCCL INFO Channel 00/0 : 3[1] -> 0[4] [send] via NET/IB/0
g28n01:770093:770455 [1] NCCL INFO Channel 01/0 : 3[1] -> 0[4] [send] via NET/IB/0
g27n18:839783:840146 [1] NCCL INFO Connected all trees
g27n18:839783:840146 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g27n18:839783:840146 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n02:792071:792451 [2] NCCL INFO Connected all rings
g28n02:792072:792450 [3] NCCL INFO Connected all rings
g28n02:792072:792450 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/IPC
g27n18:839785:840147 [2] NCCL INFO Connected all trees
g27n18:839785:840147 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g27n18:839785:840147 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n02:792069:792449 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/IPC
g31n03:688511:688866 [2] NCCL INFO Connected all rings
g31n03:688513:688867 [4] NCCL INFO Connected all rings
g31n01:617760:618122 [4] NCCL INFO comm 0x1d09effb0 rank 2 nranks 4 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x6496ef7985dfcdea - Init START
g28n02:792072:792450 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/IPC
g31n05:704345:704706 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g31n05:704345:704706 [1] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
g31n05:704345:704706 [1] NCCL INFO P2P Chunksize set to 131072
g28n04:841284:841648 [3] NCCL INFO Connected all trees
g28n04:841284:841648 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g28n04:841284:841648 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n04:841283:841647 [2] NCCL INFO Connected all trees
g28n04:841283:841647 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g28n04:841283:841647 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n02:792069:792449 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/IPC
g28n02:792071:792451 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/IPC
g31n03:688514:688869 [5] NCCL INFO Connected all rings
g31n03:688514:688869 [5] NCCL INFO Channel 00/0 : 3[5] -> 2[4] via P2P/IPC
g28n04:841280:841645 [0] NCCL INFO Connected all trees
g28n04:841280:841645 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g28n04:841280:841645 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n02:736463:736825 [0] NCCL INFO Connected all rings
g31n05:704344:704705 [0] NCCL INFO Trees [0] 3/-1/-1->2->0 [1] 3/0/-1->2->-1
g31n05:704344:704705 [0] NCCL INFO P2P Chunksize set to 131072
g31n05:704344:704705 [0] NCCL INFO Channel 00/0 : 1[5] -> 2[0] [receive] via NET/IB/0
g31n05:704344:704705 [0] NCCL INFO Channel 01/0 : 1[5] -> 2[0] [receive] via NET/IB/0
g31n05:704344:704705 [0] NCCL INFO Channel 00/0 : 2[0] -> 3[1] via P2P/IPC
g28n04:841281:841649 [1] NCCL INFO Connected all trees
g28n04:841281:841649 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g28n04:841281:841649 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n03:688512:688868 [3] NCCL INFO Connected all rings
g31n02:736467:736826 [4] NCCL INFO Connected all trees
g31n02:736467:736826 [4] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g31n02:736467:736826 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n02:736465:736828 [2] NCCL INFO Connected all rings
g27n17:842061:842427 [1] NCCL INFO Channel 00/0 : 3[1] -> 0[4] [send] via NET/IB/0
g27n17:842061:842427 [1] NCCL INFO Channel 01/0 : 3[1] -> 0[4] [send] via NET/IB/0
g31n05:704344:704705 [0] NCCL INFO Channel 01/0 : 2[0] -> 3[1] via P2P/IPC
g31n02:736464:736827 [1] NCCL INFO Connected all rings
g28n03:859954:860314 [1] NCCL INFO Connected all rings
g28n03:859954:860314 [1] NCCL INFO Channel 00/0 : 3[1] -> 2[0] via P2P/IPC
g31n04:688229:688589 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g31n04:688229:688589 [5] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
g31n04:688229:688589 [5] NCCL INFO P2P Chunksize set to 131072
g31n02:736466:736829 [3] NCCL INFO Connected all rings
g31n02:736466:736829 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/IPC
g26n02:851591:851954 [2] NCCL INFO Connected all trees
g26n02:851591:851954 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g26n02:851591:851954 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n04:688228:688586 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g31n04:688228:688586 [4] NCCL INFO Channel 00/02 :    0   1   2   3
g31n04:688228:688586 [4] NCCL INFO Channel 01/02 :    0   1   2   3
g31n04:688228:688586 [4] NCCL INFO Trees [0] 1/2/-1->0->-1 [1] 1/-1/-1->0->2
g31n04:688228:688586 [4] NCCL INFO P2P Chunksize set to 131072
g31n04:688228:688586 [4] NCCL INFO Channel 00/0 : 3[1] -> 0[4] [receive] via NET/IB/1
g31n04:688228:688586 [4] NCCL INFO Channel 01/0 : 3[1] -> 0[4] [receive] via NET/IB/1
g31n04:688228:688586 [4] NCCL INFO Channel 00/0 : 0[4] -> 1[5] via P2P/IPC
g26n02:851590:851953 [1] NCCL INFO Connected all trees
g26n02:851590:851953 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g26n02:851590:851953 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n04:688228:688586 [4] NCCL INFO Channel 01/0 : 0[4] -> 1[5] via P2P/IPC
g31n03:688514:688869 [5] NCCL INFO Channel 01/0 : 3[5] -> 2[4] via P2P/IPC
g31n03:688509:688870 [0] NCCL INFO Connected all trees
g31n03:688509:688870 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g31n03:688509:688870 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n02:792071:792451 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/IPC
g31n03:688513:688867 [4] NCCL INFO Channel 00/0 : 2[4] -> 1[3] via P2P/IPC
g31n02:736466:736829 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/IPC
g28n02:792076:792448 [5] NCCL INFO Connected all rings
g28n02:792076:792448 [5] NCCL INFO Channel 00/0 : 1[5] -> 0[4] via P2P/IPC
g31n05:704345:704706 [1] NCCL INFO Channel 00/0 : 3[1] -> 0[4] [send] via NET/IB/0
g31n05:704345:704706 [1] NCCL INFO Channel 01/0 : 3[1] -> 0[4] [send] via NET/IB/0
g31n04:688224:688588 [0] NCCL INFO comm 0x1a3f8dbc0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 404000 commId 0x51f76f63babe37f6 - Init COMPLETE
g31n03:688512:688868 [3] NCCL INFO Channel 00/0 : 1[3] -> 0[2] via P2P/IPC
g31n04:688225:688591 [1] NCCL INFO comm 0x1c1758ac0 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 405000 commId 0x51f76f63babe37f6 - Init COMPLETE
g31n03:688513:688867 [4] NCCL INFO Channel 01/0 : 2[4] -> 1[3] via P2P/IPC
g31n04:688226:688590 [2] NCCL INFO comm 0x1f177ecf0 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 406000 commId 0x51f76f63babe37f6 - Init COMPLETE
g31n04:688227:688592 [3] NCCL INFO comm 0x1a9395f60 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x51f76f63babe37f6 - Init COMPLETE
g31n02:736465:736828 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/IPC
g31n02:736464:736827 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/IPC
g26n02:851589:851951 [0] NCCL INFO Connected all trees
g26n02:851589:851951 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g26n02:851589:851951 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n03:688512:688868 [3] NCCL INFO Channel 01/0 : 1[3] -> 0[2] via P2P/IPC
g31n04:688229:688589 [5] NCCL INFO Channel 00/0 : 1[5] -> 2[0] [send] via NET/IB/1
g31n04:688229:688589 [5] NCCL INFO Channel 01/0 : 1[5] -> 2[0] [send] via NET/IB/1
g31n02:736465:736828 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/IPC
g31n02:736464:736827 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/IPC
g28n03:859953:860315 [0] NCCL INFO Connected all rings
g28n03:859953:860315 [0] NCCL INFO Channel 00/0 : 0[4] -> 2[0] [receive] via NET/IB/0
g28n03:859953:860315 [0] NCCL INFO Channel 01/0 : 0[4] -> 2[0] [receive] via NET/IB/0
g28n03:859953:860315 [0] NCCL INFO Channel 00/0 : 2[0] -> 0[4] [send] via NET/IB/0
g28n03:859953:860315 [0] NCCL INFO Channel 01/0 : 2[0] -> 0[4] [send] via NET/IB/0
g27n18:839787:840148 [3] NCCL INFO comm 0x1c5fc4570 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x73978e8b82b8f783 - Init COMPLETE
g27n18:839782:840144 [0] NCCL INFO comm 0x1f0e7d9b0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 404000 commId 0x73978e8b82b8f783 - Init COMPLETE
g28n03:859954:860314 [1] NCCL INFO Channel 01/0 : 3[1] -> 2[0] via P2P/IPC
g27n18:839783:840146 [1] NCCL INFO comm 0x172d0cf10 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 405000 commId 0x73978e8b82b8f783 - Init COMPLETE
g27n18:839785:840147 [2] NCCL INFO comm 0x1d5635800 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 406000 commId 0x73978e8b82b8f783 - Init COMPLETE
g28n02:792076:792448 [5] NCCL INFO Channel 01/0 : 1[5] -> 0[4] via P2P/IPC
g28n02:792075:792446 [4] NCCL INFO Connected all rings
g28n02:792075:792446 [4] NCCL INFO Channel 00/0 : 2[0] -> 0[4] [receive] via NET/IB/1
g28n02:792075:792446 [4] NCCL INFO Channel 01/0 : 2[0] -> 0[4] [receive] via NET/IB/1
g28n02:792075:792446 [4] NCCL INFO Channel 00/0 : 0[4] -> 2[0] [send] via NET/IB/1
g28n02:792075:792446 [4] NCCL INFO Channel 01/0 : 0[4] -> 2[0] [send] via NET/IB/1
g31n02:736467:736826 [4] NCCL INFO comm 0x1b4510c80 rank 0 nranks 4 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x2f29f621d04a681f - Init COMPLETE
g28n02:792068:792447 [0] NCCL INFO Connected all trees
g28n02:792068:792447 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g28n02:792068:792447 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n02:736468:736830 [5] NCCL INFO comm 0x1c2c763e0 rank 1 nranks 4 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x2f29f621d04a681f - Init COMPLETE
g28n02:792072:792450 [3] NCCL INFO Connected all trees
g28n02:792072:792450 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g28n02:792072:792450 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g27n18:839790:840149 [5] NCCL INFO Connected all rings
g27n18:839790:840149 [5] NCCL INFO Channel 00/0 : 1[5] -> 0[4] via P2P/IPC
g31n03:688514:688869 [5] NCCL INFO Connected all trees
g31n03:688514:688869 [5] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g31n03:688514:688869 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g27n18:839790:840149 [5] NCCL INFO Channel 01/0 : 1[5] -> 0[4] via P2P/IPC
g28n04:841284:841648 [3] NCCL INFO comm 0x1e08ad070 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xbe090fc4d31d3749 - Init COMPLETE
g28n04:841280:841645 [0] NCCL INFO comm 0x1b87df1f0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 404000 commId 0xbe090fc4d31d3749 - Init COMPLETE
g28n04:841281:841649 [1] NCCL INFO comm 0x1831f6510 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 405000 commId 0xbe090fc4d31d3749 - Init COMPLETE
g28n04:841283:841647 [2] NCCL INFO comm 0x199f72eb0 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 406000 commId 0xbe090fc4d31d3749 - Init COMPLETE
g26n02:851592:851955 [3] NCCL INFO comm 0x1b2e847f0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xf34d505a1105311f - Init COMPLETE
g26n02:851589:851951 [0] NCCL INFO comm 0x1db256390 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 404000 commId 0xf34d505a1105311f - Init COMPLETE
g26n02:851590:851953 [1] NCCL INFO comm 0x1be2792e0 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 405000 commId 0xf34d505a1105311f - Init COMPLETE
g26n02:851591:851954 [2] NCCL INFO comm 0x1c1236480 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 406000 commId 0xf34d505a1105311f - Init COMPLETE
g31n03:688510:688871 [1] NCCL INFO comm 0x1ada76bf0 rank 3 nranks 4 cudaDev 1 nvmlDev 1 busId 405000 commId 0x2f29f621d04a681f - Init COMPLETE
g26n02:851594:851956 [5] NCCL INFO Connected all rings
g26n02:851594:851956 [5] NCCL INFO Channel 00/0 : 1[5] -> 0[4] via P2P/IPC
g31n03:688509:688870 [0] NCCL INFO comm 0x1c26bd050 rank 2 nranks 4 cudaDev 0 nvmlDev 0 busId 404000 commId 0x2f29f621d04a681f - Init COMPLETE
g28n04:841288:841650 [5] NCCL INFO Connected all rings
g28n04:841288:841650 [5] NCCL INFO Channel 00/0 : 1[5] -> 0[4] via P2P/IPC
g31n02:736466:736829 [3] NCCL INFO Connected all trees
g31n02:736466:736829 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g31n02:736466:736829 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n03:688511:688866 [2] NCCL INFO Connected all trees
g31n03:688511:688866 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g31n03:688511:688866 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n03:688513:688867 [4] NCCL INFO Connected all trees
g31n03:688513:688867 [4] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g31n03:688513:688867 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g26n02:851594:851956 [5] NCCL INFO Channel 01/0 : 1[5] -> 0[4] via P2P/IPC
g31n02:736463:736825 [0] NCCL INFO Connected all trees
g31n02:736463:736825 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g31n02:736463:736825 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n04:841288:841650 [5] NCCL INFO Channel 01/0 : 1[5] -> 0[4] via P2P/IPC
g28n02:792069:792449 [1] NCCL INFO Connected all trees
g28n02:792069:792449 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g28n02:792069:792449 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n02:792071:792451 [2] NCCL INFO Connected all trees
g28n02:792071:792451 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g28n02:792071:792451 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n02:736465:736828 [2] NCCL INFO Connected all trees
g31n02:736465:736828 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g31n02:736465:736828 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n02:736464:736827 [1] NCCL INFO Connected all trees
g31n02:736464:736827 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g31n02:736464:736827 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n03:688512:688868 [3] NCCL INFO Connected all trees
g31n03:688512:688868 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g31n03:688512:688868 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n05:704345:704706 [1] NCCL INFO Connected all rings
g31n05:704345:704706 [1] NCCL INFO Channel 00/0 : 3[1] -> 2[0] via P2P/IPC
g31n01:617757:618124 [1] NCCL INFO Connected all rings
g31n01:617757:618124 [1] NCCL INFO Channel 00/0 : 3[1] -> 2[0] via P2P/IPC
g31n04:688229:688589 [5] NCCL INFO Connected all rings
g31n04:688229:688589 [5] NCCL INFO Channel 00/0 : 1[5] -> 0[4] via P2P/IPC
g27n17:842061:842427 [1] NCCL INFO Connected all rings
g27n17:842061:842427 [1] NCCL INFO Channel 00/0 : 3[1] -> 2[0] via P2P/IPC
g28n01:770093:770455 [1] NCCL INFO Connected all rings
g28n01:770093:770455 [1] NCCL INFO Channel 00/0 : 3[1] -> 2[0] via P2P/IPC
g31n05:704345:704706 [1] NCCL INFO Channel 01/0 : 3[1] -> 2[0] via P2P/IPC
g28n04:841287:841646 [4] NCCL INFO Connected all rings
g28n04:841287:841646 [4] NCCL INFO Channel 00/0 : 2[0] -> 0[4] [receive] via NET/IB/1
g28n04:841287:841646 [4] NCCL INFO Channel 01/0 : 2[0] -> 0[4] [receive] via NET/IB/1
g28n04:841287:841646 [4] NCCL INFO Channel 00/0 : 0[4] -> 2[0] [send] via NET/IB/1
g28n04:841287:841646 [4] NCCL INFO Channel 01/0 : 0[4] -> 2[0] [send] via NET/IB/1
g28n02:792076:792448 [5] NCCL INFO Connected all trees
g28n02:792076:792448 [5] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g28n02:792076:792448 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n04:688229:688589 [5] NCCL INFO Channel 01/0 : 1[5] -> 0[4] via P2P/IPC
g27n18:839789:840145 [4] NCCL INFO Connected all rings
g27n18:839789:840145 [4] NCCL INFO Channel 00/0 : 2[0] -> 0[4] [receive] via NET/IB/1
g27n18:839789:840145 [4] NCCL INFO Channel 01/0 : 2[0] -> 0[4] [receive] via NET/IB/1
g27n18:839789:840145 [4] NCCL INFO Channel 00/0 : 0[4] -> 2[0] [send] via NET/IB/1
g27n18:839789:840145 [4] NCCL INFO Channel 01/0 : 0[4] -> 2[0] [send] via NET/IB/1
g26n02:851593:851952 [4] NCCL INFO Connected all rings
g26n02:851593:851952 [4] NCCL INFO Channel 00/0 : 2[0] -> 0[4] [receive] via NET/IB/1
g26n02:851593:851952 [4] NCCL INFO Channel 01/0 : 2[0] -> 0[4] [receive] via NET/IB/1
g26n02:851593:851952 [4] NCCL INFO Channel 00/0 : 0[4] -> 2[0] [send] via NET/IB/1
g26n02:851593:851952 [4] NCCL INFO Channel 01/0 : 0[4] -> 2[0] [send] via NET/IB/1
g31n04:688228:688586 [4] NCCL INFO Connected all rings
g31n04:688228:688586 [4] NCCL INFO Channel 00/0 : 2[0] -> 0[4] [receive] via NET/IB/1
g31n04:688228:688586 [4] NCCL INFO Channel 01/0 : 2[0] -> 0[4] [receive] via NET/IB/1
g31n04:688228:688586 [4] NCCL INFO Channel 00/0 : 0[4] -> 2[0] [send] via NET/IB/1
g31n04:688228:688586 [4] NCCL INFO Channel 01/0 : 0[4] -> 2[0] [send] via NET/IB/1
g31n05:704344:704705 [0] NCCL INFO Connected all rings
g31n05:704344:704705 [0] NCCL INFO Channel 00/0 : 0[4] -> 2[0] [receive] via NET/IB/0
g31n05:704344:704705 [0] NCCL INFO Channel 01/0 : 0[4] -> 2[0] [receive] via NET/IB/0
g31n05:704344:704705 [0] NCCL INFO Channel 00/0 : 2[0] -> 0[4] [send] via NET/IB/0
g31n05:704344:704705 [0] NCCL INFO Channel 01/0 : 2[0] -> 0[4] [send] via NET/IB/0
g31n01:617757:618124 [1] NCCL INFO Channel 01/0 : 3[1] -> 2[0] via P2P/IPC
g28n01:770092:770456 [0] NCCL INFO Connected all rings
g28n01:770092:770456 [0] NCCL INFO Channel 00/0 : 0[4] -> 2[0] [receive] via NET/IB/0
g28n01:770092:770456 [0] NCCL INFO Channel 01/0 : 0[4] -> 2[0] [receive] via NET/IB/0
g28n01:770092:770456 [0] NCCL INFO Channel 00/0 : 2[0] -> 0[4] [send] via NET/IB/0
g28n01:770092:770456 [0] NCCL INFO Channel 01/0 : 2[0] -> 0[4] [send] via NET/IB/0
g28n01:770093:770455 [1] NCCL INFO Channel 01/0 : 3[1] -> 2[0] via P2P/IPC
g31n01:617756:618123 [0] NCCL INFO Connected all rings
g31n01:617756:618123 [0] NCCL INFO Channel 00/0 : 0[4] -> 2[0] [receive] via NET/IB/0
g31n01:617756:618123 [0] NCCL INFO Channel 01/0 : 0[4] -> 2[0] [receive] via NET/IB/0
g31n01:617756:618123 [0] NCCL INFO Channel 00/0 : 2[0] -> 0[4] [send] via NET/IB/0
g31n01:617756:618123 [0] NCCL INFO Channel 01/0 : 2[0] -> 0[4] [send] via NET/IB/0
g32n03:753098:753454 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g32n03:753098:753454 [2] NCCL INFO Channel 00/02 :    0   1   2   3
g32n03:753098:753454 [2] NCCL INFO Channel 01/02 :    0   1   2   3
g32n03:753098:753454 [2] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
g32n03:753098:753454 [2] NCCL INFO P2P Chunksize set to 524288
g27n17:842061:842427 [1] NCCL INFO Channel 01/0 : 3[1] -> 2[0] via P2P/IPC
g32n03:753099:753456 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g32n03:753099:753456 [3] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
g32n03:753099:753456 [3] NCCL INFO P2P Chunksize set to 524288
g28n04:841288:841650 [5] NCCL INFO Connected all trees
g28n04:841288:841650 [5] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g28n04:841288:841650 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g32n03:753101:753457 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g32n03:753101:753457 [5] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
g32n03:753101:753457 [5] NCCL INFO P2P Chunksize set to 524288
g27n18:839790:840149 [5] NCCL INFO Connected all trees
g27n18:839790:840149 [5] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g27n18:839790:840149 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g27n17:842060:842428 [0] NCCL INFO Connected all rings
g27n17:842060:842428 [0] NCCL INFO Channel 00/0 : 0[4] -> 2[0] [receive] via NET/IB/0
g27n17:842060:842428 [0] NCCL INFO Channel 01/0 : 0[4] -> 2[0] [receive] via NET/IB/0
g27n17:842060:842428 [0] NCCL INFO Channel 00/0 : 2[0] -> 0[4] [send] via NET/IB/0
g27n17:842060:842428 [0] NCCL INFO Channel 01/0 : 2[0] -> 0[4] [send] via NET/IB/0
g26n02:851594:851956 [5] NCCL INFO Connected all trees
g26n02:851594:851956 [5] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g26n02:851594:851956 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n03:688511:688866 [2] NCCL INFO comm 0x13beb20d0 rank 0 nranks 4 cudaDev 2 nvmlDev 2 busId 406000 commId 0xc44305de50f5a865 - Init COMPLETE
g28n02:792068:792447 [0] NCCL INFO comm 0x19ec87f70 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 404000 commId 0x9a4d1656c0da603a - Init COMPLETE
g31n05:704345:704706 [1] NCCL INFO Connected all trees
g31n05:704345:704706 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g31n05:704345:704706 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n03:688512:688868 [3] NCCL INFO comm 0x1d6615380 rank 1 nranks 4 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xc44305de50f5a865 - Init COMPLETE
g28n02:792071:792451 [2] NCCL INFO comm 0x1c4989c80 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 406000 commId 0x9a4d1656c0da603a - Init COMPLETE
g31n03:688513:688867 [4] NCCL INFO comm 0x1ddc15580 rank 2 nranks 4 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xc44305de50f5a865 - Init COMPLETE
g31n03:688514:688869 [5] NCCL INFO comm 0x1e41ff050 rank 3 nranks 4 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xc44305de50f5a865 - Init COMPLETE
g28n02:792069:792449 [1] NCCL INFO comm 0x1ca612370 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 405000 commId 0x9a4d1656c0da603a - Init COMPLETE
g31n04:688229:688589 [5] NCCL INFO Connected all trees
g31n04:688229:688589 [5] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g31n04:688229:688589 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g32n03:753098:753454 [2] NCCL INFO Channel 00/0 : 0[2] -> 1[3] via P2P/IPC
g28n02:792072:792450 [3] NCCL INFO comm 0x1cac2f0b0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x9a4d1656c0da603a - Init COMPLETE
g31n02:736466:736829 [3] NCCL INFO comm 0x1c192b010 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xc1f28ebe8d1bf8e6 - Init COMPLETE
g32n03:753099:753456 [3] NCCL INFO Channel 00/0 : 1[3] -> 2[4] via P2P/IPC
g31n02:736464:736827 [1] NCCL INFO comm 0x188c20060 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 405000 commId 0xc1f28ebe8d1bf8e6 - Init COMPLETE
g32n03:753101:753457 [5] NCCL INFO Channel 00/0 : 3[5] -> 0[2] via P2P/IPC
g31n02:736465:736828 [2] NCCL INFO comm 0x1c42b31e0 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 406000 commId 0xc1f28ebe8d1bf8e6 - Init COMPLETE
g31n02:736463:736825 [0] NCCL INFO comm 0x1149c5dc0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 404000 commId 0xc1f28ebe8d1bf8e6 - Init COMPLETE
g28n03:859954:860314 [1] NCCL INFO Connected all trees
g28n03:859954:860314 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g28n03:859954:860314 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g32n03:753098:753454 [2] NCCL INFO Channel 01/0 : 0[2] -> 1[3] via P2P/IPC
g32n03:753099:753456 [3] NCCL INFO Channel 01/0 : 1[3] -> 2[4] via P2P/IPC
g32n03:753101:753457 [5] NCCL INFO Channel 01/0 : 3[5] -> 0[2] via P2P/IPC
g32n03:753098:753454 [2] NCCL INFO Connected all rings
g25n18:898852:899316 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g25n18:898852:899316 [0] NCCL INFO Channel 00/02 :    0   1   2   3
g25n18:898852:899316 [0] NCCL INFO Channel 01/02 :    0   1   2   3
g25n18:898852:899316 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
g25n18:898852:899316 [0] NCCL INFO P2P Chunksize set to 524288
g25n18:898855:899320 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g25n18:898855:899320 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
g25n18:898855:899320 [3] NCCL INFO P2P Chunksize set to 524288
g26n01:811336:811690 [5] NCCL INFO Channel 00/0 : 3[5] -> 0[2] via P2P/IPC
g26n01:811336:811690 [5] NCCL INFO Channel 01/0 : 3[5] -> 0[2] via P2P/IPC
g25n18:898852:899316 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/IPC
g25n18:898855:899320 [3] NCCL INFO Channel 00/0 : 3[3] -> 0[0] via P2P/IPC
g25n18:898852:899316 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/IPC
g25n18:898855:899320 [3] NCCL INFO Channel 01/0 : 3[3] -> 0[0] via P2P/IPC
g28n01:770093:770455 [1] NCCL INFO Connected all trees
g28n01:770093:770455 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g28n01:770093:770455 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g26n01:811335:811691 [4] NCCL INFO Connected all rings
g31n01:617757:618124 [1] NCCL INFO Connected all trees
g31n01:617757:618124 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g31n01:617757:618124 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g26n01:811333:811686 [2] NCCL INFO Connected all rings
g26n01:811336:811690 [5] NCCL INFO Connected all rings
g26n01:811336:811690 [5] NCCL INFO Channel 00/0 : 3[5] -> 2[4] via P2P/IPC
g28n02:792075:792446 [4] NCCL INFO Connected all trees
g28n02:792075:792446 [4] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g28n02:792075:792446 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g26n01:811336:811690 [5] NCCL INFO Channel 01/0 : 3[5] -> 2[4] via P2P/IPC
g31n04:688228:688586 [4] NCCL INFO Connected all trees
g31n04:688228:688586 [4] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g31n04:688228:688586 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g26n01:811335:811691 [4] NCCL INFO Channel 00/0 : 2[4] -> 1[3] via P2P/IPC
g31n05:704344:704705 [0] NCCL INFO Connected all trees
g31n05:704344:704705 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g31n05:704344:704705 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g26n01:811335:811691 [4] NCCL INFO Channel 01/0 : 2[4] -> 1[3] via P2P/IPC
g27n17:842061:842427 [1] NCCL INFO Connected all trees
g27n17:842061:842427 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g27n17:842061:842427 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n03:859953:860315 [0] NCCL INFO Connected all trees
g28n03:859953:860315 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g28n03:859953:860315 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g32n02:753684:754052 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g32n02:753684:754052 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
g32n02:753684:754052 [1] NCCL INFO P2P Chunksize set to 524288
g26n01:811333:811686 [2] NCCL INFO Connected all trees
g26n01:811333:811686 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g26n01:811333:811686 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g32n02:753685:754050 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g32n02:753685:754050 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
g32n02:753685:754050 [2] NCCL INFO P2P Chunksize set to 524288
g32n02:753683:754048 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g32n02:753683:754048 [0] NCCL INFO Channel 00/02 :    0   1   2   3
g32n02:753683:754048 [0] NCCL INFO Channel 01/02 :    0   1   2   3
g32n02:753683:754048 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
g32n02:753683:754048 [0] NCCL INFO P2P Chunksize set to 524288
g32n02:753686:754051 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g32n02:753686:754051 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
g32n02:753686:754051 [3] NCCL INFO P2P Chunksize set to 524288
g28n02:792075:792446 [4] NCCL INFO comm 0x1a15cd580 rank 0 nranks 4 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xef632f2039341538 - Init COMPLETE
g28n02:792076:792448 [5] NCCL INFO comm 0x1e913d960 rank 1 nranks 4 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xef632f2039341538 - Init COMPLETE
g31n04:688228:688586 [4] NCCL INFO comm 0x1565aef70 rank 0 nranks 4 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xcc471a684c90aea8 - Init COMPLETE
g31n04:688229:688589 [5] NCCL INFO comm 0x19bc4ea10 rank 1 nranks 4 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xcc471a684c90aea8 - Init COMPLETE
g31n05:704349:704703 [5] NCCL INFO Channel 00/0 : 3[5] -> 0[2] via P2P/IPC
g31n05:704349:704703 [5] NCCL INFO Channel 01/0 : 3[5] -> 0[2] via P2P/IPC
g32n02:753684:754052 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/IPC
g31n05:704344:704705 [0] NCCL INFO comm 0x1e8507d10 rank 2 nranks 4 cudaDev 0 nvmlDev 0 busId 404000 commId 0xcc471a684c90aea8 - Init COMPLETE
g32n02:753685:754050 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/IPC
g31n05:704345:704706 [1] NCCL INFO comm 0x1a17ecfd0 rank 3 nranks 4 cudaDev 1 nvmlDev 1 busId 405000 commId 0xcc471a684c90aea8 - Init COMPLETE
g32n02:753683:754048 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/IPC
g32n02:753686:754051 [3] NCCL INFO Channel 00/0 : 3[3] -> 0[0] via P2P/IPC
g32n02:753683:754048 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/IPC
g28n03:859953:860315 [0] NCCL INFO comm 0x1e469cd80 rank 2 nranks 4 cudaDev 0 nvmlDev 0 busId 404000 commId 0xef632f2039341538 - Init COMPLETE
g32n02:753685:754050 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/IPC
g28n03:859954:860314 [1] NCCL INFO comm 0x1483f6020 rank 3 nranks 4 cudaDev 1 nvmlDev 1 busId 405000 commId 0xef632f2039341538 - Init COMPLETE
g32n02:753684:754052 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/IPC
g32n02:753686:754051 [3] NCCL INFO Channel 01/0 : 3[3] -> 0[0] via P2P/IPC
g28n03:859956:860313 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g28n03:859956:860313 [2] NCCL INFO Channel 00/02 :    0   1   2   3
g28n03:859956:860313 [2] NCCL INFO Channel 01/02 :    0   1   2   3
g28n03:859956:860313 [2] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
g28n03:859956:860313 [2] NCCL INFO P2P Chunksize set to 524288
g28n03:859960:860316 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g28n03:859960:860316 [4] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
g28n03:859960:860316 [4] NCCL INFO P2P Chunksize set to 524288
g28n03:859961:860317 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g28n03:859961:860317 [5] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
g28n03:859961:860317 [5] NCCL INFO P2P Chunksize set to 524288
g28n03:859958:860318 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g28n03:859958:860318 [3] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
g28n03:859958:860318 [3] NCCL INFO P2P Chunksize set to 524288
g26n01:811334:811689 [3] NCCL INFO Connected all trees
g26n01:811334:811689 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g26n01:811334:811689 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n05:704348:704702 [4] NCCL INFO Connected all rings
g26n01:811335:811691 [4] NCCL INFO Connected all trees
g26n01:811335:811691 [4] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g26n01:811335:811691 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n05:704349:704703 [5] NCCL INFO Connected all rings
g31n05:704349:704703 [5] NCCL INFO Channel 00/0 : 3[5] -> 2[4] via P2P/IPC
g28n03:859960:860316 [4] NCCL INFO Channel 00/0 : 2[4] -> 3[5] via P2P/IPC
g28n03:859961:860317 [5] NCCL INFO Channel 00/0 : 3[5] -> 0[2] via P2P/IPC
g26n01:811336:811690 [5] NCCL INFO Connected all trees
g26n01:811336:811690 [5] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g26n01:811336:811690 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n03:859956:860313 [2] NCCL INFO Channel 00/0 : 0[2] -> 1[3] via P2P/IPC
g28n03:859958:860318 [3] NCCL INFO Channel 00/0 : 1[3] -> 2[4] via P2P/IPC
g31n05:704349:704703 [5] NCCL INFO Channel 01/0 : 3[5] -> 2[4] via P2P/IPC
g28n03:859960:860316 [4] NCCL INFO Channel 01/0 : 2[4] -> 3[5] via P2P/IPC
g28n03:859961:860317 [5] NCCL INFO Channel 01/0 : 3[5] -> 0[2] via P2P/IPC
g31n05:704346:704701 [2] NCCL INFO Connected all rings
g31n05:704348:704702 [4] NCCL INFO Channel 00/0 : 2[4] -> 1[3] via P2P/IPC
g28n03:859958:860318 [3] NCCL INFO Channel 01/0 : 1[3] -> 2[4] via P2P/IPC
g28n03:859956:860313 [2] NCCL INFO Channel 01/0 : 0[2] -> 1[3] via P2P/IPC
g27n18:839789:840145 [4] NCCL INFO Connected all trees
g27n18:839789:840145 [4] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g27n18:839789:840145 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n05:704348:704702 [4] NCCL INFO Channel 01/0 : 2[4] -> 1[3] via P2P/IPC
g26n02:851593:851952 [4] NCCL INFO Connected all trees
g26n02:851593:851952 [4] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g26n02:851593:851952 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g32n02:753685:754050 [2] NCCL INFO Connected all rings
g32n02:753684:754052 [1] NCCL INFO Connected all rings
g26n01:811332:811688 [1] NCCL INFO Connected all rings
g26n01:811332:811688 [1] NCCL INFO Channel 00/0 : 3[1] -> 2[0] via P2P/IPC
g32n02:753683:754048 [0] NCCL INFO Connected all rings
g32n02:753686:754051 [3] NCCL INFO Connected all rings
g32n02:753686:754051 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/IPC
g26n01:811332:811688 [1] NCCL INFO Channel 01/0 : 3[1] -> 2[0] via P2P/IPC
g32n02:753686:754051 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/IPC
g28n04:841287:841646 [4] NCCL INFO Connected all trees
g28n04:841287:841646 [4] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g28n04:841287:841646 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n01:770092:770456 [0] NCCL INFO Connected all trees
g28n01:770092:770456 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g28n01:770092:770456 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g32n02:753685:754050 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/IPC
g32n02:753684:754052 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/IPC
g26n01:811333:811686 [2] NCCL INFO comm 0x1a555de30 rank 0 nranks 4 cudaDev 2 nvmlDev 2 busId 406000 commId 0x34c28deeb8633891 - Init COMPLETE
g26n01:811334:811689 [3] NCCL INFO comm 0x1b20bc7d0 rank 1 nranks 4 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x34c28deeb8633891 - Init COMPLETE
g26n01:811335:811691 [4] NCCL INFO comm 0x186131c30 rank 2 nranks 4 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x34c28deeb8633891 - Init COMPLETE
g26n01:811336:811690 [5] NCCL INFO comm 0x1bb0983c0 rank 3 nranks 4 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x34c28deeb8633891 - Init COMPLETE
g25n18:898853:899318 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g25n18:898853:899318 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
g25n18:898853:899318 [1] NCCL INFO P2P Chunksize set to 524288
g25n18:898857:899317 [5] NCCL INFO Connected all rings
g25n18:898857:899317 [5] NCCL INFO Channel 00/0 : 1[5] -> 0[4] via P2P/IPC
g32n02:753685:754050 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/IPC
g31n05:704349:704703 [5] NCCL INFO Connected all trees
g31n05:704349:704703 [5] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g31n05:704349:704703 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g32n02:753684:754052 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/IPC
g25n18:898854:899319 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g25n18:898854:899319 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
g25n18:898854:899319 [2] NCCL INFO P2P Chunksize set to 524288
g31n05:704346:704701 [2] NCCL INFO Connected all trees
g31n05:704346:704701 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g31n05:704346:704701 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g26n01:811331:811687 [0] NCCL INFO Connected all rings
g26n01:811331:811687 [0] NCCL INFO Channel 00/0 : 0[4] -> 2[0] [receive] via NET/IB/0
g26n01:811331:811687 [0] NCCL INFO Channel 01/0 : 0[4] -> 2[0] [receive] via NET/IB/0
g26n01:811331:811687 [0] NCCL INFO Channel 00/0 : 2[0] -> 0[4] [send] via NET/IB/0
g26n01:811331:811687 [0] NCCL INFO Channel 01/0 : 2[0] -> 0[4] [send] via NET/IB/0
g28n03:859960:860316 [4] NCCL INFO Connected all rings
g27n18:839789:840145 [4] NCCL INFO comm 0x1b1aa78a0 rank 0 nranks 4 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x4e7980dedb654e20 - Init COMPLETE
g27n18:839790:840149 [5] NCCL INFO comm 0x166dcf1e0 rank 1 nranks 4 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x4e7980dedb654e20 - Init COMPLETE
g25n18:898856:899314 [4] NCCL INFO Connected all rings
g25n18:898856:899314 [4] NCCL INFO Channel 00/0 : 2[0] -> 0[4] [receive] via NET/IB/1
g25n18:898856:899314 [4] NCCL INFO Channel 01/0 : 2[0] -> 0[4] [receive] via NET/IB/1
g25n18:898856:899314 [4] NCCL INFO Channel 00/0 : 0[4] -> 2[0] [send] via NET/IB/1
g25n18:898856:899314 [4] NCCL INFO Channel 01/0 : 0[4] -> 2[0] [send] via NET/IB/1
g28n03:859956:860313 [2] NCCL INFO Connected all rings
g26n02:851593:851952 [4] NCCL INFO comm 0x1c3358f80 rank 0 nranks 4 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xdde4cc035ecac97b - Init COMPLETE
g28n03:859958:860318 [3] NCCL INFO Connected all rings
g27n17:842060:842428 [0] NCCL INFO Connected all trees
g27n17:842060:842428 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g27n17:842060:842428 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g26n02:851594:851956 [5] NCCL INFO comm 0x1e5c2c0d0 rank 1 nranks 4 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xdde4cc035ecac97b - Init COMPLETE
g25n18:898857:899317 [5] NCCL INFO Channel 01/0 : 1[5] -> 0[4] via P2P/IPC
g31n05:704348:704702 [4] NCCL INFO Connected all trees
g31n05:704348:704702 [4] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g31n05:704348:704702 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g25n18:898853:899318 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/IPC
g25n18:898854:899319 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/IPC
g28n03:859960:860316 [4] NCCL INFO Channel 00/0 : 2[4] -> 1[3] via P2P/IPC
g28n03:859961:860317 [5] NCCL INFO Connected all rings
g28n03:859961:860317 [5] NCCL INFO Channel 00/0 : 3[5] -> 2[4] via P2P/IPC
g28n04:841287:841646 [4] NCCL INFO comm 0x1c1b8ea30 rank 0 nranks 4 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x24db34cf1c66119e - Init COMPLETE
g28n04:841288:841650 [5] NCCL INFO comm 0x1db0038c0 rank 1 nranks 4 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x24db34cf1c66119e - Init COMPLETE
g31n01:617756:618123 [0] NCCL INFO Connected all trees
g31n01:617756:618123 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g31n01:617756:618123 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g26n01:811332:811688 [1] NCCL INFO Connected all trees
g26n01:811332:811688 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g26n01:811332:811688 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n03:859958:860318 [3] NCCL INFO Channel 00/0 : 1[3] -> 0[2] via P2P/IPC
g31n05:704347:704704 [3] NCCL INFO Connected all trees
g31n05:704347:704704 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g31n05:704347:704704 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n03:859960:860316 [4] NCCL INFO Channel 01/0 : 2[4] -> 1[3] via P2P/IPC
g28n03:859961:860317 [5] NCCL INFO Channel 01/0 : 3[5] -> 2[4] via P2P/IPC
g25n18:898854:899319 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/IPC
g28n03:859958:860318 [3] NCCL INFO Channel 01/0 : 1[3] -> 0[2] via P2P/IPC
g32n02:753686:754051 [3] NCCL INFO Connected all trees
g32n02:753686:754051 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g32n02:753686:754051 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g32n02:753683:754048 [0] NCCL INFO Connected all trees
g32n02:753683:754048 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g32n02:753683:754048 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g25n18:898853:899318 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/IPC
g28n01:770092:770456 [0] NCCL INFO comm 0x1c169c380 rank 2 nranks 4 cudaDev 0 nvmlDev 0 busId 404000 commId 0x4e7980dedb654e20 - Init COMPLETE
g28n01:770093:770455 [1] NCCL INFO comm 0x18d46f060 rank 3 nranks 4 cudaDev 1 nvmlDev 1 busId 405000 commId 0x4e7980dedb654e20 - Init COMPLETE
g25n18:898857:899317 [5] NCCL INFO Connected all trees
g25n18:898857:899317 [5] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g25n18:898857:899317 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g32n02:753685:754050 [2] NCCL INFO Connected all trees
g32n02:753685:754050 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g32n02:753685:754050 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g32n02:753684:754052 [1] NCCL INFO Connected all trees
g32n02:753684:754052 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g32n02:753684:754052 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n03:859961:860317 [5] NCCL INFO Connected all trees
g28n03:859961:860317 [5] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g28n03:859961:860317 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g27n17:842060:842428 [0] NCCL INFO comm 0x1bda86a80 rank 2 nranks 4 cudaDev 0 nvmlDev 0 busId 404000 commId 0xdde4cc035ecac97b - Init COMPLETE
g27n17:842061:842427 [1] NCCL INFO comm 0x1a8ffd220 rank 3 nranks 4 cudaDev 1 nvmlDev 1 busId 405000 commId 0xdde4cc035ecac97b - Init COMPLETE
g28n03:859956:860313 [2] NCCL INFO Connected all trees
g28n03:859956:860313 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g28n03:859956:860313 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n03:859960:860316 [4] NCCL INFO Connected all trees
g28n03:859960:860316 [4] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g28n03:859960:860316 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g25n18:898855:899320 [3] NCCL INFO Connected all rings
g25n18:898855:899320 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/IPC
g31n05:704346:704701 [2] NCCL INFO comm 0x1c5c00e80 rank 0 nranks 4 cudaDev 2 nvmlDev 2 busId 406000 commId 0x91e4d8ed552aed23 - Init COMPLETE
g31n05:704347:704704 [3] NCCL INFO comm 0x1e03c8b10 rank 1 nranks 4 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x91e4d8ed552aed23 - Init COMPLETE
g31n05:704348:704702 [4] NCCL INFO comm 0x19a8819a0 rank 2 nranks 4 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x91e4d8ed552aed23 - Init COMPLETE
g31n05:704349:704703 [5] NCCL INFO comm 0x1b57dc530 rank 3 nranks 4 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x91e4d8ed552aed23 - Init COMPLETE
g28n03:859958:860318 [3] NCCL INFO Connected all trees
g28n03:859958:860318 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g28n03:859958:860318 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n01:617757:618124 [1] NCCL INFO comm 0x1a2ccf8f0 rank 3 nranks 4 cudaDev 1 nvmlDev 1 busId 405000 commId 0x24db34cf1c66119e - Init COMPLETE
g31n01:617756:618123 [0] NCCL INFO comm 0x1e436b870 rank 2 nranks 4 cudaDev 0 nvmlDev 0 busId 404000 commId 0x24db34cf1c66119e - Init COMPLETE
g25n18:898854:899319 [2] NCCL INFO Connected all rings
g25n18:898855:899320 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/IPC
g25n18:898853:899318 [1] NCCL INFO Connected all rings
g28n01:770095:770451 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g28n01:770095:770451 [2] NCCL INFO Channel 00/02 :    0   1   2   3
g28n01:770095:770451 [2] NCCL INFO Channel 01/02 :    0   1   2   3
g28n01:770095:770451 [2] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
g28n01:770095:770451 [2] NCCL INFO P2P Chunksize set to 524288
g32n02:753686:754051 [3] NCCL INFO comm 0x18678d940 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x6b4cabcca34779f8 - Init COMPLETE
g28n01:770096:770453 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g28n01:770096:770453 [3] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
g28n01:770096:770453 [3] NCCL INFO P2P Chunksize set to 524288
g28n01:770099:770454 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g28n01:770099:770454 [4] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
g28n01:770099:770454 [4] NCCL INFO P2P Chunksize set to 524288
g32n02:753683:754048 [0] NCCL INFO comm 0x19a809900 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 404000 commId 0x6b4cabcca34779f8 - Init COMPLETE
g32n02:753684:754052 [1] NCCL INFO comm 0x1bb2d1710 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 405000 commId 0x6b4cabcca34779f8 - Init COMPLETE
g32n02:753685:754050 [2] NCCL INFO comm 0x1b4859150 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 406000 commId 0x6b4cabcca34779f8 - Init COMPLETE
g28n01:770100:770452 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g28n01:770100:770452 [5] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
g28n01:770100:770452 [5] NCCL INFO P2P Chunksize set to 524288
g25n18:898854:899319 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/IPC
g25n18:898854:899319 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/IPC
g25n18:898853:899318 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/IPC
g27n17:842067:842426 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g27n17:842067:842426 [4] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
g27n17:842067:842426 [4] NCCL INFO P2P Chunksize set to 524288
g27n17:842068:842425 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g27n17:842068:842425 [5] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
g27n17:842068:842425 [5] NCCL INFO P2P Chunksize set to 524288
g27n17:842063:842423 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g27n17:842063:842423 [2] NCCL INFO Channel 00/02 :    0   1   2   3
g27n17:842063:842423 [2] NCCL INFO Channel 01/02 :    0   1   2   3
g27n17:842063:842423 [2] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
g27n17:842063:842423 [2] NCCL INFO P2P Chunksize set to 524288
g28n01:770095:770451 [2] NCCL INFO Channel 00/0 : 0[2] -> 1[3] via P2P/IPC
g27n17:842064:842424 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g27n17:842064:842424 [3] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
g27n17:842064:842424 [3] NCCL INFO P2P Chunksize set to 524288
g25n18:898852:899316 [0] NCCL INFO Connected all rings
g28n01:770096:770453 [3] NCCL INFO Channel 00/0 : 1[3] -> 2[4] via P2P/IPC
g25n18:898853:899318 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/IPC
g28n01:770099:770454 [4] NCCL INFO Channel 00/0 : 2[4] -> 3[5] via P2P/IPC
g28n01:770100:770452 [5] NCCL INFO Channel 00/0 : 3[5] -> 0[2] via P2P/IPC
g28n01:770095:770451 [2] NCCL INFO Channel 01/0 : 0[2] -> 1[3] via P2P/IPC
g28n01:770099:770454 [4] NCCL INFO Channel 01/0 : 2[4] -> 3[5] via P2P/IPC
g28n03:859956:860313 [2] NCCL INFO comm 0x1a7e047b0 rank 0 nranks 4 cudaDev 2 nvmlDev 2 busId 406000 commId 0x6617c45e414945a1 - Init COMPLETE
g28n01:770096:770453 [3] NCCL INFO Channel 01/0 : 1[3] -> 2[4] via P2P/IPC
g28n03:859958:860318 [3] NCCL INFO comm 0x1cdb076e0 rank 1 nranks 4 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x6617c45e414945a1 - Init COMPLETE
g28n01:770100:770452 [5] NCCL INFO Channel 01/0 : 3[5] -> 0[2] via P2P/IPC
g28n03:859961:860317 [5] NCCL INFO comm 0x1cda5ebf0 rank 3 nranks 4 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x6617c45e414945a1 - Init COMPLETE
g28n03:859960:860316 [4] NCCL INFO comm 0x1cb689e00 rank 2 nranks 4 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x6617c45e414945a1 - Init COMPLETE
g27n17:842064:842424 [3] NCCL INFO Channel 00/0 : 1[3] -> 2[4] via P2P/IPC
g27n17:842068:842425 [5] NCCL INFO Channel 00/0 : 3[5] -> 0[2] via P2P/IPC
g27n17:842067:842426 [4] NCCL INFO Channel 00/0 : 2[4] -> 3[5] via P2P/IPC
g27n17:842063:842423 [2] NCCL INFO Channel 00/0 : 0[2] -> 1[3] via P2P/IPC
g32n03:753100:753455 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g32n03:753100:753455 [4] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
g32n03:753100:753455 [4] NCCL INFO P2P Chunksize set to 524288
g25n18:898855:899320 [3] NCCL INFO Connected all trees
g25n18:898855:899320 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g25n18:898855:899320 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g27n17:842067:842426 [4] NCCL INFO Channel 01/0 : 2[4] -> 3[5] via P2P/IPC
g27n17:842068:842425 [5] NCCL INFO Channel 01/0 : 3[5] -> 0[2] via P2P/IPC
g27n17:842063:842423 [2] NCCL INFO Channel 01/0 : 0[2] -> 1[3] via P2P/IPC
g27n17:842064:842424 [3] NCCL INFO Channel 01/0 : 1[3] -> 2[4] via P2P/IPC
g26n01:811331:811687 [0] NCCL INFO Connected all trees
g26n01:811331:811687 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g26n01:811331:811687 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n01:617758:618119 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g31n01:617758:618119 [2] NCCL INFO Channel 00/02 :    0   1   2   3
g31n01:617758:618119 [2] NCCL INFO Channel 01/02 :    0   1   2   3
g31n01:617758:618119 [2] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
g31n01:617758:618119 [2] NCCL INFO P2P Chunksize set to 524288
g31n01:617760:618122 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g31n01:617760:618122 [4] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
g31n01:617760:618122 [4] NCCL INFO P2P Chunksize set to 524288
g31n01:617761:618121 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g31n01:617761:618121 [5] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
g31n01:617761:618121 [5] NCCL INFO P2P Chunksize set to 524288
g31n01:617759:618120 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g31n01:617759:618120 [3] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
g31n01:617759:618120 [3] NCCL INFO P2P Chunksize set to 524288
g25n18:898853:899318 [1] NCCL INFO Connected all trees
g25n18:898853:899318 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g25n18:898853:899318 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n01:770096:770453 [3] NCCL INFO Connected all rings
g28n01:770099:770454 [4] NCCL INFO Connected all rings
g31n01:617759:618120 [3] NCCL INFO Channel 00/0 : 1[3] -> 2[4] via P2P/IPC
g25n18:898854:899319 [2] NCCL INFO Connected all trees
g25n18:898854:899319 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g25n18:898854:899319 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n01:617761:618121 [5] NCCL INFO Channel 00/0 : 3[5] -> 0[2] via P2P/IPC
g28n01:770095:770451 [2] NCCL INFO Connected all rings
g25n18:898852:899316 [0] NCCL INFO Connected all trees
g25n18:898852:899316 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g25n18:898852:899316 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n01:617758:618119 [2] NCCL INFO Channel 00/0 : 0[2] -> 1[3] via P2P/IPC
g31n01:617760:618122 [4] NCCL INFO Channel 00/0 : 2[4] -> 3[5] via P2P/IPC
g28n01:770100:770452 [5] NCCL INFO Connected all rings
g28n01:770100:770452 [5] NCCL INFO Channel 00/0 : 3[5] -> 2[4] via P2P/IPC
g25n18:898856:899314 [4] NCCL INFO Connected all trees
g25n18:898856:899314 [4] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g25n18:898856:899314 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n01:617760:618122 [4] NCCL INFO Channel 01/0 : 2[4] -> 3[5] via P2P/IPC
g27n17:842067:842426 [4] NCCL INFO Connected all rings
g27n17:842068:842425 [5] NCCL INFO Connected all rings
g27n17:842068:842425 [5] NCCL INFO Channel 00/0 : 3[5] -> 2[4] via P2P/IPC
g31n01:617758:618119 [2] NCCL INFO Channel 01/0 : 0[2] -> 1[3] via P2P/IPC
g31n01:617761:618121 [5] NCCL INFO Channel 01/0 : 3[5] -> 0[2] via P2P/IPC
g31n01:617759:618120 [3] NCCL INFO Channel 01/0 : 1[3] -> 2[4] via P2P/IPC
g28n01:770100:770452 [5] NCCL INFO Channel 01/0 : 3[5] -> 2[4] via P2P/IPC
g28n01:770099:770454 [4] NCCL INFO Channel 00/0 : 2[4] -> 1[3] via P2P/IPC
g28n01:770096:770453 [3] NCCL INFO Channel 00/0 : 1[3] -> 0[2] via P2P/IPC
g28n01:770099:770454 [4] NCCL INFO Channel 01/0 : 2[4] -> 1[3] via P2P/IPC
g28n01:770096:770453 [3] NCCL INFO Channel 01/0 : 1[3] -> 0[2] via P2P/IPC
g27n17:842068:842425 [5] NCCL INFO Channel 01/0 : 3[5] -> 2[4] via P2P/IPC
g27n17:842063:842423 [2] NCCL INFO Connected all rings
g27n17:842064:842424 [3] NCCL INFO Connected all rings
g27n17:842067:842426 [4] NCCL INFO Channel 00/0 : 2[4] -> 1[3] via P2P/IPC
g27n17:842067:842426 [4] NCCL INFO Channel 01/0 : 2[4] -> 1[3] via P2P/IPC
g27n17:842064:842424 [3] NCCL INFO Channel 00/0 : 1[3] -> 0[2] via P2P/IPC
g27n17:842064:842424 [3] NCCL INFO Channel 01/0 : 1[3] -> 0[2] via P2P/IPC
g25n18:898855:899320 [3] NCCL INFO comm 0x1c3d15ad0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x9a30262117f4b78d - Init COMPLETE
g25n18:898854:899319 [2] NCCL INFO comm 0x1bb59ed90 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 406000 commId 0x9a30262117f4b78d - Init COMPLETE
g25n18:898856:899314 [4] NCCL INFO comm 0x14d79c2c0 rank 0 nranks 4 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x960bdb9dee6b6c07 - Init COMPLETE
g25n18:898853:899318 [1] NCCL INFO comm 0x1bc6392e0 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 405000 commId 0x9a30262117f4b78d - Init COMPLETE
g25n18:898852:899316 [0] NCCL INFO comm 0x13b80adf0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 404000 commId 0x9a30262117f4b78d - Init COMPLETE
g25n18:898857:899317 [5] NCCL INFO comm 0x193769450 rank 1 nranks 4 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x960bdb9dee6b6c07 - Init COMPLETE
g31n01:617759:618120 [3] NCCL INFO Connected all rings
g31n01:617760:618122 [4] NCCL INFO Connected all rings
g26n01:811331:811687 [0] NCCL INFO comm 0x1ad514000 rank 2 nranks 4 cudaDev 0 nvmlDev 0 busId 404000 commId 0x960bdb9dee6b6c07 - Init COMPLETE
g26n01:811332:811688 [1] NCCL INFO comm 0x1b327bab0 rank 3 nranks 4 cudaDev 1 nvmlDev 1 busId 405000 commId 0x960bdb9dee6b6c07 - Init COMPLETE
g28n01:770100:770452 [5] NCCL INFO Connected all trees
g28n01:770100:770452 [5] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g28n01:770100:770452 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n01:770095:770451 [2] NCCL INFO Connected all trees
g28n01:770095:770451 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g28n01:770095:770451 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g27n17:842068:842425 [5] NCCL INFO Connected all trees
g27n17:842068:842425 [5] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g27n17:842068:842425 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n01:617758:618119 [2] NCCL INFO Connected all rings
g31n01:617761:618121 [5] NCCL INFO Connected all rings
g31n01:617761:618121 [5] NCCL INFO Channel 00/0 : 3[5] -> 2[4] via P2P/IPC
g28n01:770099:770454 [4] NCCL INFO Connected all trees
g28n01:770099:770454 [4] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g28n01:770099:770454 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n01:770096:770453 [3] NCCL INFO Connected all trees
g28n01:770096:770453 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g28n01:770096:770453 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n01:617759:618120 [3] NCCL INFO Channel 00/0 : 1[3] -> 0[2] via P2P/IPC
g31n01:617760:618122 [4] NCCL INFO Channel 00/0 : 2[4] -> 1[3] via P2P/IPC
g31n01:617761:618121 [5] NCCL INFO Channel 01/0 : 3[5] -> 2[4] via P2P/IPC
g27n17:842063:842423 [2] NCCL INFO Connected all trees
g27n17:842063:842423 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g27n17:842063:842423 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n01:617759:618120 [3] NCCL INFO Channel 01/0 : 1[3] -> 0[2] via P2P/IPC
g31n01:617760:618122 [4] NCCL INFO Channel 01/0 : 2[4] -> 1[3] via P2P/IPC
g27n17:842067:842426 [4] NCCL INFO Connected all trees
g27n17:842067:842426 [4] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g27n17:842067:842426 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g27n17:842064:842424 [3] NCCL INFO Connected all trees
g27n17:842064:842424 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g27n17:842064:842424 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n01:617758:618119 [2] NCCL INFO Connected all trees
g31n01:617758:618119 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g31n01:617758:618119 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n01:770095:770451 [2] NCCL INFO comm 0x1c174a7c0 rank 0 nranks 4 cudaDev 2 nvmlDev 2 busId 406000 commId 0x8bd3134fb91d287 - Init COMPLETE
g28n01:770100:770452 [5] NCCL INFO comm 0x1aecc7130 rank 3 nranks 4 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x8bd3134fb91d287 - Init COMPLETE
g28n01:770099:770454 [4] NCCL INFO comm 0x1b200cec0 rank 2 nranks 4 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x8bd3134fb91d287 - Init COMPLETE
g28n01:770096:770453 [3] NCCL INFO comm 0x1e55f3610 rank 1 nranks 4 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x8bd3134fb91d287 - Init COMPLETE
g31n01:617761:618121 [5] NCCL INFO Connected all trees
g31n01:617761:618121 [5] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g31n01:617761:618121 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n01:617759:618120 [3] NCCL INFO Connected all trees
g31n01:617759:618120 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g31n01:617759:618120 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n01:617760:618122 [4] NCCL INFO Connected all trees
g31n01:617760:618122 [4] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g31n01:617760:618122 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g27n17:842063:842423 [2] NCCL INFO comm 0x19acce640 rank 0 nranks 4 cudaDev 2 nvmlDev 2 busId 406000 commId 0x943bcf09c0d96019 - Init COMPLETE
g27n17:842068:842425 [5] NCCL INFO comm 0x1c1331490 rank 3 nranks 4 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x943bcf09c0d96019 - Init COMPLETE
g27n17:842067:842426 [4] NCCL INFO comm 0x18b8774c0 rank 2 nranks 4 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x943bcf09c0d96019 - Init COMPLETE
g27n17:842064:842424 [3] NCCL INFO comm 0x1c9f9a2c0 rank 1 nranks 4 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x943bcf09c0d96019 - Init COMPLETE
g32n03:753100:753455 [4] NCCL INFO Channel 00/0 : 2[4] -> 3[5] via P2P/IPC
g32n03:753100:753455 [4] NCCL INFO Channel 01/0 : 2[4] -> 3[5] via P2P/IPC
g31n01:617758:618119 [2] NCCL INFO comm 0x1d6f348b0 rank 0 nranks 4 cudaDev 2 nvmlDev 2 busId 406000 commId 0x6496ef7985dfcdea - Init COMPLETE
g31n01:617759:618120 [3] NCCL INFO comm 0x1aaef9f10 rank 1 nranks 4 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x6496ef7985dfcdea - Init COMPLETE
g31n01:617760:618122 [4] NCCL INFO comm 0x1d09effb0 rank 2 nranks 4 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x6496ef7985dfcdea - Init COMPLETE
g31n01:617761:618121 [5] NCCL INFO comm 0x1c2e6ad20 rank 3 nranks 4 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x6496ef7985dfcdea - Init COMPLETE
g32n03:753101:753457 [5] NCCL INFO Connected all rings
g32n03:753101:753457 [5] NCCL INFO Channel 00/0 : 3[5] -> 2[4] via P2P/IPC
g32n03:753100:753455 [4] NCCL INFO Connected all rings
g32n03:753099:753456 [3] NCCL INFO Connected all rings
g32n03:753100:753455 [4] NCCL INFO Channel 00/0 : 2[4] -> 1[3] via P2P/IPC
g32n03:753099:753456 [3] NCCL INFO Channel 00/0 : 1[3] -> 0[2] via P2P/IPC
g32n03:753100:753455 [4] NCCL INFO Channel 01/0 : 2[4] -> 1[3] via P2P/IPC
g32n03:753099:753456 [3] NCCL INFO Channel 01/0 : 1[3] -> 0[2] via P2P/IPC
g32n03:753098:753454 [2] NCCL INFO Connected all trees
g32n03:753098:753454 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g32n03:753098:753454 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g32n03:753101:753457 [5] NCCL INFO Channel 01/0 : 3[5] -> 2[4] via P2P/IPC
g32n03:753099:753456 [3] NCCL INFO Connected all trees
g32n03:753099:753456 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g32n03:753099:753456 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g32n03:753100:753455 [4] NCCL INFO Connected all trees
g32n03:753100:753455 [4] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g32n03:753100:753455 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g32n03:753101:753457 [5] NCCL INFO Connected all trees
g32n03:753101:753457 [5] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
g32n03:753101:753457 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g32n03:753098:753454 [2] NCCL INFO comm 0x1adb48f90 rank 0 nranks 4 cudaDev 2 nvmlDev 2 busId 406000 commId 0xe7e649e6bd56d868 - Init COMPLETE
g32n03:753101:753457 [5] NCCL INFO comm 0x1e4bddc60 rank 3 nranks 4 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xe7e649e6bd56d868 - Init COMPLETE
g32n03:753099:753456 [3] NCCL INFO comm 0x15fcd7ca0 rank 1 nranks 4 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xe7e649e6bd56d868 - Init COMPLETE
g32n03:753100:753455 [4] NCCL INFO comm 0x1a72efe10 rank 2 nranks 4 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xe7e649e6bd56d868 - Init COMPLETE
[after dataloaders are built] datetime: 2024-03-04 12:19:13 
done with setup ...
training ...
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (10855.02, 10905.08)
    train/valid/test-data-iterators-setup ..........: (640.08, 915.32)
[before the start of training step] datetime: 2024-03-04 12:19:13 
[2024-03-04 12:19:13,167] [INFO] [checkpointing.py:529:forward] Activation Checkpointing Information
[2024-03-04 12:19:13,168] [INFO] [checkpointing.py:530:forward] ----Partition Activations False, CPU CHECKPOINTING False
[2024-03-04 12:19:13,168] [INFO] [checkpointing.py:531:forward] ----contiguous Memory Checkpointing False with 32 total layers
[2024-03-04 12:19:13,168] [INFO] [checkpointing.py:533:forward] ----Synchronization False
[2024-03-04 12:19:13,168] [INFO] [checkpointing.py:534:forward] ----Profiling time in checkpointing False
g27n17:842060:842446 [0] NCCL INFO Using network IB
g27n17:842060:842446 [0] NCCL INFO comm 0x1a5bc3740 rank 2 nranks 8 cudaDev 0 nvmlDev 0 busId 404000 commId 0x849436df72f1a6f9 - Init START
g27n17:842061:842441 [1] NCCL INFO Using network IB
g27n17:842061:842441 [1] NCCL INFO comm 0x17b3b25b0 rank 3 nranks 8 cudaDev 1 nvmlDev 1 busId 405000 commId 0x849436df72f1a6f9 - Init START
g27n17:842063:842442 [2] NCCL INFO Using network IB
g27n17:842063:842442 [2] NCCL INFO comm 0x15b2cf720 rank 4 nranks 8 cudaDev 2 nvmlDev 2 busId 406000 commId 0x849436df72f1a6f9 - Init START
g31n03:688512:688885 [3] NCCL INFO Using network IB
g31n03:688512:688885 [3] NCCL INFO comm 0x1b0af45c0 rank 5 nranks 8 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x4fddb6043ee42895 - Init START
g31n03:688513:688886 [4] NCCL INFO Using network IB
g31n03:688513:688886 [4] NCCL INFO comm 0x1d6a7ec10 rank 6 nranks 8 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x4fddb6043ee42895 - Init START
g31n03:688514:688888 [5] NCCL INFO Using network IB
g31n03:688514:688888 [5] NCCL INFO comm 0x1aa49d4d0 rank 7 nranks 8 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x4fddb6043ee42895 - Init START
g27n17:842064:842445 [3] NCCL INFO Using network IB
g27n17:842064:842445 [3] NCCL INFO comm 0x1ac0c2910 rank 5 nranks 8 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x849436df72f1a6f9 - Init START
g27n17:842068:842443 [5] NCCL INFO Using network IB
g27n17:842068:842443 [5] NCCL INFO comm 0x18789bc70 rank 7 nranks 8 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x849436df72f1a6f9 - Init START
g27n17:842067:842444 [4] NCCL INFO Using network IB
g27n17:842067:842444 [4] NCCL INFO comm 0x13be271f0 rank 6 nranks 8 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x849436df72f1a6f9 - Init START
g28n04:841284:841669 [3] NCCL INFO Using network IB
g28n04:841284:841669 [3] NCCL INFO comm 0x172053c50 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xfaf114ba6bb27b13 - Init START
g28n04:841287:841667 [4] NCCL INFO Using network IB
g28n04:841287:841667 [4] NCCL INFO comm 0x1c2760730 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xfaf114ba6bb27b13 - Init START
g28n04:841283:841665 [2] NCCL INFO Using network IB
g28n04:841283:841665 [2] NCCL INFO comm 0x1a6eb0e70 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 406000 commId 0xfaf114ba6bb27b13 - Init START
g31n03:688509:688884 [0] NCCL INFO Using network IB
g31n03:688509:688884 [0] NCCL INFO comm 0x19aa93f30 rank 2 nranks 8 cudaDev 0 nvmlDev 0 busId 404000 commId 0x4fddb6043ee42895 - Init START
g28n04:841288:841668 [5] NCCL INFO Using network IB
g28n04:841288:841668 [5] NCCL INFO comm 0x1e20f2950 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xfaf114ba6bb27b13 - Init START
g31n03:688511:688887 [2] NCCL INFO Using network IB
g31n03:688511:688887 [2] NCCL INFO comm 0x1b3b8f9a0 rank 4 nranks 8 cudaDev 2 nvmlDev 2 busId 406000 commId 0x4fddb6043ee42895 - Init START
g28n04:841280:841664 [0] NCCL INFO Using network IB
g28n04:841280:841664 [0] NCCL INFO comm 0x17ed95820 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 404000 commId 0xfaf114ba6bb27b13 - Init START
g31n03:688510:688889 [1] NCCL INFO Using network IB
g31n03:688510:688889 [1] NCCL INFO comm 0x18db46b90 rank 3 nranks 8 cudaDev 1 nvmlDev 1 busId 405000 commId 0x4fddb6043ee42895 - Init START
g28n04:841281:841666 [1] NCCL INFO Using network IB
g28n04:841281:841666 [1] NCCL INFO comm 0x168f5e5a0 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 405000 commId 0xfaf114ba6bb27b13 - Init START
g31n02:736465:736845 [2] NCCL INFO Using network IB
g31n02:736465:736845 [2] NCCL INFO comm 0x1a27132b0 rank 6 nranks 8 cudaDev 2 nvmlDev 2 busId 406000 commId 0xadee5768a93b5235 - Init START
g31n02:736464:736846 [1] NCCL INFO Using network IB
g31n02:736464:736846 [1] NCCL INFO comm 0x187cde490 rank 5 nranks 8 cudaDev 1 nvmlDev 1 busId 405000 commId 0xadee5768a93b5235 - Init START
g26n02:851592:851974 [3] NCCL INFO Using network IB
g26n02:851592:851974 [3] NCCL INFO comm 0x175230230 rank 7 nranks 8 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xb1b45bf195242dd5 - Init START
g31n02:736463:736849 [0] NCCL INFO Using network IB
g31n02:736463:736849 [0] NCCL INFO comm 0x121dc03a0 rank 4 nranks 8 cudaDev 0 nvmlDev 0 busId 404000 commId 0xadee5768a93b5235 - Init START
g31n02:736466:736847 [3] NCCL INFO Using network IB
g31n02:736466:736847 [3] NCCL INFO comm 0x189c6f9a0 rank 7 nranks 8 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xadee5768a93b5235 - Init START
g31n02:736467:736844 [4] NCCL INFO Using network IB
g31n02:736467:736844 [4] NCCL INFO comm 0x18275d150 rank 0 nranks 8 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x4fddb6043ee42895 - Init START
g31n02:736468:736848 [5] NCCL INFO Using network IB
g31n02:736468:736848 [5] NCCL INFO comm 0x1bfa60370 rank 1 nranks 8 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x4fddb6043ee42895 - Init START
g26n02:851589:851973 [0] NCCL INFO Using network IB
g26n02:851589:851973 [0] NCCL INFO comm 0x1b3133720 rank 4 nranks 8 cudaDev 0 nvmlDev 0 busId 404000 commId 0xb1b45bf195242dd5 - Init START
g25n18:898855:899337 [3] NCCL INFO Using network IB
g25n18:898855:899337 [3] NCCL INFO comm 0x1cacd3ea0 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x9ac9af8d1af9f7d1 - Init START
g26n01:811333:811705 [2] NCCL INFO Using network IB
g26n01:811333:811705 [2] NCCL INFO comm 0x16d89b070 rank 0 nranks 8 cudaDev 2 nvmlDev 2 busId 406000 commId 0xb1b45bf195242dd5 - Init START
g25n18:898857:899338 [5] NCCL INFO Using network IB
g25n18:898857:899338 [5] NCCL INFO comm 0x141bea260 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x9ac9af8d1af9f7d1 - Init START
g26n01:811332:811710 [1] NCCL INFO Using network IB
g26n01:811332:811710 [1] NCCL INFO comm 0x19b304620 rank 7 nranks 8 cudaDev 1 nvmlDev 1 busId 405000 commId 0x9ac9af8d1af9f7d1 - Init START
g26n02:851591:851972 [2] NCCL INFO Using network IB
g26n02:851591:851972 [2] NCCL INFO comm 0x187784770 rank 6 nranks 8 cudaDev 2 nvmlDev 2 busId 406000 commId 0xb1b45bf195242dd5 - Init START
g26n01:811331:811709 [0] NCCL INFO Using network IB
g26n01:811331:811709 [0] NCCL INFO comm 0x1ac2bdef0 rank 6 nranks 8 cudaDev 0 nvmlDev 0 busId 404000 commId 0x9ac9af8d1af9f7d1 - Init START
g26n02:851590:851971 [1] NCCL INFO Using network IB
g26n02:851590:851971 [1] NCCL INFO comm 0x19e3219a0 rank 5 nranks 8 cudaDev 1 nvmlDev 1 busId 405000 commId 0xb1b45bf195242dd5 - Init START
g26n01:811336:811706 [5] NCCL INFO Using network IB
g26n01:811336:811706 [5] NCCL INFO comm 0x18328de30 rank 3 nranks 8 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xb1b45bf195242dd5 - Init START
g31n04:688228:688611 [4] NCCL INFO Using network IB
g26n01:811334:811707 [3] NCCL INFO Using network IB
g26n01:811334:811707 [3] NCCL INFO comm 0x17a32c4c0 rank 1 nranks 8 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xb1b45bf195242dd5 - Init START
g31n04:688229:688610 [5] NCCL INFO Using network IB
g26n01:811335:811708 [4] NCCL INFO Using network IB
g26n01:811335:811708 [4] NCCL INFO comm 0x14e64ec20 rank 2 nranks 8 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xb1b45bf195242dd5 - Init START
g31n04:688227:688609 [3] NCCL INFO Using network IB
g31n04:688226:688607 [2] NCCL INFO Using network IB
g25n18:898853:899335 [1] NCCL INFO Using network IB
g25n18:898853:899335 [1] NCCL INFO comm 0x17ca397b0 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 405000 commId 0x9ac9af8d1af9f7d1 - Init START
g31n04:688225:688608 [1] NCCL INFO Using network IB
g31n04:688224:688606 [0] NCCL INFO Using network IB
g26n02:851594:851975 [5] NCCL INFO Using network IB
g26n02:851594:851975 [5] NCCL INFO comm 0x1d3bca8e0 rank 1 nranks 8 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x849436df72f1a6f9 - Init START
g26n02:851593:851970 [4] NCCL INFO Using network IB
g26n02:851593:851970 [4] NCCL INFO comm 0x17d888830 rank 0 nranks 8 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x849436df72f1a6f9 - Init START
g31n01:617757:618140 [1] NCCL INFO Using network IB
g31n01:617757:618140 [1] NCCL INFO comm 0x1afbccf60 rank 7 nranks 8 cudaDev 1 nvmlDev 1 busId 405000 commId 0xfaf114ba6bb27b13 - Init START
g31n01:617758:618138 [2] NCCL INFO Using network IB
g31n01:617758:618138 [2] NCCL INFO comm 0x1993350a0 rank 0 nranks 8 cudaDev 2 nvmlDev 2 busId 406000 commId 0xadee5768a93b5235 - Init START
g31n01:617756:618139 [0] NCCL INFO Using network IB
g31n01:617756:618139 [0] NCCL INFO comm 0x19e8800d0 rank 6 nranks 8 cudaDev 0 nvmlDev 0 busId 404000 commId 0xfaf114ba6bb27b13 - Init START
g28n01:770099:770473 [4] NCCL INFO Using network IB
g28n01:770099:770473 [4] NCCL INFO comm 0x1b8fd8c70 rank 2 nranks 8 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x16d0e2c5f1ddffab - Init START
g28n01:770100:770471 [5] NCCL INFO Using network IB
g28n01:770100:770471 [5] NCCL INFO comm 0x196de93a0 rank 3 nranks 8 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x16d0e2c5f1ddffab - Init START
g28n01:770096:770472 [3] NCCL INFO Using network IB
g28n01:770096:770472 [3] NCCL INFO comm 0x1c5693460 rank 1 nranks 8 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x16d0e2c5f1ddffab - Init START
g25n18:898854:899336 [2] NCCL INFO Using network IB
g25n18:898854:899336 [2] NCCL INFO comm 0x1b2447840 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 406000 commId 0x9ac9af8d1af9f7d1 - Init START
g31n01:617759:618141 [3] NCCL INFO Using network IB
g31n01:617759:618141 [3] NCCL INFO comm 0x1414fec90 rank 1 nranks 8 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xadee5768a93b5235 - Init START
g31n01:617761:618142 [5] NCCL INFO Using network IB
g31n01:617761:618142 [5] NCCL INFO comm 0x1c9e35f20 rank 3 nranks 8 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xadee5768a93b5235 - Init START
g32n02:753687:754066 [4] NCCL INFO Using network IB
g28n01:770092:770474 [0] NCCL INFO Using network IB
g28n01:770092:770474 [0] NCCL INFO comm 0x199996c50 rank 6 nranks 8 cudaDev 0 nvmlDev 0 busId 404000 commId 0x7b81f3d89584e3c1 - Init START
g32n02:753688:754067 [5] NCCL INFO Using network IB
g28n01:770093:770475 [1] NCCL INFO Using network IB
g28n01:770093:770475 [1] NCCL INFO comm 0x14f7c5220 rank 7 nranks 8 cudaDev 1 nvmlDev 1 busId 405000 commId 0x7b81f3d89584e3c1 - Init START
g28n01:770095:770470 [2] NCCL INFO Using network IB
g28n01:770095:770470 [2] NCCL INFO comm 0x1c858e5b0 rank 0 nranks 8 cudaDev 2 nvmlDev 2 busId 406000 commId 0x16d0e2c5f1ddffab - Init START
g28n03:859958:860335 [3] NCCL INFO Using network IB
g28n03:859958:860335 [3] NCCL INFO comm 0x1bbed8b60 rank 5 nranks 8 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x5feb56817484c8aa - Init START
g28n03:859960:860334 [4] NCCL INFO Using network IB
g28n03:859960:860334 [4] NCCL INFO comm 0x1da5b81b0 rank 6 nranks 8 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x5feb56817484c8aa - Init START
g28n03:859961:860333 [5] NCCL INFO Using network IB
g28n03:859961:860333 [5] NCCL INFO comm 0x1ad87dee0 rank 7 nranks 8 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x5feb56817484c8aa - Init START
g28n03:859954:860336 [1] NCCL INFO Using network IB
g28n03:859954:860336 [1] NCCL INFO comm 0x155ed7bc0 rank 3 nranks 8 cudaDev 1 nvmlDev 1 busId 405000 commId 0x5feb56817484c8aa - Init START
g28n03:859953:860331 [0] NCCL INFO Using network IB
g28n03:859953:860331 [0] NCCL INFO comm 0x1be814ae0 rank 2 nranks 8 cudaDev 0 nvmlDev 0 busId 404000 commId 0x5feb56817484c8aa - Init START
g27n18:839785:840164 [2] NCCL INFO Using network IB
g27n18:839785:840164 [2] NCCL INFO comm 0x1bfa82790 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 406000 commId 0x7b81f3d89584e3c1 - Init START
g28n03:859956:860332 [2] NCCL INFO Using network IB
g28n03:859956:860332 [2] NCCL INFO comm 0x187c22830 rank 4 nranks 8 cudaDev 2 nvmlDev 2 busId 406000 commId 0x5feb56817484c8aa - Init START
g31n01:617760:618143 [4] NCCL INFO Using network IB
g31n01:617760:618143 [4] NCCL INFO comm 0x1b2aabdf0 rank 2 nranks 8 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xadee5768a93b5235 - Init START
g27n18:839782:840163 [0] NCCL INFO Using network IB
g27n18:839782:840163 [0] NCCL INFO comm 0x1f5b69550 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 404000 commId 0x7b81f3d89584e3c1 - Init START
g32n02:753685:754070 [2] NCCL INFO Using network IB
g28n02:792072:792470 [3] NCCL INFO Using network IB
g28n02:792072:792470 [3] NCCL INFO comm 0x1c7ecd950 rank 7 nranks 8 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x16d0e2c5f1ddffab - Init START
g28n02:792075:792465 [4] NCCL INFO Using network IB
g28n02:792075:792465 [4] NCCL INFO comm 0x1a62ac930 rank 0 nranks 8 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x5feb56817484c8aa - Init START
g28n02:792076:792466 [5] NCCL INFO Using network IB
g28n02:792076:792466 [5] NCCL INFO comm 0x1bb65d370 rank 1 nranks 8 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x5feb56817484c8aa - Init START
g28n02:792068:792468 [0] NCCL INFO Using network IB
g28n02:792068:792468 [0] NCCL INFO comm 0x176c99080 rank 4 nranks 8 cudaDev 0 nvmlDev 0 busId 404000 commId 0x16d0e2c5f1ddffab - Init START
g27n18:839787:840165 [3] NCCL INFO Using network IB
g27n18:839787:840165 [3] NCCL INFO comm 0x1c2dad6e0 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x7b81f3d89584e3c1 - Init START
g27n18:839789:840166 [4] NCCL INFO Using network IB
g27n18:839789:840166 [4] NCCL INFO comm 0x199bc7b20 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x7b81f3d89584e3c1 - Init START
g28n02:792069:792467 [1] NCCL INFO Using network IB
g28n02:792069:792467 [1] NCCL INFO comm 0x1cb637770 rank 5 nranks 8 cudaDev 1 nvmlDev 1 busId 405000 commId 0x16d0e2c5f1ddffab - Init START
g28n02:792071:792469 [2] NCCL INFO Using network IB
g28n02:792071:792469 [2] NCCL INFO comm 0x18aedcad0 rank 6 nranks 8 cudaDev 2 nvmlDev 2 busId 406000 commId 0x16d0e2c5f1ddffab - Init START
g31n05:704345:704721 [1] NCCL INFO Using network IB
g31n05:704345:704721 [1] NCCL INFO comm 0x181596770 rank 7 nranks 8 cudaDev 1 nvmlDev 1 busId 405000 commId 0xf41855dae56807 - Init START
g27n18:839783:840167 [1] NCCL INFO Using network IB
g27n18:839783:840167 [1] NCCL INFO comm 0x1807e2220 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 405000 commId 0x7b81f3d89584e3c1 - Init START
g27n18:839790:840168 [5] NCCL INFO Using network IB
g27n18:839790:840168 [5] NCCL INFO comm 0x1751b3cb0 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x7b81f3d89584e3c1 - Init START
g25n18:898856:899339 [4] NCCL INFO Using network IB
g25n18:898856:899339 [4] NCCL INFO comm 0x15bb9ed70 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x9ac9af8d1af9f7d1 - Init START
g25n18:898852:899334 [0] NCCL INFO Using network IB
g25n18:898852:899334 [0] NCCL INFO comm 0x149bed730 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 404000 commId 0x9ac9af8d1af9f7d1 - Init START
g32n03:753099:753472 [3] NCCL INFO Using network IB
g32n03:753099:753472 [3] NCCL INFO comm 0x16dc4e720 rank 5 nranks 8 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xb5380ca77106f99c - Init START
g32n03:753100:753471 [4] NCCL INFO Using network IB
g32n03:753100:753471 [4] NCCL INFO comm 0x16f7c4820 rank 6 nranks 8 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xb5380ca77106f99c - Init START
g32n03:753096:753474 [0] NCCL INFO Using network IB
g32n03:753096:753474 [0] NCCL INFO comm 0x1ae639330 rank 2 nranks 8 cudaDev 0 nvmlDev 0 busId 404000 commId 0xb5380ca77106f99c - Init START
g32n03:753097:753475 [1] NCCL INFO Using network IB
g32n03:753097:753475 [1] NCCL INFO comm 0x1f46e03e0 rank 3 nranks 8 cudaDev 1 nvmlDev 1 busId 405000 commId 0xb5380ca77106f99c - Init START
g32n03:753098:753473 [2] NCCL INFO Using network IB
g32n03:753098:753473 [2] NCCL INFO comm 0x17bd77e50 rank 4 nranks 8 cudaDev 2 nvmlDev 2 busId 406000 commId 0xb5380ca77106f99c - Init START
g32n03:753101:753470 [5] NCCL INFO Using network IB
g32n03:753101:753470 [5] NCCL INFO comm 0x1e9d0d8a0 rank 7 nranks 8 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xb5380ca77106f99c - Init START
g31n05:704348:704723 [4] NCCL INFO Using network IB
g31n05:704348:704723 [4] NCCL INFO comm 0x1a1370120 rank 2 nranks 8 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xe6c91641295cac8b - Init START
g31n05:704349:704724 [5] NCCL INFO Using network IB
g31n05:704349:704724 [5] NCCL INFO comm 0x1ba4d3220 rank 3 nranks 8 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xe6c91641295cac8b - Init START
g31n05:704344:704722 [0] NCCL INFO Using network IB
g31n05:704344:704722 [0] NCCL INFO comm 0x1aaa97d20 rank 6 nranks 8 cudaDev 0 nvmlDev 0 busId 404000 commId 0xf41855dae56807 - Init START
g31n05:704344:704722 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g31n02:736468:736848 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g31n05:704346:704720 [2] NCCL INFO Using network IB
g31n05:704346:704720 [2] NCCL INFO comm 0x18de49fe0 rank 0 nranks 8 cudaDev 2 nvmlDev 2 busId 406000 commId 0xe6c91641295cac8b - Init START
g31n02:736467:736844 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g31n05:704347:704725 [3] NCCL INFO Using network IB
g31n05:704347:704725 [3] NCCL INFO comm 0x1c84b2b20 rank 1 nranks 8 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xe6c91641295cac8b - Init START
g26n01:811331:811709 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g26n01:811332:811710 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g31n04:688228:688611 [4] NCCL INFO comm 0x1c05d1270 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xf41855dae56807 - Init START
g31n04:688229:688610 [5] NCCL INFO comm 0x18bfefa10 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xf41855dae56807 - Init START
g31n04:688227:688609 [3] NCCL INFO comm 0x1697e59f0 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xf41855dae56807 - Init START
g31n04:688226:688607 [2] NCCL INFO comm 0x1e908ca60 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 406000 commId 0xf41855dae56807 - Init START
g31n04:688225:688608 [1] NCCL INFO comm 0x1a7751e20 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 405000 commId 0xf41855dae56807 - Init START
g31n04:688224:688606 [0] NCCL INFO comm 0x18c0165b0 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 404000 commId 0xf41855dae56807 - Init START
g26n02:851594:851975 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g26n02:851593:851970 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g32n02:753684:754069 [1] NCCL INFO Using network IB
g32n02:753684:754069 [1] NCCL INFO comm 0x1c1f888e0 rank 5 nranks 8 cudaDev 1 nvmlDev 1 busId 405000 commId 0xe6c91641295cac8b - Init START
g32n02:753683:754068 [0] NCCL INFO Using network IB
g32n02:753683:754068 [0] NCCL INFO comm 0x18aba5ab0 rank 4 nranks 8 cudaDev 0 nvmlDev 0 busId 404000 commId 0xe6c91641295cac8b - Init START
g31n01:617757:618140 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g31n01:617756:618139 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g32n02:753686:754071 [3] NCCL INFO Using network IB
g32n02:753686:754071 [3] NCCL INFO comm 0x156b86be0 rank 7 nranks 8 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xe6c91641295cac8b - Init START
g28n01:770092:770474 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g28n01:770093:770475 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g32n02:753687:754066 [4] NCCL INFO comm 0x1921bdbb0 rank 0 nranks 8 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xb5380ca77106f99c - Init START
g32n02:753688:754067 [5] NCCL INFO comm 0x172ebf770 rank 1 nranks 8 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xb5380ca77106f99c - Init START
g32n02:753685:754070 [2] NCCL INFO comm 0x1bafda560 rank 6 nranks 8 cudaDev 2 nvmlDev 2 busId 406000 commId 0xe6c91641295cac8b - Init START
g28n02:792075:792465 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g28n02:792076:792466 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g28n02:792068:792468 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g28n02:792071:792469 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g31n05:704345:704721 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g31n02:736464:736846 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g31n02:736465:736845 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g26n02:851592:851974 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g31n02:736463:736849 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g31n05:704348:704723 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g31n05:704349:704724 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g31n02:736466:736847 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g31n05:704346:704720 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g31n05:704347:704725 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g26n02:851589:851973 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g26n01:811333:811705 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g26n01:811336:811706 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g26n02:851590:851971 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g26n01:811334:811707 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g26n02:851591:851972 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g26n01:811335:811708 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g32n02:753684:754069 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g28n01:770100:770471 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g28n01:770100:770471 [5] NCCL INFO Trees [0] 0/-1/-1->3->2 [1] 0/-1/-1->3->2 [2] 0/-1/-1->3->2 [3] 0/-1/-1->3->2
g28n01:770100:770471 [5] NCCL INFO P2P Chunksize set to 131072
g28n01:770099:770473 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g28n01:770099:770473 [4] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1
g28n01:770099:770473 [4] NCCL INFO P2P Chunksize set to 131072
g32n02:753683:754068 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g28n01:770096:770472 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g28n01:770096:770472 [3] NCCL INFO Trees [0] 2/4/-1->1->-1 [1] 2/4/-1->1->-1 [2] 2/-1/-1->1->4 [3] 2/-1/-1->1->4
g28n01:770096:770472 [3] NCCL INFO P2P Chunksize set to 131072
g28n02:792069:792467 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g28n02:792069:792467 [1] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4
g28n02:792069:792467 [1] NCCL INFO P2P Chunksize set to 131072
g28n01:770095:770470 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g28n01:770095:770470 [2] NCCL INFO Channel 00/04 :    0   1   2   3   4   5   6   7
g28n01:770095:770470 [2] NCCL INFO Channel 01/04 :    0   7   6   5   4   1   2   3
g28n01:770095:770470 [2] NCCL INFO Channel 02/04 :    0   1   2   3   4   5   6   7
g28n01:770095:770470 [2] NCCL INFO Channel 03/04 :    0   7   6   5   4   1   2   3
g28n01:770095:770470 [2] NCCL INFO Trees [0] -1/-1/-1->0->3 [1] -1/-1/-1->0->3 [2] -1/-1/-1->0->3 [3] -1/-1/-1->0->3
g28n01:770095:770470 [2] NCCL INFO P2P Chunksize set to 131072
g28n01:770095:770470 [2] NCCL INFO Channel 00/0 : 7[3] -> 0[2] [receive] via NET/IB/0
g28n01:770095:770470 [2] NCCL INFO Channel 02/0 : 7[3] -> 0[2] [receive] via NET/IB/0
g28n01:770095:770470 [2] NCCL INFO Channel 00/0 : 0[2] -> 1[3] via P2P/IPC
g28n02:792071:792469 [2] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5
g28n02:792071:792469 [2] NCCL INFO P2P Chunksize set to 131072
g31n01:617758:618138 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g28n02:792072:792470 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g28n02:792072:792470 [3] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] -1/-1/-1->7->6 [3] -1/-1/-1->7->6
g28n02:792072:792470 [3] NCCL INFO P2P Chunksize set to 131072
g28n02:792068:792468 [0] NCCL INFO Trees [0] 5/-1/-1->4->1 [1] 5/-1/-1->4->1 [2] 5/1/-1->4->-1 [3] 5/1/-1->4->-1
g28n02:792068:792468 [0] NCCL INFO P2P Chunksize set to 131072
g28n02:792068:792468 [0] NCCL INFO Channel 00/0 : 3[5] -> 4[0] [receive] via NET/IB/0
g28n02:792068:792468 [0] NCCL INFO Channel 02/0 : 3[5] -> 4[0] [receive] via NET/IB/0
g28n02:792068:792468 [0] NCCL INFO Channel 00/0 : 4[0] -> 5[1] via P2P/IPC
g31n01:617759:618141 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g31n01:617761:618142 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g32n02:753688:754067 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g28n01:770095:770470 [2] NCCL INFO Channel 02/0 : 0[2] -> 1[3] via P2P/IPC
g32n02:753686:754071 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g32n02:753687:754066 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g28n02:792068:792468 [0] NCCL INFO Channel 02/0 : 4[0] -> 5[1] via P2P/IPC
g28n03:859954:860336 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g28n02:792071:792469 [2] NCCL INFO Channel 00/0 : 6[2] -> 7[3] via P2P/IPC
g31n01:617760:618143 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g28n02:792069:792467 [1] NCCL INFO Channel 00/0 : 5[1] -> 6[2] via P2P/IPC
g28n01:770096:770472 [3] NCCL INFO Channel 00/0 : 1[3] -> 2[4] via P2P/IPC
g32n02:753685:754070 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g28n02:792071:792469 [2] NCCL INFO Channel 02/0 : 6[2] -> 7[3] via P2P/IPC
g28n02:792069:792467 [1] NCCL INFO Channel 02/0 : 5[1] -> 6[2] via P2P/IPC
g28n02:792072:792470 [3] NCCL INFO Channel 00/0 : 7[3] -> 0[2] [send] via NET/IB/1
g28n02:792072:792470 [3] NCCL INFO Channel 02/0 : 7[3] -> 0[2] [send] via NET/IB/1
g28n01:770096:770472 [3] NCCL INFO Channel 01/0 : 1[3] -> 2[4] via P2P/IPC
g28n01:770099:770473 [4] NCCL INFO Channel 00/0 : 2[4] -> 3[5] via P2P/IPC
g28n01:770096:770472 [3] NCCL INFO Channel 02/0 : 1[3] -> 2[4] via P2P/IPC
g28n02:792072:792470 [3] NCCL INFO Channel 01/0 : 0[2] -> 7[3] [receive] via NET/IB/3
g28n02:792072:792470 [3] NCCL INFO Channel 03/0 : 0[2] -> 7[3] [receive] via NET/IB/3
g28n02:792072:792470 [3] NCCL INFO Channel 01/0 : 7[3] -> 6[2] via P2P/IPC
g28n01:770099:770473 [4] NCCL INFO Channel 01/0 : 2[4] -> 3[5] via P2P/IPC
g28n01:770096:770472 [3] NCCL INFO Channel 03/0 : 1[3] -> 2[4] via P2P/IPC
g28n02:792072:792470 [3] NCCL INFO Channel 03/0 : 7[3] -> 6[2] via P2P/IPC
g28n01:770099:770473 [4] NCCL INFO Channel 02/0 : 2[4] -> 3[5] via P2P/IPC
g28n02:792071:792469 [2] NCCL INFO Channel 01/0 : 6[2] -> 5[1] via P2P/IPC
g28n02:792069:792467 [1] NCCL INFO Channel 01/0 : 5[1] -> 4[0] via P2P/IPC
g28n01:770099:770473 [4] NCCL INFO Channel 03/0 : 2[4] -> 3[5] via P2P/IPC
g28n02:792071:792469 [2] NCCL INFO Channel 03/0 : 6[2] -> 5[1] via P2P/IPC
g28n02:792069:792467 [1] NCCL INFO Channel 03/0 : 5[1] -> 4[0] via P2P/IPC
g28n01:770100:770471 [5] NCCL INFO Channel 00/0 : 3[5] -> 4[0] [send] via NET/IB/1
g28n01:770100:770471 [5] NCCL INFO Channel 02/0 : 3[5] -> 4[0] [send] via NET/IB/1
g28n01:770100:770471 [5] NCCL INFO Channel 01/0 : 3[5] -> 0[2] via P2P/IPC
g28n01:770096:770472 [3] NCCL INFO Channel 01/0 : 4[0] -> 1[3] [receive] via NET/IB/3
g28n01:770096:770472 [3] NCCL INFO Channel 03/0 : 4[0] -> 1[3] [receive] via NET/IB/3
g28n02:792068:792468 [0] NCCL INFO Channel 01/0 : 4[0] -> 1[3] [send] via NET/IB/2
g28n02:792068:792468 [0] NCCL INFO Channel 03/0 : 4[0] -> 1[3] [send] via NET/IB/2
g28n01:770100:770471 [5] NCCL INFO Channel 03/0 : 3[5] -> 0[2] via P2P/IPC
g28n01:770095:770470 [2] NCCL INFO Channel 01/0 : 0[2] -> 7[3] [send] via NET/IB/2
g28n01:770095:770470 [2] NCCL INFO Channel 03/0 : 0[2] -> 7[3] [send] via NET/IB/2
g28n02:792071:792469 [2] NCCL INFO Connected all rings
g31n02:736465:736845 [2] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5
g31n02:736465:736845 [2] NCCL INFO P2P Chunksize set to 131072
g31n01:617760:618143 [4] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1
g31n01:617760:618143 [4] NCCL INFO P2P Chunksize set to 131072
g31n02:736464:736846 [1] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4
g31n02:736464:736846 [1] NCCL INFO P2P Chunksize set to 131072
g31n01:617761:618142 [5] NCCL INFO Trees [0] 0/-1/-1->3->2 [1] 0/-1/-1->3->2 [2] 0/-1/-1->3->2 [3] 0/-1/-1->3->2
g31n01:617761:618142 [5] NCCL INFO P2P Chunksize set to 131072
g31n02:736466:736847 [3] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] -1/-1/-1->7->6 [3] -1/-1/-1->7->6
g31n02:736466:736847 [3] NCCL INFO P2P Chunksize set to 131072
g31n01:617759:618141 [3] NCCL INFO Trees [0] 2/4/-1->1->-1 [1] 2/4/-1->1->-1 [2] 2/-1/-1->1->4 [3] 2/-1/-1->1->4
g31n01:617759:618141 [3] NCCL INFO P2P Chunksize set to 131072
g31n02:736463:736849 [0] NCCL INFO Trees [0] 5/-1/-1->4->1 [1] 5/-1/-1->4->1 [2] 5/1/-1->4->-1 [3] 5/1/-1->4->-1
g31n02:736463:736849 [0] NCCL INFO P2P Chunksize set to 131072
g31n02:736463:736849 [0] NCCL INFO Channel 00/0 : 3[5] -> 4[0] [receive] via NET/IB/0
g31n02:736463:736849 [0] NCCL INFO Channel 02/0 : 3[5] -> 4[0] [receive] via NET/IB/0
g31n02:736463:736849 [0] NCCL INFO Channel 00/0 : 4[0] -> 5[1] via P2P/IPC
g31n01:617758:618138 [2] NCCL INFO Channel 00/04 :    0   1   2   3   4   5   6   7
g31n01:617758:618138 [2] NCCL INFO Channel 01/04 :    0   7   6   5   4   1   2   3
g31n01:617758:618138 [2] NCCL INFO Channel 02/04 :    0   1   2   3   4   5   6   7
g31n01:617758:618138 [2] NCCL INFO Channel 03/04 :    0   7   6   5   4   1   2   3
g31n01:617758:618138 [2] NCCL INFO Trees [0] -1/-1/-1->0->3 [1] -1/-1/-1->0->3 [2] -1/-1/-1->0->3 [3] -1/-1/-1->0->3
g31n01:617758:618138 [2] NCCL INFO P2P Chunksize set to 131072
g31n01:617758:618138 [2] NCCL INFO Channel 00/0 : 7[3] -> 0[2] [receive] via NET/IB/0
g31n01:617758:618138 [2] NCCL INFO Channel 02/0 : 7[3] -> 0[2] [receive] via NET/IB/0
g31n01:617758:618138 [2] NCCL INFO Channel 00/0 : 0[2] -> 1[3] via P2P/IPC
g31n02:736463:736849 [0] NCCL INFO Channel 02/0 : 4[0] -> 5[1] via P2P/IPC
g31n01:617758:618138 [2] NCCL INFO Channel 02/0 : 0[2] -> 1[3] via P2P/IPC
g28n02:792071:792469 [2] NCCL INFO Channel 01/0 : 6[2] -> 7[3] via P2P/IPC
g28n01:770099:770473 [4] NCCL INFO Connected all rings
g31n02:736464:736846 [1] NCCL INFO Channel 00/0 : 5[1] -> 6[2] via P2P/IPC
g31n02:736465:736845 [2] NCCL INFO Channel 00/0 : 6[2] -> 7[3] via P2P/IPC
g31n02:736464:736846 [1] NCCL INFO Channel 02/0 : 5[1] -> 6[2] via P2P/IPC
g31n02:736465:736845 [2] NCCL INFO Channel 02/0 : 6[2] -> 7[3] via P2P/IPC
g31n01:617759:618141 [3] NCCL INFO Channel 00/0 : 1[3] -> 2[4] via P2P/IPC
g26n02:851592:851974 [3] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] -1/-1/-1->7->6 [3] -1/-1/-1->7->6
g26n02:851592:851974 [3] NCCL INFO P2P Chunksize set to 131072
g26n01:811334:811707 [3] NCCL INFO Trees [0] 2/4/-1->1->-1 [1] 2/4/-1->1->-1 [2] 2/-1/-1->1->4 [3] 2/-1/-1->1->4
g26n01:811334:811707 [3] NCCL INFO P2P Chunksize set to 131072
g26n01:811336:811706 [5] NCCL INFO Trees [0] 0/-1/-1->3->2 [1] 0/-1/-1->3->2 [2] 0/-1/-1->3->2 [3] 0/-1/-1->3->2
g26n01:811336:811706 [5] NCCL INFO P2P Chunksize set to 131072
g26n02:851591:851972 [2] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5
g26n02:851591:851972 [2] NCCL INFO P2P Chunksize set to 131072
g26n01:811335:811708 [4] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1
g26n01:811335:811708 [4] NCCL INFO P2P Chunksize set to 131072
g26n02:851590:851971 [1] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4
g26n02:851590:851971 [1] NCCL INFO P2P Chunksize set to 131072
g26n01:811333:811705 [2] NCCL INFO Channel 00/04 :    0   1   2   3   4   5   6   7
g26n01:811333:811705 [2] NCCL INFO Channel 01/04 :    0   7   6   5   4   1   2   3
g26n01:811333:811705 [2] NCCL INFO Channel 02/04 :    0   1   2   3   4   5   6   7
g26n01:811333:811705 [2] NCCL INFO Channel 03/04 :    0   7   6   5   4   1   2   3
g26n01:811333:811705 [2] NCCL INFO Trees [0] -1/-1/-1->0->3 [1] -1/-1/-1->0->3 [2] -1/-1/-1->0->3 [3] -1/-1/-1->0->3
g26n01:811333:811705 [2] NCCL INFO P2P Chunksize set to 131072
g26n01:811333:811705 [2] NCCL INFO Channel 00/0 : 7[3] -> 0[2] [receive] via NET/IB/0
g26n01:811333:811705 [2] NCCL INFO Channel 02/0 : 7[3] -> 0[2] [receive] via NET/IB/0
g26n01:811333:811705 [2] NCCL INFO Channel 00/0 : 0[2] -> 1[3] via P2P/IPC
g26n02:851589:851973 [0] NCCL INFO Trees [0] 5/-1/-1->4->1 [1] 5/-1/-1->4->1 [2] 5/1/-1->4->-1 [3] 5/1/-1->4->-1
g26n02:851589:851973 [0] NCCL INFO P2P Chunksize set to 131072
g26n02:851589:851973 [0] NCCL INFO Channel 00/0 : 3[5] -> 4[0] [receive] via NET/IB/0
g26n02:851589:851973 [0] NCCL INFO Channel 02/0 : 3[5] -> 4[0] [receive] via NET/IB/0
g26n02:851589:851973 [0] NCCL INFO Channel 00/0 : 4[0] -> 5[1] via P2P/IPC
g31n01:617759:618141 [3] NCCL INFO Channel 01/0 : 1[3] -> 2[4] via P2P/IPC
g31n01:617760:618143 [4] NCCL INFO Channel 00/0 : 2[4] -> 3[5] via P2P/IPC
g31n02:736466:736847 [3] NCCL INFO Channel 00/0 : 7[3] -> 0[2] [send] via NET/IB/1
g31n02:736466:736847 [3] NCCL INFO Channel 02/0 : 7[3] -> 0[2] [send] via NET/IB/1
g31n02:736466:736847 [3] NCCL INFO Channel 01/0 : 0[2] -> 7[3] [receive] via NET/IB/3
g31n02:736466:736847 [3] NCCL INFO Channel 03/0 : 0[2] -> 7[3] [receive] via NET/IB/3
g31n02:736466:736847 [3] NCCL INFO Channel 01/0 : 7[3] -> 6[2] via P2P/IPC
g28n01:770100:770471 [5] NCCL INFO Connected all rings
g31n01:617759:618141 [3] NCCL INFO Channel 02/0 : 1[3] -> 2[4] via P2P/IPC
g28n02:792071:792469 [2] NCCL INFO Channel 03/0 : 6[2] -> 7[3] via P2P/IPC
g31n01:617760:618143 [4] NCCL INFO Channel 01/0 : 2[4] -> 3[5] via P2P/IPC
g31n01:617759:618141 [3] NCCL INFO Channel 03/0 : 1[3] -> 2[4] via P2P/IPC
g26n01:811333:811705 [2] NCCL INFO Channel 02/0 : 0[2] -> 1[3] via P2P/IPC
g28n02:792069:792467 [1] NCCL INFO Connected all rings
g31n01:617760:618143 [4] NCCL INFO Channel 02/0 : 2[4] -> 3[5] via P2P/IPC
g26n02:851589:851973 [0] NCCL INFO Channel 02/0 : 4[0] -> 5[1] via P2P/IPC
g31n02:736466:736847 [3] NCCL INFO Channel 03/0 : 7[3] -> 6[2] via P2P/IPC
g26n02:851591:851972 [2] NCCL INFO Channel 00/0 : 6[2] -> 7[3] via P2P/IPC
g26n02:851590:851971 [1] NCCL INFO Channel 00/0 : 5[1] -> 6[2] via P2P/IPC
g31n01:617760:618143 [4] NCCL INFO Channel 03/0 : 2[4] -> 3[5] via P2P/IPC
g31n02:736464:736846 [1] NCCL INFO Channel 01/0 : 5[1] -> 4[0] via P2P/IPC
g26n01:811334:811707 [3] NCCL INFO Channel 00/0 : 1[3] -> 2[4] via P2P/IPC
g31n02:736465:736845 [2] NCCL INFO Channel 01/0 : 6[2] -> 5[1] via P2P/IPC
g28n01:770099:770473 [4] NCCL INFO Channel 00/0 : 2[4] -> 1[3] via P2P/IPC
g28n02:792069:792467 [1] NCCL INFO Channel 01/0 : 5[1] -> 6[2] via P2P/IPC
g26n02:851591:851972 [2] NCCL INFO Channel 02/0 : 6[2] -> 7[3] via P2P/IPC
g26n02:851590:851971 [1] NCCL INFO Channel 02/0 : 5[1] -> 6[2] via P2P/IPC
g31n02:736464:736846 [1] NCCL INFO Channel 03/0 : 5[1] -> 4[0] via P2P/IPC
g31n02:736465:736845 [2] NCCL INFO Channel 03/0 : 6[2] -> 5[1] via P2P/IPC
g28n02:792069:792467 [1] NCCL INFO Channel 03/0 : 5[1] -> 6[2] via P2P/IPC
g28n01:770099:770473 [4] NCCL INFO Channel 01/0 : 2[4] -> 1[3] via P2P/IPC
g26n01:811334:811707 [3] NCCL INFO Channel 01/0 : 1[3] -> 2[4] via P2P/IPC
g31n01:617761:618142 [5] NCCL INFO Channel 00/0 : 3[5] -> 4[0] [send] via NET/IB/1
g31n01:617761:618142 [5] NCCL INFO Channel 02/0 : 3[5] -> 4[0] [send] via NET/IB/1
g31n01:617761:618142 [5] NCCL INFO Channel 01/0 : 3[5] -> 0[2] via P2P/IPC
g28n01:770099:770473 [4] NCCL INFO Channel 02/0 : 2[4] -> 1[3] via P2P/IPC
g31n01:617759:618141 [3] NCCL INFO Channel 01/0 : 4[0] -> 1[3] [receive] via NET/IB/3
g31n01:617759:618141 [3] NCCL INFO Channel 03/0 : 4[0] -> 1[3] [receive] via NET/IB/3
g26n02:851592:851974 [3] NCCL INFO Channel 00/0 : 7[3] -> 0[2] [send] via NET/IB/1
g26n02:851592:851974 [3] NCCL INFO Channel 02/0 : 7[3] -> 0[2] [send] via NET/IB/1
g26n02:851592:851974 [3] NCCL INFO Channel 01/0 : 0[2] -> 7[3] [receive] via NET/IB/3
g26n02:851592:851974 [3] NCCL INFO Channel 03/0 : 0[2] -> 7[3] [receive] via NET/IB/3
g26n02:851592:851974 [3] NCCL INFO Channel 01/0 : 7[3] -> 6[2] via P2P/IPC
g26n01:811334:811707 [3] NCCL INFO Channel 02/0 : 1[3] -> 2[4] via P2P/IPC
g31n02:736463:736849 [0] NCCL INFO Channel 01/0 : 4[0] -> 1[3] [send] via NET/IB/2
g31n02:736463:736849 [0] NCCL INFO Channel 03/0 : 4[0] -> 1[3] [send] via NET/IB/2
g26n01:811335:811708 [4] NCCL INFO Channel 00/0 : 2[4] -> 3[5] via P2P/IPC
g28n01:770099:770473 [4] NCCL INFO Channel 03/0 : 2[4] -> 1[3] via P2P/IPC
g26n01:811334:811707 [3] NCCL INFO Channel 03/0 : 1[3] -> 2[4] via P2P/IPC
g26n01:811335:811708 [4] NCCL INFO Channel 01/0 : 2[4] -> 3[5] via P2P/IPC
g31n01:617761:618142 [5] NCCL INFO Channel 03/0 : 3[5] -> 0[2] via P2P/IPC
g26n02:851592:851974 [3] NCCL INFO Channel 03/0 : 7[3] -> 6[2] via P2P/IPC
g26n01:811335:811708 [4] NCCL INFO Channel 02/0 : 2[4] -> 3[5] via P2P/IPC
g26n02:851591:851972 [2] NCCL INFO Channel 01/0 : 6[2] -> 5[1] via P2P/IPC
g26n02:851590:851971 [1] NCCL INFO Channel 01/0 : 5[1] -> 4[0] via P2P/IPC
g26n01:811335:811708 [4] NCCL INFO Channel 03/0 : 2[4] -> 3[5] via P2P/IPC
g26n02:851591:851972 [2] NCCL INFO Channel 03/0 : 6[2] -> 5[1] via P2P/IPC
g26n02:851590:851971 [1] NCCL INFO Channel 03/0 : 5[1] -> 4[0] via P2P/IPC
g31n01:617758:618138 [2] NCCL INFO Channel 01/0 : 0[2] -> 7[3] [send] via NET/IB/2
g31n01:617758:618138 [2] NCCL INFO Channel 03/0 : 0[2] -> 7[3] [send] via NET/IB/2
g26n01:811336:811706 [5] NCCL INFO Channel 00/0 : 3[5] -> 4[0] [send] via NET/IB/1
g26n01:811336:811706 [5] NCCL INFO Channel 02/0 : 3[5] -> 4[0] [send] via NET/IB/1
g26n01:811336:811706 [5] NCCL INFO Channel 01/0 : 3[5] -> 0[2] via P2P/IPC
g31n02:736465:736845 [2] NCCL INFO Connected all rings
g26n01:811334:811707 [3] NCCL INFO Channel 01/0 : 4[0] -> 1[3] [receive] via NET/IB/3
g26n01:811334:811707 [3] NCCL INFO Channel 03/0 : 4[0] -> 1[3] [receive] via NET/IB/3
g26n02:851589:851973 [0] NCCL INFO Channel 01/0 : 4[0] -> 1[3] [send] via NET/IB/2
g26n02:851589:851973 [0] NCCL INFO Channel 03/0 : 4[0] -> 1[3] [send] via NET/IB/2
g26n01:811336:811706 [5] NCCL INFO Channel 03/0 : 3[5] -> 0[2] via P2P/IPC
g28n02:792068:792468 [0] NCCL INFO Connected all rings
g28n02:792068:792468 [0] NCCL INFO Channel 01/0 : 4[0] -> 5[1] via P2P/IPC
g28n02:792068:792468 [0] NCCL INFO Channel 03/0 : 4[0] -> 5[1] via P2P/IPC
g28n01:770095:770470 [2] NCCL INFO Connected all rings
g28n01:770095:770470 [2] NCCL INFO Channel 00/0 : 0[2] -> 3[5] via P2P/IPC
g31n01:617760:618143 [4] NCCL INFO Connected all rings
g26n02:851591:851972 [2] NCCL INFO Connected all rings
g28n01:770095:770470 [2] NCCL INFO Channel 01/0 : 0[2] -> 3[5] via P2P/IPC
g26n01:811333:811705 [2] NCCL INFO Channel 01/0 : 0[2] -> 7[3] [send] via NET/IB/2
g26n01:811333:811705 [2] NCCL INFO Channel 03/0 : 0[2] -> 7[3] [send] via NET/IB/2
g28n01:770095:770470 [2] NCCL INFO Channel 02/0 : 0[2] -> 3[5] via P2P/IPC
g28n01:770096:770472 [3] NCCL INFO Connected all rings
g28n01:770096:770472 [3] NCCL INFO Channel 00/0 : 1[3] -> 4[0] [send] via NET/IB/3
g28n01:770096:770472 [3] NCCL INFO Channel 01/0 : 1[3] -> 4[0] [send] via NET/IB/0
g28n01:770096:770472 [3] NCCL INFO Channel 02/0 : 1[3] -> 4[0] [send] via NET/IB/3
g28n01:770096:770472 [3] NCCL INFO Channel 03/0 : 1[3] -> 4[0] [send] via NET/IB/0
g28n01:770096:770472 [3] NCCL INFO Channel 00/0 : 4[0] -> 1[3] [receive] via NET/IB/3
g28n01:770096:770472 [3] NCCL INFO Channel 02/0 : 4[0] -> 1[3] [receive] via NET/IB/3
g31n02:736464:736846 [1] NCCL INFO Connected all rings
g28n02:792068:792468 [0] NCCL INFO Channel 00/0 : 1[3] -> 4[0] [receive] via NET/IB/0
g28n02:792068:792468 [0] NCCL INFO Channel 01/0 : 1[3] -> 4[0] [receive] via NET/IB/3
g28n02:792068:792468 [0] NCCL INFO Channel 02/0 : 1[3] -> 4[0] [receive] via NET/IB/0
g28n02:792068:792468 [0] NCCL INFO Channel 03/0 : 1[3] -> 4[0] [receive] via NET/IB/3
g28n02:792068:792468 [0] NCCL INFO Channel 00/0 : 4[0] -> 1[3] [send] via NET/IB/0
g28n02:792068:792468 [0] NCCL INFO Channel 02/0 : 4[0] -> 1[3] [send] via NET/IB/0
g28n01:770095:770470 [2] NCCL INFO Channel 03/0 : 0[2] -> 3[5] via P2P/IPC
g28n02:792072:792470 [3] NCCL INFO Connected all rings
g31n02:736465:736845 [2] NCCL INFO Channel 01/0 : 6[2] -> 7[3] via P2P/IPC
g28n02:792069:792467 [1] NCCL INFO Channel 00/0 : 5[1] -> 4[0] via P2P/IPC
g28n01:770100:770471 [5] NCCL INFO Channel 00/0 : 3[5] -> 0[2] via P2P/IPC
g28n02:792069:792467 [1] NCCL INFO Channel 02/0 : 5[1] -> 4[0] via P2P/IPC
g31n01:617760:618143 [4] NCCL INFO Channel 00/0 : 2[4] -> 1[3] via P2P/IPC
g31n02:736465:736845 [2] NCCL INFO Channel 03/0 : 6[2] -> 7[3] via P2P/IPC
g31n02:736464:736846 [1] NCCL INFO Channel 01/0 : 5[1] -> 6[2] via P2P/IPC
g31n01:617760:618143 [4] NCCL INFO Channel 01/0 : 2[4] -> 1[3] via P2P/IPC
g28n02:792072:792470 [3] NCCL INFO Channel 00/0 : 7[3] -> 6[2] via P2P/IPC
g31n02:736464:736846 [1] NCCL INFO Channel 03/0 : 5[1] -> 6[2] via P2P/IPC
g26n02:851591:851972 [2] NCCL INFO Channel 01/0 : 6[2] -> 7[3] via P2P/IPC
g28n01:770100:770471 [5] NCCL INFO Channel 02/0 : 3[5] -> 0[2] via P2P/IPC
g31n01:617760:618143 [4] NCCL INFO Channel 02/0 : 2[4] -> 1[3] via P2P/IPC
g31n01:617760:618143 [4] NCCL INFO Channel 03/0 : 2[4] -> 1[3] via P2P/IPC
g26n01:811335:811708 [4] NCCL INFO Connected all rings
g28n02:792072:792470 [3] NCCL INFO Channel 02/0 : 7[3] -> 6[2] via P2P/IPC
g26n02:851591:851972 [2] NCCL INFO Channel 03/0 : 6[2] -> 7[3] via P2P/IPC
g28n01:770100:770471 [5] NCCL INFO Channel 00/0 : 3[5] -> 2[4] via P2P/IPC
g28n02:792071:792469 [2] NCCL INFO Channel 00/0 : 6[2] -> 5[1] via P2P/IPC
g26n01:811336:811706 [5] NCCL INFO Connected all rings
g28n01:770100:770471 [5] NCCL INFO Channel 01/0 : 3[5] -> 2[4] via P2P/IPC
g28n02:792071:792469 [2] NCCL INFO Channel 02/0 : 6[2] -> 5[1] via P2P/IPC
g26n02:851590:851971 [1] NCCL INFO Connected all rings
g31n05:704347:704725 [3] NCCL INFO Trees [0] 2/4/-1->1->-1 [1] 2/4/-1->1->-1 [2] 2/-1/-1->1->4 [3] 2/-1/-1->1->4
g31n05:704347:704725 [3] NCCL INFO P2P Chunksize set to 131072
g32n02:753685:754070 [2] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5
g32n02:753685:754070 [2] NCCL INFO P2P Chunksize set to 131072
g31n05:704348:704723 [4] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1
g31n05:704348:704723 [4] NCCL INFO P2P Chunksize set to 131072
g31n01:617761:618142 [5] NCCL INFO Connected all rings
g32n02:753684:754069 [1] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4
g32n02:753684:754069 [1] NCCL INFO P2P Chunksize set to 131072
g28n01:770100:770471 [5] NCCL INFO Channel 02/0 : 3[5] -> 2[4] via P2P/IPC
g31n05:704349:704724 [5] NCCL INFO Trees [0] 0/-1/-1->3->2 [1] 0/-1/-1->3->2 [2] 0/-1/-1->3->2 [3] 0/-1/-1->3->2
g31n05:704349:704724 [5] NCCL INFO P2P Chunksize set to 131072
g32n02:753686:754071 [3] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] -1/-1/-1->7->6 [3] -1/-1/-1->7->6
g32n02:753686:754071 [3] NCCL INFO P2P Chunksize set to 131072
g26n02:851590:851971 [1] NCCL INFO Channel 01/0 : 5[1] -> 6[2] via P2P/IPC
g31n05:704346:704720 [2] NCCL INFO Channel 00/04 :    0   1   2   3   4   5   6   7
g31n05:704346:704720 [2] NCCL INFO Channel 01/04 :    0   7   6   5   4   1   2   3
g31n05:704346:704720 [2] NCCL INFO Channel 02/04 :    0   1   2   3   4   5   6   7
g31n05:704346:704720 [2] NCCL INFO Channel 03/04 :    0   7   6   5   4   1   2   3
g31n05:704346:704720 [2] NCCL INFO Trees [0] -1/-1/-1->0->3 [1] -1/-1/-1->0->3 [2] -1/-1/-1->0->3 [3] -1/-1/-1->0->3
g31n05:704346:704720 [2] NCCL INFO P2P Chunksize set to 131072
g31n05:704346:704720 [2] NCCL INFO Channel 00/0 : 7[3] -> 0[2] [receive] via NET/IB/0
g31n05:704346:704720 [2] NCCL INFO Channel 02/0 : 7[3] -> 0[2] [receive] via NET/IB/0
g31n05:704346:704720 [2] NCCL INFO Channel 00/0 : 0[2] -> 1[3] via P2P/IPC
g32n02:753683:754068 [0] NCCL INFO Trees [0] 5/-1/-1->4->1 [1] 5/-1/-1->4->1 [2] 5/1/-1->4->-1 [3] 5/1/-1->4->-1
g32n02:753683:754068 [0] NCCL INFO P2P Chunksize set to 131072
g32n02:753683:754068 [0] NCCL INFO Channel 00/0 : 3[5] -> 4[0] [receive] via NET/IB/0
g32n02:753683:754068 [0] NCCL INFO Channel 02/0 : 3[5] -> 4[0] [receive] via NET/IB/0
g32n02:753683:754068 [0] NCCL INFO Channel 00/0 : 4[0] -> 5[1] via P2P/IPC
g26n01:811335:811708 [4] NCCL INFO Channel 00/0 : 2[4] -> 1[3] via P2P/IPC
g26n02:851590:851971 [1] NCCL INFO Channel 03/0 : 5[1] -> 6[2] via P2P/IPC
g28n01:770100:770471 [5] NCCL INFO Channel 03/0 : 3[5] -> 2[4] via P2P/IPC
g28n01:770095:770470 [2] NCCL INFO Connected all trees
g28n01:770095:770470 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g28n01:770095:770470 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g26n01:811335:811708 [4] NCCL INFO Channel 01/0 : 2[4] -> 1[3] via P2P/IPC
g31n05:704346:704720 [2] NCCL INFO Channel 02/0 : 0[2] -> 1[3] via P2P/IPC
g32n02:753683:754068 [0] NCCL INFO Channel 02/0 : 4[0] -> 5[1] via P2P/IPC
g26n01:811335:811708 [4] NCCL INFO Channel 02/0 : 2[4] -> 1[3] via P2P/IPC
g26n01:811335:811708 [4] NCCL INFO Channel 03/0 : 2[4] -> 1[3] via P2P/IPC
g32n02:753685:754070 [2] NCCL INFO Channel 00/0 : 6[2] -> 7[3] via P2P/IPC
g32n02:753684:754069 [1] NCCL INFO Channel 00/0 : 5[1] -> 6[2] via P2P/IPC
g31n05:704347:704725 [3] NCCL INFO Channel 00/0 : 1[3] -> 2[4] via P2P/IPC
g31n02:736463:736849 [0] NCCL INFO Connected all rings
g31n02:736463:736849 [0] NCCL INFO Channel 01/0 : 4[0] -> 5[1] via P2P/IPC
g32n02:753685:754070 [2] NCCL INFO Channel 02/0 : 6[2] -> 7[3] via P2P/IPC
g32n02:753684:754069 [1] NCCL INFO Channel 02/0 : 5[1] -> 6[2] via P2P/IPC
g31n02:736463:736849 [0] NCCL INFO Channel 03/0 : 4[0] -> 5[1] via P2P/IPC
g31n05:704347:704725 [3] NCCL INFO Channel 01/0 : 1[3] -> 2[4] via P2P/IPC
g31n01:617758:618138 [2] NCCL INFO Connected all rings
g31n01:617758:618138 [2] NCCL INFO Channel 00/0 : 0[2] -> 3[5] via P2P/IPC
g32n02:753686:754071 [3] NCCL INFO Channel 00/0 : 7[3] -> 0[2] [send] via NET/IB/1
g32n02:753686:754071 [3] NCCL INFO Channel 02/0 : 7[3] -> 0[2] [send] via NET/IB/1
g32n02:753686:754071 [3] NCCL INFO Channel 01/0 : 0[2] -> 7[3] [receive] via NET/IB/3
g32n02:753686:754071 [3] NCCL INFO Channel 03/0 : 0[2] -> 7[3] [receive] via NET/IB/3
g32n02:753686:754071 [3] NCCL INFO Channel 01/0 : 7[3] -> 6[2] via P2P/IPC
g31n05:704348:704723 [4] NCCL INFO Channel 00/0 : 2[4] -> 3[5] via P2P/IPC
g31n05:704347:704725 [3] NCCL INFO Channel 02/0 : 1[3] -> 2[4] via P2P/IPC
g31n01:617758:618138 [2] NCCL INFO Channel 01/0 : 0[2] -> 3[5] via P2P/IPC
g28n02:792072:792470 [3] NCCL INFO Connected all trees
g28n02:792072:792470 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g28n02:792072:792470 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g31n05:704348:704723 [4] NCCL INFO Channel 01/0 : 2[4] -> 3[5] via P2P/IPC
g31n05:704347:704725 [3] NCCL INFO Channel 03/0 : 1[3] -> 2[4] via P2P/IPC
g31n01:617759:618141 [3] NCCL INFO Connected all rings
g31n01:617759:618141 [3] NCCL INFO Channel 00/0 : 1[3] -> 4[0] [send] via NET/IB/3
g31n01:617759:618141 [3] NCCL INFO Channel 01/0 : 1[3] -> 4[0] [send] via NET/IB/0
g31n01:617759:618141 [3] NCCL INFO Channel 02/0 : 1[3] -> 4[0] [send] via NET/IB/3
g31n01:617759:618141 [3] NCCL INFO Channel 03/0 : 1[3] -> 4[0] [send] via NET/IB/0
g31n01:617759:618141 [3] NCCL INFO Channel 00/0 : 4[0] -> 1[3] [receive] via NET/IB/3
g31n01:617759:618141 [3] NCCL INFO Channel 02/0 : 4[0] -> 1[3] [receive] via NET/IB/3
g31n01:617758:618138 [2] NCCL INFO Channel 02/0 : 0[2] -> 3[5] via P2P/IPC
g32n02:753686:754071 [3] NCCL INFO Channel 03/0 : 7[3] -> 6[2] via P2P/IPC
g31n02:736463:736849 [0] NCCL INFO Channel 00/0 : 1[3] -> 4[0] [receive] via NET/IB/0
g31n02:736463:736849 [0] NCCL INFO Channel 01/0 : 1[3] -> 4[0] [receive] via NET/IB/3
g31n02:736463:736849 [0] NCCL INFO Channel 02/0 : 1[3] -> 4[0] [receive] via NET/IB/0
g31n02:736463:736849 [0] NCCL INFO Channel 03/0 : 1[3] -> 4[0] [receive] via NET/IB/3
g31n02:736463:736849 [0] NCCL INFO Channel 00/0 : 4[0] -> 1[3] [send] via NET/IB/0
g31n02:736463:736849 [0] NCCL INFO Channel 02/0 : 4[0] -> 1[3] [send] via NET/IB/0
g31n05:704348:704723 [4] NCCL INFO Channel 02/0 : 2[4] -> 3[5] via P2P/IPC
g31n02:736466:736847 [3] NCCL INFO Connected all rings
g28n02:792069:792467 [1] NCCL INFO Connected all trees
g28n02:792069:792467 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g28n02:792069:792467 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g31n01:617758:618138 [2] NCCL INFO Channel 03/0 : 0[2] -> 3[5] via P2P/IPC
g32n02:753685:754070 [2] NCCL INFO Channel 01/0 : 6[2] -> 5[1] via P2P/IPC
g32n02:753684:754069 [1] NCCL INFO Channel 01/0 : 5[1] -> 4[0] via P2P/IPC
g31n05:704348:704723 [4] NCCL INFO Channel 03/0 : 2[4] -> 3[5] via P2P/IPC
g32n02:753685:754070 [2] NCCL INFO Channel 03/0 : 6[2] -> 5[1] via P2P/IPC
g31n02:736464:736846 [1] NCCL INFO Channel 00/0 : 5[1] -> 4[0] via P2P/IPC
g32n02:753684:754069 [1] NCCL INFO Channel 03/0 : 5[1] -> 4[0] via P2P/IPC
g28n02:792071:792469 [2] NCCL INFO Connected all trees
g28n02:792071:792469 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g28n02:792071:792469 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g26n02:851589:851973 [0] NCCL INFO Connected all rings
g26n02:851589:851973 [0] NCCL INFO Channel 01/0 : 4[0] -> 5[1] via P2P/IPC
g31n01:617761:618142 [5] NCCL INFO Channel 00/0 : 3[5] -> 0[2] via P2P/IPC
g31n02:736464:736846 [1] NCCL INFO Channel 02/0 : 5[1] -> 4[0] via P2P/IPC
g31n05:704349:704724 [5] NCCL INFO Channel 00/0 : 3[5] -> 4[0] [send] via NET/IB/1
g31n05:704349:704724 [5] NCCL INFO Channel 02/0 : 3[5] -> 4[0] [send] via NET/IB/1
g31n05:704349:704724 [5] NCCL INFO Channel 01/0 : 3[5] -> 0[2] via P2P/IPC
g26n02:851589:851973 [0] NCCL INFO Channel 03/0 : 4[0] -> 5[1] via P2P/IPC
g31n02:736466:736847 [3] NCCL INFO Channel 00/0 : 7[3] -> 6[2] via P2P/IPC
g32n02:753683:754068 [0] NCCL INFO Channel 01/0 : 4[0] -> 1[3] [send] via NET/IB/2
g32n02:753683:754068 [0] NCCL INFO Channel 03/0 : 4[0] -> 1[3] [send] via NET/IB/2
g26n01:811333:811705 [2] NCCL INFO Connected all rings
g26n01:811333:811705 [2] NCCL INFO Channel 00/0 : 0[2] -> 3[5] via P2P/IPC
g31n05:704347:704725 [3] NCCL INFO Channel 01/0 : 4[0] -> 1[3] [receive] via NET/IB/3
g31n05:704347:704725 [3] NCCL INFO Channel 03/0 : 4[0] -> 1[3] [receive] via NET/IB/3
g31n01:617761:618142 [5] NCCL INFO Channel 02/0 : 3[5] -> 0[2] via P2P/IPC
g26n01:811333:811705 [2] NCCL INFO Channel 01/0 : 0[2] -> 3[5] via P2P/IPC
g28n01:770099:770473 [4] NCCL INFO Connected all trees
g28n01:770099:770473 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g28n01:770099:770473 [4] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g28n01:770100:770471 [5] NCCL INFO Connected all trees
g28n01:770100:770471 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g28n01:770100:770471 [5] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g31n02:736466:736847 [3] NCCL INFO Channel 02/0 : 7[3] -> 6[2] via P2P/IPC
g26n01:811334:811707 [3] NCCL INFO Connected all rings
g26n01:811334:811707 [3] NCCL INFO Channel 00/0 : 1[3] -> 4[0] [send] via NET/IB/3
g26n01:811334:811707 [3] NCCL INFO Channel 01/0 : 1[3] -> 4[0] [send] via NET/IB/0
g26n01:811334:811707 [3] NCCL INFO Channel 02/0 : 1[3] -> 4[0] [send] via NET/IB/3
g26n01:811334:811707 [3] NCCL INFO Channel 03/0 : 1[3] -> 4[0] [send] via NET/IB/0
g26n01:811334:811707 [3] NCCL INFO Channel 00/0 : 4[0] -> 1[3] [receive] via NET/IB/3
g26n01:811334:811707 [3] NCCL INFO Channel 02/0 : 4[0] -> 1[3] [receive] via NET/IB/3
g26n01:811333:811705 [2] NCCL INFO Channel 02/0 : 0[2] -> 3[5] via P2P/IPC
g31n01:617761:618142 [5] NCCL INFO Channel 00/0 : 3[5] -> 2[4] via P2P/IPC
g26n02:851589:851973 [0] NCCL INFO Channel 00/0 : 1[3] -> 4[0] [receive] via NET/IB/0
g26n02:851589:851973 [0] NCCL INFO Channel 01/0 : 1[3] -> 4[0] [receive] via NET/IB/3
g26n02:851589:851973 [0] NCCL INFO Channel 02/0 : 1[3] -> 4[0] [receive] via NET/IB/0
g26n02:851589:851973 [0] NCCL INFO Channel 03/0 : 1[3] -> 4[0] [receive] via NET/IB/3
g26n02:851589:851973 [0] NCCL INFO Channel 00/0 : 4[0] -> 1[3] [send] via NET/IB/0
g26n02:851589:851973 [0] NCCL INFO Channel 02/0 : 4[0] -> 1[3] [send] via NET/IB/0
g26n02:851592:851974 [3] NCCL INFO Connected all rings
g31n02:736465:736845 [2] NCCL INFO Channel 00/0 : 6[2] -> 5[1] via P2P/IPC
g26n01:811333:811705 [2] NCCL INFO Channel 03/0 : 0[2] -> 3[5] via P2P/IPC
g26n02:851590:851971 [1] NCCL INFO Channel 00/0 : 5[1] -> 4[0] via P2P/IPC
g31n02:736465:736845 [2] NCCL INFO Channel 02/0 : 6[2] -> 5[1] via P2P/IPC
g31n05:704349:704724 [5] NCCL INFO Channel 03/0 : 3[5] -> 0[2] via P2P/IPC
g26n02:851590:851971 [1] NCCL INFO Channel 02/0 : 5[1] -> 4[0] via P2P/IPC
g31n01:617761:618142 [5] NCCL INFO Channel 01/0 : 3[5] -> 2[4] via P2P/IPC
g26n01:811336:811706 [5] NCCL INFO Channel 00/0 : 3[5] -> 0[2] via P2P/IPC
g32n02:753685:754070 [2] NCCL INFO Connected all rings
g31n01:617761:618142 [5] NCCL INFO Channel 02/0 : 3[5] -> 2[4] via P2P/IPC
g26n02:851592:851974 [3] NCCL INFO Channel 00/0 : 7[3] -> 6[2] via P2P/IPC
g26n01:811336:811706 [5] NCCL INFO Channel 02/0 : 3[5] -> 0[2] via P2P/IPC
g31n01:617761:618142 [5] NCCL INFO Channel 03/0 : 3[5] -> 2[4] via P2P/IPC
g31n05:704346:704720 [2] NCCL INFO Channel 01/0 : 0[2] -> 7[3] [send] via NET/IB/2
g31n05:704346:704720 [2] NCCL INFO Channel 03/0 : 0[2] -> 7[3] [send] via NET/IB/2
g31n01:617758:618138 [2] NCCL INFO Connected all trees
g31n01:617758:618138 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g31n01:617758:618138 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g26n02:851592:851974 [3] NCCL INFO Channel 02/0 : 7[3] -> 6[2] via P2P/IPC
g31n05:704348:704723 [4] NCCL INFO Connected all rings
g26n02:851591:851972 [2] NCCL INFO Channel 00/0 : 6[2] -> 5[1] via P2P/IPC
g31n02:736466:736847 [3] NCCL INFO Connected all trees
g31n02:736466:736847 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g31n02:736466:736847 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g26n02:851591:851972 [2] NCCL INFO Channel 02/0 : 6[2] -> 5[1] via P2P/IPC
g32n02:753685:754070 [2] NCCL INFO Channel 01/0 : 6[2] -> 7[3] via P2P/IPC
g26n01:811336:811706 [5] NCCL INFO Channel 00/0 : 3[5] -> 2[4] via P2P/IPC
g26n01:811336:811706 [5] NCCL INFO Channel 01/0 : 3[5] -> 2[4] via P2P/IPC
g32n02:753685:754070 [2] NCCL INFO Channel 03/0 : 6[2] -> 7[3] via P2P/IPC
g26n01:811333:811705 [2] NCCL INFO Connected all trees
g26n01:811333:811705 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g26n01:811333:811705 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g26n01:811336:811706 [5] NCCL INFO Channel 02/0 : 3[5] -> 2[4] via P2P/IPC
g32n02:753684:754069 [1] NCCL INFO Connected all rings
g26n01:811336:811706 [5] NCCL INFO Channel 03/0 : 3[5] -> 2[4] via P2P/IPC
g31n05:704348:704723 [4] NCCL INFO Channel 00/0 : 2[4] -> 1[3] via P2P/IPC
g31n05:704348:704723 [4] NCCL INFO Channel 01/0 : 2[4] -> 1[3] via P2P/IPC
g32n02:753684:754069 [1] NCCL INFO Channel 01/0 : 5[1] -> 6[2] via P2P/IPC
g31n02:736464:736846 [1] NCCL INFO Connected all trees
g31n02:736464:736846 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g31n02:736464:736846 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g28n02:792068:792468 [0] NCCL INFO Connected all trees
g28n02:792068:792468 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g28n02:792068:792468 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g31n05:704348:704723 [4] NCCL INFO Channel 02/0 : 2[4] -> 1[3] via P2P/IPC
g28n01:770096:770472 [3] NCCL INFO Connected all trees
g28n01:770096:770472 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g28n01:770096:770472 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g32n02:753684:754069 [1] NCCL INFO Channel 03/0 : 5[1] -> 6[2] via P2P/IPC
g31n02:736465:736845 [2] NCCL INFO Connected all trees
g31n02:736465:736845 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g31n02:736465:736845 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g31n05:704348:704723 [4] NCCL INFO Channel 03/0 : 2[4] -> 1[3] via P2P/IPC
g31n01:617760:618143 [4] NCCL INFO Connected all trees
g31n01:617760:618143 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g31n01:617760:618143 [4] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g31n01:617761:618142 [5] NCCL INFO Connected all trees
g31n01:617761:618142 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g31n01:617761:618142 [5] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g26n02:851592:851974 [3] NCCL INFO Connected all trees
g26n02:851592:851974 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g26n02:851592:851974 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g31n05:704349:704724 [5] NCCL INFO Connected all rings
g26n02:851590:851971 [1] NCCL INFO Connected all trees
g26n02:851590:851971 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g26n02:851590:851971 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g26n02:851591:851972 [2] NCCL INFO Connected all trees
g26n02:851591:851972 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g26n02:851591:851972 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g27n17:842060:842446 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g27n17:842063:842442 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g27n17:842061:842441 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g28n01:770096:770472 [3] NCCL INFO comm 0x1c5693460 rank 1 nranks 8 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x16d0e2c5f1ddffab - Init COMPLETE
g28n01:770100:770471 [5] NCCL INFO comm 0x196de93a0 rank 3 nranks 8 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x16d0e2c5f1ddffab - Init COMPLETE
g28n01:770099:770473 [4] NCCL INFO comm 0x1b8fd8c70 rank 2 nranks 8 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x16d0e2c5f1ddffab - Init COMPLETE
g28n01:770095:770470 [2] NCCL INFO comm 0x1c858e5b0 rank 0 nranks 8 cudaDev 2 nvmlDev 2 busId 406000 commId 0x16d0e2c5f1ddffab - Init COMPLETE
g28n01:770095:770484 [2] NCCL INFO Channel 02/1 : 7[3] -> 0[2] [receive] via NET/IB/0/Shared
g28n01:770095:770484 [2] NCCL INFO Channel 03/1 : 7[3] -> 0[2] [receive] via NET/IB/2/Shared
g28n01:770095:770484 [2] NCCL INFO Channel 00/1 : 0[2] -> 1[3] via P2P/IPC
g31n03:688512:688885 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g26n01:811335:811708 [4] NCCL INFO Connected all trees
g26n01:811335:811708 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g26n01:811335:811708 [4] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g27n17:842068:842443 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g28n02:792071:792469 [2] NCCL INFO comm 0x18aedcad0 rank 6 nranks 8 cudaDev 2 nvmlDev 2 busId 406000 commId 0x16d0e2c5f1ddffab - Init COMPLETE
g31n03:688513:688886 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g28n04:841284:841669 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g31n03:688514:688888 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g26n01:811336:811706 [5] NCCL INFO Connected all trees
g26n01:811336:811706 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g26n01:811336:811706 [5] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g28n02:792069:792467 [1] NCCL INFO comm 0x1cb637770 rank 5 nranks 8 cudaDev 1 nvmlDev 1 busId 405000 commId 0x16d0e2c5f1ddffab - Init COMPLETE
g27n17:842064:842445 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g28n04:841283:841665 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g27n17:842067:842444 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g28n02:792068:792468 [0] NCCL INFO comm 0x176c99080 rank 4 nranks 8 cudaDev 0 nvmlDev 0 busId 404000 commId 0x16d0e2c5f1ddffab - Init COMPLETE
g28n02:792068:792480 [0] NCCL INFO Channel 02/1 : 3[5] -> 4[0] [receive] via NET/IB/0/Shared
g28n02:792068:792480 [0] NCCL INFO Channel 03/1 : 3[5] -> 4[0] [receive] via NET/IB/2/Shared
g28n02:792068:792480 [0] NCCL INFO Channel 00/1 : 4[0] -> 5[1] via P2P/IPC
g31n03:688510:688889 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g28n04:841287:841667 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g31n03:688511:688887 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g28n02:792072:792470 [3] NCCL INFO comm 0x1c7ecd950 rank 7 nranks 8 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x16d0e2c5f1ddffab - Init COMPLETE
g28n01:770095:770484 [2] NCCL INFO Channel 01/1 : 0[2] -> 1[3] via P2P/IPC
g31n03:688509:688884 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g28n04:841288:841668 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g28n04:841280:841664 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g28n04:841281:841666 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g32n02:753683:754068 [0] NCCL INFO Connected all rings
g32n02:753683:754068 [0] NCCL INFO Channel 01/0 : 4[0] -> 5[1] via P2P/IPC
g31n05:704347:704725 [3] NCCL INFO Connected all rings
g31n05:704347:704725 [3] NCCL INFO Channel 00/0 : 1[3] -> 4[0] [send] via NET/IB/3
g31n05:704347:704725 [3] NCCL INFO Channel 01/0 : 1[3] -> 4[0] [send] via NET/IB/0
g31n05:704347:704725 [3] NCCL INFO Channel 02/0 : 1[3] -> 4[0] [send] via NET/IB/3
g31n05:704347:704725 [3] NCCL INFO Channel 03/0 : 1[3] -> 4[0] [send] via NET/IB/0
g32n02:753683:754068 [0] NCCL INFO Channel 03/0 : 4[0] -> 5[1] via P2P/IPC
g28n01:770096:770485 [3] NCCL INFO Channel 00/1 : 1[3] -> 2[4] via P2P/IPC
g28n02:792068:792480 [0] NCCL INFO Channel 01/1 : 4[0] -> 5[1] via P2P/IPC
g28n01:770099:770487 [4] NCCL INFO Channel 00/1 : 2[4] -> 3[5] via P2P/IPC
g25n18:898855:899337 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g25n18:898857:899338 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g28n02:792071:792479 [2] NCCL INFO Channel 00/1 : 6[2] -> 7[3] via P2P/IPC
g28n01:770099:770487 [4] NCCL INFO Channel 01/1 : 2[4] -> 3[5] via P2P/IPC
g28n02:792069:792481 [1] NCCL INFO Channel 00/1 : 5[1] -> 6[2] via P2P/IPC
g28n01:770096:770485 [3] NCCL INFO Channel 01/1 : 1[3] -> 2[4] via P2P/IPC
g25n18:898853:899335 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g31n05:704347:704725 [3] NCCL INFO Channel 00/0 : 4[0] -> 1[3] [receive] via NET/IB/3
g31n05:704347:704725 [3] NCCL INFO Channel 02/0 : 4[0] -> 1[3] [receive] via NET/IB/3
g28n02:792071:792479 [2] NCCL INFO Channel 01/1 : 6[2] -> 7[3] via P2P/IPC
g31n04:688225:688608 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g28n02:792069:792481 [1] NCCL INFO Channel 01/1 : 5[1] -> 6[2] via P2P/IPC
g31n04:688224:688606 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g32n02:753683:754068 [0] NCCL INFO Channel 00/0 : 1[3] -> 4[0] [receive] via NET/IB/0
g32n02:753683:754068 [0] NCCL INFO Channel 01/0 : 1[3] -> 4[0] [receive] via NET/IB/3
g32n02:753683:754068 [0] NCCL INFO Channel 02/0 : 1[3] -> 4[0] [receive] via NET/IB/0
g32n02:753683:754068 [0] NCCL INFO Channel 03/0 : 1[3] -> 4[0] [receive] via NET/IB/3
g32n02:753683:754068 [0] NCCL INFO Channel 00/0 : 4[0] -> 1[3] [send] via NET/IB/0
g32n02:753683:754068 [0] NCCL INFO Channel 02/0 : 4[0] -> 1[3] [send] via NET/IB/0
g32n02:753684:754069 [1] NCCL INFO Channel 00/0 : 5[1] -> 4[0] via P2P/IPC
g28n01:770096:770485 [3] NCCL INFO Channel 02/1 : 7[3] -> 1[3] [receive] via NET/IB/1/Shared
g28n01:770096:770485 [3] NCCL INFO Channel 03/1 : 7[3] -> 1[3] [receive] via NET/IB/3/Shared
g28n01:770096:770485 [3] NCCL INFO Channel 00/1 : 1[3] -> 3[5] via P2P/IPC
g28n01:770100:770486 [5] NCCL INFO Channel 02/1 : 3[5] -> 4[0] [send] via NET/IB/1/Shared
g28n01:770100:770486 [5] NCCL INFO Channel 03/1 : 3[5] -> 4[0] [send] via NET/IB/3/Shared
g31n05:704346:704720 [2] NCCL INFO Connected all rings
g31n05:704346:704720 [2] NCCL INFO Channel 00/0 : 0[2] -> 3[5] via P2P/IPC
g28n01:770095:770484 [2] NCCL INFO Channel 02/1 : 6[2] -> 0[2] [receive] via NET/IB/0/Shared
g28n01:770095:770484 [2] NCCL INFO Channel 03/1 : 6[2] -> 0[2] [receive] via NET/IB/2/Shared
g28n01:770095:770484 [2] NCCL INFO Channel 00/1 : 0[2] -> 2[4] via P2P/IPC
g25n18:898854:899336 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g28n02:792072:792482 [3] NCCL INFO Channel 02/1 : 7[3] -> 0[2] [send] via NET/IB/1/Shared
g28n02:792072:792482 [3] NCCL INFO Channel 03/1 : 7[3] -> 0[2] [send] via NET/IB/3/Shared
g28n01:770095:770484 [2] NCCL INFO Channel 01/1 : 0[2] -> 2[4] via P2P/IPC
g32n02:753684:754069 [1] NCCL INFO Channel 02/0 : 5[1] -> 4[0] via P2P/IPC
g28n02:792068:792480 [0] NCCL INFO Channel 02/1 : 2[4] -> 4[0] [receive] via NET/IB/0/Shared
g28n02:792068:792480 [0] NCCL INFO Channel 03/1 : 2[4] -> 4[0] [receive] via NET/IB/2/Shared
g28n02:792068:792480 [0] NCCL INFO Channel 00/1 : 4[0] -> 6[2] via P2P/IPC
g28n02:792069:792481 [1] NCCL INFO Channel 02/1 : 3[5] -> 5[1] [receive] via NET/IB/2/Shared
g28n02:792069:792481 [1] NCCL INFO Channel 03/1 : 3[5] -> 5[1] [receive] via NET/IB/0/Shared
g28n02:792069:792481 [1] NCCL INFO Channel 00/1 : 5[1] -> 7[3] via P2P/IPC
g31n05:704346:704720 [2] NCCL INFO Channel 01/0 : 0[2] -> 3[5] via P2P/IPC
g31n02:736463:736849 [0] NCCL INFO Connected all trees
g31n02:736463:736849 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g31n02:736463:736849 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g28n03:859958:860335 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g28n03:859960:860334 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g28n03:859961:860333 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g28n03:859953:860331 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g28n01:770096:770485 [3] NCCL INFO Channel 01/1 : 1[3] -> 3[5] via P2P/IPC
g28n03:859956:860332 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g27n18:839785:840164 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g31n05:704346:704720 [2] NCCL INFO Channel 02/0 : 0[2] -> 3[5] via P2P/IPC
g28n02:792068:792480 [0] NCCL INFO Channel 01/1 : 4[0] -> 6[2] via P2P/IPC
g28n02:792069:792481 [1] NCCL INFO Channel 01/1 : 5[1] -> 7[3] via P2P/IPC
g27n18:839782:840163 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g32n02:753686:754071 [3] NCCL INFO Connected all rings
g28n02:792072:792482 [3] NCCL INFO Channel 02/1 : 7[3] -> 1[3] [send] via NET/IB/1/Shared
g28n02:792072:792482 [3] NCCL INFO Channel 03/1 : 7[3] -> 1[3] [send] via NET/IB/3/Shared
g28n02:792069:792481 [1] NCCL INFO Channel 02/1 : 2[4] -> 5[1] [receive] via NET/IB/2/Shared
g28n02:792071:792479 [2] NCCL INFO Channel 02/1 : 6[2] -> 0[2] [send] via NET/IB/0/Shared
g28n02:792071:792479 [2] NCCL INFO Channel 03/1 : 6[2] -> 0[2] [send] via NET/IB/2/Shared
g28n02:792071:792479 [2] NCCL INFO Channel 02/1 : 3[5] -> 6[2] [receive] via NET/IB/0/Shared
g28n01:770100:770486 [5] NCCL INFO Channel 02/1 : 3[5] -> 5[1] [send] via NET/IB/1/Shared
g28n01:770100:770486 [5] NCCL INFO Channel 03/1 : 3[5] -> 5[1] [send] via NET/IB/3/Shared
g27n18:839787:840165 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g31n05:704346:704720 [2] NCCL INFO Channel 03/0 : 0[2] -> 3[5] via P2P/IPC
g31n01:617759:618141 [3] NCCL INFO Connected all trees
g31n01:617759:618141 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g31n01:617759:618141 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g28n02:792068:792480 [0] NCCL INFO Channel 02/1 : 1[3] -> 4[0] [receive] via NET/IB/0/Shared
g28n02:792068:792480 [0] NCCL INFO Channel 03/1 : 1[3] -> 4[0] [receive] via NET/IB/2/Shared
g28n02:792068:792480 [0] NCCL INFO Channel 00/1 : 4[0] -> 7[3] via P2P/IPC
g27n18:839789:840166 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g28n01:770095:770484 [2] NCCL INFO Channel 02/1 : 5[1] -> 0[2] [receive] via NET/IB/0/Shared
g28n01:770095:770484 [2] NCCL INFO Channel 03/1 : 5[1] -> 0[2] [receive] via NET/IB/2/Shared
g28n01:770095:770484 [2] NCCL INFO Channel 00/1 : 0[2] -> 3[5] via P2P/IPC
g28n01:770095:770484 [2] NCCL INFO Channel 01/1 : 0[2] -> 3[5] via P2P/IPC
g28n02:792068:792480 [0] NCCL INFO Channel 01/1 : 4[0] -> 7[3] via P2P/IPC
g31n05:704349:704724 [5] NCCL INFO Channel 00/0 : 3[5] -> 0[2] via P2P/IPC
g32n02:753686:754071 [3] NCCL INFO Channel 00/0 : 7[3] -> 6[2] via P2P/IPC
g28n01:770095:770484 [2] NCCL INFO Channel 02/1 : 4[0] -> 0[2] [receive] via NET/IB/0/Shared
g28n01:770095:770484 [2] NCCL INFO Channel 03/1 : 4[0] -> 0[2] [receive] via NET/IB/2/Shared
g28n01:770095:770484 [2] NCCL INFO Channel 02/1 : 0[2] -> 4[0] [send] via NET/IB/0/Shared
g28n01:770095:770484 [2] NCCL INFO Channel 03/1 : 0[2] -> 4[0] [send] via NET/IB/2/Shared
g31n05:704349:704724 [5] NCCL INFO Channel 02/0 : 3[5] -> 0[2] via P2P/IPC
g28n02:792068:792480 [0] NCCL INFO Channel 02/1 : 0[2] -> 4[0] [receive] via NET/IB/0/Shared
g28n02:792068:792480 [0] NCCL INFO Channel 03/1 : 0[2] -> 4[0] [receive] via NET/IB/2/Shared
g28n02:792068:792480 [0] NCCL INFO Channel 02/1 : 4[0] -> 0[2] [send] via NET/IB/0/Shared
g28n02:792068:792480 [0] NCCL INFO Channel 03/1 : 4[0] -> 0[2] [send] via NET/IB/2/Shared
g32n02:753686:754071 [3] NCCL INFO Channel 02/0 : 7[3] -> 6[2] via P2P/IPC
g28n01:770100:770486 [5] NCCL INFO Channel 02/1 : 3[5] -> 6[2] [send] via NET/IB/1/Shared
g28n01:770100:770486 [5] NCCL INFO Channel 03/1 : 3[5] -> 6[2] [send] via NET/IB/3/Shared
g28n01:770100:770486 [5] NCCL INFO Channel 02/1 : 7[3] -> 3[5] [receive] via NET/IB/1/Shared
g28n01:770100:770486 [5] NCCL INFO Channel 03/1 : 7[3] -> 3[5] [receive] via NET/IB/3/Shared
g28n01:770100:770486 [5] NCCL INFO Channel 02/1 : 3[5] -> 7[3] [send] via NET/IB/1/Shared
g28n01:770100:770486 [5] NCCL INFO Channel 03/1 : 3[5] -> 7[3] [send] via NET/IB/3/Shared
g28n01:770100:770486 [5] NCCL INFO Channel 02/1 : 6[2] -> 3[5] [receive] via NET/IB/1/Shared
g28n01:770100:770486 [5] NCCL INFO Channel 03/1 : 6[2] -> 3[5] [receive] via NET/IB/3/Shared
g28n01:770100:770486 [5] NCCL INFO Channel 00/1 : 3[5] -> 0[2] via P2P/IPC
g28n02:792072:792482 [3] NCCL INFO Channel 02/1 : 7[3] -> 2[4] [send] via NET/IB/1/Shared
g28n02:792072:792482 [3] NCCL INFO Channel 03/1 : 7[3] -> 2[4] [send] via NET/IB/3/Shared
g28n02:792072:792482 [3] NCCL INFO Channel 02/1 : 3[5] -> 7[3] [receive] via NET/IB/1/Shared
g28n02:792072:792482 [3] NCCL INFO Channel 03/1 : 3[5] -> 7[3] [receive] via NET/IB/3/Shared
g28n02:792072:792482 [3] NCCL INFO Channel 02/1 : 7[3] -> 3[5] [send] via NET/IB/1/Shared
g28n02:792072:792482 [3] NCCL INFO Channel 03/1 : 7[3] -> 3[5] [send] via NET/IB/3/Shared
g28n02:792072:792482 [3] NCCL INFO Channel 02/1 : 2[4] -> 7[3] [receive] via NET/IB/1/Shared
g28n02:792072:792482 [3] NCCL INFO Channel 03/1 : 2[4] -> 7[3] [receive] via NET/IB/3/Shared
g28n02:792072:792482 [3] NCCL INFO Channel 00/1 : 7[3] -> 4[0] via P2P/IPC
g26n02:851589:851973 [0] NCCL INFO Connected all trees
g26n02:851589:851973 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g26n02:851589:851973 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g32n02:753685:754070 [2] NCCL INFO Channel 00/0 : 6[2] -> 5[1] via P2P/IPC
g31n05:704349:704724 [5] NCCL INFO Channel 00/0 : 3[5] -> 2[4] via P2P/IPC
g28n01:770100:770486 [5] NCCL INFO Channel 01/1 : 3[5] -> 0[2] via P2P/IPC
g28n02:792072:792482 [3] NCCL INFO Channel 01/1 : 7[3] -> 4[0] via P2P/IPC
g32n02:753685:754070 [2] NCCL INFO Channel 02/0 : 6[2] -> 5[1] via P2P/IPC
g31n02:736464:736846 [1] NCCL INFO comm 0x187cde490 rank 5 nranks 8 cudaDev 1 nvmlDev 1 busId 405000 commId 0xadee5768a93b5235 - Init COMPLETE
g31n05:704349:704724 [5] NCCL INFO Channel 01/0 : 3[5] -> 2[4] via P2P/IPC
g31n02:736463:736849 [0] NCCL INFO comm 0x121dc03a0 rank 4 nranks 8 cudaDev 0 nvmlDev 0 busId 404000 commId 0xadee5768a93b5235 - Init COMPLETE
g31n02:736463:736861 [0] NCCL INFO Channel 02/1 : 3[5] -> 4[0] [receive] via NET/IB/0/Shared
g31n02:736463:736861 [0] NCCL INFO Channel 03/1 : 3[5] -> 4[0] [receive] via NET/IB/2/Shared
g31n02:736463:736861 [0] NCCL INFO Channel 00/1 : 4[0] -> 5[1] via P2P/IPC
g26n01:811334:811707 [3] NCCL INFO Connected all trees
g26n01:811334:811707 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g26n01:811334:811707 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g31n05:704349:704724 [5] NCCL INFO Channel 02/0 : 3[5] -> 2[4] via P2P/IPC
g28n01:770095:770484 [2] NCCL INFO Channel 02/1 : 0[2] -> 5[1] [send] via NET/IB/0/Shared
g28n01:770095:770484 [2] NCCL INFO Channel 03/1 : 0[2] -> 5[1] [send] via NET/IB/2/Shared
g28n01:770099:770487 [4] NCCL INFO Channel 02/1 : 2[4] -> 4[0] [send] via NET/IB/3/Shared
g28n01:770099:770487 [4] NCCL INFO Channel 03/1 : 2[4] -> 4[0] [send] via NET/IB/1/Shared
g28n01:770099:770487 [4] NCCL INFO Channel 02/1 : 7[3] -> 2[4] [receive] via NET/IB/3/Shared
g28n01:770099:770487 [4] NCCL INFO Channel 03/1 : 7[3] -> 2[4] [receive] via NET/IB/1/Shared
g28n01:770099:770487 [4] NCCL INFO Channel 02/1 : 2[4] -> 5[1] [send] via NET/IB/3/Shared
g28n01:770099:770487 [4] NCCL INFO Channel 03/1 : 2[4] -> 5[1] [send] via NET/IB/1/Shared
g28n01:770099:770487 [4] NCCL INFO Channel 02/1 : 6[2] -> 2[4] [receive] via NET/IB/3/Shared
g28n01:770099:770487 [4] NCCL INFO Channel 03/1 : 6[2] -> 2[4] [receive] via NET/IB/1/Shared
g28n01:770099:770487 [4] NCCL INFO Channel 02/1 : 2[4] -> 6[2] [send] via NET/IB/3/Shared
g28n01:770099:770487 [4] NCCL INFO Channel 03/1 : 2[4] -> 6[2] [send] via NET/IB/1/Shared
g28n01:770099:770487 [4] NCCL INFO Channel 02/1 : 5[1] -> 2[4] [receive] via NET/IB/3/Shared
g28n01:770099:770487 [4] NCCL INFO Channel 03/1 : 5[1] -> 2[4] [receive] via NET/IB/1/Shared
g28n01:770099:770487 [4] NCCL INFO Channel 02/1 : 2[4] -> 7[3] [send] via NET/IB/3/Shared
g28n01:770099:770487 [4] NCCL INFO Channel 03/1 : 2[4] -> 7[3] [send] via NET/IB/1/Shared
g28n01:770099:770487 [4] NCCL INFO Channel 02/1 : 4[0] -> 2[4] [receive] via NET/IB/3/Shared
g28n01:770099:770487 [4] NCCL INFO Channel 03/1 : 4[0] -> 2[4] [receive] via NET/IB/1/Shared
g28n01:770099:770487 [4] NCCL INFO Channel 00/1 : 2[4] -> 0[2] via P2P/IPC
g31n02:736465:736845 [2] NCCL INFO comm 0x1a27132b0 rank 6 nranks 8 cudaDev 2 nvmlDev 2 busId 406000 commId 0xadee5768a93b5235 - Init COMPLETE
g28n01:770096:770485 [3] NCCL INFO Channel 02/1 : 6[2] -> 1[3] [receive] via NET/IB/1/Shared
g28n01:770096:770485 [3] NCCL INFO Channel 03/1 : 6[2] -> 1[3] [receive] via NET/IB/3/Shared
g28n01:770096:770485 [3] NCCL INFO Channel 02/1 : 1[3] -> 4[0] [send] via NET/IB/1/Shared
g28n01:770096:770485 [3] NCCL INFO Channel 03/1 : 1[3] -> 4[0] [send] via NET/IB/3/Shared
g28n01:770096:770485 [3] NCCL INFO Channel 02/1 : 5[1] -> 1[3] [receive] via NET/IB/1/Shared
g28n01:770096:770485 [3] NCCL INFO Channel 03/1 : 5[1] -> 1[3] [receive] via NET/IB/3/Shared
g28n01:770096:770485 [3] NCCL INFO Channel 02/1 : 1[3] -> 5[1] [send] via NET/IB/1/Shared
g28n01:770096:770485 [3] NCCL INFO Channel 03/1 : 1[3] -> 5[1] [send] via NET/IB/3/Shared
g28n01:770096:770485 [3] NCCL INFO Channel 02/1 : 4[0] -> 1[3] [receive] via NET/IB/1/Shared
g28n01:770096:770485 [3] NCCL INFO Channel 03/1 : 4[0] -> 1[3] [receive] via NET/IB/3/Shared
g28n01:770096:770485 [3] NCCL INFO Channel 02/1 : 1[3] -> 6[2] [send] via NET/IB/1/Shared
g28n01:770096:770485 [3] NCCL INFO Channel 03/1 : 1[3] -> 6[2] [send] via NET/IB/3/Shared
g28n02:792069:792481 [1] NCCL INFO Channel 03/1 : 2[4] -> 5[1] [receive] via NET/IB/0/Shared
g28n02:792069:792481 [1] NCCL INFO Channel 02/1 : 5[1] -> 0[2] [send] via NET/IB/2/Shared
g28n02:792069:792481 [1] NCCL INFO Channel 03/1 : 5[1] -> 0[2] [send] via NET/IB/0/Shared
g28n02:792069:792481 [1] NCCL INFO Channel 02/1 : 1[3] -> 5[1] [receive] via NET/IB/2/Shared
g28n02:792069:792481 [1] NCCL INFO Channel 03/1 : 1[3] -> 5[1] [receive] via NET/IB/0/Shared
g28n02:792069:792481 [1] NCCL INFO Channel 02/1 : 5[1] -> 1[3] [send] via NET/IB/2/Shared
g28n02:792069:792481 [1] NCCL INFO Channel 03/1 : 5[1] -> 1[3] [send] via NET/IB/0/Shared
g28n02:792069:792481 [1] NCCL INFO Channel 02/1 : 0[2] -> 5[1] [receive] via NET/IB/2/Shared
g28n02:792069:792481 [1] NCCL INFO Channel 03/1 : 0[2] -> 5[1] [receive] via NET/IB/0/Shared
g28n02:792069:792481 [1] NCCL INFO Channel 02/1 : 5[1] -> 2[4] [send] via NET/IB/2/Shared
g28n02:792069:792481 [1] NCCL INFO Channel 03/1 : 5[1] -> 2[4] [send] via NET/IB/0/Shared
g28n01:770100:770486 [5] NCCL INFO Channel 02/1 : 5[1] -> 3[5] [receive] via NET/IB/1/Shared
g28n01:770100:770486 [5] NCCL INFO Channel 03/1 : 5[1] -> 3[5] [receive] via NET/IB/3/Shared
g28n01:770100:770486 [5] NCCL INFO Channel 00/1 : 3[5] -> 1[3] via P2P/IPC
g28n02:792071:792479 [2] NCCL INFO Channel 03/1 : 3[5] -> 6[2] [receive] via NET/IB/2/Shared
g28n02:792071:792479 [2] NCCL INFO Channel 02/1 : 6[2] -> 1[3] [send] via NET/IB/0/Shared
g28n02:792071:792479 [2] NCCL INFO Channel 03/1 : 6[2] -> 1[3] [send] via NET/IB/2/Shared
g28n02:792071:792479 [2] NCCL INFO Channel 02/1 : 2[4] -> 6[2] [receive] via NET/IB/0/Shared
g28n02:792071:792479 [2] NCCL INFO Channel 03/1 : 2[4] -> 6[2] [receive] via NET/IB/2/Shared
g28n02:792071:792479 [2] NCCL INFO Channel 02/1 : 6[2] -> 2[4] [send] via NET/IB/0/Shared
g28n02:792071:792479 [2] NCCL INFO Channel 03/1 : 6[2] -> 2[4] [send] via NET/IB/2/Shared
g28n02:792071:792479 [2] NCCL INFO Channel 02/1 : 1[3] -> 6[2] [receive] via NET/IB/0/Shared
g28n02:792071:792479 [2] NCCL INFO Channel 03/1 : 1[3] -> 6[2] [receive] via NET/IB/2/Shared
g28n02:792071:792479 [2] NCCL INFO Channel 02/1 : 6[2] -> 3[5] [send] via NET/IB/0/Shared
g28n02:792071:792479 [2] NCCL INFO Channel 03/1 : 6[2] -> 3[5] [send] via NET/IB/2/Shared
g28n02:792071:792479 [2] NCCL INFO Channel 02/1 : 0[2] -> 6[2] [receive] via NET/IB/0/Shared
g28n02:792071:792479 [2] NCCL INFO Channel 03/1 : 0[2] -> 6[2] [receive] via NET/IB/2/Shared
g28n02:792071:792479 [2] NCCL INFO Channel 00/1 : 6[2] -> 4[0] via P2P/IPC
g31n05:704346:704720 [2] NCCL INFO Connected all trees
g31n05:704346:704720 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g31n05:704346:704720 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g28n02:792068:792480 [0] NCCL INFO Channel 02/1 : 4[0] -> 1[3] [send] via NET/IB/0/Shared
g28n02:792068:792480 [0] NCCL INFO Channel 03/1 : 4[0] -> 1[3] [send] via NET/IB/2/Shared
g31n05:704349:704724 [5] NCCL INFO Channel 03/0 : 3[5] -> 2[4] via P2P/IPC
g31n02:736466:736847 [3] NCCL INFO comm 0x189c6f9a0 rank 7 nranks 8 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xadee5768a93b5235 - Init COMPLETE
g28n02:792072:792482 [3] NCCL INFO Channel 02/1 : 1[3] -> 7[3] [receive] via NET/IB/1/Shared
g28n02:792072:792482 [3] NCCL INFO Channel 03/1 : 1[3] -> 7[3] [receive] via NET/IB/3/Shared
g28n02:792072:792482 [3] NCCL INFO Channel 00/1 : 7[3] -> 5[1] via P2P/IPC
g31n02:736463:736861 [0] NCCL INFO Channel 01/1 : 4[0] -> 5[1] via P2P/IPC
g31n02:736464:736858 [1] NCCL INFO Channel 00/1 : 5[1] -> 6[2] via P2P/IPC
g28n01:770099:770487 [4] NCCL INFO Channel 01/1 : 2[4] -> 0[2] via P2P/IPC
g28n02:792071:792479 [2] NCCL INFO Channel 01/1 : 6[2] -> 4[0] via P2P/IPC
g28n01:770100:770486 [5] NCCL INFO Channel 01/1 : 3[5] -> 1[3] via P2P/IPC
g28n02:792072:792482 [3] NCCL INFO Channel 01/1 : 7[3] -> 5[1] via P2P/IPC
g31n02:736465:736860 [2] NCCL INFO Channel 00/1 : 6[2] -> 7[3] via P2P/IPC
g31n02:736464:736858 [1] NCCL INFO Channel 01/1 : 5[1] -> 6[2] via P2P/IPC
g27n18:839783:840167 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g31n02:736465:736860 [2] NCCL INFO Channel 01/1 : 6[2] -> 7[3] via P2P/IPC
g31n01:617760:618143 [4] NCCL INFO comm 0x1b2aabdf0 rank 2 nranks 8 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xadee5768a93b5235 - Init COMPLETE
g31n01:617759:618141 [3] NCCL INFO comm 0x1414fec90 rank 1 nranks 8 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xadee5768a93b5235 - Init COMPLETE
g31n01:617761:618142 [5] NCCL INFO comm 0x1c9e35f20 rank 3 nranks 8 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xadee5768a93b5235 - Init COMPLETE
g31n01:617758:618138 [2] NCCL INFO comm 0x1993350a0 rank 0 nranks 8 cudaDev 2 nvmlDev 2 busId 406000 commId 0xadee5768a93b5235 - Init COMPLETE
g31n01:617758:618152 [2] NCCL INFO Channel 02/1 : 7[3] -> 0[2] [receive] via NET/IB/0/Shared
g31n01:617758:618152 [2] NCCL INFO Channel 03/1 : 7[3] -> 0[2] [receive] via NET/IB/2/Shared
g31n01:617758:618152 [2] NCCL INFO Channel 00/1 : 0[2] -> 1[3] via P2P/IPC
g28n01:770096:770485 [3] NCCL INFO Channel 02/1 : 1[3] -> 7[3] [send] via NET/IB/1/Shared
g28n01:770096:770485 [3] NCCL INFO Channel 03/1 : 1[3] -> 7[3] [send] via NET/IB/3/Shared
g28n01:770095:770484 [2] NCCL INFO Channel 02/1 : 0[2] -> 6[2] [send] via NET/IB/0/Shared
g28n01:770095:770484 [2] NCCL INFO Channel 03/1 : 0[2] -> 6[2] [send] via NET/IB/2/Shared
g28n01:770100:770486 [5] NCCL INFO Channel 02/1 : 4[0] -> 3[5] [receive] via NET/IB/1/Shared
g28n01:770100:770486 [5] NCCL INFO Channel 03/1 : 4[0] -> 3[5] [receive] via NET/IB/3/Shared
g28n01:770100:770486 [5] NCCL INFO Channel 00/1 : 3[5] -> 2[4] via P2P/IPC
g28n02:792068:792480 [0] NCCL INFO Channel 02/1 : 4[0] -> 2[4] [send] via NET/IB/0/Shared
g28n02:792068:792480 [0] NCCL INFO Channel 03/1 : 4[0] -> 2[4] [send] via NET/IB/2/Shared
g28n02:792069:792481 [1] NCCL INFO Channel 02/1 : 5[1] -> 3[5] [send] via NET/IB/2/Shared
g28n02:792069:792481 [1] NCCL INFO Channel 03/1 : 5[1] -> 3[5] [send] via NET/IB/0/Shared
g27n18:839790:840168 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g28n02:792072:792482 [3] NCCL INFO Channel 02/1 : 0[2] -> 7[3] [receive] via NET/IB/1/Shared
g28n02:792072:792482 [3] NCCL INFO Channel 03/1 : 0[2] -> 7[3] [receive] via NET/IB/3/Shared
g28n02:792072:792482 [3] NCCL INFO Channel 00/1 : 7[3] -> 6[2] via P2P/IPC
g31n02:736464:736858 [1] NCCL INFO Channel 02/1 : 3[5] -> 5[1] [receive] via NET/IB/2/Shared
g31n02:736464:736858 [1] NCCL INFO Channel 03/1 : 3[5] -> 5[1] [receive] via NET/IB/0/Shared
g31n02:736464:736858 [1] NCCL INFO Channel 00/1 : 5[1] -> 7[3] via P2P/IPC
g25n18:898856:899339 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g31n02:736464:736858 [1] NCCL INFO Channel 01/1 : 5[1] -> 7[3] via P2P/IPC
g31n01:617758:618152 [2] NCCL INFO Channel 01/1 : 0[2] -> 1[3] via P2P/IPC
g26n02:851591:851972 [2] NCCL INFO comm 0x187784770 rank 6 nranks 8 cudaDev 2 nvmlDev 2 busId 406000 commId 0xb1b45bf195242dd5 - Init COMPLETE
g28n01:770100:770486 [5] NCCL INFO Channel 01/1 : 3[5] -> 2[4] via P2P/IPC
g26n02:851590:851971 [1] NCCL INFO comm 0x19e3219a0 rank 5 nranks 8 cudaDev 1 nvmlDev 1 busId 405000 commId 0xb1b45bf195242dd5 - Init COMPLETE
g26n02:851589:851973 [0] NCCL INFO comm 0x1b3133720 rank 4 nranks 8 cudaDev 0 nvmlDev 0 busId 404000 commId 0xb1b45bf195242dd5 - Init COMPLETE
g26n02:851589:851985 [0] NCCL INFO Channel 02/1 : 3[5] -> 4[0] [receive] via NET/IB/0/Shared
g26n02:851589:851985 [0] NCCL INFO Channel 03/1 : 3[5] -> 4[0] [receive] via NET/IB/2/Shared
g26n02:851589:851985 [0] NCCL INFO Channel 00/1 : 4[0] -> 5[1] via P2P/IPC
g31n01:617759:618153 [3] NCCL INFO Channel 00/1 : 1[3] -> 2[4] via P2P/IPC
g26n02:851592:851974 [3] NCCL INFO comm 0x175230230 rank 7 nranks 8 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xb1b45bf195242dd5 - Init COMPLETE
g31n01:617760:618155 [4] NCCL INFO Channel 00/1 : 2[4] -> 3[5] via P2P/IPC
g28n02:792072:792482 [3] NCCL INFO Channel 01/1 : 7[3] -> 6[2] via P2P/IPC
g31n02:736466:736859 [3] NCCL INFO Channel 02/1 : 7[3] -> 0[2] [send] via NET/IB/1/Shared
g31n02:736466:736859 [3] NCCL INFO Channel 03/1 : 7[3] -> 0[2] [send] via NET/IB/3/Shared
g28n01:770096:770485 [3] NCCL INFO Channel 00/1 : 1[3] -> 0[2] via P2P/IPC
g32n02:753686:754071 [3] NCCL INFO Connected all trees
g32n02:753686:754071 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g32n02:753686:754071 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g31n01:617760:618155 [4] NCCL INFO Channel 01/1 : 2[4] -> 3[5] via P2P/IPC
g31n01:617759:618153 [3] NCCL INFO Channel 01/1 : 1[3] -> 2[4] via P2P/IPC
g28n02:792071:792479 [2] NCCL INFO Channel 00/1 : 6[2] -> 5[1] via P2P/IPC
g26n01:811334:811707 [3] NCCL INFO comm 0x17a32c4c0 rank 1 nranks 8 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xb1b45bf195242dd5 - Init COMPLETE
g31n02:736463:736861 [0] NCCL INFO Channel 02/1 : 2[4] -> 4[0] [receive] via NET/IB/0/Shared
g31n02:736463:736861 [0] NCCL INFO Channel 03/1 : 2[4] -> 4[0] [receive] via NET/IB/2/Shared
g31n02:736463:736861 [0] NCCL INFO Channel 00/1 : 4[0] -> 6[2] via P2P/IPC
g26n01:811336:811706 [5] NCCL INFO comm 0x18328de30 rank 3 nranks 8 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xb1b45bf195242dd5 - Init COMPLETE
g28n02:792069:792481 [1] NCCL INFO Channel 00/1 : 5[1] -> 4[0] via P2P/IPC
g32n02:753685:754070 [2] NCCL INFO Connected all trees
g32n02:753685:754070 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g32n02:753685:754070 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g26n01:811335:811708 [4] NCCL INFO comm 0x14e64ec20 rank 2 nranks 8 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xb1b45bf195242dd5 - Init COMPLETE
g28n01:770096:770485 [3] NCCL INFO Channel 01/1 : 1[3] -> 0[2] via P2P/IPC
g26n02:851589:851985 [0] NCCL INFO Channel 01/1 : 4[0] -> 5[1] via P2P/IPC
g31n02:736463:736861 [0] NCCL INFO Channel 01/1 : 4[0] -> 6[2] via P2P/IPC
g26n01:811333:811705 [2] NCCL INFO comm 0x16d89b070 rank 0 nranks 8 cudaDev 2 nvmlDev 2 busId 406000 commId 0xb1b45bf195242dd5 - Init COMPLETE
g26n01:811333:811720 [2] NCCL INFO Channel 02/1 : 7[3] -> 0[2] [receive] via NET/IB/0/Shared
g26n01:811333:811720 [2] NCCL INFO Channel 03/1 : 7[3] -> 0[2] [receive] via NET/IB/2/Shared
g26n01:811333:811720 [2] NCCL INFO Channel 00/1 : 0[2] -> 1[3] via P2P/IPC
g28n01:770099:770487 [4] NCCL INFO Channel 00/1 : 2[4] -> 1[3] via P2P/IPC
g28n02:792071:792479 [2] NCCL INFO Channel 01/1 : 6[2] -> 5[1] via P2P/IPC
g28n01:770095:770484 [2] NCCL INFO Channel 02/1 : 0[2] -> 7[3] [send] via NET/IB/0/Shared
g28n01:770095:770484 [2] NCCL INFO Channel 03/1 : 0[2] -> 7[3] [send] via NET/IB/2/Shared
g32n02:753684:754069 [1] NCCL INFO Connected all trees
g32n02:753684:754069 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g32n02:753684:754069 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g28n01:770099:770487 [4] NCCL INFO Channel 01/1 : 2[4] -> 1[3] via P2P/IPC
g28n02:792069:792481 [1] NCCL INFO Channel 01/1 : 5[1] -> 4[0] via P2P/IPC
g31n01:617761:618154 [5] NCCL INFO Channel 02/1 : 3[5] -> 4[0] [send] via NET/IB/1/Shared
g31n01:617761:618154 [5] NCCL INFO Channel 03/1 : 3[5] -> 4[0] [send] via NET/IB/3/Shared
g26n02:851590:851984 [1] NCCL INFO Channel 00/1 : 5[1] -> 6[2] via P2P/IPC
g31n01:617758:618152 [2] NCCL INFO Channel 02/1 : 6[2] -> 0[2] [receive] via NET/IB/0/Shared
g31n01:617758:618152 [2] NCCL INFO Channel 03/1 : 6[2] -> 0[2] [receive] via NET/IB/2/Shared
g31n01:617758:618152 [2] NCCL INFO Channel 00/1 : 0[2] -> 2[4] via P2P/IPC
g26n02:851591:851987 [2] NCCL INFO Channel 00/1 : 6[2] -> 7[3] via P2P/IPC
g31n01:617759:618153 [3] NCCL INFO Channel 02/1 : 7[3] -> 1[3] [receive] via NET/IB/1/Shared
g31n01:617759:618153 [3] NCCL INFO Channel 03/1 : 7[3] -> 1[3] [receive] via NET/IB/3/Shared
g31n01:617759:618153 [3] NCCL INFO Channel 00/1 : 1[3] -> 3[5] via P2P/IPC
g26n02:851590:851984 [1] NCCL INFO Channel 01/1 : 5[1] -> 6[2] via P2P/IPC
g26n01:811333:811720 [2] NCCL INFO Channel 01/1 : 0[2] -> 1[3] via P2P/IPC
g28n02:792068:792480 [0] NCCL INFO Channel 02/1 : 4[0] -> 3[5] [send] via NET/IB/0/Shared
g28n02:792068:792480 [0] NCCL INFO Channel 03/1 : 4[0] -> 3[5] [send] via NET/IB/2/Shared
g26n02:851591:851987 [2] NCCL INFO Channel 01/1 : 6[2] -> 7[3] via P2P/IPC
g31n01:617759:618153 [3] NCCL INFO Channel 01/1 : 1[3] -> 3[5] via P2P/IPC
g26n01:811334:811722 [3] NCCL INFO Channel 00/1 : 1[3] -> 2[4] via P2P/IPC
g26n01:811335:811723 [4] NCCL INFO Channel 00/1 : 2[4] -> 3[5] via P2P/IPC
g31n05:704348:704723 [4] NCCL INFO Connected all trees
g31n05:704348:704723 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g31n05:704348:704723 [4] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g31n05:704349:704724 [5] NCCL INFO Connected all trees
g31n05:704349:704724 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g31n05:704349:704724 [5] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g31n01:617758:618152 [2] NCCL INFO Channel 01/1 : 0[2] -> 2[4] via P2P/IPC
g26n01:811335:811723 [4] NCCL INFO Channel 01/1 : 2[4] -> 3[5] via P2P/IPC
g26n01:811334:811722 [3] NCCL INFO Channel 01/1 : 1[3] -> 2[4] via P2P/IPC
g26n02:851592:851986 [3] NCCL INFO Channel 02/1 : 7[3] -> 0[2] [send] via NET/IB/1/Shared
g26n02:851592:851986 [3] NCCL INFO Channel 03/1 : 7[3] -> 0[2] [send] via NET/IB/3/Shared
g26n02:851590:851984 [1] NCCL INFO Channel 02/1 : 3[5] -> 5[1] [receive] via NET/IB/2/Shared
g26n02:851590:851984 [1] NCCL INFO Channel 03/1 : 3[5] -> 5[1] [receive] via NET/IB/0/Shared
g26n02:851590:851984 [1] NCCL INFO Channel 00/1 : 5[1] -> 7[3] via P2P/IPC
g26n02:851589:851985 [0] NCCL INFO Channel 02/1 : 2[4] -> 4[0] [receive] via NET/IB/0/Shared
g26n02:851589:851985 [0] NCCL INFO Channel 03/1 : 2[4] -> 4[0] [receive] via NET/IB/2/Shared
g26n02:851589:851985 [0] NCCL INFO Channel 00/1 : 4[0] -> 6[2] via P2P/IPC
g31n01:617761:618154 [5] NCCL INFO Channel 02/1 : 3[5] -> 5[1] [send] via NET/IB/1/Shared
g31n01:617761:618154 [5] NCCL INFO Channel 03/1 : 3[5] -> 5[1] [send] via NET/IB/3/Shared
g31n02:736466:736859 [3] NCCL INFO Channel 02/1 : 7[3] -> 1[3] [send] via NET/IB/1/Shared
g31n02:736466:736859 [3] NCCL INFO Channel 03/1 : 7[3] -> 1[3] [send] via NET/IB/3/Shared
g31n02:736463:736861 [0] NCCL INFO Channel 02/1 : 1[3] -> 4[0] [receive] via NET/IB/0/Shared
g31n02:736463:736861 [0] NCCL INFO Channel 03/1 : 1[3] -> 4[0] [receive] via NET/IB/2/Shared
g31n02:736463:736861 [0] NCCL INFO Channel 00/1 : 4[0] -> 7[3] via P2P/IPC
g31n01:617758:618152 [2] NCCL INFO Channel 02/1 : 5[1] -> 0[2] [receive] via NET/IB/0/Shared
g31n01:617758:618152 [2] NCCL INFO Channel 03/1 : 5[1] -> 0[2] [receive] via NET/IB/2/Shared
g31n01:617758:618152 [2] NCCL INFO Channel 00/1 : 0[2] -> 3[5] via P2P/IPC
g26n01:811336:811721 [5] NCCL INFO Channel 02/1 : 3[5] -> 4[0] [send] via NET/IB/1/Shared
g26n01:811336:811721 [5] NCCL INFO Channel 03/1 : 3[5] -> 4[0] [send] via NET/IB/3/Shared
g26n02:851590:851984 [1] NCCL INFO Channel 01/1 : 5[1] -> 7[3] via P2P/IPC
g26n01:811333:811720 [2] NCCL INFO Channel 02/1 : 6[2] -> 0[2] [receive] via NET/IB/0/Shared
g26n01:811333:811720 [2] NCCL INFO Channel 03/1 : 6[2] -> 0[2] [receive] via NET/IB/2/Shared
g26n01:811333:811720 [2] NCCL INFO Channel 00/1 : 0[2] -> 2[4] via P2P/IPC
g26n02:851589:851985 [0] NCCL INFO Channel 01/1 : 4[0] -> 6[2] via P2P/IPC
g26n01:811334:811722 [3] NCCL INFO Channel 02/1 : 7[3] -> 1[3] [receive] via NET/IB/1/Shared
g26n01:811334:811722 [3] NCCL INFO Channel 03/1 : 7[3] -> 1[3] [receive] via NET/IB/3/Shared
g26n01:811334:811722 [3] NCCL INFO Channel 00/1 : 1[3] -> 3[5] via P2P/IPC
g31n02:736463:736861 [0] NCCL INFO Channel 01/1 : 4[0] -> 7[3] via P2P/IPC
g31n01:617758:618152 [2] NCCL INFO Channel 01/1 : 0[2] -> 3[5] via P2P/IPC
g26n01:811333:811720 [2] NCCL INFO Channel 01/1 : 0[2] -> 2[4] via P2P/IPC
g26n01:811334:811722 [3] NCCL INFO Channel 01/1 : 1[3] -> 3[5] via P2P/IPC
g31n01:617758:618152 [2] NCCL INFO Channel 02/1 : 4[0] -> 0[2] [receive] via NET/IB/0/Shared
g31n01:617758:618152 [2] NCCL INFO Channel 03/1 : 4[0] -> 0[2] [receive] via NET/IB/2/Shared
g31n01:617758:618152 [2] NCCL INFO Channel 02/1 : 0[2] -> 4[0] [send] via NET/IB/0/Shared
g31n01:617758:618152 [2] NCCL INFO Channel 03/1 : 0[2] -> 4[0] [send] via NET/IB/2/Shared
g31n02:736463:736861 [0] NCCL INFO Channel 02/1 : 0[2] -> 4[0] [receive] via NET/IB/0/Shared
g31n02:736463:736861 [0] NCCL INFO Channel 03/1 : 0[2] -> 4[0] [receive] via NET/IB/2/Shared
g31n02:736463:736861 [0] NCCL INFO Channel 02/1 : 4[0] -> 0[2] [send] via NET/IB/0/Shared
g31n02:736463:736861 [0] NCCL INFO Channel 03/1 : 4[0] -> 0[2] [send] via NET/IB/2/Shared
g31n01:617761:618154 [5] NCCL INFO Channel 02/1 : 3[5] -> 6[2] [send] via NET/IB/1/Shared
g31n01:617761:618154 [5] NCCL INFO Channel 03/1 : 3[5] -> 6[2] [send] via NET/IB/3/Shared
g31n01:617761:618154 [5] NCCL INFO Channel 02/1 : 7[3] -> 3[5] [receive] via NET/IB/1/Shared
g31n01:617761:618154 [5] NCCL INFO Channel 03/1 : 7[3] -> 3[5] [receive] via NET/IB/3/Shared
g31n01:617761:618154 [5] NCCL INFO Channel 02/1 : 3[5] -> 7[3] [send] via NET/IB/1/Shared
g31n01:617761:618154 [5] NCCL INFO Channel 03/1 : 3[5] -> 7[3] [send] via NET/IB/3/Shared
g31n01:617761:618154 [5] NCCL INFO Channel 02/1 : 6[2] -> 3[5] [receive] via NET/IB/1/Shared
g31n01:617761:618154 [5] NCCL INFO Channel 03/1 : 6[2] -> 3[5] [receive] via NET/IB/3/Shared
g31n01:617761:618154 [5] NCCL INFO Channel 00/1 : 3[5] -> 0[2] via P2P/IPC
g31n02:736466:736859 [3] NCCL INFO Channel 02/1 : 7[3] -> 2[4] [send] via NET/IB/1/Shared
g31n02:736466:736859 [3] NCCL INFO Channel 03/1 : 7[3] -> 2[4] [send] via NET/IB/3/Shared
g31n02:736466:736859 [3] NCCL INFO Channel 02/1 : 3[5] -> 7[3] [receive] via NET/IB/1/Shared
g31n02:736466:736859 [3] NCCL INFO Channel 03/1 : 3[5] -> 7[3] [receive] via NET/IB/3/Shared
g31n02:736466:736859 [3] NCCL INFO Channel 02/1 : 7[3] -> 3[5] [send] via NET/IB/1/Shared
g31n02:736466:736859 [3] NCCL INFO Channel 03/1 : 7[3] -> 3[5] [send] via NET/IB/3/Shared
g31n02:736466:736859 [3] NCCL INFO Channel 02/1 : 2[4] -> 7[3] [receive] via NET/IB/1/Shared
g31n02:736466:736859 [3] NCCL INFO Channel 03/1 : 2[4] -> 7[3] [receive] via NET/IB/3/Shared
g31n02:736466:736859 [3] NCCL INFO Channel 00/1 : 7[3] -> 4[0] via P2P/IPC
g26n02:851592:851986 [3] NCCL INFO Channel 02/1 : 7[3] -> 1[3] [send] via NET/IB/1/Shared
g26n02:851592:851986 [3] NCCL INFO Channel 03/1 : 7[3] -> 1[3] [send] via NET/IB/3/Shared
g26n01:811336:811721 [5] NCCL INFO Channel 02/1 : 3[5] -> 5[1] [send] via NET/IB/1/Shared
g26n01:811336:811721 [5] NCCL INFO Channel 03/1 : 3[5] -> 5[1] [send] via NET/IB/3/Shared
g31n02:736466:736859 [3] NCCL INFO Channel 01/1 : 7[3] -> 4[0] via P2P/IPC
g31n01:617761:618154 [5] NCCL INFO Channel 01/1 : 3[5] -> 0[2] via P2P/IPC
g26n01:811333:811720 [2] NCCL INFO Channel 02/1 : 5[1] -> 0[2] [receive] via NET/IB/0/Shared
g26n01:811333:811720 [2] NCCL INFO Channel 03/1 : 5[1] -> 0[2] [receive] via NET/IB/2/Shared
g26n01:811333:811720 [2] NCCL INFO Channel 00/1 : 0[2] -> 3[5] via P2P/IPC
g26n02:851589:851985 [0] NCCL INFO Channel 02/1 : 1[3] -> 4[0] [receive] via NET/IB/0/Shared
g26n02:851589:851985 [0] NCCL INFO Channel 03/1 : 1[3] -> 4[0] [receive] via NET/IB/2/Shared
g26n02:851589:851985 [0] NCCL INFO Channel 00/1 : 4[0] -> 7[3] via P2P/IPC
g26n02:851589:851985 [0] NCCL INFO Channel 01/1 : 4[0] -> 7[3] via P2P/IPC
g26n01:811333:811720 [2] NCCL INFO Channel 01/1 : 0[2] -> 3[5] via P2P/IPC
g31n01:617759:618153 [3] NCCL INFO Channel 02/1 : 6[2] -> 1[3] [receive] via NET/IB/1/Shared
g31n01:617759:618153 [3] NCCL INFO Channel 03/1 : 6[2] -> 1[3] [receive] via NET/IB/3/Shared
g31n01:617759:618153 [3] NCCL INFO Channel 02/1 : 1[3] -> 4[0] [send] via NET/IB/1/Shared
g31n01:617759:618153 [3] NCCL INFO Channel 03/1 : 1[3] -> 4[0] [send] via NET/IB/3/Shared
g31n01:617759:618153 [3] NCCL INFO Channel 02/1 : 5[1] -> 1[3] [receive] via NET/IB/1/Shared
g31n01:617759:618153 [3] NCCL INFO Channel 03/1 : 5[1] -> 1[3] [receive] via NET/IB/3/Shared
g31n01:617759:618153 [3] NCCL INFO Channel 02/1 : 1[3] -> 5[1] [send] via NET/IB/1/Shared
g31n01:617759:618153 [3] NCCL INFO Channel 03/1 : 1[3] -> 5[1] [send] via NET/IB/3/Shared
g31n01:617759:618153 [3] NCCL INFO Channel 02/1 : 4[0] -> 1[3] [receive] via NET/IB/1/Shared
g31n01:617759:618153 [3] NCCL INFO Channel 03/1 : 4[0] -> 1[3] [receive] via NET/IB/3/Shared
g31n01:617759:618153 [3] NCCL INFO Channel 02/1 : 1[3] -> 6[2] [send] via NET/IB/1/Shared
g31n01:617759:618153 [3] NCCL INFO Channel 03/1 : 1[3] -> 6[2] [send] via NET/IB/3/Shared
g31n01:617758:618152 [2] NCCL INFO Channel 02/1 : 0[2] -> 5[1] [send] via NET/IB/0/Shared
g31n01:617758:618152 [2] NCCL INFO Channel 03/1 : 0[2] -> 5[1] [send] via NET/IB/2/Shared
g31n02:736464:736858 [1] NCCL INFO Channel 02/1 : 2[4] -> 5[1] [receive] via NET/IB/2/Shared
g31n02:736464:736858 [1] NCCL INFO Channel 03/1 : 2[4] -> 5[1] [receive] via NET/IB/0/Shared
g31n02:736464:736858 [1] NCCL INFO Channel 02/1 : 5[1] -> 0[2] [send] via NET/IB/2/Shared
g31n02:736464:736858 [1] NCCL INFO Channel 03/1 : 5[1] -> 0[2] [send] via NET/IB/0/Shared
g31n02:736464:736858 [1] NCCL INFO Channel 02/1 : 1[3] -> 5[1] [receive] via NET/IB/2/Shared
g31n02:736464:736858 [1] NCCL INFO Channel 03/1 : 1[3] -> 5[1] [receive] via NET/IB/0/Shared
g31n02:736464:736858 [1] NCCL INFO Channel 02/1 : 5[1] -> 1[3] [send] via NET/IB/2/Shared
g31n02:736464:736858 [1] NCCL INFO Channel 03/1 : 5[1] -> 1[3] [send] via NET/IB/0/Shared
g31n02:736464:736858 [1] NCCL INFO Channel 02/1 : 0[2] -> 5[1] [receive] via NET/IB/2/Shared
g31n02:736464:736858 [1] NCCL INFO Channel 03/1 : 0[2] -> 5[1] [receive] via NET/IB/0/Shared
g31n02:736464:736858 [1] NCCL INFO Channel 02/1 : 5[1] -> 2[4] [send] via NET/IB/2/Shared
g31n02:736464:736858 [1] NCCL INFO Channel 03/1 : 5[1] -> 2[4] [send] via NET/IB/0/Shared
g31n02:736463:736861 [0] NCCL INFO Channel 02/1 : 4[0] -> 1[3] [send] via NET/IB/0/Shared
g31n02:736463:736861 [0] NCCL INFO Channel 03/1 : 4[0] -> 1[3] [send] via NET/IB/2/Shared
g31n01:617760:618155 [4] NCCL INFO Channel 02/1 : 2[4] -> 4[0] [send] via NET/IB/3/Shared
g31n01:617760:618155 [4] NCCL INFO Channel 03/1 : 2[4] -> 4[0] [send] via NET/IB/1/Shared
g31n01:617760:618155 [4] NCCL INFO Channel 02/1 : 7[3] -> 2[4] [receive] via NET/IB/3/Shared
g31n01:617760:618155 [4] NCCL INFO Channel 03/1 : 7[3] -> 2[4] [receive] via NET/IB/1/Shared
g31n01:617760:618155 [4] NCCL INFO Channel 02/1 : 2[4] -> 5[1] [send] via NET/IB/3/Shared
g31n01:617760:618155 [4] NCCL INFO Channel 03/1 : 2[4] -> 5[1] [send] via NET/IB/1/Shared
g31n01:617760:618155 [4] NCCL INFO Channel 02/1 : 6[2] -> 2[4] [receive] via NET/IB/3/Shared
g31n01:617760:618155 [4] NCCL INFO Channel 03/1 : 6[2] -> 2[4] [receive] via NET/IB/1/Shared
g31n01:617760:618155 [4] NCCL INFO Channel 02/1 : 2[4] -> 6[2] [send] via NET/IB/3/Shared
g31n01:617760:618155 [4] NCCL INFO Channel 03/1 : 2[4] -> 6[2] [send] via NET/IB/1/Shared
g31n01:617760:618155 [4] NCCL INFO Channel 02/1 : 5[1] -> 2[4] [receive] via NET/IB/3/Shared
g31n01:617760:618155 [4] NCCL INFO Channel 03/1 : 5[1] -> 2[4] [receive] via NET/IB/1/Shared
g31n01:617760:618155 [4] NCCL INFO Channel 02/1 : 2[4] -> 7[3] [send] via NET/IB/3/Shared
g31n01:617760:618155 [4] NCCL INFO Channel 03/1 : 2[4] -> 7[3] [send] via NET/IB/1/Shared
g31n01:617760:618155 [4] NCCL INFO Channel 02/1 : 4[0] -> 2[4] [receive] via NET/IB/3/Shared
g31n01:617760:618155 [4] NCCL INFO Channel 03/1 : 4[0] -> 2[4] [receive] via NET/IB/1/Shared
g31n01:617760:618155 [4] NCCL INFO Channel 00/1 : 2[4] -> 0[2] via P2P/IPC
g31n02:736465:736860 [2] NCCL INFO Channel 02/1 : 6[2] -> 0[2] [send] via NET/IB/0/Shared
g31n02:736465:736860 [2] NCCL INFO Channel 03/1 : 6[2] -> 0[2] [send] via NET/IB/2/Shared
g31n02:736465:736860 [2] NCCL INFO Channel 02/1 : 3[5] -> 6[2] [receive] via NET/IB/0/Shared
g31n02:736465:736860 [2] NCCL INFO Channel 03/1 : 3[5] -> 6[2] [receive] via NET/IB/2/Shared
g31n02:736465:736860 [2] NCCL INFO Channel 02/1 : 6[2] -> 1[3] [send] via NET/IB/0/Shared
g31n02:736465:736860 [2] NCCL INFO Channel 03/1 : 6[2] -> 1[3] [send] via NET/IB/2/Shared
g31n02:736465:736860 [2] NCCL INFO Channel 02/1 : 2[4] -> 6[2] [receive] via NET/IB/0/Shared
g31n02:736465:736860 [2] NCCL INFO Channel 03/1 : 2[4] -> 6[2] [receive] via NET/IB/2/Shared
g31n02:736465:736860 [2] NCCL INFO Channel 02/1 : 6[2] -> 2[4] [send] via NET/IB/0/Shared
g31n02:736465:736860 [2] NCCL INFO Channel 03/1 : 6[2] -> 2[4] [send] via NET/IB/2/Shared
g31n02:736465:736860 [2] NCCL INFO Channel 02/1 : 1[3] -> 6[2] [receive] via NET/IB/0/Shared
g31n02:736465:736860 [2] NCCL INFO Channel 03/1 : 1[3] -> 6[2] [receive] via NET/IB/2/Shared
g31n02:736465:736860 [2] NCCL INFO Channel 02/1 : 6[2] -> 3[5] [send] via NET/IB/0/Shared
g31n02:736465:736860 [2] NCCL INFO Channel 03/1 : 6[2] -> 3[5] [send] via NET/IB/2/Shared
g31n02:736465:736860 [2] NCCL INFO Channel 02/1 : 0[2] -> 6[2] [receive] via NET/IB/0/Shared
g31n02:736465:736860 [2] NCCL INFO Channel 03/1 : 0[2] -> 6[2] [receive] via NET/IB/2/Shared
g31n02:736465:736860 [2] NCCL INFO Channel 00/1 : 6[2] -> 4[0] via P2P/IPC
g31n02:736466:736859 [3] NCCL INFO Channel 02/1 : 1[3] -> 7[3] [receive] via NET/IB/1/Shared
g31n02:736466:736859 [3] NCCL INFO Channel 03/1 : 1[3] -> 7[3] [receive] via NET/IB/3/Shared
g31n02:736466:736859 [3] NCCL INFO Channel 00/1 : 7[3] -> 5[1] via P2P/IPC
g26n01:811333:811720 [2] NCCL INFO Channel 02/1 : 4[0] -> 0[2] [receive] via NET/IB/0/Shared
g26n01:811333:811720 [2] NCCL INFO Channel 03/1 : 4[0] -> 0[2] [receive] via NET/IB/2/Shared
g26n01:811333:811720 [2] NCCL INFO Channel 02/1 : 0[2] -> 4[0] [send] via NET/IB/0/Shared
g26n01:811333:811720 [2] NCCL INFO Channel 03/1 : 0[2] -> 4[0] [send] via NET/IB/2/Shared
g26n02:851589:851985 [0] NCCL INFO Channel 02/1 : 0[2] -> 4[0] [receive] via NET/IB/0/Shared
g26n02:851589:851985 [0] NCCL INFO Channel 03/1 : 0[2] -> 4[0] [receive] via NET/IB/2/Shared
g26n02:851589:851985 [0] NCCL INFO Channel 02/1 : 4[0] -> 0[2] [send] via NET/IB/0/Shared
g26n02:851589:851985 [0] NCCL INFO Channel 03/1 : 4[0] -> 0[2] [send] via NET/IB/2/Shared
g31n01:617761:618154 [5] NCCL INFO Channel 02/1 : 5[1] -> 3[5] [receive] via NET/IB/1/Shared
g31n01:617761:618154 [5] NCCL INFO Channel 03/1 : 5[1] -> 3[5] [receive] via NET/IB/3/Shared
g31n01:617761:618154 [5] NCCL INFO Channel 00/1 : 3[5] -> 1[3] via P2P/IPC
g26n02:851592:851986 [3] NCCL INFO Channel 02/1 : 7[3] -> 2[4] [send] via NET/IB/1/Shared
g26n02:851592:851986 [3] NCCL INFO Channel 03/1 : 7[3] -> 2[4] [send] via NET/IB/3/Shared
g26n02:851592:851986 [3] NCCL INFO Channel 02/1 : 3[5] -> 7[3] [receive] via NET/IB/1/Shared
g26n02:851592:851986 [3] NCCL INFO Channel 03/1 : 3[5] -> 7[3] [receive] via NET/IB/3/Shared
g26n02:851592:851986 [3] NCCL INFO Channel 02/1 : 7[3] -> 3[5] [send] via NET/IB/1/Shared
g26n02:851592:851986 [3] NCCL INFO Channel 03/1 : 7[3] -> 3[5] [send] via NET/IB/3/Shared
g26n02:851592:851986 [3] NCCL INFO Channel 02/1 : 2[4] -> 7[3] [receive] via NET/IB/1/Shared
g26n02:851592:851986 [3] NCCL INFO Channel 03/1 : 2[4] -> 7[3] [receive] via NET/IB/3/Shared
g26n02:851592:851986 [3] NCCL INFO Channel 00/1 : 7[3] -> 4[0] via P2P/IPC
g26n01:811336:811721 [5] NCCL INFO Channel 02/1 : 3[5] -> 6[2] [send] via NET/IB/1/Shared
g26n01:811336:811721 [5] NCCL INFO Channel 03/1 : 3[5] -> 6[2] [send] via NET/IB/3/Shared
g26n01:811336:811721 [5] NCCL INFO Channel 02/1 : 7[3] -> 3[5] [receive] via NET/IB/1/Shared
g26n01:811336:811721 [5] NCCL INFO Channel 03/1 : 7[3] -> 3[5] [receive] via NET/IB/3/Shared
g26n01:811336:811721 [5] NCCL INFO Channel 02/1 : 3[5] -> 7[3] [send] via NET/IB/1/Shared
g26n01:811336:811721 [5] NCCL INFO Channel 03/1 : 3[5] -> 7[3] [send] via NET/IB/3/Shared
g26n01:811336:811721 [5] NCCL INFO Channel 02/1 : 6[2] -> 3[5] [receive] via NET/IB/1/Shared
g26n01:811336:811721 [5] NCCL INFO Channel 03/1 : 6[2] -> 3[5] [receive] via NET/IB/3/Shared
g26n01:811336:811721 [5] NCCL INFO Channel 00/1 : 3[5] -> 0[2] via P2P/IPC
g31n01:617760:618155 [4] NCCL INFO Channel 01/1 : 2[4] -> 0[2] via P2P/IPC
g31n01:617761:618154 [5] NCCL INFO Channel 01/1 : 3[5] -> 1[3] via P2P/IPC
g31n02:736465:736860 [2] NCCL INFO Channel 01/1 : 6[2] -> 4[0] via P2P/IPC
g26n01:811336:811721 [5] NCCL INFO Channel 01/1 : 3[5] -> 0[2] via P2P/IPC
g26n02:851592:851986 [3] NCCL INFO Channel 01/1 : 7[3] -> 4[0] via P2P/IPC
g31n02:736466:736859 [3] NCCL INFO Channel 01/1 : 7[3] -> 5[1] via P2P/IPC
g31n01:617758:618152 [2] NCCL INFO Channel 02/1 : 0[2] -> 6[2] [send] via NET/IB/0/Shared
g31n01:617758:618152 [2] NCCL INFO Channel 03/1 : 0[2] -> 6[2] [send] via NET/IB/2/Shared
g31n01:617759:618153 [3] NCCL INFO Channel 02/1 : 1[3] -> 7[3] [send] via NET/IB/1/Shared
g31n01:617759:618153 [3] NCCL INFO Channel 03/1 : 1[3] -> 7[3] [send] via NET/IB/3/Shared
g31n01:617761:618154 [5] NCCL INFO Channel 02/1 : 4[0] -> 3[5] [receive] via NET/IB/1/Shared
g31n01:617761:618154 [5] NCCL INFO Channel 03/1 : 4[0] -> 3[5] [receive] via NET/IB/3/Shared
g31n01:617761:618154 [5] NCCL INFO Channel 00/1 : 3[5] -> 2[4] via P2P/IPC
g26n01:811335:811723 [4] NCCL INFO Channel 02/1 : 2[4] -> 4[0] [send] via NET/IB/3/Shared
g26n01:811335:811723 [4] NCCL INFO Channel 03/1 : 2[4] -> 4[0] [send] via NET/IB/1/Shared
g26n01:811335:811723 [4] NCCL INFO Channel 02/1 : 7[3] -> 2[4] [receive] via NET/IB/3/Shared
g26n01:811335:811723 [4] NCCL INFO Channel 03/1 : 7[3] -> 2[4] [receive] via NET/IB/1/Shared
g26n01:811335:811723 [4] NCCL INFO Channel 02/1 : 2[4] -> 5[1] [send] via NET/IB/3/Shared
g26n01:811335:811723 [4] NCCL INFO Channel 03/1 : 2[4] -> 5[1] [send] via NET/IB/1/Shared
g26n01:811335:811723 [4] NCCL INFO Channel 02/1 : 6[2] -> 2[4] [receive] via NET/IB/3/Shared
g26n01:811335:811723 [4] NCCL INFO Channel 03/1 : 6[2] -> 2[4] [receive] via NET/IB/1/Shared
g26n01:811335:811723 [4] NCCL INFO Channel 02/1 : 2[4] -> 6[2] [send] via NET/IB/3/Shared
g26n01:811335:811723 [4] NCCL INFO Channel 03/1 : 2[4] -> 6[2] [send] via NET/IB/1/Shared
g26n01:811335:811723 [4] NCCL INFO Channel 02/1 : 5[1] -> 2[4] [receive] via NET/IB/3/Shared
g26n01:811335:811723 [4] NCCL INFO Channel 03/1 : 5[1] -> 2[4] [receive] via NET/IB/1/Shared
g26n01:811335:811723 [4] NCCL INFO Channel 02/1 : 2[4] -> 7[3] [send] via NET/IB/3/Shared
g26n01:811335:811723 [4] NCCL INFO Channel 03/1 : 2[4] -> 7[3] [send] via NET/IB/1/Shared
g26n01:811335:811723 [4] NCCL INFO Channel 02/1 : 4[0] -> 2[4] [receive] via NET/IB/3/Shared
g26n01:811335:811723 [4] NCCL INFO Channel 03/1 : 4[0] -> 2[4] [receive] via NET/IB/1/Shared
g26n01:811335:811723 [4] NCCL INFO Channel 00/1 : 2[4] -> 0[2] via P2P/IPC
g26n01:811334:811722 [3] NCCL INFO Channel 02/1 : 6[2] -> 1[3] [receive] via NET/IB/1/Shared
g26n01:811334:811722 [3] NCCL INFO Channel 03/1 : 6[2] -> 1[3] [receive] via NET/IB/3/Shared
g26n01:811334:811722 [3] NCCL INFO Channel 02/1 : 1[3] -> 4[0] [send] via NET/IB/1/Shared
g26n01:811334:811722 [3] NCCL INFO Channel 03/1 : 1[3] -> 4[0] [send] via NET/IB/3/Shared
g26n01:811334:811722 [3] NCCL INFO Channel 02/1 : 5[1] -> 1[3] [receive] via NET/IB/1/Shared
g26n01:811334:811722 [3] NCCL INFO Channel 03/1 : 5[1] -> 1[3] [receive] via NET/IB/3/Shared
g26n01:811334:811722 [3] NCCL INFO Channel 02/1 : 1[3] -> 5[1] [send] via NET/IB/1/Shared
g26n01:811334:811722 [3] NCCL INFO Channel 03/1 : 1[3] -> 5[1] [send] via NET/IB/3/Shared
g26n01:811334:811722 [3] NCCL INFO Channel 02/1 : 4[0] -> 1[3] [receive] via NET/IB/1/Shared
g26n01:811334:811722 [3] NCCL INFO Channel 03/1 : 4[0] -> 1[3] [receive] via NET/IB/3/Shared
g26n01:811334:811722 [3] NCCL INFO Channel 02/1 : 1[3] -> 6[2] [send] via NET/IB/1/Shared
g26n01:811334:811722 [3] NCCL INFO Channel 03/1 : 1[3] -> 6[2] [send] via NET/IB/3/Shared
g26n02:851590:851984 [1] NCCL INFO Channel 02/1 : 2[4] -> 5[1] [receive] via NET/IB/2/Shared
g26n02:851590:851984 [1] NCCL INFO Channel 03/1 : 2[4] -> 5[1] [receive] via NET/IB/0/Shared
g26n02:851590:851984 [1] NCCL INFO Channel 02/1 : 5[1] -> 0[2] [send] via NET/IB/2/Shared
g26n02:851590:851984 [1] NCCL INFO Channel 03/1 : 5[1] -> 0[2] [send] via NET/IB/0/Shared
g26n02:851590:851984 [1] NCCL INFO Channel 02/1 : 1[3] -> 5[1] [receive] via NET/IB/2/Shared
g26n02:851590:851984 [1] NCCL INFO Channel 03/1 : 1[3] -> 5[1] [receive] via NET/IB/0/Shared
g26n02:851590:851984 [1] NCCL INFO Channel 02/1 : 5[1] -> 1[3] [send] via NET/IB/2/Shared
g26n02:851590:851984 [1] NCCL INFO Channel 03/1 : 5[1] -> 1[3] [send] via NET/IB/0/Shared
g26n02:851590:851984 [1] NCCL INFO Channel 02/1 : 0[2] -> 5[1] [receive] via NET/IB/2/Shared
g26n02:851590:851984 [1] NCCL INFO Channel 03/1 : 0[2] -> 5[1] [receive] via NET/IB/0/Shared
g26n02:851590:851984 [1] NCCL INFO Channel 02/1 : 5[1] -> 2[4] [send] via NET/IB/2/Shared
g26n02:851590:851984 [1] NCCL INFO Channel 03/1 : 5[1] -> 2[4] [send] via NET/IB/0/Shared
g26n01:811333:811720 [2] NCCL INFO Channel 02/1 : 0[2] -> 5[1] [send] via NET/IB/0/Shared
g26n01:811333:811720 [2] NCCL INFO Channel 03/1 : 0[2] -> 5[1] [send] via NET/IB/2/Shared
g31n02:736463:736861 [0] NCCL INFO Channel 02/1 : 4[0] -> 2[4] [send] via NET/IB/0/Shared
g31n02:736463:736861 [0] NCCL INFO Channel 03/1 : 4[0] -> 2[4] [send] via NET/IB/2/Shared
g26n02:851589:851985 [0] NCCL INFO Channel 02/1 : 4[0] -> 1[3] [send] via NET/IB/0/Shared
g26n02:851589:851985 [0] NCCL INFO Channel 03/1 : 4[0] -> 1[3] [send] via NET/IB/2/Shared
g26n01:811336:811721 [5] NCCL INFO Channel 02/1 : 5[1] -> 3[5] [receive] via NET/IB/1/Shared
g26n01:811336:811721 [5] NCCL INFO Channel 03/1 : 5[1] -> 3[5] [receive] via NET/IB/3/Shared
g26n01:811336:811721 [5] NCCL INFO Channel 00/1 : 3[5] -> 1[3] via P2P/IPC
g31n05:704347:704725 [3] NCCL INFO Connected all trees
g31n05:704347:704725 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g31n05:704347:704725 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g31n02:736464:736858 [1] NCCL INFO Channel 02/1 : 5[1] -> 3[5] [send] via NET/IB/2/Shared
g31n02:736464:736858 [1] NCCL INFO Channel 03/1 : 5[1] -> 3[5] [send] via NET/IB/0/Shared
g26n02:851591:851987 [2] NCCL INFO Channel 02/1 : 6[2] -> 0[2] [send] via NET/IB/0/Shared
g26n02:851591:851987 [2] NCCL INFO Channel 03/1 : 6[2] -> 0[2] [send] via NET/IB/2/Shared
g26n02:851591:851987 [2] NCCL INFO Channel 02/1 : 3[5] -> 6[2] [receive] via NET/IB/0/Shared
g26n02:851591:851987 [2] NCCL INFO Channel 03/1 : 3[5] -> 6[2] [receive] via NET/IB/2/Shared
g26n02:851591:851987 [2] NCCL INFO Channel 02/1 : 6[2] -> 1[3] [send] via NET/IB/0/Shared
g26n02:851591:851987 [2] NCCL INFO Channel 03/1 : 6[2] -> 1[3] [send] via NET/IB/2/Shared
g26n02:851591:851987 [2] NCCL INFO Channel 02/1 : 2[4] -> 6[2] [receive] via NET/IB/0/Shared
g26n02:851591:851987 [2] NCCL INFO Channel 03/1 : 2[4] -> 6[2] [receive] via NET/IB/2/Shared
g26n02:851591:851987 [2] NCCL INFO Channel 02/1 : 6[2] -> 2[4] [send] via NET/IB/0/Shared
g26n02:851591:851987 [2] NCCL INFO Channel 03/1 : 6[2] -> 2[4] [send] via NET/IB/2/Shared
g26n02:851591:851987 [2] NCCL INFO Channel 02/1 : 1[3] -> 6[2] [receive] via NET/IB/0/Shared
g26n02:851591:851987 [2] NCCL INFO Channel 03/1 : 1[3] -> 6[2] [receive] via NET/IB/2/Shared
g26n02:851591:851987 [2] NCCL INFO Channel 02/1 : 6[2] -> 3[5] [send] via NET/IB/0/Shared
g26n02:851591:851987 [2] NCCL INFO Channel 03/1 : 6[2] -> 3[5] [send] via NET/IB/2/Shared
g26n02:851591:851987 [2] NCCL INFO Channel 02/1 : 0[2] -> 6[2] [receive] via NET/IB/0/Shared
g26n02:851591:851987 [2] NCCL INFO Channel 03/1 : 0[2] -> 6[2] [receive] via NET/IB/2/Shared
g26n02:851591:851987 [2] NCCL INFO Channel 00/1 : 6[2] -> 4[0] via P2P/IPC
g31n02:736466:736859 [3] NCCL INFO Channel 02/1 : 0[2] -> 7[3] [receive] via NET/IB/1/Shared
g31n02:736466:736859 [3] NCCL INFO Channel 03/1 : 0[2] -> 7[3] [receive] via NET/IB/3/Shared
g31n02:736466:736859 [3] NCCL INFO Channel 00/1 : 7[3] -> 6[2] via P2P/IPC
g26n02:851592:851986 [3] NCCL INFO Channel 02/1 : 1[3] -> 7[3] [receive] via NET/IB/1/Shared
g26n02:851592:851986 [3] NCCL INFO Channel 03/1 : 1[3] -> 7[3] [receive] via NET/IB/3/Shared
g26n02:851592:851986 [3] NCCL INFO Channel 00/1 : 7[3] -> 5[1] via P2P/IPC
g31n01:617761:618154 [5] NCCL INFO Channel 01/1 : 3[5] -> 2[4] via P2P/IPC
g26n01:811335:811723 [4] NCCL INFO Channel 01/1 : 2[4] -> 0[2] via P2P/IPC
g32n02:753683:754068 [0] NCCL INFO Connected all trees
g32n02:753683:754068 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g32n02:753683:754068 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g26n01:811336:811721 [5] NCCL INFO Channel 01/1 : 3[5] -> 1[3] via P2P/IPC
g26n02:851591:851987 [2] NCCL INFO Channel 01/1 : 6[2] -> 4[0] via P2P/IPC
g26n02:851592:851986 [3] NCCL INFO Channel 01/1 : 7[3] -> 5[1] via P2P/IPC
g31n02:736466:736859 [3] NCCL INFO Channel 01/1 : 7[3] -> 6[2] via P2P/IPC
g31n01:617759:618153 [3] NCCL INFO Channel 00/1 : 1[3] -> 0[2] via P2P/IPC
g31n01:617760:618155 [4] NCCL INFO Channel 00/1 : 2[4] -> 1[3] via P2P/IPC
g25n18:898852:899334 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g31n02:736465:736860 [2] NCCL INFO Channel 00/1 : 6[2] -> 5[1] via P2P/IPC
g31n02:736464:736858 [1] NCCL INFO Channel 00/1 : 5[1] -> 4[0] via P2P/IPC
g31n01:617759:618153 [3] NCCL INFO Channel 01/1 : 1[3] -> 0[2] via P2P/IPC
g31n01:617760:618155 [4] NCCL INFO Channel 01/1 : 2[4] -> 1[3] via P2P/IPC
g31n02:736465:736860 [2] NCCL INFO Channel 01/1 : 6[2] -> 5[1] via P2P/IPC
g26n02:851589:851985 [0] NCCL INFO Channel 02/1 : 4[0] -> 2[4] [send] via NET/IB/0/Shared
g26n02:851589:851985 [0] NCCL INFO Channel 03/1 : 4[0] -> 2[4] [send] via NET/IB/2/Shared
g31n02:736464:736858 [1] NCCL INFO Channel 01/1 : 5[1] -> 4[0] via P2P/IPC
g26n02:851590:851984 [1] NCCL INFO Channel 02/1 : 5[1] -> 3[5] [send] via NET/IB/2/Shared
g26n02:851590:851984 [1] NCCL INFO Channel 03/1 : 5[1] -> 3[5] [send] via NET/IB/0/Shared
g26n02:851592:851986 [3] NCCL INFO Channel 02/1 : 0[2] -> 7[3] [receive] via NET/IB/1/Shared
g26n02:851592:851986 [3] NCCL INFO Channel 03/1 : 0[2] -> 7[3] [receive] via NET/IB/3/Shared
g26n02:851592:851986 [3] NCCL INFO Channel 00/1 : 7[3] -> 6[2] via P2P/IPC
g26n01:811333:811720 [2] NCCL INFO Channel 02/1 : 0[2] -> 6[2] [send] via NET/IB/0/Shared
g26n01:811333:811720 [2] NCCL INFO Channel 03/1 : 0[2] -> 6[2] [send] via NET/IB/2/Shared
g26n01:811334:811722 [3] NCCL INFO Channel 02/1 : 1[3] -> 7[3] [send] via NET/IB/1/Shared
g26n01:811334:811722 [3] NCCL INFO Channel 03/1 : 1[3] -> 7[3] [send] via NET/IB/3/Shared
g26n01:811336:811721 [5] NCCL INFO Channel 02/1 : 4[0] -> 3[5] [receive] via NET/IB/1/Shared
g26n01:811336:811721 [5] NCCL INFO Channel 03/1 : 4[0] -> 3[5] [receive] via NET/IB/3/Shared
g26n01:811336:811721 [5] NCCL INFO Channel 00/1 : 3[5] -> 2[4] via P2P/IPC
g31n01:617758:618152 [2] NCCL INFO Channel 02/1 : 0[2] -> 7[3] [send] via NET/IB/0/Shared
g31n01:617758:618152 [2] NCCL INFO Channel 03/1 : 0[2] -> 7[3] [send] via NET/IB/2/Shared
g31n02:736463:736861 [0] NCCL INFO Channel 02/1 : 4[0] -> 3[5] [send] via NET/IB/0/Shared
g31n02:736463:736861 [0] NCCL INFO Channel 03/1 : 4[0] -> 3[5] [send] via NET/IB/2/Shared
g26n02:851592:851986 [3] NCCL INFO Channel 01/1 : 7[3] -> 6[2] via P2P/IPC
g26n01:811336:811721 [5] NCCL INFO Channel 01/1 : 3[5] -> 2[4] via P2P/IPC
g26n02:851590:851984 [1] NCCL INFO Channel 00/1 : 5[1] -> 4[0] via P2P/IPC
g26n02:851591:851987 [2] NCCL INFO Channel 00/1 : 6[2] -> 5[1] via P2P/IPC
g26n01:811335:811723 [4] NCCL INFO Channel 00/1 : 2[4] -> 1[3] via P2P/IPC
g26n01:811334:811722 [3] NCCL INFO Channel 00/1 : 1[3] -> 0[2] via P2P/IPC
g26n02:851590:851984 [1] NCCL INFO Channel 01/1 : 5[1] -> 4[0] via P2P/IPC
g26n02:851591:851987 [2] NCCL INFO Channel 01/1 : 6[2] -> 5[1] via P2P/IPC
g26n01:811335:811723 [4] NCCL INFO Channel 01/1 : 2[4] -> 1[3] via P2P/IPC
g26n01:811334:811722 [3] NCCL INFO Channel 01/1 : 1[3] -> 0[2] via P2P/IPC
g26n02:851589:851985 [0] NCCL INFO Channel 02/1 : 4[0] -> 3[5] [send] via NET/IB/0/Shared
g26n02:851589:851985 [0] NCCL INFO Channel 03/1 : 4[0] -> 3[5] [send] via NET/IB/2/Shared
g31n05:704347:704725 [3] NCCL INFO comm 0x1c84b2b20 rank 1 nranks 8 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xe6c91641295cac8b - Init COMPLETE
g31n05:704348:704723 [4] NCCL INFO comm 0x1a1370120 rank 2 nranks 8 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xe6c91641295cac8b - Init COMPLETE
g31n05:704349:704724 [5] NCCL INFO comm 0x1ba4d3220 rank 3 nranks 8 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xe6c91641295cac8b - Init COMPLETE
g31n05:704346:704720 [2] NCCL INFO comm 0x18de49fe0 rank 0 nranks 8 cudaDev 2 nvmlDev 2 busId 406000 commId 0xe6c91641295cac8b - Init COMPLETE
g31n05:704346:704734 [2] NCCL INFO Channel 02/1 : 7[3] -> 0[2] [receive] via NET/IB/0/Shared
g31n05:704346:704734 [2] NCCL INFO Channel 03/1 : 7[3] -> 0[2] [receive] via NET/IB/2/Shared
g31n05:704346:704734 [2] NCCL INFO Channel 00/1 : 0[2] -> 1[3] via P2P/IPC
g26n01:811333:811720 [2] NCCL INFO Channel 02/1 : 0[2] -> 7[3] [send] via NET/IB/0/Shared
g26n01:811333:811720 [2] NCCL INFO Channel 03/1 : 0[2] -> 7[3] [send] via NET/IB/2/Shared
g31n05:704346:704734 [2] NCCL INFO Channel 01/1 : 0[2] -> 1[3] via P2P/IPC
g32n02:753686:754071 [3] NCCL INFO comm 0x156b86be0 rank 7 nranks 8 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xe6c91641295cac8b - Init COMPLETE
g32n02:753685:754070 [2] NCCL INFO comm 0x1bafda560 rank 6 nranks 8 cudaDev 2 nvmlDev 2 busId 406000 commId 0xe6c91641295cac8b - Init COMPLETE
g32n02:753683:754068 [0] NCCL INFO comm 0x18aba5ab0 rank 4 nranks 8 cudaDev 0 nvmlDev 0 busId 404000 commId 0xe6c91641295cac8b - Init COMPLETE
g32n02:753683:754083 [0] NCCL INFO Channel 02/1 : 3[5] -> 4[0] [receive] via NET/IB/0/Shared
g32n02:753683:754083 [0] NCCL INFO Channel 03/1 : 3[5] -> 4[0] [receive] via NET/IB/2/Shared
g32n02:753683:754083 [0] NCCL INFO Channel 00/1 : 4[0] -> 5[1] via P2P/IPC
g32n02:753684:754069 [1] NCCL INFO comm 0x1c1f888e0 rank 5 nranks 8 cudaDev 1 nvmlDev 1 busId 405000 commId 0xe6c91641295cac8b - Init COMPLETE
g31n05:704347:704737 [3] NCCL INFO Channel 00/1 : 1[3] -> 2[4] via P2P/IPC
g31n05:704348:704736 [4] NCCL INFO Channel 00/1 : 2[4] -> 3[5] via P2P/IPC
g31n05:704348:704736 [4] NCCL INFO Channel 01/1 : 2[4] -> 3[5] via P2P/IPC
g31n05:704347:704737 [3] NCCL INFO Channel 01/1 : 1[3] -> 2[4] via P2P/IPC
g32n02:753683:754083 [0] NCCL INFO Channel 01/1 : 4[0] -> 5[1] via P2P/IPC
g32n02:753685:754082 [2] NCCL INFO Channel 00/1 : 6[2] -> 7[3] via P2P/IPC
g32n02:753684:754081 [1] NCCL INFO Channel 00/1 : 5[1] -> 6[2] via P2P/IPC
g32n02:753685:754082 [2] NCCL INFO Channel 01/1 : 6[2] -> 7[3] via P2P/IPC
g31n05:704349:704735 [5] NCCL INFO Channel 02/1 : 3[5] -> 4[0] [send] via NET/IB/1/Shared
g31n05:704349:704735 [5] NCCL INFO Channel 03/1 : 3[5] -> 4[0] [send] via NET/IB/3/Shared
g32n02:753684:754081 [1] NCCL INFO Channel 01/1 : 5[1] -> 6[2] via P2P/IPC
g31n05:704347:704737 [3] NCCL INFO Channel 02/1 : 7[3] -> 1[3] [receive] via NET/IB/1/Shared
g31n05:704347:704737 [3] NCCL INFO Channel 03/1 : 7[3] -> 1[3] [receive] via NET/IB/3/Shared
g31n05:704347:704737 [3] NCCL INFO Channel 00/1 : 1[3] -> 3[5] via P2P/IPC
g31n05:704346:704734 [2] NCCL INFO Channel 02/1 : 6[2] -> 0[2] [receive] via NET/IB/0/Shared
g31n05:704346:704734 [2] NCCL INFO Channel 03/1 : 6[2] -> 0[2] [receive] via NET/IB/2/Shared
g31n05:704346:704734 [2] NCCL INFO Channel 00/1 : 0[2] -> 2[4] via P2P/IPC
g31n05:704347:704737 [3] NCCL INFO Channel 01/1 : 1[3] -> 3[5] via P2P/IPC
g32n02:753686:754080 [3] NCCL INFO Channel 02/1 : 7[3] -> 0[2] [send] via NET/IB/1/Shared
g32n02:753686:754080 [3] NCCL INFO Channel 03/1 : 7[3] -> 0[2] [send] via NET/IB/3/Shared
g31n05:704346:704734 [2] NCCL INFO Channel 01/1 : 0[2] -> 2[4] via P2P/IPC
g32n02:753683:754083 [0] NCCL INFO Channel 02/1 : 2[4] -> 4[0] [receive] via NET/IB/0/Shared
g32n02:753683:754083 [0] NCCL INFO Channel 03/1 : 2[4] -> 4[0] [receive] via NET/IB/2/Shared
g32n02:753683:754083 [0] NCCL INFO Channel 00/1 : 4[0] -> 6[2] via P2P/IPC
g32n02:753684:754081 [1] NCCL INFO Channel 02/1 : 3[5] -> 5[1] [receive] via NET/IB/2/Shared
g32n02:753684:754081 [1] NCCL INFO Channel 03/1 : 3[5] -> 5[1] [receive] via NET/IB/0/Shared
g32n02:753684:754081 [1] NCCL INFO Channel 00/1 : 5[1] -> 7[3] via P2P/IPC
g32n02:753683:754083 [0] NCCL INFO Channel 01/1 : 4[0] -> 6[2] via P2P/IPC
g32n02:753684:754081 [1] NCCL INFO Channel 01/1 : 5[1] -> 7[3] via P2P/IPC
g28n02:792076:792466 [5] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
g28n02:792076:792466 [5] NCCL INFO P2P Chunksize set to 131072
g31n05:704349:704735 [5] NCCL INFO Channel 02/1 : 3[5] -> 5[1] [send] via NET/IB/1/Shared
g31n05:704349:704735 [5] NCCL INFO Channel 03/1 : 3[5] -> 5[1] [send] via NET/IB/3/Shared
g28n02:792075:792465 [4] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7
g28n02:792075:792465 [4] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7
g28n02:792075:792465 [4] NCCL INFO Trees [0] 1/2/-1->0->-1 [1] 1/-1/-1->0->2
g28n02:792075:792465 [4] NCCL INFO P2P Chunksize set to 131072
g28n02:792075:792465 [4] NCCL INFO Channel 00/0 : 7[5] -> 0[4] [receive] via NET/IB/1
g28n02:792075:792465 [4] NCCL INFO Channel 01/0 : 7[5] -> 0[4] [receive] via NET/IB/1
g28n02:792075:792465 [4] NCCL INFO Channel 00/0 : 0[4] -> 1[5] via P2P/IPC
g32n02:753686:754080 [3] NCCL INFO Channel 02/1 : 7[3] -> 1[3] [send] via NET/IB/1/Shared
g32n02:753686:754080 [3] NCCL INFO Channel 03/1 : 7[3] -> 1[3] [send] via NET/IB/3/Shared
g31n05:704346:704734 [2] NCCL INFO Channel 02/1 : 5[1] -> 0[2] [receive] via NET/IB/0/Shared
g31n05:704346:704734 [2] NCCL INFO Channel 03/1 : 5[1] -> 0[2] [receive] via NET/IB/2/Shared
g31n05:704346:704734 [2] NCCL INFO Channel 00/1 : 0[2] -> 3[5] via P2P/IPC
g32n02:753683:754083 [0] NCCL INFO Channel 02/1 : 1[3] -> 4[0] [receive] via NET/IB/0/Shared
g32n02:753683:754083 [0] NCCL INFO Channel 03/1 : 1[3] -> 4[0] [receive] via NET/IB/2/Shared
g32n02:753683:754083 [0] NCCL INFO Channel 00/1 : 4[0] -> 7[3] via P2P/IPC
g28n02:792075:792465 [4] NCCL INFO Channel 01/0 : 0[4] -> 1[5] via P2P/IPC
g31n05:704346:704734 [2] NCCL INFO Channel 01/1 : 0[2] -> 3[5] via P2P/IPC
g28n03:859954:860336 [1] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2
g28n03:859954:860336 [1] NCCL INFO P2P Chunksize set to 131072
g28n03:859956:860332 [2] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3
g28n03:859956:860332 [2] NCCL INFO P2P Chunksize set to 131072
g28n03:859960:860334 [4] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5
g28n03:859960:860334 [4] NCCL INFO P2P Chunksize set to 131072
g32n02:753683:754083 [0] NCCL INFO Channel 01/1 : 4[0] -> 7[3] via P2P/IPC
g28n03:859961:860333 [5] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6
g28n03:859961:860333 [5] NCCL INFO P2P Chunksize set to 131072
g28n03:859958:860335 [3] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4
g28n03:859958:860335 [3] NCCL INFO P2P Chunksize set to 131072
g28n03:859953:860331 [0] NCCL INFO Trees [0] 3/-1/-1->2->0 [1] 3/0/-1->2->-1
g28n03:859953:860331 [0] NCCL INFO P2P Chunksize set to 131072
g28n03:859953:860331 [0] NCCL INFO Channel 00/0 : 1[5] -> 2[0] [receive] via NET/IB/0
g28n03:859953:860331 [0] NCCL INFO Channel 01/0 : 1[5] -> 2[0] [receive] via NET/IB/0
g28n03:859953:860331 [0] NCCL INFO Channel 00/0 : 2[0] -> 3[1] via P2P/IPC
g31n05:704346:704734 [2] NCCL INFO Channel 02/1 : 4[0] -> 0[2] [receive] via NET/IB/0/Shared
g31n05:704346:704734 [2] NCCL INFO Channel 03/1 : 4[0] -> 0[2] [receive] via NET/IB/2/Shared
g31n05:704346:704734 [2] NCCL INFO Channel 02/1 : 0[2] -> 4[0] [send] via NET/IB/0/Shared
g31n05:704346:704734 [2] NCCL INFO Channel 03/1 : 0[2] -> 4[0] [send] via NET/IB/2/Shared
g32n02:753683:754083 [0] NCCL INFO Channel 02/1 : 0[2] -> 4[0] [receive] via NET/IB/0/Shared
g32n02:753683:754083 [0] NCCL INFO Channel 03/1 : 0[2] -> 4[0] [receive] via NET/IB/2/Shared
g32n02:753683:754083 [0] NCCL INFO Channel 02/1 : 4[0] -> 0[2] [send] via NET/IB/0/Shared
g32n02:753683:754083 [0] NCCL INFO Channel 03/1 : 4[0] -> 0[2] [send] via NET/IB/2/Shared
g31n05:704349:704735 [5] NCCL INFO Channel 02/1 : 3[5] -> 6[2] [send] via NET/IB/1/Shared
g31n05:704349:704735 [5] NCCL INFO Channel 03/1 : 3[5] -> 6[2] [send] via NET/IB/3/Shared
g31n05:704349:704735 [5] NCCL INFO Channel 02/1 : 7[3] -> 3[5] [receive] via NET/IB/1/Shared
g31n05:704349:704735 [5] NCCL INFO Channel 03/1 : 7[3] -> 3[5] [receive] via NET/IB/3/Shared
g31n05:704349:704735 [5] NCCL INFO Channel 02/1 : 3[5] -> 7[3] [send] via NET/IB/1/Shared
g31n05:704349:704735 [5] NCCL INFO Channel 03/1 : 3[5] -> 7[3] [send] via NET/IB/3/Shared
g31n05:704349:704735 [5] NCCL INFO Channel 02/1 : 6[2] -> 3[5] [receive] via NET/IB/1/Shared
g31n05:704349:704735 [5] NCCL INFO Channel 03/1 : 6[2] -> 3[5] [receive] via NET/IB/3/Shared
g31n05:704349:704735 [5] NCCL INFO Channel 00/1 : 3[5] -> 0[2] via P2P/IPC
g32n02:753686:754080 [3] NCCL INFO Channel 02/1 : 7[3] -> 2[4] [send] via NET/IB/1/Shared
g32n02:753686:754080 [3] NCCL INFO Channel 03/1 : 7[3] -> 2[4] [send] via NET/IB/3/Shared
g32n02:753686:754080 [3] NCCL INFO Channel 02/1 : 3[5] -> 7[3] [receive] via NET/IB/1/Shared
g32n02:753686:754080 [3] NCCL INFO Channel 03/1 : 3[5] -> 7[3] [receive] via NET/IB/3/Shared
g32n02:753686:754080 [3] NCCL INFO Channel 02/1 : 7[3] -> 3[5] [send] via NET/IB/1/Shared
g32n02:753686:754080 [3] NCCL INFO Channel 03/1 : 7[3] -> 3[5] [send] via NET/IB/3/Shared
g32n02:753686:754080 [3] NCCL INFO Channel 02/1 : 2[4] -> 7[3] [receive] via NET/IB/1/Shared
g32n02:753686:754080 [3] NCCL INFO Channel 03/1 : 2[4] -> 7[3] [receive] via NET/IB/3/Shared
g32n02:753686:754080 [3] NCCL INFO Channel 00/1 : 7[3] -> 4[0] via P2P/IPC
g28n03:859953:860331 [0] NCCL INFO Channel 01/0 : 2[0] -> 3[1] via P2P/IPC
g31n05:704349:704735 [5] NCCL INFO Channel 01/1 : 3[5] -> 0[2] via P2P/IPC
g32n02:753686:754080 [3] NCCL INFO Channel 01/1 : 7[3] -> 4[0] via P2P/IPC
g28n02:792076:792466 [5] NCCL INFO Channel 00/0 : 1[5] -> 2[0] [send] via NET/IB/1
g28n02:792076:792466 [5] NCCL INFO Channel 01/0 : 1[5] -> 2[0] [send] via NET/IB/1
g28n03:859954:860336 [1] NCCL INFO Channel 00/0 : 3[1] -> 4[2] via P2P/IPC
g28n03:859956:860332 [2] NCCL INFO Channel 00/0 : 4[2] -> 5[3] via P2P/IPC
g28n03:859960:860334 [4] NCCL INFO Channel 00/0 : 6[4] -> 7[5] via P2P/IPC
g28n03:859958:860335 [3] NCCL INFO Channel 00/0 : 5[3] -> 6[4] via P2P/IPC
g31n05:704347:704737 [3] NCCL INFO Channel 02/1 : 6[2] -> 1[3] [receive] via NET/IB/1/Shared
g31n05:704347:704737 [3] NCCL INFO Channel 03/1 : 6[2] -> 1[3] [receive] via NET/IB/3/Shared
g31n05:704347:704737 [3] NCCL INFO Channel 02/1 : 1[3] -> 4[0] [send] via NET/IB/1/Shared
g31n05:704347:704737 [3] NCCL INFO Channel 03/1 : 1[3] -> 4[0] [send] via NET/IB/3/Shared
g31n05:704347:704737 [3] NCCL INFO Channel 02/1 : 5[1] -> 1[3] [receive] via NET/IB/1/Shared
g31n05:704347:704737 [3] NCCL INFO Channel 03/1 : 5[1] -> 1[3] [receive] via NET/IB/3/Shared
g31n05:704347:704737 [3] NCCL INFO Channel 02/1 : 1[3] -> 5[1] [send] via NET/IB/1/Shared
g31n05:704347:704737 [3] NCCL INFO Channel 03/1 : 1[3] -> 5[1] [send] via NET/IB/3/Shared
g31n05:704347:704737 [3] NCCL INFO Channel 02/1 : 4[0] -> 1[3] [receive] via NET/IB/1/Shared
g31n05:704347:704737 [3] NCCL INFO Channel 03/1 : 4[0] -> 1[3] [receive] via NET/IB/3/Shared
g31n05:704347:704737 [3] NCCL INFO Channel 02/1 : 1[3] -> 6[2] [send] via NET/IB/1/Shared
g31n05:704347:704737 [3] NCCL INFO Channel 03/1 : 1[3] -> 6[2] [send] via NET/IB/3/Shared
g32n02:753685:754082 [2] NCCL INFO Channel 02/1 : 6[2] -> 0[2] [send] via NET/IB/0/Shared
g32n02:753685:754082 [2] NCCL INFO Channel 03/1 : 6[2] -> 0[2] [send] via NET/IB/2/Shared
g32n02:753685:754082 [2] NCCL INFO Channel 02/1 : 3[5] -> 6[2] [receive] via NET/IB/0/Shared
g32n02:753685:754082 [2] NCCL INFO Channel 03/1 : 3[5] -> 6[2] [receive] via NET/IB/2/Shared
g32n02:753685:754082 [2] NCCL INFO Channel 02/1 : 6[2] -> 1[3] [send] via NET/IB/0/Shared
g32n02:753685:754082 [2] NCCL INFO Channel 03/1 : 6[2] -> 1[3] [send] via NET/IB/2/Shared
g32n02:753685:754082 [2] NCCL INFO Channel 02/1 : 2[4] -> 6[2] [receive] via NET/IB/0/Shared
g32n02:753685:754082 [2] NCCL INFO Channel 03/1 : 2[4] -> 6[2] [receive] via NET/IB/2/Shared
g32n02:753685:754082 [2] NCCL INFO Channel 02/1 : 6[2] -> 2[4] [send] via NET/IB/0/Shared
g32n02:753685:754082 [2] NCCL INFO Channel 03/1 : 6[2] -> 2[4] [send] via NET/IB/2/Shared
g32n02:753685:754082 [2] NCCL INFO Channel 02/1 : 1[3] -> 6[2] [receive] via NET/IB/0/Shared
g32n02:753685:754082 [2] NCCL INFO Channel 03/1 : 1[3] -> 6[2] [receive] via NET/IB/2/Shared
g32n02:753685:754082 [2] NCCL INFO Channel 02/1 : 6[2] -> 3[5] [send] via NET/IB/0/Shared
g32n02:753685:754082 [2] NCCL INFO Channel 03/1 : 6[2] -> 3[5] [send] via NET/IB/2/Shared
g32n02:753685:754082 [2] NCCL INFO Channel 02/1 : 0[2] -> 6[2] [receive] via NET/IB/0/Shared
g32n02:753685:754082 [2] NCCL INFO Channel 03/1 : 0[2] -> 6[2] [receive] via NET/IB/2/Shared
g32n02:753685:754082 [2] NCCL INFO Channel 00/1 : 6[2] -> 4[0] via P2P/IPC
g31n05:704348:704736 [4] NCCL INFO Channel 02/1 : 2[4] -> 4[0] [send] via NET/IB/3/Shared
g31n05:704348:704736 [4] NCCL INFO Channel 03/1 : 2[4] -> 4[0] [send] via NET/IB/1/Shared
g31n05:704348:704736 [4] NCCL INFO Channel 02/1 : 7[3] -> 2[4] [receive] via NET/IB/3/Shared
g31n05:704348:704736 [4] NCCL INFO Channel 03/1 : 7[3] -> 2[4] [receive] via NET/IB/1/Shared
g31n05:704348:704736 [4] NCCL INFO Channel 02/1 : 2[4] -> 5[1] [send] via NET/IB/3/Shared
g31n05:704348:704736 [4] NCCL INFO Channel 03/1 : 2[4] -> 5[1] [send] via NET/IB/1/Shared
g31n05:704348:704736 [4] NCCL INFO Channel 02/1 : 6[2] -> 2[4] [receive] via NET/IB/3/Shared
g31n05:704348:704736 [4] NCCL INFO Channel 03/1 : 6[2] -> 2[4] [receive] via NET/IB/1/Shared
g31n05:704348:704736 [4] NCCL INFO Channel 02/1 : 2[4] -> 6[2] [send] via NET/IB/3/Shared
g31n05:704348:704736 [4] NCCL INFO Channel 03/1 : 2[4] -> 6[2] [send] via NET/IB/1/Shared
g31n05:704348:704736 [4] NCCL INFO Channel 02/1 : 5[1] -> 2[4] [receive] via NET/IB/3/Shared
g31n05:704348:704736 [4] NCCL INFO Channel 03/1 : 5[1] -> 2[4] [receive] via NET/IB/1/Shared
g31n05:704348:704736 [4] NCCL INFO Channel 02/1 : 2[4] -> 7[3] [send] via NET/IB/3/Shared
g31n05:704348:704736 [4] NCCL INFO Channel 03/1 : 2[4] -> 7[3] [send] via NET/IB/1/Shared
g31n05:704348:704736 [4] NCCL INFO Channel 02/1 : 4[0] -> 2[4] [receive] via NET/IB/3/Shared
g31n05:704348:704736 [4] NCCL INFO Channel 03/1 : 4[0] -> 2[4] [receive] via NET/IB/1/Shared
g31n05:704348:704736 [4] NCCL INFO Channel 00/1 : 2[4] -> 0[2] via P2P/IPC
g31n05:704346:704734 [2] NCCL INFO Channel 02/1 : 0[2] -> 5[1] [send] via NET/IB/0/Shared
g31n05:704346:704734 [2] NCCL INFO Channel 03/1 : 0[2] -> 5[1] [send] via NET/IB/2/Shared
g32n02:753683:754083 [0] NCCL INFO Channel 02/1 : 4[0] -> 1[3] [send] via NET/IB/0/Shared
g32n02:753683:754083 [0] NCCL INFO Channel 03/1 : 4[0] -> 1[3] [send] via NET/IB/2/Shared
g31n05:704349:704735 [5] NCCL INFO Channel 02/1 : 5[1] -> 3[5] [receive] via NET/IB/1/Shared
g31n05:704349:704735 [5] NCCL INFO Channel 03/1 : 5[1] -> 3[5] [receive] via NET/IB/3/Shared
g31n05:704349:704735 [5] NCCL INFO Channel 00/1 : 3[5] -> 1[3] via P2P/IPC
g32n02:753684:754081 [1] NCCL INFO Channel 02/1 : 2[4] -> 5[1] [receive] via NET/IB/2/Shared
g32n02:753684:754081 [1] NCCL INFO Channel 03/1 : 2[4] -> 5[1] [receive] via NET/IB/0/Shared
g32n02:753684:754081 [1] NCCL INFO Channel 02/1 : 5[1] -> 0[2] [send] via NET/IB/2/Shared
g32n02:753684:754081 [1] NCCL INFO Channel 03/1 : 5[1] -> 0[2] [send] via NET/IB/0/Shared
g32n02:753684:754081 [1] NCCL INFO Channel 02/1 : 1[3] -> 5[1] [receive] via NET/IB/2/Shared
g32n02:753684:754081 [1] NCCL INFO Channel 03/1 : 1[3] -> 5[1] [receive] via NET/IB/0/Shared
g32n02:753684:754081 [1] NCCL INFO Channel 02/1 : 5[1] -> 1[3] [send] via NET/IB/2/Shared
g32n02:753684:754081 [1] NCCL INFO Channel 03/1 : 5[1] -> 1[3] [send] via NET/IB/0/Shared
g32n02:753684:754081 [1] NCCL INFO Channel 02/1 : 0[2] -> 5[1] [receive] via NET/IB/2/Shared
g32n02:753684:754081 [1] NCCL INFO Channel 03/1 : 0[2] -> 5[1] [receive] via NET/IB/0/Shared
g32n02:753684:754081 [1] NCCL INFO Channel 02/1 : 5[1] -> 2[4] [send] via NET/IB/2/Shared
g32n02:753684:754081 [1] NCCL INFO Channel 03/1 : 5[1] -> 2[4] [send] via NET/IB/0/Shared
g28n03:859954:860336 [1] NCCL INFO Channel 01/0 : 3[1] -> 4[2] via P2P/IPC
g32n02:753686:754080 [3] NCCL INFO Channel 02/1 : 1[3] -> 7[3] [receive] via NET/IB/1/Shared
g32n02:753686:754080 [3] NCCL INFO Channel 03/1 : 1[3] -> 7[3] [receive] via NET/IB/3/Shared
g32n02:753686:754080 [3] NCCL INFO Channel 00/1 : 7[3] -> 5[1] via P2P/IPC
g28n03:859956:860332 [2] NCCL INFO Channel 01/0 : 4[2] -> 5[3] via P2P/IPC
g28n03:859960:860334 [4] NCCL INFO Channel 01/0 : 6[4] -> 7[5] via P2P/IPC
g28n03:859958:860335 [3] NCCL INFO Channel 01/0 : 5[3] -> 6[4] via P2P/IPC
g31n05:704348:704736 [4] NCCL INFO Channel 01/1 : 2[4] -> 0[2] via P2P/IPC
g32n02:753685:754082 [2] NCCL INFO Channel 01/1 : 6[2] -> 4[0] via P2P/IPC
g31n05:704349:704735 [5] NCCL INFO Channel 01/1 : 3[5] -> 1[3] via P2P/IPC
g32n02:753686:754080 [3] NCCL INFO Channel 01/1 : 7[3] -> 5[1] via P2P/IPC
g28n03:859961:860333 [5] NCCL INFO Channel 00/0 : 7[5] -> 0[4] [send] via NET/IB/1
g28n03:859961:860333 [5] NCCL INFO Channel 01/0 : 7[5] -> 0[4] [send] via NET/IB/1
g31n05:704347:704737 [3] NCCL INFO Channel 02/1 : 1[3] -> 7[3] [send] via NET/IB/1/Shared
g31n05:704347:704737 [3] NCCL INFO Channel 03/1 : 1[3] -> 7[3] [send] via NET/IB/3/Shared
g31n05:704346:704734 [2] NCCL INFO Channel 02/1 : 0[2] -> 6[2] [send] via NET/IB/0/Shared
g31n05:704346:704734 [2] NCCL INFO Channel 03/1 : 0[2] -> 6[2] [send] via NET/IB/2/Shared
g32n02:753683:754083 [0] NCCL INFO Channel 02/1 : 4[0] -> 2[4] [send] via NET/IB/0/Shared
g32n02:753683:754083 [0] NCCL INFO Channel 03/1 : 4[0] -> 2[4] [send] via NET/IB/2/Shared
g31n05:704349:704735 [5] NCCL INFO Channel 02/1 : 4[0] -> 3[5] [receive] via NET/IB/1/Shared
g31n05:704349:704735 [5] NCCL INFO Channel 03/1 : 4[0] -> 3[5] [receive] via NET/IB/3/Shared
g31n05:704349:704735 [5] NCCL INFO Channel 00/1 : 3[5] -> 2[4] via P2P/IPC
g32n02:753684:754081 [1] NCCL INFO Channel 02/1 : 5[1] -> 3[5] [send] via NET/IB/2/Shared
g32n02:753684:754081 [1] NCCL INFO Channel 03/1 : 5[1] -> 3[5] [send] via NET/IB/0/Shared
g32n02:753686:754080 [3] NCCL INFO Channel 02/1 : 0[2] -> 7[3] [receive] via NET/IB/1/Shared
g32n02:753686:754080 [3] NCCL INFO Channel 03/1 : 0[2] -> 7[3] [receive] via NET/IB/3/Shared
g32n02:753686:754080 [3] NCCL INFO Channel 00/1 : 7[3] -> 6[2] via P2P/IPC
g31n05:704349:704735 [5] NCCL INFO Channel 01/1 : 3[5] -> 2[4] via P2P/IPC
g32n02:753686:754080 [3] NCCL INFO Channel 01/1 : 7[3] -> 6[2] via P2P/IPC
g31n01:617757:618140 [1] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6
g31n01:617757:618140 [1] NCCL INFO P2P Chunksize set to 131072
g31n05:704347:704737 [3] NCCL INFO Channel 00/1 : 1[3] -> 0[2] via P2P/IPC
g31n05:704348:704736 [4] NCCL INFO Channel 00/1 : 2[4] -> 1[3] via P2P/IPC
g32n02:753685:754082 [2] NCCL INFO Channel 00/1 : 6[2] -> 5[1] via P2P/IPC
g32n02:753684:754081 [1] NCCL INFO Channel 00/1 : 5[1] -> 4[0] via P2P/IPC
g31n01:617756:618139 [0] NCCL INFO Trees [0] 7/-1/-1->6->0 [1] 7/0/-1->6->-1
g31n01:617756:618139 [0] NCCL INFO P2P Chunksize set to 131072
g31n01:617756:618139 [0] NCCL INFO Channel 00/0 : 5[5] -> 6[0] [receive] via NET/IB/0
g31n01:617756:618139 [0] NCCL INFO Channel 01/0 : 5[5] -> 6[0] [receive] via NET/IB/0
g31n01:617756:618139 [0] NCCL INFO Channel 00/0 : 6[0] -> 7[1] via P2P/IPC
g28n03:859956:860332 [2] NCCL INFO Connected all rings
g28n03:859960:860334 [4] NCCL INFO Connected all rings
g31n05:704347:704737 [3] NCCL INFO Channel 01/1 : 1[3] -> 0[2] via P2P/IPC
g32n02:753685:754082 [2] NCCL INFO Channel 01/1 : 6[2] -> 5[1] via P2P/IPC
g31n05:704348:704736 [4] NCCL INFO Channel 01/1 : 2[4] -> 1[3] via P2P/IPC
g32n02:753684:754081 [1] NCCL INFO Channel 01/1 : 5[1] -> 4[0] via P2P/IPC
g31n01:617756:618139 [0] NCCL INFO Channel 01/0 : 6[0] -> 7[1] via P2P/IPC
g28n04:841281:841666 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
g28n04:841281:841666 [1] NCCL INFO P2P Chunksize set to 131072
g28n04:841283:841665 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
g28n04:841283:841665 [2] NCCL INFO P2P Chunksize set to 131072
g28n04:841287:841667 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3
g28n04:841287:841667 [4] NCCL INFO P2P Chunksize set to 131072
g31n05:704346:704734 [2] NCCL INFO Channel 02/1 : 0[2] -> 7[3] [send] via NET/IB/0/Shared
g31n05:704346:704734 [2] NCCL INFO Channel 03/1 : 0[2] -> 7[3] [send] via NET/IB/2/Shared
g28n04:841288:841668 [5] NCCL INFO Trees [0] -1/-1/-1->5->4 [1] -1/-1/-1->5->4
g28n04:841288:841668 [5] NCCL INFO P2P Chunksize set to 131072
g28n04:841284:841669 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2
g28n04:841284:841669 [3] NCCL INFO P2P Chunksize set to 131072
g28n04:841280:841664 [0] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7
g28n04:841280:841664 [0] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7
g28n04:841280:841664 [0] NCCL INFO Trees [0] 1/6/-1->0->-1 [1] 1/-1/-1->0->6
g28n04:841280:841664 [0] NCCL INFO P2P Chunksize set to 131072
g28n04:841280:841664 [0] NCCL INFO Channel 00/0 : 7[1] -> 0[0] [receive] via NET/IB/0
g28n04:841280:841664 [0] NCCL INFO Channel 01/0 : 7[1] -> 0[0] [receive] via NET/IB/0
g28n04:841280:841664 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/IPC
g32n02:753683:754083 [0] NCCL INFO Channel 02/1 : 4[0] -> 3[5] [send] via NET/IB/0/Shared
g32n02:753683:754083 [0] NCCL INFO Channel 03/1 : 4[0] -> 3[5] [send] via NET/IB/2/Shared
g28n04:841280:841664 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/IPC
g28n03:859956:860332 [2] NCCL INFO Channel 00/0 : 4[2] -> 3[1] via P2P/IPC
g28n03:859954:860336 [1] NCCL INFO Connected all rings
g31n01:617757:618140 [1] NCCL INFO Channel 00/0 : 7[1] -> 0[0] [send] via NET/IB/0
g31n01:617757:618140 [1] NCCL INFO Channel 01/0 : 7[1] -> 0[0] [send] via NET/IB/0
g28n03:859958:860335 [3] NCCL INFO Connected all rings
g28n04:841281:841666 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/IPC
g28n04:841287:841667 [4] NCCL INFO Channel 00/0 : 4[4] -> 5[5] via P2P/IPC
g28n04:841284:841669 [3] NCCL INFO Channel 00/0 : 3[3] -> 4[4] via P2P/IPC
g28n04:841283:841665 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/IPC
g28n03:859956:860332 [2] NCCL INFO Channel 01/0 : 4[2] -> 3[1] via P2P/IPC
g28n04:841281:841666 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/IPC
g28n04:841287:841667 [4] NCCL INFO Channel 01/0 : 4[4] -> 5[5] via P2P/IPC
g28n04:841284:841669 [3] NCCL INFO Channel 01/0 : 3[3] -> 4[4] via P2P/IPC
g28n04:841283:841665 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/IPC
g28n03:859960:860334 [4] NCCL INFO Channel 00/0 : 6[4] -> 5[3] via P2P/IPC
g28n03:859954:860336 [1] NCCL INFO Channel 00/0 : 3[1] -> 2[0] via P2P/IPC
g26n02:851594:851975 [5] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
g26n02:851594:851975 [5] NCCL INFO P2P Chunksize set to 131072
g28n03:859958:860335 [3] NCCL INFO Channel 00/0 : 5[3] -> 4[2] via P2P/IPC
g28n03:859960:860334 [4] NCCL INFO Channel 01/0 : 6[4] -> 5[3] via P2P/IPC
g31n02:736468:736848 [5] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
g31n02:736468:736848 [5] NCCL INFO P2P Chunksize set to 131072
g26n02:851593:851970 [4] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7
g26n02:851593:851970 [4] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7
g26n02:851593:851970 [4] NCCL INFO Trees [0] 1/2/-1->0->-1 [1] 1/-1/-1->0->2
g26n02:851593:851970 [4] NCCL INFO P2P Chunksize set to 131072
g26n02:851593:851970 [4] NCCL INFO Channel 00/0 : 7[5] -> 0[4] [receive] via NET/IB/1
g26n02:851593:851970 [4] NCCL INFO Channel 01/0 : 7[5] -> 0[4] [receive] via NET/IB/1
g26n02:851593:851970 [4] NCCL INFO Channel 00/0 : 0[4] -> 1[5] via P2P/IPC
g28n03:859954:860336 [1] NCCL INFO Channel 01/0 : 3[1] -> 2[0] via P2P/IPC
g28n03:859958:860335 [3] NCCL INFO Channel 01/0 : 5[3] -> 4[2] via P2P/IPC
g28n04:841288:841668 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[0] [send] via NET/IB/1
g28n04:841288:841668 [5] NCCL INFO Channel 01/0 : 5[5] -> 6[0] [send] via NET/IB/1
g26n02:851593:851970 [4] NCCL INFO Channel 01/0 : 0[4] -> 1[5] via P2P/IPC
g31n02:736467:736844 [4] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7
g31n02:736467:736844 [4] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7
g31n02:736467:736844 [4] NCCL INFO Trees [0] 1/2/-1->0->-1 [1] 1/-1/-1->0->2
g31n02:736467:736844 [4] NCCL INFO P2P Chunksize set to 131072
g31n02:736467:736844 [4] NCCL INFO Channel 00/0 : 7[5] -> 0[4] [receive] via NET/IB/1
g31n02:736467:736844 [4] NCCL INFO Channel 01/0 : 7[5] -> 0[4] [receive] via NET/IB/1
g31n02:736467:736844 [4] NCCL INFO Channel 00/0 : 0[4] -> 1[5] via P2P/IPC
g27n17:842067:842444 [4] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5
g27n17:842067:842444 [4] NCCL INFO P2P Chunksize set to 131072
g27n17:842064:842445 [3] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4
g27n17:842064:842445 [3] NCCL INFO P2P Chunksize set to 131072
g31n03:688514:688888 [5] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6
g31n03:688514:688888 [5] NCCL INFO P2P Chunksize set to 131072
g27n17:842063:842442 [2] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3
g27n17:842063:842442 [2] NCCL INFO P2P Chunksize set to 131072
g31n03:688513:688886 [4] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5
g31n03:688513:688886 [4] NCCL INFO P2P Chunksize set to 131072
g31n02:736467:736844 [4] NCCL INFO Channel 01/0 : 0[4] -> 1[5] via P2P/IPC
g27n17:842061:842441 [1] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2
g27n17:842061:842441 [1] NCCL INFO P2P Chunksize set to 131072
g27n17:842060:842446 [0] NCCL INFO Trees [0] 3/-1/-1->2->0 [1] 3/0/-1->2->-1
g27n17:842060:842446 [0] NCCL INFO P2P Chunksize set to 131072
g27n17:842060:842446 [0] NCCL INFO Channel 00/0 : 1[5] -> 2[0] [receive] via NET/IB/0
g27n17:842060:842446 [0] NCCL INFO Channel 01/0 : 1[5] -> 2[0] [receive] via NET/IB/0
g27n17:842060:842446 [0] NCCL INFO Channel 00/0 : 2[0] -> 3[1] via P2P/IPC
g31n03:688510:688889 [1] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2
g31n03:688510:688889 [1] NCCL INFO P2P Chunksize set to 131072
g31n03:688509:688884 [0] NCCL INFO Trees [0] 3/-1/-1->2->0 [1] 3/0/-1->2->-1
g31n03:688509:688884 [0] NCCL INFO P2P Chunksize set to 131072
g31n03:688509:688884 [0] NCCL INFO Channel 00/0 : 1[5] -> 2[0] [receive] via NET/IB/0
g31n03:688509:688884 [0] NCCL INFO Channel 01/0 : 1[5] -> 2[0] [receive] via NET/IB/0
g31n03:688509:688884 [0] NCCL INFO Channel 00/0 : 2[0] -> 3[1] via P2P/IPC
g31n03:688512:688885 [3] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4
g31n03:688512:688885 [3] NCCL INFO P2P Chunksize set to 131072
g27n17:842068:842443 [5] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6
g27n17:842068:842443 [5] NCCL INFO P2P Chunksize set to 131072
g28n02:792076:792466 [5] NCCL INFO Connected all rings
g28n02:792076:792466 [5] NCCL INFO Channel 00/0 : 1[5] -> 0[4] via P2P/IPC
g32n03:753099:753472 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g32n03:753100:753471 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g31n03:688509:688884 [0] NCCL INFO Channel 01/0 : 2[0] -> 3[1] via P2P/IPC
g31n03:688510:688889 [1] NCCL INFO Channel 00/0 : 3[1] -> 4[2] via P2P/IPC
g28n02:792076:792466 [5] NCCL INFO Channel 01/0 : 1[5] -> 0[4] via P2P/IPC
g31n03:688511:688887 [2] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3
g31n03:688511:688887 [2] NCCL INFO P2P Chunksize set to 131072
g27n17:842060:842446 [0] NCCL INFO Channel 01/0 : 2[0] -> 3[1] via P2P/IPC
g32n03:753096:753474 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g26n01:811332:811710 [1] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6
g26n01:811332:811710 [1] NCCL INFO P2P Chunksize set to 131072
g32n03:753098:753473 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g32n03:753097:753475 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g32n03:753101:753470 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g31n02:736468:736848 [5] NCCL INFO Channel 00/0 : 1[5] -> 2[0] [send] via NET/IB/1
g31n02:736468:736848 [5] NCCL INFO Channel 01/0 : 1[5] -> 2[0] [send] via NET/IB/1
g26n02:851594:851975 [5] NCCL INFO Channel 00/0 : 1[5] -> 2[0] [send] via NET/IB/1
g26n02:851594:851975 [5] NCCL INFO Channel 01/0 : 1[5] -> 2[0] [send] via NET/IB/1
g31n03:688513:688886 [4] NCCL INFO Channel 00/0 : 6[4] -> 7[5] via P2P/IPC
g26n01:811331:811709 [0] NCCL INFO Trees [0] 7/-1/-1->6->0 [1] 7/0/-1->6->-1
g26n01:811331:811709 [0] NCCL INFO P2P Chunksize set to 131072
g26n01:811331:811709 [0] NCCL INFO Channel 00/0 : 5[5] -> 6[0] [receive] via NET/IB/0
g26n01:811331:811709 [0] NCCL INFO Channel 01/0 : 5[5] -> 6[0] [receive] via NET/IB/0
g26n01:811331:811709 [0] NCCL INFO Channel 00/0 : 6[0] -> 7[1] via P2P/IPC
g28n04:841281:841666 [1] NCCL INFO Connected all rings
g28n02:792075:792465 [4] NCCL INFO Connected all rings
g28n02:792075:792465 [4] NCCL INFO Channel 00/0 : 0[4] -> 2[0] [send] via NET/IB/1
g28n02:792075:792465 [4] NCCL INFO Channel 01/0 : 0[4] -> 2[0] [send] via NET/IB/1
g28n02:792075:792465 [4] NCCL INFO Channel 00/0 : 2[0] -> 0[4] [receive] via NET/IB/1
g28n02:792075:792465 [4] NCCL INFO Channel 01/0 : 2[0] -> 0[4] [receive] via NET/IB/1
g27n17:842067:842444 [4] NCCL INFO Channel 00/0 : 6[4] -> 7[5] via P2P/IPC
g25n18:898855:899337 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2
g25n18:898855:899337 [3] NCCL INFO P2P Chunksize set to 131072
g28n03:859961:860333 [5] NCCL INFO Connected all rings
g28n03:859961:860333 [5] NCCL INFO Channel 00/0 : 7[5] -> 6[4] via P2P/IPC
g28n04:841287:841667 [4] NCCL INFO Connected all rings
g27n17:842064:842445 [3] NCCL INFO Channel 00/0 : 5[3] -> 6[4] via P2P/IPC
g26n01:811331:811709 [0] NCCL INFO Channel 01/0 : 6[0] -> 7[1] via P2P/IPC
g28n03:859953:860331 [0] NCCL INFO Connected all rings
g28n03:859953:860331 [0] NCCL INFO Channel 00/0 : 0[4] -> 2[0] [receive] via NET/IB/0
g28n03:859953:860331 [0] NCCL INFO Channel 01/0 : 0[4] -> 2[0] [receive] via NET/IB/0
g28n03:859953:860331 [0] NCCL INFO Channel 00/0 : 2[0] -> 0[4] [send] via NET/IB/0
g28n03:859953:860331 [0] NCCL INFO Channel 01/0 : 2[0] -> 0[4] [send] via NET/IB/0
g27n17:842063:842442 [2] NCCL INFO Channel 00/0 : 4[2] -> 5[3] via P2P/IPC
g25n18:898857:899338 [5] NCCL INFO Trees [0] -1/-1/-1->5->4 [1] -1/-1/-1->5->4
g25n18:898857:899338 [5] NCCL INFO P2P Chunksize set to 131072
g28n03:859956:860332 [2] NCCL INFO Connected all trees
g28n03:859956:860332 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g28n03:859956:860332 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n04:688228:688611 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g27n17:842061:842441 [1] NCCL INFO Channel 00/0 : 3[1] -> 4[2] via P2P/IPC
g28n03:859958:860335 [3] NCCL INFO Connected all trees
g28n03:859958:860335 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g28n03:859958:860335 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n04:688229:688610 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g31n03:688512:688885 [3] NCCL INFO Channel 00/0 : 5[3] -> 6[4] via P2P/IPC
g28n03:859961:860333 [5] NCCL INFO Channel 01/0 : 7[5] -> 6[4] via P2P/IPC
g31n04:688227:688609 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g31n03:688513:688886 [4] NCCL INFO Channel 01/0 : 6[4] -> 7[5] via P2P/IPC
g25n18:898853:899335 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
g25n18:898853:899335 [1] NCCL INFO P2P Chunksize set to 131072
g31n03:688510:688889 [1] NCCL INFO Channel 01/0 : 3[1] -> 4[2] via P2P/IPC
g27n17:842067:842444 [4] NCCL INFO Channel 01/0 : 6[4] -> 7[5] via P2P/IPC
g25n18:898854:899336 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
g25n18:898854:899336 [2] NCCL INFO P2P Chunksize set to 131072
g27n17:842064:842445 [3] NCCL INFO Channel 01/0 : 5[3] -> 6[4] via P2P/IPC
g31n04:688226:688607 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g25n18:898852:899334 [0] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7
g25n18:898852:899334 [0] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7
g25n18:898852:899334 [0] NCCL INFO Trees [0] 1/6/-1->0->-1 [1] 1/-1/-1->0->6
g25n18:898852:899334 [0] NCCL INFO P2P Chunksize set to 131072
g25n18:898852:899334 [0] NCCL INFO Channel 00/0 : 7[1] -> 0[0] [receive] via NET/IB/0
g25n18:898852:899334 [0] NCCL INFO Channel 01/0 : 7[1] -> 0[0] [receive] via NET/IB/0
g25n18:898852:899334 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/IPC
g27n17:842061:842441 [1] NCCL INFO Channel 01/0 : 3[1] -> 4[2] via P2P/IPC
g25n18:898856:899339 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3
g25n18:898856:899339 [4] NCCL INFO P2P Chunksize set to 131072
g27n17:842063:842442 [2] NCCL INFO Channel 01/0 : 4[2] -> 5[3] via P2P/IPC
g31n03:688514:688888 [5] NCCL INFO Channel 00/0 : 7[5] -> 0[4] [send] via NET/IB/1
g31n03:688514:688888 [5] NCCL INFO Channel 01/0 : 7[5] -> 0[4] [send] via NET/IB/1
g31n03:688511:688887 [2] NCCL INFO Channel 00/0 : 4[2] -> 5[3] via P2P/IPC
g28n02:792076:792466 [5] NCCL INFO Connected all trees
g28n02:792076:792466 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g28n02:792076:792466 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n03:688512:688885 [3] NCCL INFO Channel 01/0 : 5[3] -> 6[4] via P2P/IPC
g25n18:898852:899334 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/IPC
g27n17:842068:842443 [5] NCCL INFO Channel 00/0 : 7[5] -> 0[4] [send] via NET/IB/1
g27n17:842068:842443 [5] NCCL INFO Channel 01/0 : 7[5] -> 0[4] [send] via NET/IB/1
g28n04:841281:841666 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/IPC
g28n04:841287:841667 [4] NCCL INFO Channel 00/0 : 4[4] -> 3[3] via P2P/IPC
g27n18:839785:840164 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
g27n18:839785:840164 [2] NCCL INFO P2P Chunksize set to 131072
g26n01:811332:811710 [1] NCCL INFO Channel 00/0 : 7[1] -> 0[0] [send] via NET/IB/0
g26n01:811332:811710 [1] NCCL INFO Channel 01/0 : 7[1] -> 0[0] [send] via NET/IB/0
g25n18:898855:899337 [3] NCCL INFO Channel 00/0 : 3[3] -> 4[4] via P2P/IPC
g28n01:770093:770475 [1] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6
g28n01:770093:770475 [1] NCCL INFO P2P Chunksize set to 131072
g25n18:898853:899335 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/IPC
g27n18:839782:840163 [0] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7
g27n18:839782:840163 [0] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7
g27n18:839782:840163 [0] NCCL INFO Trees [0] 1/6/-1->0->-1 [1] 1/-1/-1->0->6
g27n18:839782:840163 [0] NCCL INFO P2P Chunksize set to 131072
g27n18:839782:840163 [0] NCCL INFO Channel 00/0 : 7[1] -> 0[0] [receive] via NET/IB/0
g27n18:839782:840163 [0] NCCL INFO Channel 01/0 : 7[1] -> 0[0] [receive] via NET/IB/0
g27n18:839782:840163 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/IPC
g31n03:688511:688887 [2] NCCL INFO Channel 01/0 : 4[2] -> 5[3] via P2P/IPC
g25n18:898854:899336 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/IPC
g28n04:841284:841669 [3] NCCL INFO Connected all rings
g25n18:898856:899339 [4] NCCL INFO Channel 00/0 : 4[4] -> 5[5] via P2P/IPC
g27n18:839787:840165 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2
g27n18:839787:840165 [3] NCCL INFO P2P Chunksize set to 131072
g27n18:839789:840166 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3
g27n18:839789:840166 [4] NCCL INFO P2P Chunksize set to 131072
g28n01:770092:770474 [0] NCCL INFO Trees [0] 7/-1/-1->6->0 [1] 7/0/-1->6->-1
g28n01:770092:770474 [0] NCCL INFO P2P Chunksize set to 131072
g28n01:770092:770474 [0] NCCL INFO Channel 00/0 : 5[5] -> 6[0] [receive] via NET/IB/0
g28n01:770092:770474 [0] NCCL INFO Channel 01/0 : 5[5] -> 6[0] [receive] via NET/IB/0
g28n01:770092:770474 [0] NCCL INFO Channel 00/0 : 6[0] -> 7[1] via P2P/IPC
g28n04:841281:841666 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/IPC
g28n04:841287:841667 [4] NCCL INFO Channel 01/0 : 4[4] -> 3[3] via P2P/IPC
g25n18:898855:899337 [3] NCCL INFO Channel 01/0 : 3[3] -> 4[4] via P2P/IPC
g25n18:898853:899335 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/IPC
g28n01:770092:770474 [0] NCCL INFO Channel 01/0 : 6[0] -> 7[1] via P2P/IPC
g25n18:898854:899336 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/IPC
g25n18:898856:899339 [4] NCCL INFO Channel 01/0 : 4[4] -> 5[5] via P2P/IPC
g27n18:839790:840168 [5] NCCL INFO Trees [0] -1/-1/-1->5->4 [1] -1/-1/-1->5->4
g27n18:839790:840168 [5] NCCL INFO P2P Chunksize set to 131072
g27n18:839783:840167 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
g27n18:839783:840167 [1] NCCL INFO P2P Chunksize set to 131072
g28n04:841288:841668 [5] NCCL INFO Connected all rings
g28n04:841288:841668 [5] NCCL INFO Channel 00/0 : 5[5] -> 4[4] via P2P/IPC
g28n04:841283:841665 [2] NCCL INFO Connected all rings
g31n01:617757:618140 [1] NCCL INFO Connected all rings
g31n01:617757:618140 [1] NCCL INFO Channel 00/0 : 7[1] -> 6[0] via P2P/IPC
g28n04:841284:841669 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/IPC
g27n17:842067:842444 [4] NCCL INFO Connected all rings
g31n01:617757:618140 [1] NCCL INFO Channel 01/0 : 7[1] -> 6[0] via P2P/IPC
g28n03:859960:860334 [4] NCCL INFO Connected all trees
g28n03:859960:860334 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g28n03:859960:860334 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g25n18:898857:899338 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[0] [send] via NET/IB/1
g25n18:898857:899338 [5] NCCL INFO Channel 01/0 : 5[5] -> 6[0] [send] via NET/IB/1
g28n04:841288:841668 [5] NCCL INFO Channel 01/0 : 5[5] -> 4[4] via P2P/IPC
g27n18:839782:840163 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/IPC
g28n04:841284:841669 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/IPC
g27n17:842064:842445 [3] NCCL INFO Connected all rings
g28n03:859961:860333 [5] NCCL INFO Connected all trees
g28n03:859961:860333 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g28n03:859961:860333 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n03:859954:860336 [1] NCCL INFO Connected all trees
g28n03:859954:860336 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g28n03:859954:860336 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n04:841283:841665 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/IPC
g28n01:770093:770475 [1] NCCL INFO Channel 00/0 : 7[1] -> 0[0] [send] via NET/IB/0
g28n01:770093:770475 [1] NCCL INFO Channel 01/0 : 7[1] -> 0[0] [send] via NET/IB/0
g31n02:736468:736848 [5] NCCL INFO Connected all rings
g31n02:736468:736848 [5] NCCL INFO Channel 00/0 : 1[5] -> 0[4] via P2P/IPC
g31n01:617756:618139 [0] NCCL INFO Connected all rings
g31n01:617756:618139 [0] NCCL INFO Channel 00/0 : 6[0] -> 0[0] [send] via NET/IB/0
g31n01:617756:618139 [0] NCCL INFO Channel 01/0 : 6[0] -> 0[0] [send] via NET/IB/0
g31n01:617756:618139 [0] NCCL INFO Channel 00/0 : 0[0] -> 6[0] [receive] via NET/IB/0
g31n01:617756:618139 [0] NCCL INFO Channel 01/0 : 0[0] -> 6[0] [receive] via NET/IB/0
g26n02:851594:851975 [5] NCCL INFO Connected all rings
g26n02:851594:851975 [5] NCCL INFO Channel 00/0 : 1[5] -> 0[4] via P2P/IPC
g27n18:839787:840165 [3] NCCL INFO Channel 00/0 : 3[3] -> 4[4] via P2P/IPC
g27n18:839789:840166 [4] NCCL INFO Channel 00/0 : 4[4] -> 5[5] via P2P/IPC
g27n18:839783:840167 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/IPC
g27n18:839785:840164 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/IPC
g31n02:736468:736848 [5] NCCL INFO Channel 01/0 : 1[5] -> 0[4] via P2P/IPC
g26n02:851594:851975 [5] NCCL INFO Channel 01/0 : 1[5] -> 0[4] via P2P/IPC
g31n03:688510:688889 [1] NCCL INFO Connected all rings
g27n18:839787:840165 [3] NCCL INFO Channel 01/0 : 3[3] -> 4[4] via P2P/IPC
g27n17:842061:842441 [1] NCCL INFO Connected all rings
g27n18:839789:840166 [4] NCCL INFO Channel 01/0 : 4[4] -> 5[5] via P2P/IPC
g27n17:842067:842444 [4] NCCL INFO Channel 00/0 : 6[4] -> 5[3] via P2P/IPC
g28n04:841280:841664 [0] NCCL INFO Connected all rings
g28n04:841280:841664 [0] NCCL INFO Channel 00/0 : 6[0] -> 0[0] [receive] via NET/IB/0
g28n04:841280:841664 [0] NCCL INFO Channel 01/0 : 6[0] -> 0[0] [receive] via NET/IB/0
g28n04:841280:841664 [0] NCCL INFO Channel 00/0 : 0[0] -> 6[0] [send] via NET/IB/0
g28n04:841280:841664 [0] NCCL INFO Channel 01/0 : 0[0] -> 6[0] [send] via NET/IB/0
g27n18:839783:840167 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/IPC
g27n18:839785:840164 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/IPC
g28n04:841288:841668 [5] NCCL INFO Connected all trees
g28n04:841288:841668 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g28n04:841288:841668 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n03:688513:688886 [4] NCCL INFO Connected all rings
g31n01:617757:618140 [1] NCCL INFO Connected all trees
g31n01:617757:618140 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g31n01:617757:618140 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g27n18:839790:840168 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[0] [send] via NET/IB/1
g27n18:839790:840168 [5] NCCL INFO Channel 01/0 : 5[5] -> 6[0] [send] via NET/IB/1
g31n03:688512:688885 [3] NCCL INFO Connected all rings
g28n04:841283:841665 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/IPC
g27n17:842064:842445 [3] NCCL INFO Channel 00/0 : 5[3] -> 4[2] via P2P/IPC
g27n17:842067:842444 [4] NCCL INFO Channel 01/0 : 6[4] -> 5[3] via P2P/IPC
g31n03:688511:688887 [2] NCCL INFO Connected all rings
g25n18:898853:899335 [1] NCCL INFO Connected all rings
g25n18:898856:899339 [4] NCCL INFO Connected all rings
g27n17:842064:842445 [3] NCCL INFO Channel 01/0 : 5[3] -> 4[2] via P2P/IPC
g27n17:842061:842441 [1] NCCL INFO Channel 00/0 : 3[1] -> 2[0] via P2P/IPC
g31n03:688510:688889 [1] NCCL INFO Channel 00/0 : 3[1] -> 2[0] via P2P/IPC
g27n17:842063:842442 [2] NCCL INFO Connected all rings
g31n05:704345:704721 [1] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6
g31n05:704345:704721 [1] NCCL INFO P2P Chunksize set to 131072
g27n17:842061:842441 [1] NCCL INFO Channel 01/0 : 3[1] -> 2[0] via P2P/IPC
g31n03:688513:688886 [4] NCCL INFO Channel 00/0 : 6[4] -> 5[3] via P2P/IPC
g31n05:704344:704722 [0] NCCL INFO Trees [0] 7/-1/-1->6->0 [1] 7/0/-1->6->-1
g31n05:704344:704722 [0] NCCL INFO P2P Chunksize set to 131072
g31n05:704344:704722 [0] NCCL INFO Channel 00/0 : 5[5] -> 6[0] [receive] via NET/IB/0
g31n05:704344:704722 [0] NCCL INFO Channel 01/0 : 5[5] -> 6[0] [receive] via NET/IB/0
g31n05:704344:704722 [0] NCCL INFO Channel 00/0 : 6[0] -> 7[1] via P2P/IPC
g25n18:898857:899338 [5] NCCL INFO Connected all rings
g25n18:898857:899338 [5] NCCL INFO Channel 00/0 : 5[5] -> 4[4] via P2P/IPC
g31n03:688510:688889 [1] NCCL INFO Channel 01/0 : 3[1] -> 2[0] via P2P/IPC
g31n03:688511:688887 [2] NCCL INFO Channel 00/0 : 4[2] -> 3[1] via P2P/IPC
g27n17:842063:842442 [2] NCCL INFO Channel 00/0 : 4[2] -> 3[1] via P2P/IPC
g25n18:898853:899335 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/IPC
g31n03:688513:688886 [4] NCCL INFO Channel 01/0 : 6[4] -> 5[3] via P2P/IPC
g31n05:704344:704722 [0] NCCL INFO Channel 01/0 : 6[0] -> 7[1] via P2P/IPC
g28n03:859953:860331 [0] NCCL INFO Connected all trees
g28n03:859953:860331 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g28n03:859953:860331 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g25n18:898854:899336 [2] NCCL INFO Connected all rings
g31n03:688512:688885 [3] NCCL INFO Channel 00/0 : 5[3] -> 4[2] via P2P/IPC
g31n04:688224:688606 [0] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7
g31n04:688224:688606 [0] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7
g31n04:688224:688606 [0] NCCL INFO Trees [0] 1/6/-1->0->-1 [1] 1/-1/-1->0->6
g31n04:688224:688606 [0] NCCL INFO P2P Chunksize set to 131072
g31n04:688224:688606 [0] NCCL INFO Channel 00/0 : 7[1] -> 0[0] [receive] via NET/IB/0
g31n04:688224:688606 [0] NCCL INFO Channel 01/0 : 7[1] -> 0[0] [receive] via NET/IB/0
g31n04:688224:688606 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/IPC
g31n03:688511:688887 [2] NCCL INFO Channel 01/0 : 4[2] -> 3[1] via P2P/IPC
g31n04:688225:688608 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
g31n04:688225:688608 [1] NCCL INFO P2P Chunksize set to 131072
g31n03:688514:688888 [5] NCCL INFO Connected all rings
g31n03:688514:688888 [5] NCCL INFO Channel 00/0 : 7[5] -> 6[4] via P2P/IPC
g27n17:842068:842443 [5] NCCL INFO Connected all rings
g27n17:842068:842443 [5] NCCL INFO Channel 00/0 : 7[5] -> 6[4] via P2P/IPC
g27n18:839787:840165 [3] NCCL INFO Connected all rings
g28n02:792075:792465 [4] NCCL INFO Connected all trees
g28n02:792075:792465 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g28n02:792075:792465 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g27n17:842063:842442 [2] NCCL INFO Channel 01/0 : 4[2] -> 3[1] via P2P/IPC
g27n17:842060:842446 [0] NCCL INFO Connected all rings
g27n17:842060:842446 [0] NCCL INFO Channel 00/0 : 0[4] -> 2[0] [receive] via NET/IB/0
g27n17:842060:842446 [0] NCCL INFO Channel 01/0 : 0[4] -> 2[0] [receive] via NET/IB/0
g27n17:842060:842446 [0] NCCL INFO Channel 00/0 : 2[0] -> 0[4] [send] via NET/IB/0
g27n17:842060:842446 [0] NCCL INFO Channel 01/0 : 2[0] -> 0[4] [send] via NET/IB/0
g27n18:839789:840166 [4] NCCL INFO Connected all rings
g31n04:688229:688610 [5] NCCL INFO Trees [0] -1/-1/-1->5->4 [1] -1/-1/-1->5->4
g31n04:688229:688610 [5] NCCL INFO P2P Chunksize set to 131072
g31n03:688512:688885 [3] NCCL INFO Channel 01/0 : 5[3] -> 4[2] via P2P/IPC
g26n02:851593:851970 [4] NCCL INFO Connected all rings
g26n02:851593:851970 [4] NCCL INFO Channel 00/0 : 0[4] -> 2[0] [send] via NET/IB/1
g26n02:851593:851970 [4] NCCL INFO Channel 01/0 : 0[4] -> 2[0] [send] via NET/IB/1
g26n02:851593:851970 [4] NCCL INFO Channel 00/0 : 2[0] -> 0[4] [receive] via NET/IB/1
g26n02:851593:851970 [4] NCCL INFO Channel 01/0 : 2[0] -> 0[4] [receive] via NET/IB/1
g31n04:688226:688607 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
g31n04:688226:688607 [2] NCCL INFO P2P Chunksize set to 131072
g31n03:688509:688884 [0] NCCL INFO Connected all rings
g31n03:688509:688884 [0] NCCL INFO Channel 00/0 : 0[4] -> 2[0] [receive] via NET/IB/0
g31n03:688509:688884 [0] NCCL INFO Channel 01/0 : 0[4] -> 2[0] [receive] via NET/IB/0
g31n03:688509:688884 [0] NCCL INFO Channel 00/0 : 2[0] -> 0[4] [send] via NET/IB/0
g31n03:688509:688884 [0] NCCL INFO Channel 01/0 : 2[0] -> 0[4] [send] via NET/IB/0
g31n02:736467:736844 [4] NCCL INFO Connected all rings
g31n02:736467:736844 [4] NCCL INFO Channel 00/0 : 0[4] -> 2[0] [send] via NET/IB/1
g31n02:736467:736844 [4] NCCL INFO Channel 01/0 : 0[4] -> 2[0] [send] via NET/IB/1
g31n02:736467:736844 [4] NCCL INFO Channel 00/0 : 2[0] -> 0[4] [receive] via NET/IB/1
g31n02:736467:736844 [4] NCCL INFO Channel 01/0 : 2[0] -> 0[4] [receive] via NET/IB/1
g31n03:688514:688888 [5] NCCL INFO Channel 01/0 : 7[5] -> 6[4] via P2P/IPC
g31n04:688228:688611 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3
g31n04:688228:688611 [4] NCCL INFO P2P Chunksize set to 131072
g27n17:842068:842443 [5] NCCL INFO Channel 01/0 : 7[5] -> 6[4] via P2P/IPC
g28n04:841283:841665 [2] NCCL INFO Connected all trees
g28n04:841283:841665 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g28n04:841283:841665 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n04:688227:688609 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2
g31n04:688227:688609 [3] NCCL INFO P2P Chunksize set to 131072
g25n18:898857:899338 [5] NCCL INFO Channel 01/0 : 5[5] -> 4[4] via P2P/IPC
g25n18:898853:899335 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/IPC
g26n01:811332:811710 [1] NCCL INFO Connected all rings
g26n01:811332:811710 [1] NCCL INFO Channel 00/0 : 7[1] -> 6[0] via P2P/IPC
g25n18:898855:899337 [3] NCCL INFO Connected all rings
g28n04:841281:841666 [1] NCCL INFO Connected all trees
g28n04:841281:841666 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g28n04:841281:841666 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g26n01:811332:811710 [1] NCCL INFO Channel 01/0 : 7[1] -> 6[0] via P2P/IPC
g28n04:841287:841667 [4] NCCL INFO Connected all trees
g28n04:841287:841667 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g28n04:841287:841667 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n04:688224:688606 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/IPC
g25n18:898854:899336 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/IPC
g27n18:839785:840164 [2] NCCL INFO Connected all rings
g25n18:898856:899339 [4] NCCL INFO Channel 00/0 : 4[4] -> 3[3] via P2P/IPC
g31n04:688225:688608 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/IPC
g26n01:811331:811709 [0] NCCL INFO Connected all rings
g26n01:811331:811709 [0] NCCL INFO Channel 00/0 : 6[0] -> 0[0] [send] via NET/IB/0
g26n01:811331:811709 [0] NCCL INFO Channel 01/0 : 6[0] -> 0[0] [send] via NET/IB/0
g26n01:811331:811709 [0] NCCL INFO Channel 00/0 : 0[0] -> 6[0] [receive] via NET/IB/0
g26n01:811331:811709 [0] NCCL INFO Channel 01/0 : 0[0] -> 6[0] [receive] via NET/IB/0
g31n05:704345:704721 [1] NCCL INFO Channel 00/0 : 7[1] -> 0[0] [send] via NET/IB/0
g31n05:704345:704721 [1] NCCL INFO Channel 01/0 : 7[1] -> 0[0] [send] via NET/IB/0
g28n04:841284:841669 [3] NCCL INFO Connected all trees
g28n04:841284:841669 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g28n04:841284:841669 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g26n02:851594:851975 [5] NCCL INFO Connected all trees
g26n02:851594:851975 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g26n02:851594:851975 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g27n18:839789:840166 [4] NCCL INFO Channel 00/0 : 4[4] -> 3[3] via P2P/IPC
g25n18:898854:899336 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/IPC
g25n18:898855:899337 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/IPC
g31n02:736468:736848 [5] NCCL INFO Connected all trees
g31n02:736468:736848 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g31n02:736468:736848 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g25n18:898856:899339 [4] NCCL INFO Channel 01/0 : 4[4] -> 3[3] via P2P/IPC
g25n18:898852:899334 [0] NCCL INFO Connected all rings
g25n18:898852:899334 [0] NCCL INFO Channel 00/0 : 6[0] -> 0[0] [receive] via NET/IB/0
g25n18:898852:899334 [0] NCCL INFO Channel 01/0 : 6[0] -> 0[0] [receive] via NET/IB/0
g25n18:898852:899334 [0] NCCL INFO Channel 00/0 : 0[0] -> 6[0] [send] via NET/IB/0
g25n18:898852:899334 [0] NCCL INFO Channel 01/0 : 0[0] -> 6[0] [send] via NET/IB/0
g31n04:688226:688607 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/IPC
g31n04:688228:688611 [4] NCCL INFO Channel 00/0 : 4[4] -> 5[5] via P2P/IPC
g31n04:688225:688608 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/IPC
g31n04:688227:688609 [3] NCCL INFO Channel 00/0 : 3[3] -> 4[4] via P2P/IPC
g27n18:839789:840166 [4] NCCL INFO Channel 01/0 : 4[4] -> 3[3] via P2P/IPC
g25n18:898855:899337 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/IPC
g31n03:688514:688888 [5] NCCL INFO Connected all trees
g31n03:688514:688888 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g31n03:688514:688888 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g32n02:753688:754067 [5] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
g32n02:753688:754067 [5] NCCL INFO P2P Chunksize set to 131072
g31n04:688226:688607 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/IPC
g27n17:842068:842443 [5] NCCL INFO Connected all trees
g27n17:842068:842443 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g27n17:842068:842443 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g27n18:839785:840164 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/IPC
g31n04:688228:688611 [4] NCCL INFO Channel 01/0 : 4[4] -> 5[5] via P2P/IPC
g31n04:688227:688609 [3] NCCL INFO Channel 01/0 : 3[3] -> 4[4] via P2P/IPC
g28n02:792076:792466 [5] NCCL INFO comm 0x1bb65d370 rank 1 nranks 8 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x5feb56817484c8aa - Init COMPLETE
g28n02:792075:792465 [4] NCCL INFO comm 0x1a62ac930 rank 0 nranks 8 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x5feb56817484c8aa - Init COMPLETE
g28n02:792075:792487 [4] NCCL INFO Channel 00/1 : 7[5] -> 0[4] [receive] via NET/IB/1/Shared
g28n02:792075:792487 [4] NCCL INFO Channel 01/1 : 7[5] -> 0[4] [receive] via NET/IB/1/Shared
g28n02:792075:792487 [4] NCCL INFO Channel 00/1 : 0[4] -> 1[5] via P2P/IPC
g27n18:839783:840167 [1] NCCL INFO Connected all rings
g32n02:753687:754066 [4] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7
g32n02:753687:754066 [4] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7
g32n02:753687:754066 [4] NCCL INFO Trees [0] 1/2/-1->0->-1 [1] 1/-1/-1->0->2
g32n02:753687:754066 [4] NCCL INFO P2P Chunksize set to 131072
g32n02:753687:754066 [4] NCCL INFO Channel 00/0 : 7[5] -> 0[4] [receive] via NET/IB/1
g32n02:753687:754066 [4] NCCL INFO Channel 01/0 : 7[5] -> 0[4] [receive] via NET/IB/1
g32n02:753687:754066 [4] NCCL INFO Channel 00/0 : 0[4] -> 1[5] via P2P/IPC
g27n18:839787:840165 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/IPC
g26n01:811332:811710 [1] NCCL INFO Connected all trees
g26n01:811332:811710 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g26n01:811332:811710 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n02:792075:792487 [4] NCCL INFO Channel 01/1 : 0[4] -> 1[5] via P2P/IPC
g32n02:753687:754066 [4] NCCL INFO Channel 01/0 : 0[4] -> 1[5] via P2P/IPC
g32n03:753101:753470 [5] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6
g32n03:753101:753470 [5] NCCL INFO P2P Chunksize set to 131072
g27n18:839785:840164 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/IPC
g32n03:753100:753471 [4] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5
g32n03:753100:753471 [4] NCCL INFO P2P Chunksize set to 131072
g32n03:753099:753472 [3] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4
g32n03:753099:753472 [3] NCCL INFO P2P Chunksize set to 131072
g32n03:753097:753475 [1] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2
g32n03:753097:753475 [1] NCCL INFO P2P Chunksize set to 131072
g28n03:859954:860336 [1] NCCL INFO comm 0x155ed7bc0 rank 3 nranks 8 cudaDev 1 nvmlDev 1 busId 405000 commId 0x5feb56817484c8aa - Init COMPLETE
g32n03:753098:753473 [2] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3
g32n03:753098:753473 [2] NCCL INFO P2P Chunksize set to 131072
g28n03:859956:860332 [2] NCCL INFO comm 0x187c22830 rank 4 nranks 8 cudaDev 2 nvmlDev 2 busId 406000 commId 0x5feb56817484c8aa - Init COMPLETE
g32n03:753096:753474 [0] NCCL INFO Trees [0] 3/-1/-1->2->0 [1] 3/0/-1->2->-1
g32n03:753096:753474 [0] NCCL INFO P2P Chunksize set to 131072
g32n03:753096:753474 [0] NCCL INFO Channel 00/0 : 1[5] -> 2[0] [receive] via NET/IB/0
g32n03:753096:753474 [0] NCCL INFO Channel 01/0 : 1[5] -> 2[0] [receive] via NET/IB/0
g32n03:753096:753474 [0] NCCL INFO Channel 00/0 : 2[0] -> 3[1] via P2P/IPC
g31n04:688229:688610 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[0] [send] via NET/IB/1
g31n04:688229:688610 [5] NCCL INFO Channel 01/0 : 5[5] -> 6[0] [send] via NET/IB/1
g28n03:859960:860334 [4] NCCL INFO comm 0x1da5b81b0 rank 6 nranks 8 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x5feb56817484c8aa - Init COMPLETE
g27n18:839787:840165 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/IPC
g28n03:859961:860333 [5] NCCL INFO comm 0x1ad87dee0 rank 7 nranks 8 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x5feb56817484c8aa - Init COMPLETE
g31n03:688512:688885 [3] NCCL INFO Connected all trees
g31n03:688512:688885 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g31n03:688512:688885 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n03:859953:860331 [0] NCCL INFO comm 0x1be814ae0 rank 2 nranks 8 cudaDev 0 nvmlDev 0 busId 404000 commId 0x5feb56817484c8aa - Init COMPLETE
g28n03:859953:860354 [0] NCCL INFO Channel 00/1 : 1[5] -> 2[0] [receive] via NET/IB/0/Shared
g28n03:859953:860354 [0] NCCL INFO Channel 01/1 : 1[5] -> 2[0] [receive] via NET/IB/2/Shared
g28n03:859953:860354 [0] NCCL INFO Channel 00/1 : 2[0] -> 3[1] via P2P/IPC
g27n18:839790:840168 [5] NCCL INFO Connected all rings
g27n18:839790:840168 [5] NCCL INFO Channel 00/0 : 5[5] -> 4[4] via P2P/IPC
g28n03:859958:860335 [3] NCCL INFO comm 0x1bbed8b60 rank 5 nranks 8 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x5feb56817484c8aa - Init COMPLETE
g27n18:839783:840167 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/IPC
g25n18:898857:899338 [5] NCCL INFO Connected all trees
g25n18:898857:899338 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g25n18:898857:899338 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g27n18:839790:840168 [5] NCCL INFO Channel 01/0 : 5[5] -> 4[4] via P2P/IPC
g27n18:839783:840167 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/IPC
g32n03:753096:753474 [0] NCCL INFO Channel 01/0 : 2[0] -> 3[1] via P2P/IPC
g28n03:859953:860354 [0] NCCL INFO Channel 01/1 : 2[0] -> 3[1] via P2P/IPC
g27n17:842061:842441 [1] NCCL INFO Connected all trees
g27n17:842061:842441 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g27n17:842061:842441 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n03:688513:688886 [4] NCCL INFO Connected all trees
g31n03:688513:688886 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g31n03:688513:688886 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g32n02:753688:754067 [5] NCCL INFO Channel 00/0 : 1[5] -> 2[0] [send] via NET/IB/1
g32n02:753688:754067 [5] NCCL INFO Channel 01/0 : 1[5] -> 2[0] [send] via NET/IB/1
g32n03:753100:753471 [4] NCCL INFO Channel 00/0 : 6[4] -> 7[5] via P2P/IPC
g32n03:753097:753475 [1] NCCL INFO Channel 00/0 : 3[1] -> 4[2] via P2P/IPC
g32n03:753098:753473 [2] NCCL INFO Channel 00/0 : 4[2] -> 5[3] via P2P/IPC
g28n03:859954:860352 [1] NCCL INFO Channel 00/1 : 3[1] -> 4[2] via P2P/IPC
g32n03:753099:753472 [3] NCCL INFO Channel 00/0 : 5[3] -> 6[4] via P2P/IPC
g27n17:842067:842444 [4] NCCL INFO Connected all trees
g27n17:842067:842444 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g27n17:842067:842444 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n03:859960:860349 [4] NCCL INFO Channel 00/1 : 6[4] -> 7[5] via P2P/IPC
g27n17:842064:842445 [3] NCCL INFO Connected all trees
g27n17:842064:842445 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g27n17:842064:842445 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n03:859956:860353 [2] NCCL INFO Channel 00/1 : 4[2] -> 5[3] via P2P/IPC
g31n03:688510:688889 [1] NCCL INFO Connected all trees
g31n03:688510:688889 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g31n03:688510:688889 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n03:859958:860351 [3] NCCL INFO Channel 00/1 : 5[3] -> 6[4] via P2P/IPC
g27n17:842063:842442 [2] NCCL INFO Connected all trees
g27n17:842063:842442 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g27n17:842063:842442 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g32n03:753100:753471 [4] NCCL INFO Channel 01/0 : 6[4] -> 7[5] via P2P/IPC
g32n03:753097:753475 [1] NCCL INFO Channel 01/0 : 3[1] -> 4[2] via P2P/IPC
g32n03:753098:753473 [2] NCCL INFO Channel 01/0 : 4[2] -> 5[3] via P2P/IPC
g28n03:859956:860353 [2] NCCL INFO Channel 01/1 : 4[2] -> 5[3] via P2P/IPC
g32n03:753099:753472 [3] NCCL INFO Channel 01/0 : 5[3] -> 6[4] via P2P/IPC
g28n03:859960:860349 [4] NCCL INFO Channel 01/1 : 6[4] -> 7[5] via P2P/IPC
g31n03:688511:688887 [2] NCCL INFO Connected all trees
g31n03:688511:688887 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g31n03:688511:688887 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n03:859954:860352 [1] NCCL INFO Channel 01/1 : 3[1] -> 4[2] via P2P/IPC
g28n03:859958:860351 [3] NCCL INFO Channel 01/1 : 5[3] -> 6[4] via P2P/IPC
g28n04:841280:841664 [0] NCCL INFO Connected all trees
g28n04:841280:841664 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g28n04:841280:841664 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n04:688228:688611 [4] NCCL INFO Connected all rings
g28n01:770093:770475 [1] NCCL INFO Connected all rings
g28n01:770093:770475 [1] NCCL INFO Channel 00/0 : 7[1] -> 6[0] via P2P/IPC
g31n04:688225:688608 [1] NCCL INFO Connected all rings
g31n04:688227:688609 [3] NCCL INFO Connected all rings
g25n18:898854:899336 [2] NCCL INFO Connected all trees
g25n18:898854:899336 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g25n18:898854:899336 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n05:704345:704721 [1] NCCL INFO Connected all rings
g31n05:704345:704721 [1] NCCL INFO Channel 00/0 : 7[1] -> 6[0] via P2P/IPC
g32n03:753101:753470 [5] NCCL INFO Channel 00/0 : 7[5] -> 0[4] [send] via NET/IB/1
g32n03:753101:753470 [5] NCCL INFO Channel 01/0 : 7[5] -> 0[4] [send] via NET/IB/1
g28n01:770093:770475 [1] NCCL INFO Channel 01/0 : 7[1] -> 6[0] via P2P/IPC
g31n01:617756:618139 [0] NCCL INFO Connected all trees
g31n01:617756:618139 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g31n01:617756:618139 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g27n18:839787:840165 [3] NCCL INFO Connected all trees
g27n18:839787:840165 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g27n18:839787:840165 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n05:704345:704721 [1] NCCL INFO Channel 01/0 : 7[1] -> 6[0] via P2P/IPC
g25n18:898855:899337 [3] NCCL INFO Connected all trees
g25n18:898855:899337 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g25n18:898855:899337 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n03:859961:860350 [5] NCCL INFO Channel 00/1 : 7[5] -> 0[4] [send] via NET/IB/1/Shared
g28n03:859961:860350 [5] NCCL INFO Channel 01/1 : 7[5] -> 0[4] [send] via NET/IB/3/Shared
g28n03:859953:860354 [0] NCCL INFO Channel 00/1 : 0[4] -> 2[0] [receive] via NET/IB/0/Shared
g28n03:859953:860354 [0] NCCL INFO Channel 01/1 : 0[4] -> 2[0] [receive] via NET/IB/2/Shared
g28n03:859953:860354 [0] NCCL INFO Channel 00/1 : 2[0] -> 4[2] via P2P/IPC
g28n03:859954:860352 [1] NCCL INFO Channel 00/1 : 1[5] -> 3[1] [receive] via NET/IB/2/Shared
g28n03:859954:860352 [1] NCCL INFO Channel 01/1 : 1[5] -> 3[1] [receive] via NET/IB/0/Shared
g28n03:859954:860352 [1] NCCL INFO Channel 00/1 : 3[1] -> 5[3] via P2P/IPC
g28n01:770092:770474 [0] NCCL INFO Connected all rings
g28n01:770092:770474 [0] NCCL INFO Channel 00/0 : 6[0] -> 0[0] [send] via NET/IB/0
g28n01:770092:770474 [0] NCCL INFO Channel 01/0 : 6[0] -> 0[0] [send] via NET/IB/0
g28n01:770092:770474 [0] NCCL INFO Channel 00/0 : 0[0] -> 6[0] [receive] via NET/IB/0
g28n01:770092:770474 [0] NCCL INFO Channel 01/0 : 0[0] -> 6[0] [receive] via NET/IB/0
g25n18:898856:899339 [4] NCCL INFO Connected all trees
g25n18:898856:899339 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g25n18:898856:899339 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g27n18:839790:840168 [5] NCCL INFO Connected all trees
g27n18:839790:840168 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g27n18:839790:840168 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g25n18:898853:899335 [1] NCCL INFO Connected all trees
g25n18:898853:899335 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g25n18:898853:899335 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g27n18:839782:840163 [0] NCCL INFO Connected all rings
g27n18:839782:840163 [0] NCCL INFO Channel 00/0 : 6[0] -> 0[0] [receive] via NET/IB/0
g27n18:839782:840163 [0] NCCL INFO Channel 01/0 : 6[0] -> 0[0] [receive] via NET/IB/0
g27n18:839782:840163 [0] NCCL INFO Channel 00/0 : 0[0] -> 6[0] [send] via NET/IB/0
g27n18:839782:840163 [0] NCCL INFO Channel 01/0 : 0[0] -> 6[0] [send] via NET/IB/0
g27n18:839789:840166 [4] NCCL INFO Connected all trees
g27n18:839789:840166 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g27n18:839789:840166 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g27n18:839785:840164 [2] NCCL INFO Connected all trees
g27n18:839785:840164 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g27n18:839785:840164 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n03:859953:860354 [0] NCCL INFO Channel 01/1 : 2[0] -> 4[2] via P2P/IPC
g28n03:859954:860352 [1] NCCL INFO Channel 01/1 : 3[1] -> 5[3] via P2P/IPC
g28n03:859956:860353 [2] NCCL INFO Channel 00/1 : 4[2] -> 6[4] via P2P/IPC
g31n04:688228:688611 [4] NCCL INFO Channel 00/0 : 4[4] -> 3[3] via P2P/IPC
g28n03:859958:860351 [3] NCCL INFO Channel 00/1 : 5[3] -> 7[5] via P2P/IPC
g31n04:688225:688608 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/IPC
g31n04:688227:688609 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/IPC
g31n04:688226:688607 [2] NCCL INFO Connected all rings
g28n01:770093:770475 [1] NCCL INFO Connected all trees
g28n01:770093:770475 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g28n01:770093:770475 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n03:859956:860353 [2] NCCL INFO Channel 01/1 : 4[2] -> 6[4] via P2P/IPC
g28n03:859958:860351 [3] NCCL INFO Channel 01/1 : 5[3] -> 7[5] via P2P/IPC
g32n03:753100:753471 [4] NCCL INFO Connected all rings
g32n03:753098:753473 [2] NCCL INFO Connected all rings
g31n04:688228:688611 [4] NCCL INFO Channel 01/0 : 4[4] -> 3[3] via P2P/IPC
g31n04:688225:688608 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/IPC
g31n04:688227:688609 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/IPC
g31n04:688226:688607 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/IPC
g27n17:842060:842446 [0] NCCL INFO Connected all trees
g27n17:842060:842446 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g27n17:842060:842446 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n01:617756:618139 [0] NCCL INFO comm 0x19e8800d0 rank 6 nranks 8 cudaDev 0 nvmlDev 0 busId 404000 commId 0xfaf114ba6bb27b13 - Init COMPLETE
g31n01:617756:618160 [0] NCCL INFO Channel 00/1 : 5[5] -> 6[0] [receive] via NET/IB/0/Shared
g31n01:617756:618160 [0] NCCL INFO Channel 01/1 : 5[5] -> 6[0] [receive] via NET/IB/0/Shared
g31n01:617756:618160 [0] NCCL INFO Channel 00/1 : 6[0] -> 7[1] via P2P/IPC
g28n03:859960:860349 [4] NCCL INFO Channel 00/1 : 6[4] -> 0[4] [send] via NET/IB/3/Shared
g28n03:859960:860349 [4] NCCL INFO Channel 01/1 : 6[4] -> 0[4] [send] via NET/IB/1/Shared
g31n01:617757:618140 [1] NCCL INFO comm 0x1afbccf60 rank 7 nranks 8 cudaDev 1 nvmlDev 1 busId 405000 commId 0xfaf114ba6bb27b13 - Init COMPLETE
g28n03:859961:860350 [5] NCCL INFO Channel 00/1 : 7[5] -> 1[5] [send] via NET/IB/1/Shared
g28n03:859961:860350 [5] NCCL INFO Channel 01/1 : 7[5] -> 1[5] [send] via NET/IB/3/Shared
g28n03:859954:860352 [1] NCCL INFO Channel 00/1 : 0[4] -> 3[1] [receive] via NET/IB/2/Shared
g28n03:859954:860352 [1] NCCL INFO Channel 01/1 : 0[4] -> 3[1] [receive] via NET/IB/0/Shared
g28n03:859954:860352 [1] NCCL INFO Channel 00/1 : 3[1] -> 6[4] via P2P/IPC
g28n03:859956:860353 [2] NCCL INFO Channel 00/1 : 1[5] -> 4[2] [receive] via NET/IB/0/Shared
g28n03:859956:860353 [2] NCCL INFO Channel 01/1 : 1[5] -> 4[2] [receive] via NET/IB/2/Shared
g28n03:859956:860353 [2] NCCL INFO Channel 00/1 : 4[2] -> 7[5] via P2P/IPC
g31n05:704344:704722 [0] NCCL INFO Connected all rings
g31n05:704344:704722 [0] NCCL INFO Channel 00/0 : 6[0] -> 0[0] [send] via NET/IB/0
g31n05:704344:704722 [0] NCCL INFO Channel 01/0 : 6[0] -> 0[0] [send] via NET/IB/0
g31n05:704344:704722 [0] NCCL INFO Channel 00/0 : 0[0] -> 6[0] [receive] via NET/IB/0
g31n05:704344:704722 [0] NCCL INFO Channel 01/0 : 0[0] -> 6[0] [receive] via NET/IB/0
g31n04:688229:688610 [5] NCCL INFO Connected all rings
g31n04:688229:688610 [5] NCCL INFO Channel 00/0 : 5[5] -> 4[4] via P2P/IPC
g31n01:617756:618160 [0] NCCL INFO Channel 01/1 : 6[0] -> 7[1] via P2P/IPC
g31n04:688226:688607 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/IPC
g31n04:688224:688606 [0] NCCL INFO Connected all rings
g31n04:688224:688606 [0] NCCL INFO Channel 00/0 : 6[0] -> 0[0] [receive] via NET/IB/0
g31n04:688224:688606 [0] NCCL INFO Channel 01/0 : 6[0] -> 0[0] [receive] via NET/IB/0
g31n04:688224:688606 [0] NCCL INFO Channel 00/0 : 0[0] -> 6[0] [send] via NET/IB/0
g31n04:688224:688606 [0] NCCL INFO Channel 01/0 : 0[0] -> 6[0] [send] via NET/IB/0
g28n04:841283:841665 [2] NCCL INFO comm 0x1a6eb0e70 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 406000 commId 0xfaf114ba6bb27b13 - Init COMPLETE
g28n04:841281:841666 [1] NCCL INFO comm 0x168f5e5a0 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 405000 commId 0xfaf114ba6bb27b13 - Init COMPLETE
g32n03:753100:753471 [4] NCCL INFO Channel 00/0 : 6[4] -> 5[3] via P2P/IPC
g28n04:841288:841668 [5] NCCL INFO comm 0x1e20f2950 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xfaf114ba6bb27b13 - Init COMPLETE
g32n03:753098:753473 [2] NCCL INFO Channel 00/0 : 4[2] -> 3[1] via P2P/IPC
g28n04:841284:841669 [3] NCCL INFO comm 0x172053c50 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xfaf114ba6bb27b13 - Init COMPLETE
g32n03:753099:753472 [3] NCCL INFO Connected all rings
g28n04:841287:841667 [4] NCCL INFO comm 0x1c2760730 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xfaf114ba6bb27b13 - Init COMPLETE
g31n03:688509:688884 [0] NCCL INFO Connected all trees
g31n03:688509:688884 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g31n03:688509:688884 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n04:841280:841664 [0] NCCL INFO comm 0x17ed95820 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 404000 commId 0xfaf114ba6bb27b13 - Init COMPLETE
g28n04:841280:841687 [0] NCCL INFO Channel 00/1 : 7[1] -> 0[0] [receive] via NET/IB/0/Shared
g28n04:841280:841687 [0] NCCL INFO Channel 01/1 : 7[1] -> 0[0] [receive] via NET/IB/2/Shared
g28n04:841280:841687 [0] NCCL INFO Channel 00/1 : 0[0] -> 1[1] via P2P/IPC
g26n02:851593:851970 [4] NCCL INFO Connected all trees
g26n02:851593:851970 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g26n02:851593:851970 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n04:688229:688610 [5] NCCL INFO Channel 01/0 : 5[5] -> 4[4] via P2P/IPC
g28n03:859954:860352 [1] NCCL INFO Channel 01/1 : 3[1] -> 6[4] via P2P/IPC
g28n03:859956:860353 [2] NCCL INFO Channel 01/1 : 4[2] -> 7[5] via P2P/IPC
g25n18:898852:899334 [0] NCCL INFO Connected all trees
g25n18:898852:899334 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g25n18:898852:899334 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g27n18:839783:840167 [1] NCCL INFO Connected all trees
g27n18:839783:840167 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g27n18:839783:840167 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n03:859953:860354 [0] NCCL INFO Channel 00/1 : 2[0] -> 5[3] via P2P/IPC
g32n03:753100:753471 [4] NCCL INFO Channel 01/0 : 6[4] -> 5[3] via P2P/IPC
g32n02:753688:754067 [5] NCCL INFO Connected all rings
g32n02:753688:754067 [5] NCCL INFO Channel 00/0 : 1[5] -> 0[4] via P2P/IPC
g28n03:859961:860350 [5] NCCL INFO Channel 00/1 : 7[5] -> 2[0] via P2P/IPC
g31n05:704345:704721 [1] NCCL INFO Connected all trees
g31n05:704345:704721 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g31n05:704345:704721 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n04:841280:841687 [0] NCCL INFO Channel 01/1 : 0[0] -> 1[1] via P2P/IPC
g28n03:859960:860349 [4] NCCL INFO Channel 00/1 : 6[4] -> 1[5] [send] via NET/IB/3/Shared
g28n03:859960:860349 [4] NCCL INFO Channel 01/1 : 6[4] -> 1[5] [send] via NET/IB/1/Shared
g32n03:753098:753473 [2] NCCL INFO Channel 01/0 : 4[2] -> 3[1] via P2P/IPC
g31n02:736467:736844 [4] NCCL INFO Connected all trees
g31n02:736467:736844 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g31n02:736467:736844 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g26n01:811331:811709 [0] NCCL INFO Connected all trees
g26n01:811331:811709 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g26n01:811331:811709 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g32n02:753688:754067 [5] NCCL INFO Channel 01/0 : 1[5] -> 0[4] via P2P/IPC
g28n03:859953:860354 [0] NCCL INFO Channel 01/1 : 2[0] -> 5[3] via P2P/IPC
g28n03:859961:860350 [5] NCCL INFO Channel 01/1 : 7[5] -> 2[0] via P2P/IPC
g28n04:841283:841686 [2] NCCL INFO Channel 00/1 : 2[2] -> 3[3] via P2P/IPC
g28n04:841281:841685 [1] NCCL INFO Channel 00/1 : 1[1] -> 2[2] via P2P/IPC
g28n04:841284:841684 [3] NCCL INFO Channel 00/1 : 3[3] -> 4[4] via P2P/IPC
g28n04:841287:841682 [4] NCCL INFO Channel 00/1 : 4[4] -> 5[5] via P2P/IPC
g32n02:753687:754066 [4] NCCL INFO Connected all rings
g32n02:753687:754066 [4] NCCL INFO Channel 00/0 : 0[4] -> 2[0] [send] via NET/IB/1
g32n02:753687:754066 [4] NCCL INFO Channel 01/0 : 0[4] -> 2[0] [send] via NET/IB/1
g32n02:753687:754066 [4] NCCL INFO Channel 00/0 : 2[0] -> 0[4] [receive] via NET/IB/1
g32n02:753687:754066 [4] NCCL INFO Channel 01/0 : 2[0] -> 0[4] [receive] via NET/IB/1
g32n03:753099:753472 [3] NCCL INFO Channel 00/0 : 5[3] -> 4[2] via P2P/IPC
g32n03:753101:753470 [5] NCCL INFO Connected all rings
g32n03:753101:753470 [5] NCCL INFO Channel 00/0 : 7[5] -> 6[4] via P2P/IPC
g28n04:841283:841686 [2] NCCL INFO Channel 01/1 : 2[2] -> 3[3] via P2P/IPC
g32n03:753097:753475 [1] NCCL INFO Connected all rings
g28n04:841281:841685 [1] NCCL INFO Channel 01/1 : 1[1] -> 2[2] via P2P/IPC
g32n03:753096:753474 [0] NCCL INFO Connected all rings
g32n03:753096:753474 [0] NCCL INFO Channel 00/0 : 0[4] -> 2[0] [receive] via NET/IB/0
g32n03:753096:753474 [0] NCCL INFO Channel 01/0 : 0[4] -> 2[0] [receive] via NET/IB/0
g32n03:753096:753474 [0] NCCL INFO Channel 00/0 : 2[0] -> 0[4] [send] via NET/IB/0
g32n03:753096:753474 [0] NCCL INFO Channel 01/0 : 2[0] -> 0[4] [send] via NET/IB/0
g28n04:841284:841684 [3] NCCL INFO Channel 01/1 : 3[3] -> 4[4] via P2P/IPC
g28n04:841287:841682 [4] NCCL INFO Channel 01/1 : 4[4] -> 5[5] via P2P/IPC
g28n03:859956:860353 [2] NCCL INFO Channel 00/1 : 0[4] -> 4[2] [receive] via NET/IB/0/Shared
g28n03:859956:860353 [2] NCCL INFO Channel 01/1 : 0[4] -> 4[2] [receive] via NET/IB/2/Shared
g28n03:859956:860353 [2] NCCL INFO Channel 00/1 : 4[2] -> 0[4] [send] via NET/IB/0/Shared
g28n03:859956:860353 [2] NCCL INFO Channel 01/1 : 4[2] -> 0[4] [send] via NET/IB/2/Shared
g28n03:859960:860349 [4] NCCL INFO Channel 00/1 : 6[4] -> 2[0] via P2P/IPC
g32n03:753099:753472 [3] NCCL INFO Channel 01/0 : 5[3] -> 4[2] via P2P/IPC
g31n04:688226:688607 [2] NCCL INFO Connected all trees
g31n04:688226:688607 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g31n04:688226:688607 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g27n17:842063:842442 [2] NCCL INFO comm 0x15b2cf720 rank 4 nranks 8 cudaDev 2 nvmlDev 2 busId 406000 commId 0x849436df72f1a6f9 - Init COMPLETE
g32n03:753101:753470 [5] NCCL INFO Channel 01/0 : 7[5] -> 6[4] via P2P/IPC
g28n03:859958:860351 [3] NCCL INFO Channel 00/1 : 5[3] -> 0[4] [send] via NET/IB/1/Shared
g28n03:859958:860351 [3] NCCL INFO Channel 01/1 : 5[3] -> 0[4] [send] via NET/IB/3/Shared
g28n03:859958:860351 [3] NCCL INFO Channel 00/1 : 1[5] -> 5[3] [receive] via NET/IB/1/Shared
g28n03:859958:860351 [3] NCCL INFO Channel 01/1 : 1[5] -> 5[3] [receive] via NET/IB/3/Shared
g28n03:859958:860351 [3] NCCL INFO Channel 00/1 : 5[3] -> 1[5] [send] via NET/IB/1/Shared
g28n03:859958:860351 [3] NCCL INFO Channel 01/1 : 5[3] -> 1[5] [send] via NET/IB/3/Shared
g28n03:859958:860351 [3] NCCL INFO Channel 00/1 : 0[4] -> 5[3] [receive] via NET/IB/1/Shared
g28n03:859958:860351 [3] NCCL INFO Channel 01/1 : 0[4] -> 5[3] [receive] via NET/IB/3/Shared
g28n03:859958:860351 [3] NCCL INFO Channel 00/1 : 5[3] -> 2[0] via P2P/IPC
g27n17:842061:842441 [1] NCCL INFO comm 0x17b3b25b0 rank 3 nranks 8 cudaDev 1 nvmlDev 1 busId 405000 commId 0x849436df72f1a6f9 - Init COMPLETE
g26n02:851594:851975 [5] NCCL INFO comm 0x1d3bca8e0 rank 1 nranks 8 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x849436df72f1a6f9 - Init COMPLETE
g27n17:842068:842443 [5] NCCL INFO comm 0x18789bc70 rank 7 nranks 8 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x849436df72f1a6f9 - Init COMPLETE
g26n02:851593:851970 [4] NCCL INFO comm 0x17d888830 rank 0 nranks 8 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x849436df72f1a6f9 - Init COMPLETE
g26n02:851593:851993 [4] NCCL INFO Channel 00/1 : 7[5] -> 0[4] [receive] via NET/IB/1/Shared
g26n02:851593:851993 [4] NCCL INFO Channel 01/1 : 7[5] -> 0[4] [receive] via NET/IB/1/Shared
g26n02:851593:851993 [4] NCCL INFO Channel 00/1 : 0[4] -> 1[5] via P2P/IPC
g27n17:842064:842445 [3] NCCL INFO comm 0x1ac0c2910 rank 5 nranks 8 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x849436df72f1a6f9 - Init COMPLETE
g27n17:842067:842444 [4] NCCL INFO comm 0x13be271f0 rank 6 nranks 8 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x849436df72f1a6f9 - Init COMPLETE
g28n04:841288:841683 [5] NCCL INFO Channel 00/1 : 5[5] -> 6[0] [send] via NET/IB/1/Shared
g28n04:841288:841683 [5] NCCL INFO Channel 01/1 : 5[5] -> 6[0] [send] via NET/IB/3/Shared
g27n17:842060:842446 [0] NCCL INFO comm 0x1a5bc3740 rank 2 nranks 8 cudaDev 0 nvmlDev 0 busId 404000 commId 0x849436df72f1a6f9 - Init COMPLETE
g27n17:842060:842461 [0] NCCL INFO Channel 00/1 : 1[5] -> 2[0] [receive] via NET/IB/0/Shared
g27n17:842060:842461 [0] NCCL INFO Channel 01/1 : 1[5] -> 2[0] [receive] via NET/IB/2/Shared
g27n17:842060:842461 [0] NCCL INFO Channel 00/1 : 2[0] -> 3[1] via P2P/IPC
g28n03:859954:860352 [1] NCCL INFO Channel 00/1 : 3[1] -> 7[5] via P2P/IPC
g26n02:851593:851993 [4] NCCL INFO Channel 01/1 : 0[4] -> 1[5] via P2P/IPC
g28n04:841280:841687 [0] NCCL INFO Channel 00/1 : 6[0] -> 0[0] [receive] via NET/IB/0/Shared
g28n04:841280:841687 [0] NCCL INFO Channel 01/1 : 6[0] -> 0[0] [receive] via NET/IB/2/Shared
g28n04:841280:841687 [0] NCCL INFO Channel 00/1 : 0[0] -> 2[2] via P2P/IPC
g28n04:841281:841685 [1] NCCL INFO Channel 00/1 : 7[1] -> 1[1] [receive] via NET/IB/2/Shared
g28n04:841281:841685 [1] NCCL INFO Channel 01/1 : 7[1] -> 1[1] [receive] via NET/IB/0/Shared
g28n04:841281:841685 [1] NCCL INFO Channel 00/1 : 1[1] -> 3[3] via P2P/IPC
g32n02:753688:754067 [5] NCCL INFO Connected all trees
g32n02:753688:754067 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g32n02:753688:754067 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g32n03:753097:753475 [1] NCCL INFO Channel 00/0 : 3[1] -> 2[0] via P2P/IPC
g28n03:859960:860349 [4] NCCL INFO Channel 01/1 : 6[4] -> 2[0] via P2P/IPC
g28n03:859958:860351 [3] NCCL INFO Channel 01/1 : 5[3] -> 2[0] via P2P/IPC
g31n03:688511:688887 [2] NCCL INFO comm 0x1b3b8f9a0 rank 4 nranks 8 cudaDev 2 nvmlDev 2 busId 406000 commId 0x4fddb6043ee42895 - Init COMPLETE
g31n03:688510:688889 [1] NCCL INFO comm 0x18db46b90 rank 3 nranks 8 cudaDev 1 nvmlDev 1 busId 405000 commId 0x4fddb6043ee42895 - Init COMPLETE
g31n03:688513:688886 [4] NCCL INFO comm 0x1d6a7ec10 rank 6 nranks 8 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x4fddb6043ee42895 - Init COMPLETE
g31n03:688514:688888 [5] NCCL INFO comm 0x1aa49d4d0 rank 7 nranks 8 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x4fddb6043ee42895 - Init COMPLETE
g28n03:859954:860352 [1] NCCL INFO Channel 01/1 : 3[1] -> 7[5] via P2P/IPC
g31n04:688228:688611 [4] NCCL INFO Connected all trees
g31n04:688228:688611 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g31n04:688228:688611 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n03:688512:688885 [3] NCCL INFO comm 0x1b0af45c0 rank 5 nranks 8 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x4fddb6043ee42895 - Init COMPLETE
g28n03:859953:860354 [0] NCCL INFO Channel 00/1 : 2[0] -> 6[4] via P2P/IPC
g31n03:688509:688884 [0] NCCL INFO comm 0x19aa93f30 rank 2 nranks 8 cudaDev 0 nvmlDev 0 busId 404000 commId 0x4fddb6043ee42895 - Init COMPLETE
g31n03:688509:688906 [0] NCCL INFO Channel 00/1 : 1[5] -> 2[0] [receive] via NET/IB/0/Shared
g31n03:688509:688906 [0] NCCL INFO Channel 01/1 : 1[5] -> 2[0] [receive] via NET/IB/2/Shared
g31n03:688509:688906 [0] NCCL INFO Channel 00/1 : 2[0] -> 3[1] via P2P/IPC
g28n03:859961:860350 [5] NCCL INFO Channel 00/1 : 7[5] -> 3[1] via P2P/IPC
g31n02:736468:736848 [5] NCCL INFO comm 0x1bfa60370 rank 1 nranks 8 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x4fddb6043ee42895 - Init COMPLETE
g27n17:842060:842461 [0] NCCL INFO Channel 01/1 : 2[0] -> 3[1] via P2P/IPC
g28n03:859953:860354 [0] NCCL INFO Channel 01/1 : 2[0] -> 6[4] via P2P/IPC
g25n18:898853:899335 [1] NCCL INFO comm 0x17ca397b0 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 405000 commId 0x9ac9af8d1af9f7d1 - Init COMPLETE
g31n02:736467:736844 [4] NCCL INFO comm 0x18275d150 rank 0 nranks 8 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x4fddb6043ee42895 - Init COMPLETE
g31n02:736467:736866 [4] NCCL INFO Channel 00/1 : 7[5] -> 0[4] [receive] via NET/IB/1/Shared
g31n02:736467:736866 [4] NCCL INFO Channel 01/1 : 7[5] -> 0[4] [receive] via NET/IB/1/Shared
g31n02:736467:736866 [4] NCCL INFO Channel 00/1 : 0[4] -> 1[5] via P2P/IPC
g28n03:859961:860350 [5] NCCL INFO Channel 01/1 : 7[5] -> 3[1] via P2P/IPC
g25n18:898854:899336 [2] NCCL INFO comm 0x1b2447840 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 406000 commId 0x9ac9af8d1af9f7d1 - Init COMPLETE
g25n18:898857:899338 [5] NCCL INFO comm 0x141bea260 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x9ac9af8d1af9f7d1 - Init COMPLETE
g27n17:842063:842460 [2] NCCL INFO Channel 00/1 : 4[2] -> 5[3] via P2P/IPC
g25n18:898855:899337 [3] NCCL INFO comm 0x1cacd3ea0 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x9ac9af8d1af9f7d1 - Init COMPLETE
g27n17:842061:842459 [1] NCCL INFO Channel 00/1 : 3[1] -> 4[2] via P2P/IPC
g26n01:811331:811709 [0] NCCL INFO comm 0x1ac2bdef0 rank 6 nranks 8 cudaDev 0 nvmlDev 0 busId 404000 commId 0x9ac9af8d1af9f7d1 - Init COMPLETE
g26n01:811331:811729 [0] NCCL INFO Channel 00/1 : 5[5] -> 6[0] [receive] via NET/IB/0/Shared
g26n01:811331:811729 [0] NCCL INFO Channel 01/1 : 5[5] -> 6[0] [receive] via NET/IB/0/Shared
g26n01:811331:811729 [0] NCCL INFO Channel 00/1 : 6[0] -> 7[1] via P2P/IPC
g25n18:898856:899339 [4] NCCL INFO comm 0x15bb9ed70 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x9ac9af8d1af9f7d1 - Init COMPLETE
g27n17:842064:842462 [3] NCCL INFO Channel 00/1 : 5[3] -> 6[4] via P2P/IPC
g26n01:811332:811710 [1] NCCL INFO comm 0x19b304620 rank 7 nranks 8 cudaDev 1 nvmlDev 1 busId 405000 commId 0x9ac9af8d1af9f7d1 - Init COMPLETE
g25n18:898852:899334 [0] NCCL INFO comm 0x149bed730 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 404000 commId 0x9ac9af8d1af9f7d1 - Init COMPLETE
g25n18:898852:899357 [0] NCCL INFO Channel 00/1 : 7[1] -> 0[0] [receive] via NET/IB/0/Shared
g25n18:898852:899357 [0] NCCL INFO Channel 01/1 : 7[1] -> 0[0] [receive] via NET/IB/2/Shared
g25n18:898852:899357 [0] NCCL INFO Channel 00/1 : 0[0] -> 1[1] via P2P/IPC
g27n17:842067:842464 [4] NCCL INFO Channel 00/1 : 6[4] -> 7[5] via P2P/IPC
g32n03:753101:753470 [5] NCCL INFO Connected all trees
g32n03:753101:753470 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g32n03:753101:753470 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n04:841280:841687 [0] NCCL INFO Channel 01/1 : 0[0] -> 2[2] via P2P/IPC
g28n01:770092:770474 [0] NCCL INFO Connected all trees
g28n01:770092:770474 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g28n01:770092:770474 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n04:841281:841685 [1] NCCL INFO Channel 01/1 : 1[1] -> 3[3] via P2P/IPC
g31n03:688509:688906 [0] NCCL INFO Channel 01/1 : 2[0] -> 3[1] via P2P/IPC
g26n01:811331:811729 [0] NCCL INFO Channel 01/1 : 6[0] -> 7[1] via P2P/IPC
g31n04:688227:688609 [3] NCCL INFO Connected all trees
g31n04:688227:688609 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g31n04:688227:688609 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g27n17:842061:842459 [1] NCCL INFO Channel 01/1 : 3[1] -> 4[2] via P2P/IPC
g32n03:753097:753475 [1] NCCL INFO Channel 01/0 : 3[1] -> 2[0] via P2P/IPC
g31n02:736467:736866 [4] NCCL INFO Channel 01/1 : 0[4] -> 1[5] via P2P/IPC
g27n17:842067:842464 [4] NCCL INFO Channel 01/1 : 6[4] -> 7[5] via P2P/IPC
g28n04:841284:841684 [3] NCCL INFO Channel 00/1 : 3[3] -> 5[5] via P2P/IPC
g27n17:842064:842462 [3] NCCL INFO Channel 01/1 : 5[3] -> 6[4] via P2P/IPC
g28n04:841283:841686 [2] NCCL INFO Channel 00/1 : 2[2] -> 4[4] via P2P/IPC
g27n17:842063:842460 [2] NCCL INFO Channel 01/1 : 4[2] -> 5[3] via P2P/IPC
g27n18:839782:840163 [0] NCCL INFO Connected all trees
g27n18:839782:840163 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g27n18:839782:840163 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n04:841283:841686 [2] NCCL INFO Channel 01/1 : 2[2] -> 4[4] via P2P/IPC
g28n04:841284:841684 [3] NCCL INFO Channel 01/1 : 3[3] -> 5[5] via P2P/IPC
g31n03:688511:688905 [2] NCCL INFO Channel 00/1 : 4[2] -> 5[3] via P2P/IPC
g28n03:859960:860349 [4] NCCL INFO Channel 00/1 : 1[5] -> 6[4] [receive] via NET/IB/3/Shared
g28n03:859960:860349 [4] NCCL INFO Channel 01/1 : 1[5] -> 6[4] [receive] via NET/IB/1/Shared
g28n03:859960:860349 [4] NCCL INFO Channel 00/1 : 6[4] -> 3[1] via P2P/IPC
g31n03:688513:688902 [4] NCCL INFO Channel 00/1 : 6[4] -> 7[5] via P2P/IPC
g31n04:688225:688608 [1] NCCL INFO Connected all trees
g31n04:688225:688608 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g31n04:688225:688608 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g25n18:898852:899357 [0] NCCL INFO Channel 01/1 : 0[0] -> 1[1] via P2P/IPC
g31n03:688512:688904 [3] NCCL INFO Channel 00/1 : 5[3] -> 6[4] via P2P/IPC
g31n04:688229:688610 [5] NCCL INFO Connected all trees
g31n04:688229:688610 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g31n04:688229:688610 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n03:688510:688907 [1] NCCL INFO Channel 00/1 : 3[1] -> 4[2] via P2P/IPC
g32n03:753100:753471 [4] NCCL INFO Connected all trees
g32n03:753100:753471 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g32n03:753100:753471 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g25n18:898853:899355 [1] NCCL INFO Channel 00/1 : 1[1] -> 2[2] via P2P/IPC
g28n03:859960:860349 [4] NCCL INFO Channel 01/1 : 6[4] -> 3[1] via P2P/IPC
g25n18:898854:899356 [2] NCCL INFO Channel 00/1 : 2[2] -> 3[3] via P2P/IPC
g25n18:898855:899353 [3] NCCL INFO Channel 00/1 : 3[3] -> 4[4] via P2P/IPC
g28n03:859953:860354 [0] NCCL INFO Channel 00/1 : 2[0] -> 7[5] via P2P/IPC
g27n17:842068:842463 [5] NCCL INFO Channel 00/1 : 7[5] -> 0[4] [send] via NET/IB/1/Shared
g27n17:842068:842463 [5] NCCL INFO Channel 01/1 : 7[5] -> 0[4] [send] via NET/IB/3/Shared
g25n18:898856:899354 [4] NCCL INFO Channel 00/1 : 4[4] -> 5[5] via P2P/IPC
g28n03:859961:860350 [5] NCCL INFO Channel 00/1 : 7[5] -> 4[2] via P2P/IPC
g31n03:688511:688905 [2] NCCL INFO Channel 01/1 : 4[2] -> 5[3] via P2P/IPC
g28n04:841287:841682 [4] NCCL INFO Channel 00/1 : 4[4] -> 6[0] [send] via NET/IB/3/Shared
g28n04:841287:841682 [4] NCCL INFO Channel 01/1 : 4[4] -> 6[0] [send] via NET/IB/1/Shared
g31n03:688512:688904 [3] NCCL INFO Channel 01/1 : 5[3] -> 6[4] via P2P/IPC
g28n04:841288:841683 [5] NCCL INFO Channel 00/1 : 5[5] -> 7[1] [send] via NET/IB/1/Shared
g28n04:841288:841683 [5] NCCL INFO Channel 01/1 : 5[5] -> 7[1] [send] via NET/IB/3/Shared
g32n03:753099:753472 [3] NCCL INFO Connected all trees
g32n03:753099:753472 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g32n03:753099:753472 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n04:841281:841685 [1] NCCL INFO Channel 00/1 : 6[0] -> 1[1] [receive] via NET/IB/2/Shared
g28n04:841281:841685 [1] NCCL INFO Channel 01/1 : 6[0] -> 1[1] [receive] via NET/IB/0/Shared
g28n04:841281:841685 [1] NCCL INFO Channel 00/1 : 1[1] -> 4[4] via P2P/IPC
g27n17:842060:842461 [0] NCCL INFO Channel 00/1 : 0[4] -> 2[0] [receive] via NET/IB/0/Shared
g27n17:842060:842461 [0] NCCL INFO Channel 01/1 : 0[4] -> 2[0] [receive] via NET/IB/2/Shared
g27n17:842060:842461 [0] NCCL INFO Channel 00/1 : 2[0] -> 4[2] via P2P/IPC
g31n03:688513:688902 [4] NCCL INFO Channel 01/1 : 6[4] -> 7[5] via P2P/IPC
g28n04:841283:841686 [2] NCCL INFO Channel 00/1 : 7[1] -> 2[2] [receive] via NET/IB/0/Shared
g28n04:841283:841686 [2] NCCL INFO Channel 01/1 : 7[1] -> 2[2] [receive] via NET/IB/2/Shared
g28n04:841283:841686 [2] NCCL INFO Channel 00/1 : 2[2] -> 5[5] via P2P/IPC
g31n03:688510:688907 [1] NCCL INFO Channel 01/1 : 3[1] -> 4[2] via P2P/IPC
g25n18:898853:899355 [1] NCCL INFO Channel 01/1 : 1[1] -> 2[2] via P2P/IPC
g27n17:842061:842459 [1] NCCL INFO Channel 00/1 : 1[5] -> 3[1] [receive] via NET/IB/2/Shared
g27n17:842061:842459 [1] NCCL INFO Channel 01/1 : 1[5] -> 3[1] [receive] via NET/IB/0/Shared
g27n17:842061:842459 [1] NCCL INFO Channel 00/1 : 3[1] -> 5[3] via P2P/IPC
g28n03:859954:860352 [1] NCCL INFO Channel 00/1 : 3[1] -> 0[4] [send] via NET/IB/2/Shared
g28n03:859954:860352 [1] NCCL INFO Channel 01/1 : 3[1] -> 0[4] [send] via NET/IB/0/Shared
g25n18:898854:899356 [2] NCCL INFO Channel 01/1 : 2[2] -> 3[3] via P2P/IPC
g28n03:859953:860354 [0] NCCL INFO Channel 01/1 : 2[0] -> 7[5] via P2P/IPC
g25n18:898855:899353 [3] NCCL INFO Channel 01/1 : 3[3] -> 4[4] via P2P/IPC
g28n03:859961:860350 [5] NCCL INFO Channel 01/1 : 7[5] -> 4[2] via P2P/IPC
g25n18:898856:899354 [4] NCCL INFO Channel 01/1 : 4[4] -> 5[5] via P2P/IPC
g28n03:859960:860349 [4] NCCL INFO Channel 00/1 : 0[4] -> 6[4] [receive] via NET/IB/3/Shared
g28n03:859960:860349 [4] NCCL INFO Channel 01/1 : 0[4] -> 6[4] [receive] via NET/IB/1/Shared
g28n03:859960:860349 [4] NCCL INFO Channel 00/1 : 6[4] -> 4[2] via P2P/IPC
g28n04:841281:841685 [1] NCCL INFO Channel 01/1 : 1[1] -> 4[4] via P2P/IPC
g31n03:688514:688903 [5] NCCL INFO Channel 00/1 : 7[5] -> 0[4] [send] via NET/IB/1/Shared
g31n03:688514:688903 [5] NCCL INFO Channel 01/1 : 7[5] -> 0[4] [send] via NET/IB/3/Shared
g28n04:841283:841686 [2] NCCL INFO Channel 01/1 : 2[2] -> 5[5] via P2P/IPC
g27n17:842060:842461 [0] NCCL INFO Channel 01/1 : 2[0] -> 4[2] via P2P/IPC
g27n17:842061:842459 [1] NCCL INFO Channel 01/1 : 3[1] -> 5[3] via P2P/IPC
g31n03:688509:688906 [0] NCCL INFO Channel 00/1 : 0[4] -> 2[0] [receive] via NET/IB/0/Shared
g31n03:688509:688906 [0] NCCL INFO Channel 01/1 : 0[4] -> 2[0] [receive] via NET/IB/2/Shared
g31n03:688509:688906 [0] NCCL INFO Channel 00/1 : 2[0] -> 4[2] via P2P/IPC
g31n03:688510:688907 [1] NCCL INFO Channel 00/1 : 1[5] -> 3[1] [receive] via NET/IB/2/Shared
g31n03:688510:688907 [1] NCCL INFO Channel 01/1 : 1[5] -> 3[1] [receive] via NET/IB/0/Shared
g31n03:688510:688907 [1] NCCL INFO Channel 00/1 : 3[1] -> 5[3] via P2P/IPC
g25n18:898857:899352 [5] NCCL INFO Channel 00/1 : 5[5] -> 6[0] [send] via NET/IB/1/Shared
g25n18:898857:899352 [5] NCCL INFO Channel 01/1 : 5[5] -> 6[0] [send] via NET/IB/3/Shared
g25n18:898853:899355 [1] NCCL INFO Channel 00/1 : 7[1] -> 1[1] [receive] via NET/IB/2/Shared
g25n18:898853:899355 [1] NCCL INFO Channel 01/1 : 7[1] -> 1[1] [receive] via NET/IB/0/Shared
g25n18:898853:899355 [1] NCCL INFO Channel 00/1 : 1[1] -> 3[3] via P2P/IPC
g27n17:842063:842460 [2] NCCL INFO Channel 00/1 : 4[2] -> 6[4] via P2P/IPC
g28n04:841280:841687 [0] NCCL INFO Channel 00/1 : 0[0] -> 3[3] via P2P/IPC
g27n17:842064:842462 [3] NCCL INFO Channel 00/1 : 5[3] -> 7[5] via P2P/IPC
g28n03:859960:860349 [4] NCCL INFO Channel 01/1 : 6[4] -> 4[2] via P2P/IPC
g28n03:859956:860353 [2] NCCL INFO Channel 00/1 : 4[2] -> 1[5] [send] via NET/IB/0/Shared
g28n03:859956:860353 [2] NCCL INFO Channel 01/1 : 4[2] -> 1[5] [send] via NET/IB/2/Shared
g28n03:859961:860350 [5] NCCL INFO Channel 00/1 : 1[5] -> 7[5] [receive] via NET/IB/1/Shared
g28n03:859961:860350 [5] NCCL INFO Channel 01/1 : 1[5] -> 7[5] [receive] via NET/IB/3/Shared
g28n03:859961:860350 [5] NCCL INFO Channel 00/1 : 7[5] -> 5[3] via P2P/IPC
g28n04:841288:841683 [5] NCCL INFO Channel 00/1 : 5[5] -> 0[0] via P2P/IPC
g27n17:842063:842460 [2] NCCL INFO Channel 01/1 : 4[2] -> 6[4] via P2P/IPC
g27n17:842064:842462 [3] NCCL INFO Channel 01/1 : 5[3] -> 7[5] via P2P/IPC
g28n04:841287:841682 [4] NCCL INFO Channel 00/1 : 4[4] -> 7[1] [send] via NET/IB/3/Shared
g28n04:841287:841682 [4] NCCL INFO Channel 01/1 : 4[4] -> 7[1] [send] via NET/IB/1/Shared
g25n18:898853:899355 [1] NCCL INFO Channel 01/1 : 1[1] -> 3[3] via P2P/IPC
g28n01:770093:770475 [1] NCCL INFO comm 0x14f7c5220 rank 7 nranks 8 cudaDev 1 nvmlDev 1 busId 405000 commId 0x7b81f3d89584e3c1 - Init COMPLETE
g28n01:770092:770474 [0] NCCL INFO comm 0x199996c50 rank 6 nranks 8 cudaDev 0 nvmlDev 0 busId 404000 commId 0x7b81f3d89584e3c1 - Init COMPLETE
g28n01:770092:770492 [0] NCCL INFO Channel 00/1 : 5[5] -> 6[0] [receive] via NET/IB/0/Shared
g28n01:770092:770492 [0] NCCL INFO Channel 01/1 : 5[5] -> 6[0] [receive] via NET/IB/0/Shared
g28n01:770092:770492 [0] NCCL INFO Channel 00/1 : 6[0] -> 7[1] via P2P/IPC
g28n04:841280:841687 [0] NCCL INFO Channel 01/1 : 0[0] -> 3[3] via P2P/IPC
g31n03:688509:688906 [0] NCCL INFO Channel 01/1 : 2[0] -> 4[2] via P2P/IPC
g31n03:688510:688907 [1] NCCL INFO Channel 01/1 : 3[1] -> 5[3] via P2P/IPC
g25n18:898852:899357 [0] NCCL INFO Channel 00/1 : 6[0] -> 0[0] [receive] via NET/IB/0/Shared
g25n18:898852:899357 [0] NCCL INFO Channel 01/1 : 6[0] -> 0[0] [receive] via NET/IB/2/Shared
g25n18:898852:899357 [0] NCCL INFO Channel 00/1 : 0[0] -> 2[2] via P2P/IPC
g28n01:770092:770492 [0] NCCL INFO Channel 01/1 : 6[0] -> 7[1] via P2P/IPC
g28n04:841288:841683 [5] NCCL INFO Channel 01/1 : 5[5] -> 0[0] via P2P/IPC
g28n03:859961:860350 [5] NCCL INFO Channel 01/1 : 7[5] -> 5[3] via P2P/IPC
g31n03:688511:688905 [2] NCCL INFO Channel 00/1 : 4[2] -> 6[4] via P2P/IPC
g25n18:898854:899356 [2] NCCL INFO Channel 00/1 : 2[2] -> 4[4] via P2P/IPC
g31n03:688512:688904 [3] NCCL INFO Channel 00/1 : 5[3] -> 7[5] via P2P/IPC
g28n03:859958:860351 [3] NCCL INFO Channel 00/1 : 5[3] -> 3[1] via P2P/IPC
g32n03:753098:753473 [2] NCCL INFO Connected all trees
g32n03:753098:753473 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g32n03:753098:753473 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g25n18:898855:899353 [3] NCCL INFO Channel 00/1 : 3[3] -> 5[5] via P2P/IPC
g28n03:859956:860353 [2] NCCL INFO Channel 00/1 : 4[2] -> 2[0] via P2P/IPC
g32n03:753097:753475 [1] NCCL INFO Connected all trees
g32n03:753097:753475 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g32n03:753097:753475 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g27n17:842067:842464 [4] NCCL INFO Channel 00/1 : 6[4] -> 0[4] [send] via NET/IB/3/Shared
g27n17:842067:842464 [4] NCCL INFO Channel 01/1 : 6[4] -> 0[4] [send] via NET/IB/1/Shared
g25n18:898852:899357 [0] NCCL INFO Channel 01/1 : 0[0] -> 2[2] via P2P/IPC
g27n17:842068:842463 [5] NCCL INFO Channel 00/1 : 7[5] -> 1[5] [send] via NET/IB/1/Shared
g27n17:842068:842463 [5] NCCL INFO Channel 01/1 : 7[5] -> 1[5] [send] via NET/IB/3/Shared
g31n04:688224:688606 [0] NCCL INFO Connected all trees
g31n04:688224:688606 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g31n04:688224:688606 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n03:859958:860351 [3] NCCL INFO Channel 01/1 : 5[3] -> 3[1] via P2P/IPC
g27n17:842061:842459 [1] NCCL INFO Channel 00/1 : 0[4] -> 3[1] [receive] via NET/IB/2/Shared
g27n17:842061:842459 [1] NCCL INFO Channel 01/1 : 0[4] -> 3[1] [receive] via NET/IB/0/Shared
g27n17:842061:842459 [1] NCCL INFO Channel 00/1 : 3[1] -> 6[4] via P2P/IPC
g28n02:792076:792488 [5] NCCL INFO Channel 00/1 : 1[5] -> 2[0] [send] via NET/IB/3/Shared
g28n02:792076:792488 [5] NCCL INFO Channel 01/1 : 1[5] -> 2[0] [send] via NET/IB/3/Shared
g28n02:792076:792488 [5] NCCL INFO Channel 00/1 : 7[5] -> 1[5] [receive] via NET/IB/3/Shared
g28n02:792076:792488 [5] NCCL INFO Channel 01/1 : 7[5] -> 1[5] [receive] via NET/IB/3/Shared
g28n02:792076:792488 [5] NCCL INFO Channel 00/1 : 1[5] -> 3[1] [send] via NET/IB/3/Shared
g28n02:792076:792488 [5] NCCL INFO Channel 01/1 : 1[5] -> 3[1] [send] via NET/IB/3/Shared
g28n02:792076:792488 [5] NCCL INFO Channel 00/1 : 6[4] -> 1[5] [receive] via NET/IB/3/Shared
g28n02:792076:792488 [5] NCCL INFO Channel 01/1 : 6[4] -> 1[5] [receive] via NET/IB/3/Shared
g28n02:792076:792488 [5] NCCL INFO Channel 00/1 : 1[5] -> 4[2] [send] via NET/IB/3/Shared
g28n02:792076:792488 [5] NCCL INFO Channel 01/1 : 1[5] -> 4[2] [send] via NET/IB/3/Shared
g28n02:792076:792488 [5] NCCL INFO Channel 00/1 : 5[3] -> 1[5] [receive] via NET/IB/3/Shared
g28n02:792076:792488 [5] NCCL INFO Channel 01/1 : 5[3] -> 1[5] [receive] via NET/IB/3/Shared
g28n02:792076:792488 [5] NCCL INFO Channel 00/1 : 1[5] -> 5[3] [send] via NET/IB/3/Shared
g28n02:792076:792488 [5] NCCL INFO Channel 01/1 : 1[5] -> 5[3] [send] via NET/IB/3/Shared
g28n02:792076:792488 [5] NCCL INFO Channel 00/1 : 4[2] -> 1[5] [receive] via NET/IB/3/Shared
g28n02:792076:792488 [5] NCCL INFO Channel 01/1 : 4[2] -> 1[5] [receive] via NET/IB/3/Shared
g28n02:792076:792488 [5] NCCL INFO Channel 00/1 : 1[5] -> 6[4] [send] via NET/IB/3/Shared
g28n02:792076:792488 [5] NCCL INFO Channel 01/1 : 1[5] -> 6[4] [send] via NET/IB/3/Shared
g28n02:792076:792488 [5] NCCL INFO Channel 00/1 : 3[1] -> 1[5] [receive] via NET/IB/3/Shared
g28n02:792076:792488 [5] NCCL INFO Channel 01/1 : 3[1] -> 1[5] [receive] via NET/IB/3/Shared
g28n02:792076:792488 [5] NCCL INFO Channel 00/1 : 1[5] -> 7[5] [send] via NET/IB/3/Shared
g28n02:792076:792488 [5] NCCL INFO Channel 01/1 : 1[5] -> 7[5] [send] via NET/IB/3/Shared
g28n02:792076:792488 [5] NCCL INFO Channel 00/1 : 2[0] -> 1[5] [receive] via NET/IB/3/Shared
g28n02:792076:792488 [5] NCCL INFO Channel 01/1 : 2[0] -> 1[5] [receive] via NET/IB/3/Shared
g28n02:792076:792488 [5] NCCL INFO Channel 00/1 : 1[5] -> 0[4] via P2P/IPC
g28n03:859956:860353 [2] NCCL INFO Channel 01/1 : 4[2] -> 2[0] via P2P/IPC
g27n17:842063:842460 [2] NCCL INFO Channel 00/1 : 1[5] -> 4[2] [receive] via NET/IB/0/Shared
g27n17:842063:842460 [2] NCCL INFO Channel 01/1 : 1[5] -> 4[2] [receive] via NET/IB/2/Shared
g27n17:842063:842460 [2] NCCL INFO Channel 00/1 : 4[2] -> 7[5] via P2P/IPC
g31n05:704344:704722 [0] NCCL INFO Connected all trees
g31n05:704344:704722 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g31n05:704344:704722 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n02:792075:792487 [4] NCCL INFO Channel 00/1 : 6[4] -> 0[4] [receive] via NET/IB/1/Shared
g28n02:792075:792487 [4] NCCL INFO Channel 01/1 : 6[4] -> 0[4] [receive] via NET/IB/1/Shared
g28n02:792075:792487 [4] NCCL INFO Channel 00/1 : 0[4] -> 2[0] [send] via NET/IB/1/Shared
g28n02:792075:792487 [4] NCCL INFO Channel 01/1 : 0[4] -> 2[0] [send] via NET/IB/1/Shared
g28n02:792075:792487 [4] NCCL INFO Channel 00/1 : 5[3] -> 0[4] [receive] via NET/IB/1/Shared
g28n02:792075:792487 [4] NCCL INFO Channel 01/1 : 5[3] -> 0[4] [receive] via NET/IB/1/Shared
g28n02:792075:792487 [4] NCCL INFO Channel 00/1 : 0[4] -> 3[1] [send] via NET/IB/1/Shared
g28n02:792075:792487 [4] NCCL INFO Channel 01/1 : 0[4] -> 3[1] [send] via NET/IB/1/Shared
g28n02:792075:792487 [4] NCCL INFO Channel 00/1 : 4[2] -> 0[4] [receive] via NET/IB/1/Shared
g28n02:792075:792487 [4] NCCL INFO Channel 01/1 : 4[2] -> 0[4] [receive] via NET/IB/1/Shared
g28n02:792075:792487 [4] NCCL INFO Channel 00/1 : 0[4] -> 4[2] [send] via NET/IB/1/Shared
g28n02:792075:792487 [4] NCCL INFO Channel 01/1 : 0[4] -> 4[2] [send] via NET/IB/1/Shared
g28n02:792075:792487 [4] NCCL INFO Channel 00/1 : 3[1] -> 0[4] [receive] via NET/IB/1/Shared
g28n02:792075:792487 [4] NCCL INFO Channel 01/1 : 3[1] -> 0[4] [receive] via NET/IB/1/Shared
g28n02:792075:792487 [4] NCCL INFO Channel 00/1 : 0[4] -> 5[3] [send] via NET/IB/1/Shared
g28n02:792075:792487 [4] NCCL INFO Channel 01/1 : 0[4] -> 5[3] [send] via NET/IB/1/Shared
g28n02:792075:792487 [4] NCCL INFO Channel 00/1 : 2[0] -> 0[4] [receive] via NET/IB/1/Shared
g28n02:792075:792487 [4] NCCL INFO Channel 01/1 : 2[0] -> 0[4] [receive] via NET/IB/1/Shared
g28n02:792075:792487 [4] NCCL INFO Channel 00/1 : 0[4] -> 6[4] [send] via NET/IB/1/Shared
g28n02:792075:792487 [4] NCCL INFO Channel 01/1 : 0[4] -> 6[4] [send] via NET/IB/1/Shared
g31n03:688511:688905 [2] NCCL INFO Channel 01/1 : 4[2] -> 6[4] via P2P/IPC
g25n18:898854:899356 [2] NCCL INFO Channel 01/1 : 2[2] -> 4[4] via P2P/IPC
g31n03:688512:688904 [3] NCCL INFO Channel 01/1 : 5[3] -> 7[5] via P2P/IPC
g27n18:839790:840168 [5] NCCL INFO comm 0x1751b3cb0 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x7b81f3d89584e3c1 - Init COMPLETE
g27n18:839787:840165 [3] NCCL INFO comm 0x1c2dad6e0 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x7b81f3d89584e3c1 - Init COMPLETE
g25n18:898855:899353 [3] NCCL INFO Channel 01/1 : 3[3] -> 5[5] via P2P/IPC
g27n18:839789:840166 [4] NCCL INFO comm 0x199bc7b20 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x7b81f3d89584e3c1 - Init COMPLETE
g27n18:839783:840167 [1] NCCL INFO comm 0x1807e2220 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 405000 commId 0x7b81f3d89584e3c1 - Init COMPLETE
g27n18:839785:840164 [2] NCCL INFO comm 0x1bfa82790 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 406000 commId 0x7b81f3d89584e3c1 - Init COMPLETE
g27n18:839782:840163 [0] NCCL INFO comm 0x1f5b69550 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 404000 commId 0x7b81f3d89584e3c1 - Init COMPLETE
g27n18:839782:840184 [0] NCCL INFO Channel 00/1 : 7[1] -> 0[0] [receive] via NET/IB/0/Shared
g27n18:839782:840184 [0] NCCL INFO Channel 01/1 : 7[1] -> 0[0] [receive] via NET/IB/2/Shared
g27n18:839782:840184 [0] NCCL INFO Channel 00/1 : 0[0] -> 1[1] via P2P/IPC
g28n04:841281:841685 [1] NCCL INFO Channel 00/1 : 1[1] -> 5[5] via P2P/IPC
g28n02:792076:792488 [5] NCCL INFO Channel 01/1 : 1[5] -> 0[4] via P2P/IPC
g28n04:841287:841682 [4] NCCL INFO Channel 00/1 : 4[4] -> 0[0] via P2P/IPC
g28n04:841283:841686 [2] NCCL INFO Channel 00/1 : 6[0] -> 2[2] [receive] via NET/IB/0/Shared
g28n04:841283:841686 [2] NCCL INFO Channel 01/1 : 6[0] -> 2[2] [receive] via NET/IB/2/Shared
g28n04:841283:841686 [2] NCCL INFO Channel 00/1 : 2[2] -> 6[0] [send] via NET/IB/0/Shared
g28n04:841283:841686 [2] NCCL INFO Channel 01/1 : 2[2] -> 6[0] [send] via NET/IB/2/Shared
g28n04:841284:841684 [3] NCCL INFO Channel 00/1 : 3[3] -> 6[0] [send] via NET/IB/1/Shared
g28n04:841284:841684 [3] NCCL INFO Channel 01/1 : 3[3] -> 6[0] [send] via NET/IB/3/Shared
g28n04:841284:841684 [3] NCCL INFO Channel 00/1 : 7[1] -> 3[3] [receive] via NET/IB/1/Shared
g28n04:841284:841684 [3] NCCL INFO Channel 01/1 : 7[1] -> 3[3] [receive] via NET/IB/3/Shared
g28n04:841284:841684 [3] NCCL INFO Channel 00/1 : 3[3] -> 7[1] [send] via NET/IB/1/Shared
g28n04:841284:841684 [3] NCCL INFO Channel 01/1 : 3[3] -> 7[1] [send] via NET/IB/3/Shared
g28n04:841284:841684 [3] NCCL INFO Channel 00/1 : 6[0] -> 3[3] [receive] via NET/IB/1/Shared
g28n04:841284:841684 [3] NCCL INFO Channel 01/1 : 6[0] -> 3[3] [receive] via NET/IB/3/Shared
g28n04:841284:841684 [3] NCCL INFO Channel 00/1 : 3[3] -> 0[0] via P2P/IPC
g27n17:842061:842459 [1] NCCL INFO Channel 01/1 : 3[1] -> 6[4] via P2P/IPC
g27n17:842063:842460 [2] NCCL INFO Channel 01/1 : 4[2] -> 7[5] via P2P/IPC
g31n03:688513:688902 [4] NCCL INFO Channel 00/1 : 6[4] -> 0[4] [send] via NET/IB/3/Shared
g31n03:688513:688902 [4] NCCL INFO Channel 01/1 : 6[4] -> 0[4] [send] via NET/IB/1/Shared
g31n03:688514:688903 [5] NCCL INFO Channel 00/1 : 7[5] -> 1[5] [send] via NET/IB/1/Shared
g31n03:688514:688903 [5] NCCL INFO Channel 01/1 : 7[5] -> 1[5] [send] via NET/IB/3/Shared
g31n03:688510:688907 [1] NCCL INFO Channel 00/1 : 0[4] -> 3[1] [receive] via NET/IB/2/Shared
g31n03:688510:688907 [1] NCCL INFO Channel 01/1 : 0[4] -> 3[1] [receive] via NET/IB/0/Shared
g31n03:688510:688907 [1] NCCL INFO Channel 00/1 : 3[1] -> 6[4] via P2P/IPC
g31n03:688511:688905 [2] NCCL INFO Channel 00/1 : 1[5] -> 4[2] [receive] via NET/IB/0/Shared
g31n03:688511:688905 [2] NCCL INFO Channel 01/1 : 1[5] -> 4[2] [receive] via NET/IB/2/Shared
g31n03:688511:688905 [2] NCCL INFO Channel 00/1 : 4[2] -> 7[5] via P2P/IPC
g28n03:859954:860352 [1] NCCL INFO Channel 00/1 : 3[1] -> 1[5] [send] via NET/IB/2/Shared
g28n03:859954:860352 [1] NCCL INFO Channel 01/1 : 3[1] -> 1[5] [send] via NET/IB/0/Shared
g28n03:859953:860354 [0] NCCL INFO Channel 00/1 : 2[0] -> 0[4] [send] via NET/IB/0/Shared
g28n03:859953:860354 [0] NCCL INFO Channel 01/1 : 2[0] -> 0[4] [send] via NET/IB/2/Shared
g25n18:898857:899352 [5] NCCL INFO Channel 00/1 : 5[5] -> 7[1] [send] via NET/IB/1/Shared
g25n18:898857:899352 [5] NCCL INFO Channel 01/1 : 5[5] -> 7[1] [send] via NET/IB/3/Shared
g28n03:859961:860350 [5] NCCL INFO Channel 00/1 : 0[4] -> 7[5] [receive] via NET/IB/1/Shared
g28n03:859961:860350 [5] NCCL INFO Channel 01/1 : 0[4] -> 7[5] [receive] via NET/IB/3/Shared
g28n03:859961:860350 [5] NCCL INFO Channel 00/1 : 7[5] -> 6[4] via P2P/IPC
g25n18:898856:899354 [4] NCCL INFO Channel 00/1 : 4[4] -> 6[0] [send] via NET/IB/3/Shared
g25n18:898856:899354 [4] NCCL INFO Channel 01/1 : 4[4] -> 6[0] [send] via NET/IB/1/Shared
g25n18:898853:899355 [1] NCCL INFO Channel 00/1 : 6[0] -> 1[1] [receive] via NET/IB/2/Shared
g25n18:898853:899355 [1] NCCL INFO Channel 01/1 : 6[0] -> 1[1] [receive] via NET/IB/0/Shared
g25n18:898853:899355 [1] NCCL INFO Channel 00/1 : 1[1] -> 4[4] via P2P/IPC
g25n18:898854:899356 [2] NCCL INFO Channel 00/1 : 7[1] -> 2[2] [receive] via NET/IB/0/Shared
g25n18:898854:899356 [2] NCCL INFO Channel 01/1 : 7[1] -> 2[2] [receive] via NET/IB/2/Shared
g25n18:898854:899356 [2] NCCL INFO Channel 00/1 : 2[2] -> 5[5] via P2P/IPC
g28n04:841281:841685 [1] NCCL INFO Channel 01/1 : 1[1] -> 5[5] via P2P/IPC
g27n18:839782:840184 [0] NCCL INFO Channel 01/1 : 0[0] -> 1[1] via P2P/IPC
g27n17:842068:842463 [5] NCCL INFO Channel 00/1 : 7[5] -> 2[0] via P2P/IPC
g28n04:841287:841682 [4] NCCL INFO Channel 01/1 : 4[4] -> 0[0] via P2P/IPC
g27n17:842060:842461 [0] NCCL INFO Channel 00/1 : 2[0] -> 5[3] via P2P/IPC
g28n04:841284:841684 [3] NCCL INFO Channel 01/1 : 3[3] -> 0[0] via P2P/IPC
g27n17:842067:842464 [4] NCCL INFO Channel 00/1 : 6[4] -> 1[5] [send] via NET/IB/3/Shared
g27n17:842067:842464 [4] NCCL INFO Channel 01/1 : 6[4] -> 1[5] [send] via NET/IB/1/Shared
g27n18:839783:840186 [1] NCCL INFO Channel 00/1 : 1[1] -> 2[2] via P2P/IPC
g27n18:839789:840182 [4] NCCL INFO Channel 00/1 : 4[4] -> 5[5] via P2P/IPC
g28n04:841280:841687 [0] NCCL INFO Channel 00/1 : 0[0] -> 4[4] via P2P/IPC
g28n04:841288:841683 [5] NCCL INFO Channel 00/1 : 5[5] -> 1[1] via P2P/IPC
g31n03:688510:688907 [1] NCCL INFO Channel 01/1 : 3[1] -> 6[4] via P2P/IPC
g31n03:688511:688905 [2] NCCL INFO Channel 01/1 : 4[2] -> 7[5] via P2P/IPC
g27n17:842060:842461 [0] NCCL INFO Channel 01/1 : 2[0] -> 5[3] via P2P/IPC
g27n18:839787:840183 [3] NCCL INFO Channel 00/1 : 3[3] -> 4[4] via P2P/IPC
g25n18:898853:899355 [1] NCCL INFO Channel 01/1 : 1[1] -> 4[4] via P2P/IPC
g27n17:842068:842463 [5] NCCL INFO Channel 01/1 : 7[5] -> 2[0] via P2P/IPC
g28n04:841280:841687 [0] NCCL INFO Channel 01/1 : 0[0] -> 4[4] via P2P/IPC
g27n18:839785:840185 [2] NCCL INFO Channel 00/1 : 2[2] -> 3[3] via P2P/IPC
g25n18:898854:899356 [2] NCCL INFO Channel 01/1 : 2[2] -> 5[5] via P2P/IPC
g28n04:841288:841683 [5] NCCL INFO Channel 01/1 : 5[5] -> 1[1] via P2P/IPC
g27n18:839783:840186 [1] NCCL INFO Channel 01/1 : 1[1] -> 2[2] via P2P/IPC
g27n18:839789:840182 [4] NCCL INFO Channel 01/1 : 4[4] -> 5[5] via P2P/IPC
g31n03:688509:688906 [0] NCCL INFO Channel 00/1 : 2[0] -> 5[3] via P2P/IPC
g25n18:898852:899357 [0] NCCL INFO Channel 00/1 : 0[0] -> 3[3] via P2P/IPC
g25n18:898857:899352 [5] NCCL INFO Channel 00/1 : 5[5] -> 0[0] via P2P/IPC
g27n18:839787:840183 [3] NCCL INFO Channel 01/1 : 3[3] -> 4[4] via P2P/IPC
g27n18:839785:840185 [2] NCCL INFO Channel 01/1 : 2[2] -> 3[3] via P2P/IPC
g31n03:688514:688903 [5] NCCL INFO Channel 00/1 : 7[5] -> 2[0] via P2P/IPC
g27n18:839790:840181 [5] NCCL INFO Channel 00/1 : 5[5] -> 6[0] [send] via NET/IB/1/Shared
g27n18:839790:840181 [5] NCCL INFO Channel 01/1 : 5[5] -> 6[0] [send] via NET/IB/3/Shared
g31n03:688513:688902 [4] NCCL INFO Channel 00/1 : 6[4] -> 1[5] [send] via NET/IB/3/Shared
g31n03:688513:688902 [4] NCCL INFO Channel 01/1 : 6[4] -> 1[5] [send] via NET/IB/1/Shared
g31n05:704345:704721 [1] NCCL INFO comm 0x181596770 rank 7 nranks 8 cudaDev 1 nvmlDev 1 busId 405000 commId 0xf41855dae56807 - Init COMPLETE
g25n18:898852:899357 [0] NCCL INFO Channel 01/1 : 0[0] -> 3[3] via P2P/IPC
g31n05:704344:704722 [0] NCCL INFO comm 0x1aaa97d20 rank 6 nranks 8 cudaDev 0 nvmlDev 0 busId 404000 commId 0xf41855dae56807 - Init COMPLETE
g31n05:704344:704743 [0] NCCL INFO Channel 00/1 : 5[5] -> 6[0] [receive] via NET/IB/0/Shared
g31n05:704344:704743 [0] NCCL INFO Channel 01/1 : 5[5] -> 6[0] [receive] via NET/IB/0/Shared
g31n05:704344:704743 [0] NCCL INFO Channel 00/1 : 6[0] -> 7[1] via P2P/IPC
g28n03:859960:860349 [4] NCCL INFO Channel 00/1 : 6[4] -> 5[3] via P2P/IPC
g25n18:898857:899352 [5] NCCL INFO Channel 01/1 : 5[5] -> 0[0] via P2P/IPC
g28n03:859954:860352 [1] NCCL INFO Channel 00/1 : 3[1] -> 2[0] via P2P/IPC
g27n17:842061:842459 [1] NCCL INFO Channel 00/1 : 3[1] -> 7[5] via P2P/IPC
g28n03:859956:860353 [2] NCCL INFO Channel 00/1 : 4[2] -> 3[1] via P2P/IPC
g27n18:839782:840184 [0] NCCL INFO Channel 00/1 : 6[0] -> 0[0] [receive] via NET/IB/0/Shared
g27n18:839782:840184 [0] NCCL INFO Channel 01/1 : 6[0] -> 0[0] [receive] via NET/IB/2/Shared
g27n18:839782:840184 [0] NCCL INFO Channel 00/1 : 0[0] -> 2[2] via P2P/IPC
g31n03:688509:688906 [0] NCCL INFO Channel 01/1 : 2[0] -> 5[3] via P2P/IPC
g28n04:841287:841682 [4] NCCL INFO Channel 00/1 : 7[1] -> 4[4] [receive] via NET/IB/3/Shared
g28n04:841287:841682 [4] NCCL INFO Channel 01/1 : 7[1] -> 4[4] [receive] via NET/IB/1/Shared
g28n04:841287:841682 [4] NCCL INFO Channel 00/1 : 4[4] -> 1[1] via P2P/IPC
g31n05:704344:704743 [0] NCCL INFO Channel 01/1 : 6[0] -> 7[1] via P2P/IPC
g27n17:842067:842464 [4] NCCL INFO Channel 00/1 : 6[4] -> 2[0] via P2P/IPC
g31n03:688514:688903 [5] NCCL INFO Channel 01/1 : 7[5] -> 2[0] via P2P/IPC
g27n17:842063:842460 [2] NCCL INFO Channel 00/1 : 0[4] -> 4[2] [receive] via NET/IB/0/Shared
g27n17:842063:842460 [2] NCCL INFO Channel 01/1 : 0[4] -> 4[2] [receive] via NET/IB/2/Shared
g27n17:842063:842460 [2] NCCL INFO Channel 00/1 : 4[2] -> 0[4] [send] via NET/IB/0/Shared
g27n17:842063:842460 [2] NCCL INFO Channel 01/1 : 4[2] -> 0[4] [send] via NET/IB/2/Shared
g27n17:842064:842462 [3] NCCL INFO Channel 00/1 : 5[3] -> 0[4] [send] via NET/IB/1/Shared
g27n17:842064:842462 [3] NCCL INFO Channel 01/1 : 5[3] -> 0[4] [send] via NET/IB/3/Shared
g27n17:842064:842462 [3] NCCL INFO Channel 00/1 : 1[5] -> 5[3] [receive] via NET/IB/1/Shared
g27n17:842064:842462 [3] NCCL INFO Channel 01/1 : 1[5] -> 5[3] [receive] via NET/IB/3/Shared
g27n17:842064:842462 [3] NCCL INFO Channel 00/1 : 5[3] -> 1[5] [send] via NET/IB/1/Shared
g27n17:842064:842462 [3] NCCL INFO Channel 01/1 : 5[3] -> 1[5] [send] via NET/IB/3/Shared
g27n17:842064:842462 [3] NCCL INFO Channel 00/1 : 0[4] -> 5[3] [receive] via NET/IB/1/Shared
g27n17:842064:842462 [3] NCCL INFO Channel 01/1 : 0[4] -> 5[3] [receive] via NET/IB/3/Shared
g27n17:842064:842462 [3] NCCL INFO Channel 00/1 : 5[3] -> 2[0] via P2P/IPC
g28n03:859960:860349 [4] NCCL INFO Channel 01/1 : 6[4] -> 5[3] via P2P/IPC
g28n03:859954:860352 [1] NCCL INFO Channel 01/1 : 3[1] -> 2[0] via P2P/IPC
g28n03:859956:860353 [2] NCCL INFO Channel 01/1 : 4[2] -> 3[1] via P2P/IPC
g31n04:688228:688611 [4] NCCL INFO comm 0x1c05d1270 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xf41855dae56807 - Init COMPLETE
g28n03:859958:860351 [3] NCCL INFO Channel 00/1 : 5[3] -> 4[2] via P2P/IPC
g31n04:688229:688610 [5] NCCL INFO comm 0x18bfefa10 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xf41855dae56807 - Init COMPLETE
g28n03:859961:860350 [5] NCCL INFO Channel 01/1 : 7[5] -> 6[4] via P2P/IPC
g31n04:688227:688609 [3] NCCL INFO comm 0x1697e59f0 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xf41855dae56807 - Init COMPLETE
g27n17:842061:842459 [1] NCCL INFO Channel 01/1 : 3[1] -> 7[5] via P2P/IPC
g31n04:688226:688607 [2] NCCL INFO comm 0x1e908ca60 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 406000 commId 0xf41855dae56807 - Init COMPLETE
g28n04:841287:841682 [4] NCCL INFO Channel 01/1 : 4[4] -> 1[1] via P2P/IPC
g31n04:688224:688606 [0] NCCL INFO comm 0x18c0165b0 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 404000 commId 0xf41855dae56807 - Init COMPLETE
g31n04:688224:688628 [0] NCCL INFO Channel 00/1 : 7[1] -> 0[0] [receive] via NET/IB/0/Shared
g31n04:688224:688628 [0] NCCL INFO Channel 01/1 : 7[1] -> 0[0] [receive] via NET/IB/2/Shared
g31n04:688224:688628 [0] NCCL INFO Channel 00/1 : 0[0] -> 1[1] via P2P/IPC
g31n04:688225:688608 [1] NCCL INFO comm 0x1a7751e20 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 405000 commId 0xf41855dae56807 - Init COMPLETE
g27n18:839783:840186 [1] NCCL INFO Channel 00/1 : 7[1] -> 1[1] [receive] via NET/IB/2/Shared
g27n18:839783:840186 [1] NCCL INFO Channel 01/1 : 7[1] -> 1[1] [receive] via NET/IB/0/Shared
g27n18:839783:840186 [1] NCCL INFO Channel 00/1 : 1[1] -> 3[3] via P2P/IPC
g25n18:898856:899354 [4] NCCL INFO Channel 00/1 : 4[4] -> 7[1] [send] via NET/IB/3/Shared
g25n18:898856:899354 [4] NCCL INFO Channel 01/1 : 4[4] -> 7[1] [send] via NET/IB/1/Shared
g27n18:839782:840184 [0] NCCL INFO Channel 01/1 : 0[0] -> 2[2] via P2P/IPC
g28n04:841288:841683 [5] NCCL INFO Channel 00/1 : 5[5] -> 2[2] via P2P/IPC
g28n03:859953:860354 [0] NCCL INFO Channel 00/1 : 2[0] -> 1[5] [send] via NET/IB/0/Shared
g28n03:859953:860354 [0] NCCL INFO Channel 01/1 : 2[0] -> 1[5] [send] via NET/IB/2/Shared
g31n03:688513:688902 [4] NCCL INFO Channel 00/1 : 6[4] -> 2[0] via P2P/IPC
g25n18:898854:899356 [2] NCCL INFO Channel 00/1 : 6[0] -> 2[2] [receive] via NET/IB/0/Shared
g25n18:898854:899356 [2] NCCL INFO Channel 01/1 : 6[0] -> 2[2] [receive] via NET/IB/2/Shared
g25n18:898854:899356 [2] NCCL INFO Channel 00/1 : 2[2] -> 6[0] [send] via NET/IB/0/Shared
g25n18:898854:899356 [2] NCCL INFO Channel 01/1 : 2[2] -> 6[0] [send] via NET/IB/2/Shared
g31n03:688512:688904 [3] NCCL INFO Channel 00/1 : 5[3] -> 0[4] [send] via NET/IB/1/Shared
g31n03:688512:688904 [3] NCCL INFO Channel 01/1 : 5[3] -> 0[4] [send] via NET/IB/3/Shared
g31n03:688512:688904 [3] NCCL INFO Channel 00/1 : 1[5] -> 5[3] [receive] via NET/IB/1/Shared
g31n03:688512:688904 [3] NCCL INFO Channel 01/1 : 1[5] -> 5[3] [receive] via NET/IB/3/Shared
g31n03:688512:688904 [3] NCCL INFO Channel 00/1 : 5[3] -> 1[5] [send] via NET/IB/1/Shared
g31n03:688512:688904 [3] NCCL INFO Channel 01/1 : 5[3] -> 1[5] [send] via NET/IB/3/Shared
g31n03:688512:688904 [3] NCCL INFO Channel 00/1 : 0[4] -> 5[3] [receive] via NET/IB/1/Shared
g31n03:688512:688904 [3] NCCL INFO Channel 01/1 : 0[4] -> 5[3] [receive] via NET/IB/3/Shared
g31n03:688512:688904 [3] NCCL INFO Channel 00/1 : 5[3] -> 2[0] via P2P/IPC
g25n18:898855:899353 [3] NCCL INFO Channel 00/1 : 3[3] -> 6[0] [send] via NET/IB/1/Shared
g25n18:898855:899353 [3] NCCL INFO Channel 01/1 : 3[3] -> 6[0] [send] via NET/IB/3/Shared
g25n18:898855:899353 [3] NCCL INFO Channel 00/1 : 7[1] -> 3[3] [receive] via NET/IB/1/Shared
g25n18:898855:899353 [3] NCCL INFO Channel 01/1 : 7[1] -> 3[3] [receive] via NET/IB/3/Shared
g25n18:898855:899353 [3] NCCL INFO Channel 00/1 : 3[3] -> 7[1] [send] via NET/IB/1/Shared
g25n18:898855:899353 [3] NCCL INFO Channel 01/1 : 3[3] -> 7[1] [send] via NET/IB/3/Shared
g25n18:898855:899353 [3] NCCL INFO Channel 00/1 : 6[0] -> 3[3] [receive] via NET/IB/1/Shared
g25n18:898855:899353 [3] NCCL INFO Channel 01/1 : 6[0] -> 3[3] [receive] via NET/IB/3/Shared
g25n18:898855:899353 [3] NCCL INFO Channel 00/1 : 3[3] -> 0[0] via P2P/IPC
g27n17:842067:842464 [4] NCCL INFO Channel 01/1 : 6[4] -> 2[0] via P2P/IPC
g28n03:859958:860351 [3] NCCL INFO Channel 01/1 : 5[3] -> 4[2] via P2P/IPC
g28n04:841280:841687 [0] NCCL INFO Channel 00/1 : 0[0] -> 5[5] via P2P/IPC
g27n17:842064:842462 [3] NCCL INFO Channel 01/1 : 5[3] -> 2[0] via P2P/IPC
g28n02:792075:792487 [4] NCCL INFO Channel 00/1 : 0[4] -> 7[5] [send] via NET/IB/1/Shared
g28n02:792075:792487 [4] NCCL INFO Channel 01/1 : 0[4] -> 7[5] [send] via NET/IB/1/Shared
g31n03:688510:688907 [1] NCCL INFO Channel 00/1 : 3[1] -> 7[5] via P2P/IPC
g28n04:841283:841686 [2] NCCL INFO Channel 00/1 : 2[2] -> 7[1] [send] via NET/IB/0/Shared
g28n04:841283:841686 [2] NCCL INFO Channel 01/1 : 2[2] -> 7[1] [send] via NET/IB/2/Shared
g31n03:688511:688905 [2] NCCL INFO Channel 00/1 : 0[4] -> 4[2] [receive] via NET/IB/0/Shared
g31n03:688511:688905 [2] NCCL INFO Channel 01/1 : 0[4] -> 4[2] [receive] via NET/IB/2/Shared
g31n03:688511:688905 [2] NCCL INFO Channel 00/1 : 4[2] -> 0[4] [send] via NET/IB/0/Shared
g31n03:688511:688905 [2] NCCL INFO Channel 01/1 : 4[2] -> 0[4] [send] via NET/IB/2/Shared
g32n03:753096:753474 [0] NCCL INFO Connected all trees
g32n03:753096:753474 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g32n03:753096:753474 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g28n04:841281:841685 [1] NCCL INFO Channel 00/1 : 1[1] -> 6[0] [send] via NET/IB/2/Shared
g28n04:841281:841685 [1] NCCL INFO Channel 01/1 : 1[1] -> 6[0] [send] via NET/IB/0/Shared
g31n03:688510:688907 [1] NCCL INFO Channel 01/1 : 3[1] -> 7[5] via P2P/IPC
g28n04:841287:841682 [4] NCCL INFO Channel 00/1 : 6[0] -> 4[4] [receive] via NET/IB/3/Shared
g28n04:841287:841682 [4] NCCL INFO Channel 01/1 : 6[0] -> 4[4] [receive] via NET/IB/1/Shared
g28n04:841287:841682 [4] NCCL INFO Channel 00/1 : 4[4] -> 2[2] via P2P/IPC
g27n17:842060:842461 [0] NCCL INFO Channel 00/1 : 2[0] -> 6[4] via P2P/IPC
g28n04:841288:841683 [5] NCCL INFO Channel 01/1 : 5[5] -> 2[2] via P2P/IPC
g27n17:842068:842463 [5] NCCL INFO Channel 00/1 : 7[5] -> 3[1] via P2P/IPC
g28n04:841280:841687 [0] NCCL INFO Channel 01/1 : 0[0] -> 5[5] via P2P/IPC
g27n18:839783:840186 [1] NCCL INFO Channel 01/1 : 1[1] -> 3[3] via P2P/IPC
g27n18:839785:840185 [2] NCCL INFO Channel 00/1 : 2[2] -> 4[4] via P2P/IPC
g27n17:842060:842461 [0] NCCL INFO Channel 01/1 : 2[0] -> 6[4] via P2P/IPC
g27n17:842068:842463 [5] NCCL INFO Channel 01/1 : 7[5] -> 3[1] via P2P/IPC
g27n18:839787:840183 [3] NCCL INFO Channel 00/1 : 3[3] -> 5[5] via P2P/IPC
g31n02:736467:736866 [4] NCCL INFO Channel 00/1 : 6[4] -> 0[4] [receive] via NET/IB/1/Shared
g31n02:736467:736866 [4] NCCL INFO Channel 01/1 : 6[4] -> 0[4] [receive] via NET/IB/1/Shared
g31n02:736467:736866 [4] NCCL INFO Channel 00/1 : 0[4] -> 2[0] [send] via NET/IB/1/Shared
g31n02:736467:736866 [4] NCCL INFO Channel 01/1 : 0[4] -> 2[0] [send] via NET/IB/1/Shared
g31n02:736467:736866 [4] NCCL INFO Channel 00/1 : 5[3] -> 0[4] [receive] via NET/IB/1/Shared
g31n02:736467:736866 [4] NCCL INFO Channel 01/1 : 5[3] -> 0[4] [receive] via NET/IB/1/Shared
g31n02:736467:736866 [4] NCCL INFO Channel 00/1 : 0[4] -> 3[1] [send] via NET/IB/1/Shared
g31n02:736467:736866 [4] NCCL INFO Channel 01/1 : 0[4] -> 3[1] [send] via NET/IB/1/Shared
g31n02:736467:736866 [4] NCCL INFO Channel 00/1 : 4[2] -> 0[4] [receive] via NET/IB/1/Shared
g31n02:736467:736866 [4] NCCL INFO Channel 01/1 : 4[2] -> 0[4] [receive] via NET/IB/1/Shared
g31n02:736467:736866 [4] NCCL INFO Channel 00/1 : 0[4] -> 4[2] [send] via NET/IB/1/Shared
g31n02:736467:736866 [4] NCCL INFO Channel 01/1 : 0[4] -> 4[2] [send] via NET/IB/1/Shared
g31n02:736467:736866 [4] NCCL INFO Channel 00/1 : 3[1] -> 0[4] [receive] via NET/IB/1/Shared
g31n02:736467:736866 [4] NCCL INFO Channel 01/1 : 3[1] -> 0[4] [receive] via NET/IB/1/Shared
g31n02:736467:736866 [4] NCCL INFO Channel 00/1 : 0[4] -> 5[3] [send] via NET/IB/1/Shared
g31n02:736467:736866 [4] NCCL INFO Channel 01/1 : 0[4] -> 5[3] [send] via NET/IB/1/Shared
g28n04:841287:841682 [4] NCCL INFO Channel 01/1 : 4[4] -> 2[2] via P2P/IPC
g26n01:811331:811729 [0] NCCL INFO Channel 00/1 : 4[4] -> 6[0] [receive] via NET/IB/0/Shared
g26n01:811331:811729 [0] NCCL INFO Channel 01/1 : 4[4] -> 6[0] [receive] via NET/IB/0/Shared
g26n01:811331:811729 [0] NCCL INFO Channel 00/1 : 6[0] -> 0[0] [send] via NET/IB/0/Shared
g26n01:811331:811729 [0] NCCL INFO Channel 01/1 : 6[0] -> 0[0] [send] via NET/IB/0/Shared
g26n01:811331:811729 [0] NCCL INFO Channel 00/1 : 3[3] -> 6[0] [receive] via NET/IB/0/Shared
g26n01:811331:811729 [0] NCCL INFO Channel 01/1 : 3[3] -> 6[0] [receive] via NET/IB/0/Shared
g26n01:811331:811729 [0] NCCL INFO Channel 00/1 : 6[0] -> 1[1] [send] via NET/IB/0/Shared
g26n01:811331:811729 [0] NCCL INFO Channel 01/1 : 6[0] -> 1[1] [send] via NET/IB/0/Shared
g26n01:811331:811729 [0] NCCL INFO Channel 00/1 : 2[2] -> 6[0] [receive] via NET/IB/0/Shared
g26n01:811331:811729 [0] NCCL INFO Channel 01/1 : 2[2] -> 6[0] [receive] via NET/IB/0/Shared
g26n01:811331:811729 [0] NCCL INFO Channel 00/1 : 6[0] -> 2[2] [send] via NET/IB/0/Shared
g26n01:811331:811729 [0] NCCL INFO Channel 01/1 : 6[0] -> 2[2] [send] via NET/IB/0/Shared
g26n01:811331:811729 [0] NCCL INFO Channel 00/1 : 1[1] -> 6[0] [receive] via NET/IB/0/Shared
g26n01:811331:811729 [0] NCCL INFO Channel 01/1 : 1[1] -> 6[0] [receive] via NET/IB/0/Shared
g26n01:811331:811729 [0] NCCL INFO Channel 00/1 : 6[0] -> 3[3] [send] via NET/IB/0/Shared
g26n01:811331:811729 [0] NCCL INFO Channel 01/1 : 6[0] -> 3[3] [send] via NET/IB/0/Shared
g31n02:736468:736867 [5] NCCL INFO Channel 00/1 : 1[5] -> 2[0] [send] via NET/IB/3/Shared
g31n02:736468:736867 [5] NCCL INFO Channel 01/1 : 1[5] -> 2[0] [send] via NET/IB/3/Shared
g31n02:736468:736867 [5] NCCL INFO Channel 00/1 : 7[5] -> 1[5] [receive] via NET/IB/3/Shared
g31n02:736468:736867 [5] NCCL INFO Channel 01/1 : 7[5] -> 1[5] [receive] via NET/IB/3/Shared
g31n02:736468:736867 [5] NCCL INFO Channel 00/1 : 1[5] -> 3[1] [send] via NET/IB/3/Shared
g31n02:736468:736867 [5] NCCL INFO Channel 01/1 : 1[5] -> 3[1] [send] via NET/IB/3/Shared
g31n02:736468:736867 [5] NCCL INFO Channel 00/1 : 6[4] -> 1[5] [receive] via NET/IB/3/Shared
g31n02:736468:736867 [5] NCCL INFO Channel 01/1 : 6[4] -> 1[5] [receive] via NET/IB/3/Shared
g31n02:736468:736867 [5] NCCL INFO Channel 00/1 : 1[5] -> 4[2] [send] via NET/IB/3/Shared
g31n02:736468:736867 [5] NCCL INFO Channel 01/1 : 1[5] -> 4[2] [send] via NET/IB/3/Shared
g31n02:736468:736867 [5] NCCL INFO Channel 00/1 : 5[3] -> 1[5] [receive] via NET/IB/3/Shared
g31n02:736468:736867 [5] NCCL INFO Channel 01/1 : 5[3] -> 1[5] [receive] via NET/IB/3/Shared
g31n02:736468:736867 [5] NCCL INFO Channel 00/1 : 1[5] -> 5[3] [send] via NET/IB/3/Shared
g31n02:736468:736867 [5] NCCL INFO Channel 01/1 : 1[5] -> 5[3] [send] via NET/IB/3/Shared
g31n02:736468:736867 [5] NCCL INFO Channel 00/1 : 4[2] -> 1[5] [receive] via NET/IB/3/Shared
g31n02:736468:736867 [5] NCCL INFO Channel 01/1 : 4[2] -> 1[5] [receive] via NET/IB/3/Shared
g31n02:736468:736867 [5] NCCL INFO Channel 00/1 : 1[5] -> 6[4] [send] via NET/IB/3/Shared
g31n02:736468:736867 [5] NCCL INFO Channel 01/1 : 1[5] -> 6[4] [send] via NET/IB/3/Shared
g25n18:898855:899353 [3] NCCL INFO Channel 01/1 : 3[3] -> 0[0] via P2P/IPC
g26n01:811332:811728 [1] NCCL INFO Channel 00/1 : 7[1] -> 0[0] [send] via NET/IB/2/Shared
g26n01:811332:811728 [1] NCCL INFO Channel 01/1 : 7[1] -> 0[0] [send] via NET/IB/2/Shared
g26n01:811332:811728 [1] NCCL INFO Channel 00/1 : 5[5] -> 7[1] [receive] via NET/IB/2/Shared
g26n01:811332:811728 [1] NCCL INFO Channel 01/1 : 5[5] -> 7[1] [receive] via NET/IB/2/Shared
g26n01:811332:811728 [1] NCCL INFO Channel 00/1 : 7[1] -> 1[1] [send] via NET/IB/2/Shared
g26n01:811332:811728 [1] NCCL INFO Channel 01/1 : 7[1] -> 1[1] [send] via NET/IB/2/Shared
g26n01:811332:811728 [1] NCCL INFO Channel 00/1 : 4[4] -> 7[1] [receive] via NET/IB/2/Shared
g26n01:811332:811728 [1] NCCL INFO Channel 01/1 : 4[4] -> 7[1] [receive] via NET/IB/2/Shared
g26n01:811332:811728 [1] NCCL INFO Channel 00/1 : 7[1] -> 2[2] [send] via NET/IB/2/Shared
g26n01:811332:811728 [1] NCCL INFO Channel 01/1 : 7[1] -> 2[2] [send] via NET/IB/2/Shared
g26n01:811332:811728 [1] NCCL INFO Channel 00/1 : 3[3] -> 7[1] [receive] via NET/IB/2/Shared
g26n01:811332:811728 [1] NCCL INFO Channel 01/1 : 3[3] -> 7[1] [receive] via NET/IB/2/Shared
g26n01:811332:811728 [1] NCCL INFO Channel 00/1 : 7[1] -> 3[3] [send] via NET/IB/2/Shared
g26n01:811332:811728 [1] NCCL INFO Channel 01/1 : 7[1] -> 3[3] [send] via NET/IB/2/Shared
g26n01:811332:811728 [1] NCCL INFO Channel 00/1 : 2[2] -> 7[1] [receive] via NET/IB/2/Shared
g26n01:811332:811728 [1] NCCL INFO Channel 01/1 : 2[2] -> 7[1] [receive] via NET/IB/2/Shared
g26n01:811332:811728 [1] NCCL INFO Channel 00/1 : 7[1] -> 4[4] [send] via NET/IB/2/Shared
g26n01:811332:811728 [1] NCCL INFO Channel 01/1 : 7[1] -> 4[4] [send] via NET/IB/2/Shared
g25n18:898857:899352 [5] NCCL INFO Channel 00/1 : 5[5] -> 1[1] via P2P/IPC
g27n18:839785:840185 [2] NCCL INFO Channel 01/1 : 2[2] -> 4[4] via P2P/IPC
g31n03:688513:688902 [4] NCCL INFO Channel 01/1 : 6[4] -> 2[0] via P2P/IPC
g27n18:839787:840183 [3] NCCL INFO Channel 01/1 : 3[3] -> 5[5] via P2P/IPC
g31n04:688224:688628 [0] NCCL INFO Channel 01/1 : 0[0] -> 1[1] via P2P/IPC
g31n03:688512:688904 [3] NCCL INFO Channel 01/1 : 5[3] -> 2[0] via P2P/IPC
g32n02:753687:754066 [4] NCCL INFO Connected all trees
g32n02:753687:754066 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
g32n02:753687:754066 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g31n04:688228:688624 [4] NCCL INFO Channel 00/1 : 4[4] -> 5[5] via P2P/IPC
g31n04:688227:688626 [3] NCCL INFO Channel 00/1 : 3[3] -> 4[4] via P2P/IPC
g31n04:688226:688629 [2] NCCL INFO Channel 00/1 : 2[2] -> 3[3] via P2P/IPC
g31n04:688225:688627 [1] NCCL INFO Channel 00/1 : 1[1] -> 2[2] via P2P/IPC
g31n04:688229:688625 [5] NCCL INFO Channel 00/1 : 5[5] -> 6[0] [send] via NET/IB/1/Shared
g31n04:688229:688625 [5] NCCL INFO Channel 01/1 : 5[5] -> 6[0] [send] via NET/IB/3/Shared
g31n03:688509:688906 [0] NCCL INFO Channel 00/1 : 2[0] -> 6[4] via P2P/IPC
g31n04:688227:688626 [3] NCCL INFO Channel 01/1 : 3[3] -> 4[4] via P2P/IPC
g31n03:688514:688903 [5] NCCL INFO Channel 00/1 : 7[5] -> 3[1] via P2P/IPC
g28n04:841288:841683 [5] NCCL INFO Channel 00/1 : 7[1] -> 5[5] [receive] via NET/IB/1/Shared
g28n04:841288:841683 [5] NCCL INFO Channel 01/1 : 7[1] -> 5[5] [receive] via NET/IB/3/Shared
g28n04:841288:841683 [5] NCCL INFO Channel 00/1 : 5[5] -> 3[3] via P2P/IPC
g25n18:898853:899355 [1] NCCL INFO Channel 00/1 : 1[1] -> 5[5] via P2P/IPC
g25n18:898856:899354 [4] NCCL INFO Channel 00/1 : 4[4] -> 0[0] via P2P/IPC
g31n04:688226:688629 [2] NCCL INFO Channel 01/1 : 2[2] -> 3[3] via P2P/IPC
g31n01:617757:618161 [1] NCCL INFO Channel 00/1 : 7[1] -> 0[0] [send] via NET/IB/2/Shared
g31n01:617757:618161 [1] NCCL INFO Channel 01/1 : 7[1] -> 0[0] [send] via NET/IB/2/Shared
g31n01:617757:618161 [1] NCCL INFO Channel 00/1 : 5[5] -> 7[1] [receive] via NET/IB/2/Shared
g31n01:617757:618161 [1] NCCL INFO Channel 01/1 : 5[5] -> 7[1] [receive] via NET/IB/2/Shared
g31n01:617757:618161 [1] NCCL INFO Channel 00/1 : 7[1] -> 1[1] [send] via NET/IB/2/Shared
g31n01:617757:618161 [1] NCCL INFO Channel 01/1 : 7[1] -> 1[1] [send] via NET/IB/2/Shared
g31n01:617757:618161 [1] NCCL INFO Channel 00/1 : 4[4] -> 7[1] [receive] via NET/IB/2/Shared
g31n01:617757:618161 [1] NCCL INFO Channel 01/1 : 4[4] -> 7[1] [receive] via NET/IB/2/Shared
g31n01:617757:618161 [1] NCCL INFO Channel 00/1 : 7[1] -> 2[2] [send] via NET/IB/2/Shared
g31n01:617757:618161 [1] NCCL INFO Channel 01/1 : 7[1] -> 2[2] [send] via NET/IB/2/Shared
g31n01:617757:618161 [1] NCCL INFO Channel 00/1 : 3[3] -> 7[1] [receive] via NET/IB/2/Shared
g31n01:617757:618161 [1] NCCL INFO Channel 01/1 : 3[3] -> 7[1] [receive] via NET/IB/2/Shared
g31n01:617757:618161 [1] NCCL INFO Channel 00/1 : 7[1] -> 3[3] [send] via NET/IB/2/Shared
g31n01:617757:618161 [1] NCCL INFO Channel 01/1 : 7[1] -> 3[3] [send] via NET/IB/2/Shared
g31n01:617757:618161 [1] NCCL INFO Channel 00/1 : 2[2] -> 7[1] [receive] via NET/IB/2/Shared
g31n01:617757:618161 [1] NCCL INFO Channel 01/1 : 2[2] -> 7[1] [receive] via NET/IB/2/Shared
g31n01:617757:618161 [1] NCCL INFO Channel 00/1 : 7[1] -> 4[4] [send] via NET/IB/2/Shared
g31n01:617757:618161 [1] NCCL INFO Channel 01/1 : 7[1] -> 4[4] [send] via NET/IB/2/Shared
g31n01:617757:618161 [1] NCCL INFO Channel 00/1 : 1[1] -> 7[1] [receive] via NET/IB/2/Shared
g31n01:617757:618161 [1] NCCL INFO Channel 01/1 : 1[1] -> 7[1] [receive] via NET/IB/2/Shared
g31n01:617757:618161 [1] NCCL INFO Channel 00/1 : 7[1] -> 5[5] [send] via NET/IB/2/Shared
g31n01:617757:618161 [1] NCCL INFO Channel 01/1 : 7[1] -> 5[5] [send] via NET/IB/2/Shared
g25n18:898852:899357 [0] NCCL INFO Channel 00/1 : 0[0] -> 4[4] via P2P/IPC
g31n04:688225:688627 [1] NCCL INFO Channel 01/1 : 1[1] -> 2[2] via P2P/IPC
g27n17:842067:842464 [4] NCCL INFO Channel 00/1 : 1[5] -> 6[4] [receive] via NET/IB/3/Shared
g27n17:842067:842464 [4] NCCL INFO Channel 01/1 : 1[5] -> 6[4] [receive] via NET/IB/1/Shared
g27n17:842067:842464 [4] NCCL INFO Channel 00/1 : 6[4] -> 3[1] via P2P/IPC
g31n01:617756:618160 [0] NCCL INFO Channel 00/1 : 4[4] -> 6[0] [receive] via NET/IB/0/Shared
g31n01:617756:618160 [0] NCCL INFO Channel 01/1 : 4[4] -> 6[0] [receive] via NET/IB/0/Shared
g31n01:617756:618160 [0] NCCL INFO Channel 00/1 : 6[0] -> 0[0] [send] via NET/IB/0/Shared
g31n01:617756:618160 [0] NCCL INFO Channel 01/1 : 6[0] -> 0[0] [send] via NET/IB/0/Shared
g31n01:617756:618160 [0] NCCL INFO Channel 00/1 : 3[3] -> 6[0] [receive] via NET/IB/0/Shared
g31n01:617756:618160 [0] NCCL INFO Channel 01/1 : 3[3] -> 6[0] [receive] via NET/IB/0/Shared
g31n01:617756:618160 [0] NCCL INFO Channel 00/1 : 6[0] -> 1[1] [send] via NET/IB/0/Shared
g31n01:617756:618160 [0] NCCL INFO Channel 01/1 : 6[0] -> 1[1] [send] via NET/IB/0/Shared
g31n01:617756:618160 [0] NCCL INFO Channel 00/1 : 2[2] -> 6[0] [receive] via NET/IB/0/Shared
g31n01:617756:618160 [0] NCCL INFO Channel 01/1 : 2[2] -> 6[0] [receive] via NET/IB/0/Shared
g31n01:617756:618160 [0] NCCL INFO Channel 00/1 : 6[0] -> 2[2] [send] via NET/IB/0/Shared
g31n01:617756:618160 [0] NCCL INFO Channel 01/1 : 6[0] -> 2[2] [send] via NET/IB/0/Shared
g31n01:617756:618160 [0] NCCL INFO Channel 00/1 : 1[1] -> 6[0] [receive] via NET/IB/0/Shared
g31n01:617756:618160 [0] NCCL INFO Channel 01/1 : 1[1] -> 6[0] [receive] via NET/IB/0/Shared
g31n01:617756:618160 [0] NCCL INFO Channel 00/1 : 6[0] -> 3[3] [send] via NET/IB/0/Shared
g31n01:617756:618160 [0] NCCL INFO Channel 01/1 : 6[0] -> 3[3] [send] via NET/IB/0/Shared
g31n01:617756:618160 [0] NCCL INFO Channel 00/1 : 0[0] -> 6[0] [receive] via NET/IB/0/Shared
g31n01:617756:618160 [0] NCCL INFO Channel 01/1 : 0[0] -> 6[0] [receive] via NET/IB/0/Shared
g31n01:617756:618160 [0] NCCL INFO Channel 00/1 : 6[0] -> 4[4] [send] via NET/IB/0/Shared
g31n01:617756:618160 [0] NCCL INFO Channel 01/1 : 6[0] -> 4[4] [send] via NET/IB/0/Shared
g31n03:688509:688906 [0] NCCL INFO Channel 01/1 : 2[0] -> 6[4] via P2P/IPC
g26n02:851594:851992 [5] NCCL INFO Channel 00/1 : 1[5] -> 2[0] [send] via NET/IB/3/Shared
g26n02:851594:851992 [5] NCCL INFO Channel 01/1 : 1[5] -> 2[0] [send] via NET/IB/3/Shared
g26n02:851594:851992 [5] NCCL INFO Channel 00/1 : 7[5] -> 1[5] [receive] via NET/IB/3/Shared
g26n02:851594:851992 [5] NCCL INFO Channel 01/1 : 7[5] -> 1[5] [receive] via NET/IB/3/Shared
g26n02:851594:851992 [5] NCCL INFO Channel 00/1 : 1[5] -> 3[1] [send] via NET/IB/3/Shared
g26n02:851594:851992 [5] NCCL INFO Channel 01/1 : 1[5] -> 3[1] [send] via NET/IB/3/Shared
g26n02:851594:851992 [5] NCCL INFO Channel 00/1 : 6[4] -> 1[5] [receive] via NET/IB/3/Shared
g26n02:851594:851992 [5] NCCL INFO Channel 01/1 : 6[4] -> 1[5] [receive] via NET/IB/3/Shared
g26n02:851594:851992 [5] NCCL INFO Channel 00/1 : 1[5] -> 4[2] [send] via NET/IB/3/Shared
g26n02:851594:851992 [5] NCCL INFO Channel 01/1 : 1[5] -> 4[2] [send] via NET/IB/3/Shared
g26n02:851594:851992 [5] NCCL INFO Channel 00/1 : 5[3] -> 1[5] [receive] via NET/IB/3/Shared
g26n02:851594:851992 [5] NCCL INFO Channel 01/1 : 5[3] -> 1[5] [receive] via NET/IB/3/Shared
g26n02:851594:851992 [5] NCCL INFO Channel 00/1 : 1[5] -> 5[3] [send] via NET/IB/3/Shared
g26n02:851594:851992 [5] NCCL INFO Channel 01/1 : 1[5] -> 5[3] [send] via NET/IB/3/Shared
g26n02:851594:851992 [5] NCCL INFO Channel 00/1 : 4[2] -> 1[5] [receive] via NET/IB/3/Shared
g26n02:851594:851992 [5] NCCL INFO Channel 01/1 : 4[2] -> 1[5] [receive] via NET/IB/3/Shared
g26n02:851594:851992 [5] NCCL INFO Channel 00/1 : 1[5] -> 6[4] [send] via NET/IB/3/Shared
g26n02:851594:851992 [5] NCCL INFO Channel 01/1 : 1[5] -> 6[4] [send] via NET/IB/3/Shared
g31n03:688514:688903 [5] NCCL INFO Channel 01/1 : 7[5] -> 3[1] via P2P/IPC
g26n02:851593:851993 [4] NCCL INFO Channel 00/1 : 6[4] -> 0[4] [receive] via NET/IB/1/Shared
g26n02:851593:851993 [4] NCCL INFO Channel 01/1 : 6[4] -> 0[4] [receive] via NET/IB/1/Shared
g26n02:851593:851993 [4] NCCL INFO Channel 00/1 : 0[4] -> 2[0] [send] via NET/IB/1/Shared
g26n02:851593:851993 [4] NCCL INFO Channel 01/1 : 0[4] -> 2[0] [send] via NET/IB/1/Shared
g26n02:851593:851993 [4] NCCL INFO Channel 00/1 : 5[3] -> 0[4] [receive] via NET/IB/1/Shared
g26n02:851593:851993 [4] NCCL INFO Channel 01/1 : 5[3] -> 0[4] [receive] via NET/IB/1/Shared
g26n02:851593:851993 [4] NCCL INFO Channel 00/1 : 0[4] -> 3[1] [send] via NET/IB/1/Shared
g26n02:851593:851993 [4] NCCL INFO Channel 01/1 : 0[4] -> 3[1] [send] via NET/IB/1/Shared
g26n02:851593:851993 [4] NCCL INFO Channel 00/1 : 4[2] -> 0[4] [receive] via NET/IB/1/Shared
g26n02:851593:851993 [4] NCCL INFO Channel 01/1 : 4[2] -> 0[4] [receive] via NET/IB/1/Shared
g26n02:851593:851993 [4] NCCL INFO Channel 00/1 : 0[4] -> 4[2] [send] via NET/IB/1/Shared
g26n02:851593:851993 [4] NCCL INFO Channel 01/1 : 0[4] -> 4[2] [send] via NET/IB/1/Shared
g26n02:851593:851993 [4] NCCL INFO Channel 00/1 : 3[1] -> 0[4] [receive] via NET/IB/1/Shared
g26n02:851593:851993 [4] NCCL INFO Channel 01/1 : 3[1] -> 0[4] [receive] via NET/IB/1/Shared
g26n02:851593:851993 [4] NCCL INFO Channel 00/1 : 0[4] -> 5[3] [send] via NET/IB/1/Shared
g26n02:851593:851993 [4] NCCL INFO Channel 01/1 : 0[4] -> 5[3] [send] via NET/IB/1/Shared
g31n04:688228:688624 [4] NCCL INFO Channel 01/1 : 4[4] -> 5[5] via P2P/IPC
g25n18:898854:899356 [2] NCCL INFO Channel 00/1 : 2[2] -> 7[1] [send] via NET/IB/0/Shared
g25n18:898854:899356 [2] NCCL INFO Channel 01/1 : 2[2] -> 7[1] [send] via NET/IB/2/Shared
g27n18:839789:840182 [4] NCCL INFO Channel 00/1 : 4[4] -> 6[0] [send] via NET/IB/3/Shared
g27n18:839789:840182 [4] NCCL INFO Channel 01/1 : 4[4] -> 6[0] [send] via NET/IB/1/Shared
g28n01:770092:770492 [0] NCCL INFO Channel 00/1 : 4[4] -> 6[0] [receive] via NET/IB/0/Shared
g28n01:770092:770492 [0] NCCL INFO Channel 01/1 : 4[4] -> 6[0] [receive] via NET/IB/0/Shared
g28n01:770092:770492 [0] NCCL INFO Channel 00/1 : 6[0] -> 0[0] [send] via NET/IB/0/Shared
g28n01:770092:770492 [0] NCCL INFO Channel 01/1 : 6[0] -> 0[0] [send] via NET/IB/0/Shared
g28n01:770092:770492 [0] NCCL INFO Channel 00/1 : 3[3] -> 6[0] [receive] via NET/IB/0/Shared
g28n01:770092:770492 [0] NCCL INFO Channel 01/1 : 3[3] -> 6[0] [receive] via NET/IB/0/Shared
g28n01:770092:770492 [0] NCCL INFO Channel 00/1 : 6[0] -> 1[1] [send] via NET/IB/0/Shared
g28n01:770092:770492 [0] NCCL INFO Channel 01/1 : 6[0] -> 1[1] [send] via NET/IB/0/Shared
g28n01:770093:770493 [1] NCCL INFO Channel 00/1 : 7[1] -> 0[0] [send] via NET/IB/2/Shared
g28n01:770093:770493 [1] NCCL INFO Channel 01/1 : 7[1] -> 0[0] [send] via NET/IB/2/Shared
g28n01:770093:770493 [1] NCCL INFO Channel 00/1 : 5[5] -> 7[1] [receive] via NET/IB/2/Shared
g28n01:770093:770493 [1] NCCL INFO Channel 01/1 : 5[5] -> 7[1] [receive] via NET/IB/2/Shared
g28n01:770093:770493 [1] NCCL INFO Channel 00/1 : 7[1] -> 1[1] [send] via NET/IB/2/Shared
g28n01:770093:770493 [1] NCCL INFO Channel 01/1 : 7[1] -> 1[1] [send] via NET/IB/2/Shared
g28n01:770093:770493 [1] NCCL INFO Channel 00/1 : 4[4] -> 7[1] [receive] via NET/IB/2/Shared
g28n01:770093:770493 [1] NCCL INFO Channel 01/1 : 4[4] -> 7[1] [receive] via NET/IB/2/Shared
g28n01:770093:770493 [1] NCCL INFO Channel 00/1 : 7[1] -> 2[2] [send] via NET/IB/2/Shared
g28n01:770093:770493 [1] NCCL INFO Channel 01/1 : 7[1] -> 2[2] [send] via NET/IB/2/Shared
g25n18:898853:899355 [1] NCCL INFO Channel 01/1 : 1[1] -> 5[5] via P2P/IPC
g27n18:839790:840181 [5] NCCL INFO Channel 00/1 : 5[5] -> 7[1] [send] via NET/IB/1/Shared
g27n18:839790:840181 [5] NCCL INFO Channel 01/1 : 5[5] -> 7[1] [send] via NET/IB/3/Shared
g27n17:842067:842464 [4] NCCL INFO Channel 01/1 : 6[4] -> 3[1] via P2P/IPC
g25n18:898856:899354 [4] NCCL INFO Channel 01/1 : 4[4] -> 0[0] via P2P/IPC
g27n18:839783:840186 [1] NCCL INFO Channel 00/1 : 6[0] -> 1[1] [receive] via NET/IB/2/Shared
g27n18:839783:840186 [1] NCCL INFO Channel 01/1 : 6[0] -> 1[1] [receive] via NET/IB/0/Shared
g27n18:839783:840186 [1] NCCL INFO Channel 00/1 : 1[1] -> 4[4] via P2P/IPC
g25n18:898852:899357 [0] NCCL INFO Channel 01/1 : 0[0] -> 4[4] via P2P/IPC
g27n18:839785:840185 [2] NCCL INFO Channel 00/1 : 7[1] -> 2[2] [receive] via NET/IB/0/Shared
g27n18:839785:840185 [2] NCCL INFO Channel 01/1 : 7[1] -> 2[2] [receive] via NET/IB/2/Shared
g27n18:839785:840185 [2] NCCL INFO Channel 00/1 : 2[2] -> 5[5] via P2P/IPC
g25n18:898857:899352 [5] NCCL INFO Channel 01/1 : 5[5] -> 1[1] via P2P/IPC
g28n04:841288:841683 [5] NCCL INFO Channel 01/1 : 5[5] -> 3[3] via P2P/IPC
g31n01:617757:618161 [1] NCCL INFO Channel 00/1 : 0[0] -> 7[1] [receive] via NET/IB/2/Shared
g31n01:617757:618161 [1] NCCL INFO Channel 01/1 : 0[0] -> 7[1] [receive] via NET/IB/2/Shared
g31n01:617757:618161 [1] NCCL INFO Channel 00/1 : 7[1] -> 6[0] via P2P/IPC
g31n03:688513:688902 [4] NCCL INFO Channel 00/1 : 1[5] -> 6[4] [receive] via NET/IB/3/Shared
g31n03:688513:688902 [4] NCCL INFO Channel 01/1 : 1[5] -> 6[4] [receive] via NET/IB/1/Shared
g31n03:688513:688902 [4] NCCL INFO Channel 00/1 : 6[4] -> 3[1] via P2P/IPC
g28n04:841284:841684 [3] NCCL INFO Channel 00/1 : 3[3] -> 1[1] via P2P/IPC
g28n04:841283:841686 [2] NCCL INFO Channel 00/1 : 2[2] -> 0[0] via P2P/IPC
g27n17:842068:842463 [5] NCCL INFO Channel 00/1 : 7[5] -> 4[2] via P2P/IPC
g27n18:839785:840185 [2] NCCL INFO Channel 01/1 : 2[2] -> 5[5] via P2P/IPC
g27n17:842060:842461 [0] NCCL INFO Channel 00/1 : 2[0] -> 7[5] via P2P/IPC
g28n04:841284:841684 [3] NCCL INFO Channel 01/1 : 3[3] -> 1[1] via P2P/IPC
g27n17:842061:842459 [1] NCCL INFO Channel 00/1 : 3[1] -> 0[4] [send] via NET/IB/2/Shared
g27n17:842061:842459 [1] NCCL INFO Channel 01/1 : 3[1] -> 0[4] [send] via NET/IB/0/Shared
g27n17:842067:842464 [4] NCCL INFO Channel 00/1 : 0[4] -> 6[4] [receive] via NET/IB/3/Shared
g27n17:842067:842464 [4] NCCL INFO Channel 01/1 : 0[4] -> 6[4] [receive] via NET/IB/1/Shared
g27n17:842067:842464 [4] NCCL INFO Channel 00/1 : 6[4] -> 4[2] via P2P/IPC
g31n01:617757:618161 [1] NCCL INFO Channel 01/1 : 7[1] -> 6[0] via P2P/IPC
g28n04:841283:841686 [2] NCCL INFO Channel 01/1 : 2[2] -> 0[0] via P2P/IPC
g31n04:688224:688628 [0] NCCL INFO Channel 00/1 : 6[0] -> 0[0] [receive] via NET/IB/0/Shared
g31n04:688224:688628 [0] NCCL INFO Channel 01/1 : 6[0] -> 0[0] [receive] via NET/IB/2/Shared
g31n04:688224:688628 [0] NCCL INFO Channel 00/1 : 0[0] -> 2[2] via P2P/IPC
g31n04:688225:688627 [1] NCCL INFO Channel 00/1 : 7[1] -> 1[1] [receive] via NET/IB/2/Shared
g31n04:688225:688627 [1] NCCL INFO Channel 01/1 : 7[1] -> 1[1] [receive] via NET/IB/0/Shared
g31n04:688225:688627 [1] NCCL INFO Channel 00/1 : 1[1] -> 3[3] via P2P/IPC
g25n18:898856:899354 [4] NCCL INFO Channel 00/1 : 7[1] -> 4[4] [receive] via NET/IB/3/Shared
g25n18:898856:899354 [4] NCCL INFO Channel 01/1 : 7[1] -> 4[4] [receive] via NET/IB/1/Shared
g25n18:898856:899354 [4] NCCL INFO Channel 00/1 : 4[4] -> 1[1] via P2P/IPC
g27n18:839783:840186 [1] NCCL INFO Channel 01/1 : 1[1] -> 4[4] via P2P/IPC
g27n18:839787:840183 [3] NCCL INFO Channel 00/1 : 3[3] -> 6[0] [send] via NET/IB/1/Shared
g27n18:839787:840183 [3] NCCL INFO Channel 01/1 : 3[3] -> 6[0] [send] via NET/IB/3/Shared
g27n18:839789:840182 [4] NCCL INFO Channel 00/1 : 4[4] -> 7[1] [send] via NET/IB/3/Shared
g27n18:839789:840182 [4] NCCL INFO Channel 01/1 : 4[4] -> 7[1] [send] via NET/IB/1/Shared
g31n03:688513:688902 [4] NCCL INFO Channel 01/1 : 6[4] -> 3[1] via P2P/IPC
g27n17:842068:842463 [5] NCCL INFO Channel 01/1 : 7[5] -> 4[2] via P2P/IPC
g27n18:839782:840184 [0] NCCL INFO Channel 00/1 : 0[0] -> 3[3] via P2P/IPC
g27n17:842060:842461 [0] NCCL INFO Channel 01/1 : 2[0] -> 7[5] via P2P/IPC
g27n17:842067:842464 [4] NCCL INFO Channel 01/1 : 6[4] -> 4[2] via P2P/IPC
g25n18:898856:899354 [4] NCCL INFO Channel 01/1 : 4[4] -> 1[1] via P2P/IPC
g27n18:839790:840181 [5] NCCL INFO Channel 00/1 : 5[5] -> 0[0] via P2P/IPC
g32n02:753688:754067 [5] NCCL INFO comm 0x172ebf770 rank 1 nranks 8 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xb5380ca77106f99c - Init COMPLETE
g32n02:753687:754066 [4] NCCL INFO comm 0x1921bdbb0 rank 0 nranks 8 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xb5380ca77106f99c - Init COMPLETE
g32n02:753687:754088 [4] NCCL INFO Channel 00/1 : 7[5] -> 0[4] [receive] via NET/IB/1/Shared
g32n02:753687:754088 [4] NCCL INFO Channel 01/1 : 7[5] -> 0[4] [receive] via NET/IB/1/Shared
g32n02:753687:754088 [4] NCCL INFO Channel 00/1 : 0[4] -> 1[5] via P2P/IPC
g31n03:688514:688903 [5] NCCL INFO Channel 00/1 : 7[5] -> 4[2] via P2P/IPC
g31n04:688224:688628 [0] NCCL INFO Channel 01/1 : 0[0] -> 2[2] via P2P/IPC
g28n04:841288:841683 [5] NCCL INFO Channel 00/1 : 6[0] -> 5[5] [receive] via NET/IB/1/Shared
g28n04:841288:841683 [5] NCCL INFO Channel 01/1 : 6[0] -> 5[5] [receive] via NET/IB/3/Shared
g28n04:841288:841683 [5] NCCL INFO Channel 00/1 : 5[5] -> 4[4] via P2P/IPC
g31n03:688509:688906 [0] NCCL INFO Channel 00/1 : 2[0] -> 7[5] via P2P/IPC
g31n04:688225:688627 [1] NCCL INFO Channel 01/1 : 1[1] -> 3[3] via P2P/IPC
g28n04:841281:841685 [1] NCCL INFO Channel 00/1 : 1[1] -> 7[1] [send] via NET/IB/2/Shared
g28n04:841281:841685 [1] NCCL INFO Channel 01/1 : 1[1] -> 7[1] [send] via NET/IB/0/Shared
g31n03:688510:688907 [1] NCCL INFO Channel 00/1 : 3[1] -> 0[4] [send] via NET/IB/2/Shared
g31n03:688510:688907 [1] NCCL INFO Channel 01/1 : 3[1] -> 0[4] [send] via NET/IB/0/Shared
g25n18:898852:899357 [0] NCCL INFO Channel 00/1 : 0[0] -> 5[5] via P2P/IPC
g28n04:841280:841687 [0] NCCL INFO Channel 00/1 : 0[0] -> 6[0] [send] via NET/IB/0/Shared
g28n04:841280:841687 [0] NCCL INFO Channel 01/1 : 0[0] -> 6[0] [send] via NET/IB/2/Shared
g31n03:688513:688902 [4] NCCL INFO Channel 00/1 : 0[4] -> 6[4] [receive] via NET/IB/3/Shared
g31n03:688513:688902 [4] NCCL INFO Channel 01/1 : 0[4] -> 6[4] [receive] via NET/IB/1/Shared
g31n03:688513:688902 [4] NCCL INFO Channel 00/1 : 6[4] -> 4[2] via P2P/IPC
g25n18:898857:899352 [5] NCCL INFO Channel 00/1 : 5[5] -> 2[2] via P2P/IPC
g32n03:753099:753472 [3] NCCL INFO comm 0x16dc4e720 rank 5 nranks 8 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xb5380ca77106f99c - Init COMPLETE
g27n17:842063:842460 [2] NCCL INFO Channel 00/1 : 4[2] -> 1[5] [send] via NET/IB/0/Shared
g27n17:842063:842460 [2] NCCL INFO Channel 01/1 : 4[2] -> 1[5] [send] via NET/IB/2/Shared
g32n03:753101:753470 [5] NCCL INFO comm 0x1e9d0d8a0 rank 7 nranks 8 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xb5380ca77106f99c - Init COMPLETE
g32n02:753687:754088 [4] NCCL INFO Channel 01/1 : 0[4] -> 1[5] via P2P/IPC
g27n17:842068:842463 [5] NCCL INFO Channel 00/1 : 1[5] -> 7[5] [receive] via NET/IB/1/Shared
g27n17:842068:842463 [5] NCCL INFO Channel 01/1 : 1[5] -> 7[5] [receive] via NET/IB/3/Shared
g27n17:842068:842463 [5] NCCL INFO Channel 00/1 : 7[5] -> 5[3] via P2P/IPC
g27n18:839782:840184 [0] NCCL INFO Channel 01/1 : 0[0] -> 3[3] via P2P/IPC
g32n03:753098:753473 [2] NCCL INFO comm 0x17bd77e50 rank 4 nranks 8 cudaDev 2 nvmlDev 2 busId 406000 commId 0xb5380ca77106f99c - Init COMPLETE
g31n04:688227:688626 [3] NCCL INFO Channel 00/1 : 3[3] -> 5[5] via P2P/IPC
g31n03:688514:688903 [5] NCCL INFO Channel 01/1 : 7[5] -> 4[2] via P2P/IPC
g31n05:704345:704742 [1] NCCL INFO Channel 00/1 : 7[1] -> 0[0] [send] via NET/IB/2/Shared
g31n05:704345:704742 [1] NCCL INFO Channel 01/1 : 7[1] -> 0[0] [send] via NET/IB/2/Shared
g31n05:704345:704742 [1] NCCL INFO Channel 00/1 : 5[5] -> 7[1] [receive] via NET/IB/2/Shared
g31n05:704345:704742 [1] NCCL INFO Channel 01/1 : 5[5] -> 7[1] [receive] via NET/IB/2/Shared
g31n05:704345:704742 [1] NCCL INFO Channel 00/1 : 7[1] -> 1[1] [send] via NET/IB/2/Shared
g31n05:704345:704742 [1] NCCL INFO Channel 01/1 : 7[1] -> 1[1] [send] via NET/IB/2/Shared
g31n05:704345:704742 [1] NCCL INFO Channel 00/1 : 4[4] -> 7[1] [receive] via NET/IB/2/Shared
g31n05:704345:704742 [1] NCCL INFO Channel 01/1 : 4[4] -> 7[1] [receive] via NET/IB/2/Shared
g31n05:704345:704742 [1] NCCL INFO Channel 00/1 : 7[1] -> 2[2] [send] via NET/IB/2/Shared
g31n05:704345:704742 [1] NCCL INFO Channel 01/1 : 7[1] -> 2[2] [send] via NET/IB/2/Shared
g32n03:753097:753475 [1] NCCL INFO comm 0x1f46e03e0 rank 3 nranks 8 cudaDev 1 nvmlDev 1 busId 405000 commId 0xb5380ca77106f99c - Init COMPLETE
g31n04:688226:688629 [2] NCCL INFO Channel 00/1 : 2[2] -> 4[4] via P2P/IPC
g31n03:688509:688906 [0] NCCL INFO Channel 01/1 : 2[0] -> 7[5] via P2P/IPC
g27n18:839790:840181 [5] NCCL INFO Channel 01/1 : 5[5] -> 0[0] via P2P/IPC
g31n03:688513:688902 [4] NCCL INFO Channel 01/1 : 6[4] -> 4[2] via P2P/IPC
g25n18:898853:899355 [1] NCCL INFO Channel 00/1 : 1[1] -> 6[0] [send] via NET/IB/2/Shared
g25n18:898853:899355 [1] NCCL INFO Channel 01/1 : 1[1] -> 6[0] [send] via NET/IB/0/Shared
g27n17:842068:842463 [5] NCCL INFO Channel 01/1 : 7[5] -> 5[3] via P2P/IPC
g25n18:898856:899354 [4] NCCL INFO Channel 00/1 : 6[0] -> 4[4] [receive] via NET/IB/3/Shared
g25n18:898856:899354 [4] NCCL INFO Channel 01/1 : 6[0] -> 4[4] [receive] via NET/IB/1/Shared
g25n18:898856:899354 [4] NCCL INFO Channel 00/1 : 4[4] -> 2[2] via P2P/IPC
g31n01:617756:618160 [0] NCCL INFO Channel 00/1 : 6[0] -> 5[5] [send] via NET/IB/0/Shared
g31n01:617756:618160 [0] NCCL INFO Channel 01/1 : 6[0] -> 5[5] [send] via NET/IB/0/Shared
g32n03:753100:753471 [4] NCCL INFO comm 0x16f7c4820 rank 6 nranks 8 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xb5380ca77106f99c - Init COMPLETE
g31n04:688226:688629 [2] NCCL INFO Channel 01/1 : 2[2] -> 4[4] via P2P/IPC
g31n04:688227:688626 [3] NCCL INFO Channel 01/1 : 3[3] -> 5[5] via P2P/IPC
g25n18:898852:899357 [0] NCCL INFO Channel 01/1 : 0[0] -> 5[5] via P2P/IPC
g28n04:841288:841683 [5] NCCL INFO Channel 01/1 : 5[5] -> 4[4] via P2P/IPC
g27n17:842063:842460 [2] NCCL INFO Channel 00/1 : 4[2] -> 2[0] via P2P/IPC
g32n03:753096:753474 [0] NCCL INFO comm 0x1ae639330 rank 2 nranks 8 cudaDev 0 nvmlDev 0 busId 404000 commId 0xb5380ca77106f99c - Init COMPLETE
g32n03:753096:753492 [0] NCCL INFO Channel 00/1 : 1[5] -> 2[0] [receive] via NET/IB/0/Shared
g32n03:753096:753492 [0] NCCL INFO Channel 01/1 : 1[5] -> 2[0] [receive] via NET/IB/2/Shared
g32n03:753096:753492 [0] NCCL INFO Channel 00/1 : 2[0] -> 3[1] via P2P/IPC
g25n18:898857:899352 [5] NCCL INFO Channel 01/1 : 5[5] -> 2[2] via P2P/IPC
g31n03:688511:688905 [2] NCCL INFO Channel 00/1 : 4[2] -> 1[5] [send] via NET/IB/0/Shared
g31n03:688511:688905 [2] NCCL INFO Channel 01/1 : 4[2] -> 1[5] [send] via NET/IB/2/Shared
g25n18:898856:899354 [4] NCCL INFO Channel 01/1 : 4[4] -> 2[2] via P2P/IPC
g28n04:841287:841682 [4] NCCL INFO Channel 00/1 : 4[4] -> 3[3] via P2P/IPC
g31n03:688514:688903 [5] NCCL INFO Channel 00/1 : 1[5] -> 7[5] [receive] via NET/IB/1/Shared
g31n03:688514:688903 [5] NCCL INFO Channel 01/1 : 1[5] -> 7[5] [receive] via NET/IB/3/Shared
g31n03:688514:688903 [5] NCCL INFO Channel 00/1 : 7[5] -> 5[3] via P2P/IPC
g28n04:841281:841685 [1] NCCL INFO Channel 00/1 : 1[1] -> 0[0] via P2P/IPC
g27n17:842064:842462 [3] NCCL INFO Channel 00/1 : 5[3] -> 3[1] via P2P/IPC
g27n18:839783:840186 [1] NCCL INFO Channel 00/1 : 1[1] -> 5[5] via P2P/IPC
g27n17:842063:842460 [2] NCCL INFO Channel 01/1 : 4[2] -> 2[0] via P2P/IPC
g27n18:839789:840182 [4] NCCL INFO Channel 00/1 : 4[4] -> 0[0] via P2P/IPC
g27n17:842064:842462 [3] NCCL INFO Channel 01/1 : 5[3] -> 3[1] via P2P/IPC
g27n18:839785:840185 [2] NCCL INFO Channel 00/1 : 6[0] -> 2[2] [receive] via NET/IB/0/Shared
g27n18:839785:840185 [2] NCCL INFO Channel 01/1 : 6[0] -> 2[2] [receive] via NET/IB/2/Shared
g27n18:839785:840185 [2] NCCL INFO Channel 00/1 : 2[2] -> 6[0] [send] via NET/IB/0/Shared
g27n18:839785:840185 [2] NCCL INFO Channel 01/1 : 2[2] -> 6[0] [send] via NET/IB/2/Shared
g27n18:839787:840183 [3] NCCL INFO Channel 00/1 : 7[1] -> 3[3] [receive] via NET/IB/1/Shared
g27n18:839787:840183 [3] NCCL INFO Channel 01/1 : 7[1] -> 3[3] [receive] via NET/IB/3/Shared
g27n18:839787:840183 [3] NCCL INFO Channel 00/1 : 3[3] -> 7[1] [send] via NET/IB/1/Shared
g27n18:839787:840183 [3] NCCL INFO Channel 01/1 : 3[3] -> 7[1] [send] via NET/IB/3/Shared
g27n18:839787:840183 [3] NCCL INFO Channel 00/1 : 6[0] -> 3[3] [receive] via NET/IB/1/Shared
g27n18:839787:840183 [3] NCCL INFO Channel 01/1 : 6[0] -> 3[3] [receive] via NET/IB/3/Shared
g27n18:839787:840183 [3] NCCL INFO Channel 00/1 : 3[3] -> 0[0] via P2P/IPC
g31n04:688228:688624 [4] NCCL INFO Channel 00/1 : 4[4] -> 6[0] [send] via NET/IB/3/Shared
g31n04:688228:688624 [4] NCCL INFO Channel 01/1 : 4[4] -> 6[0] [send] via NET/IB/1/Shared
g31n04:688229:688625 [5] NCCL INFO Channel 00/1 : 5[5] -> 7[1] [send] via NET/IB/1/Shared
g31n04:688229:688625 [5] NCCL INFO Channel 01/1 : 5[5] -> 7[1] [send] via NET/IB/3/Shared
g25n18:898857:899352 [5] NCCL INFO Channel 00/1 : 7[1] -> 5[5] [receive] via NET/IB/1/Shared
g25n18:898857:899352 [5] NCCL INFO Channel 01/1 : 7[1] -> 5[5] [receive] via NET/IB/3/Shared
g25n18:898857:899352 [5] NCCL INFO Channel 00/1 : 5[5] -> 3[3] via P2P/IPC
g31n04:688225:688627 [1] NCCL INFO Channel 00/1 : 6[0] -> 1[1] [receive] via NET/IB/2/Shared
g31n04:688225:688627 [1] NCCL INFO Channel 01/1 : 6[0] -> 1[1] [receive] via NET/IB/0/Shared
g31n04:688225:688627 [1] NCCL INFO Channel 00/1 : 1[1] -> 4[4] via P2P/IPC
g31n04:688226:688629 [2] NCCL INFO Channel 00/1 : 7[1] -> 2[2] [receive] via NET/IB/0/Shared
g31n04:688226:688629 [2] NCCL INFO Channel 01/1 : 7[1] -> 2[2] [receive] via NET/IB/2/Shared
g31n04:688226:688629 [2] NCCL INFO Channel 00/1 : 2[2] -> 5[5] via P2P/IPC
g26n02:851594:851992 [5] NCCL INFO Channel 00/1 : 3[1] -> 1[5] [receive] via NET/IB/3/Shared
g26n02:851594:851992 [5] NCCL INFO Channel 01/1 : 3[1] -> 1[5] [receive] via NET/IB/3/Shared
g26n02:851594:851992 [5] NCCL INFO Channel 00/1 : 1[5] -> 7[5] [send] via NET/IB/3/Shared
g26n02:851594:851992 [5] NCCL INFO Channel 01/1 : 1[5] -> 7[5] [send] via NET/IB/3/Shared
g26n02:851594:851992 [5] NCCL INFO Channel 00/1 : 2[0] -> 1[5] [receive] via NET/IB/3/Shared
g26n02:851594:851992 [5] NCCL INFO Channel 01/1 : 2[0] -> 1[5] [receive] via NET/IB/3/Shared
g26n02:851594:851992 [5] NCCL INFO Channel 00/1 : 1[5] -> 0[4] via P2P/IPC
g26n02:851593:851993 [4] NCCL INFO Channel 00/1 : 2[0] -> 0[4] [receive] via NET/IB/1/Shared
g26n02:851593:851993 [4] NCCL INFO Channel 01/1 : 2[0] -> 0[4] [receive] via NET/IB/1/Shared
g26n02:851593:851993 [4] NCCL INFO Channel 00/1 : 0[4] -> 6[4] [send] via NET/IB/1/Shared
g26n02:851593:851993 [4] NCCL INFO Channel 01/1 : 0[4] -> 6[4] [send] via NET/IB/1/Shared
g32n03:753096:753492 [0] NCCL INFO Channel 01/1 : 2[0] -> 3[1] via P2P/IPC
g28n04:841287:841682 [4] NCCL INFO Channel 01/1 : 4[4] -> 3[3] via P2P/IPC
g31n03:688514:688903 [5] NCCL INFO Channel 01/1 : 7[5] -> 5[3] via P2P/IPC
g32n03:753099:753489 [3] NCCL INFO Channel 00/1 : 5[3] -> 6[4] via P2P/IPC
g28n04:841281:841685 [1] NCCL INFO Channel 01/1 : 1[1] -> 0[0] via P2P/IPC
g32n03:753097:753493 [1] NCCL INFO Channel 00/1 : 3[1] -> 4[2] via P2P/IPC
g28n04:841283:841686 [2] NCCL INFO Channel 00/1 : 2[2] -> 1[1] via P2P/IPC
g32n03:753098:753491 [2] NCCL INFO Channel 00/1 : 4[2] -> 5[3] via P2P/IPC
g26n02:851594:851992 [5] NCCL INFO Channel 01/1 : 1[5] -> 0[4] via P2P/IPC
g31n03:688511:688905 [2] NCCL INFO Channel 00/1 : 4[2] -> 2[0] via P2P/IPC
g32n03:753100:753490 [4] NCCL INFO Channel 00/1 : 6[4] -> 7[5] via P2P/IPC
g27n18:839783:840186 [1] NCCL INFO Channel 01/1 : 1[1] -> 5[5] via P2P/IPC
g31n03:688512:688904 [3] NCCL INFO Channel 00/1 : 5[3] -> 3[1] via P2P/IPC
g25n18:898857:899352 [5] NCCL INFO Channel 01/1 : 5[5] -> 3[3] via P2P/IPC
g27n17:842060:842461 [0] NCCL INFO Channel 00/1 : 2[0] -> 0[4] [send] via NET/IB/0/Shared
g27n17:842060:842461 [0] NCCL INFO Channel 01/1 : 2[0] -> 0[4] [send] via NET/IB/2/Shared
g32n03:753099:753489 [3] NCCL INFO Channel 01/1 : 5[3] -> 6[4] via P2P/IPC
g27n18:839789:840182 [4] NCCL INFO Channel 01/1 : 4[4] -> 0[0] via P2P/IPC
g27n17:842061:842459 [1] NCCL INFO Channel 00/1 : 3[1] -> 1[5] [send] via NET/IB/2/Shared
g27n17:842061:842459 [1] NCCL INFO Channel 01/1 : 3[1] -> 1[5] [send] via NET/IB/0/Shared
g31n04:688225:688627 [1] NCCL INFO Channel 01/1 : 1[1] -> 4[4] via P2P/IPC
g32n03:753097:753493 [1] NCCL INFO Channel 01/1 : 3[1] -> 4[2] via P2P/IPC
g31n04:688226:688629 [2] NCCL INFO Channel 01/1 : 2[2] -> 5[5] via P2P/IPC
g27n17:842068:842463 [5] NCCL INFO Channel 00/1 : 0[4] -> 7[5] [receive] via NET/IB/1/Shared
g27n17:842068:842463 [5] NCCL INFO Channel 01/1 : 0[4] -> 7[5] [receive] via NET/IB/3/Shared
g27n17:842068:842463 [5] NCCL INFO Channel 00/1 : 7[5] -> 6[4] via P2P/IPC
g26n01:811332:811728 [1] NCCL INFO Channel 00/1 : 1[1] -> 7[1] [receive] via NET/IB/2/Shared
g26n01:811332:811728 [1] NCCL INFO Channel 01/1 : 1[1] -> 7[1] [receive] via NET/IB/2/Shared
g26n01:811332:811728 [1] NCCL INFO Channel 00/1 : 7[1] -> 5[5] [send] via NET/IB/2/Shared
g26n01:811332:811728 [1] NCCL INFO Channel 01/1 : 7[1] -> 5[5] [send] via NET/IB/2/Shared
g26n01:811332:811728 [1] NCCL INFO Channel 00/1 : 0[0] -> 7[1] [receive] via NET/IB/2/Shared
g26n01:811332:811728 [1] NCCL INFO Channel 01/1 : 0[0] -> 7[1] [receive] via NET/IB/2/Shared
g26n01:811332:811728 [1] NCCL INFO Channel 00/1 : 7[1] -> 6[0] via P2P/IPC
g32n03:753098:753491 [2] NCCL INFO Channel 01/1 : 4[2] -> 5[3] via P2P/IPC
g31n03:688511:688905 [2] NCCL INFO Channel 01/1 : 4[2] -> 2[0] via P2P/IPC
g27n18:839787:840183 [3] NCCL INFO Channel 01/1 : 3[3] -> 0[0] via P2P/IPC
g26n01:811331:811729 [0] NCCL INFO Channel 00/1 : 0[0] -> 6[0] [receive] via NET/IB/0/Shared
g26n01:811331:811729 [0] NCCL INFO Channel 01/1 : 0[0] -> 6[0] [receive] via NET/IB/0/Shared
g26n01:811331:811729 [0] NCCL INFO Channel 00/1 : 6[0] -> 4[4] [send] via NET/IB/0/Shared
g26n01:811331:811729 [0] NCCL INFO Channel 01/1 : 6[0] -> 4[4] [send] via NET/IB/0/Shared
g32n03:753100:753490 [4] NCCL INFO Channel 01/1 : 6[4] -> 7[5] via P2P/IPC
g31n03:688512:688904 [3] NCCL INFO Channel 01/1 : 5[3] -> 3[1] via P2P/IPC
g25n18:898855:899353 [3] NCCL INFO Channel 00/1 : 3[3] -> 1[1] via P2P/IPC
g26n01:811332:811728 [1] NCCL INFO Channel 01/1 : 7[1] -> 6[0] via P2P/IPC
g25n18:898854:899356 [2] NCCL INFO Channel 00/1 : 2[2] -> 0[0] via P2P/IPC
g28n04:841280:841687 [0] NCCL INFO Channel 00/1 : 0[0] -> 7[1] [send] via NET/IB/0/Shared
g28n04:841280:841687 [0] NCCL INFO Channel 01/1 : 0[0] -> 7[1] [send] via NET/IB/2/Shared
g28n04:841283:841686 [2] NCCL INFO Channel 01/1 : 2[2] -> 1[1] via P2P/IPC
g27n18:839782:840184 [0] NCCL INFO Channel 00/1 : 0[0] -> 4[4] via P2P/IPC
g31n02:736468:736867 [5] NCCL INFO Channel 00/1 : 3[1] -> 1[5] [receive] via NET/IB/3/Shared
g31n02:736468:736867 [5] NCCL INFO Channel 01/1 : 3[1] -> 1[5] [receive] via NET/IB/3/Shared
g31n02:736468:736867 [5] NCCL INFO Channel 00/1 : 1[5] -> 7[5] [send] via NET/IB/3/Shared
g31n02:736468:736867 [5] NCCL INFO Channel 01/1 : 1[5] -> 7[5] [send] via NET/IB/3/Shared
g31n02:736468:736867 [5] NCCL INFO Channel 00/1 : 2[0] -> 1[5] [receive] via NET/IB/3/Shared
g31n02:736468:736867 [5] NCCL INFO Channel 01/1 : 2[0] -> 1[5] [receive] via NET/IB/3/Shared
g31n02:736468:736867 [5] NCCL INFO Channel 00/1 : 1[5] -> 0[4] via P2P/IPC
g28n04:841284:841684 [3] NCCL INFO Channel 00/1 : 3[3] -> 2[2] via P2P/IPC
g27n18:839790:840181 [5] NCCL INFO Channel 00/1 : 5[5] -> 1[1] via P2P/IPC
g31n02:736467:736866 [4] NCCL INFO Channel 00/1 : 2[0] -> 0[4] [receive] via NET/IB/1/Shared
g31n02:736467:736866 [4] NCCL INFO Channel 01/1 : 2[0] -> 0[4] [receive] via NET/IB/1/Shared
g31n02:736467:736866 [4] NCCL INFO Channel 00/1 : 0[4] -> 6[4] [send] via NET/IB/1/Shared
g31n02:736467:736866 [4] NCCL INFO Channel 01/1 : 0[4] -> 6[4] [send] via NET/IB/1/Shared
g25n18:898855:899353 [3] NCCL INFO Channel 01/1 : 3[3] -> 1[1] via P2P/IPC
g31n04:688224:688628 [0] NCCL INFO Channel 00/1 : 0[0] -> 3[3] via P2P/IPC
g25n18:898854:899356 [2] NCCL INFO Channel 01/1 : 2[2] -> 0[0] via P2P/IPC
g27n18:839782:840184 [0] NCCL INFO Channel 01/1 : 0[0] -> 4[4] via P2P/IPC
g27n18:839790:840181 [5] NCCL INFO Channel 01/1 : 5[5] -> 1[1] via P2P/IPC
g31n02:736468:736867 [5] NCCL INFO Channel 01/1 : 1[5] -> 0[4] via P2P/IPC
g31n04:688229:688625 [5] NCCL INFO Channel 00/1 : 5[5] -> 0[0] via P2P/IPC
g27n17:842068:842463 [5] NCCL INFO Channel 01/1 : 7[5] -> 6[4] via P2P/IPC
g32n03:753101:753488 [5] NCCL INFO Channel 00/1 : 7[5] -> 0[4] [send] via NET/IB/1/Shared
g32n03:753101:753488 [5] NCCL INFO Channel 01/1 : 7[5] -> 0[4] [send] via NET/IB/3/Shared
g31n04:688228:688624 [4] NCCL INFO Channel 00/1 : 4[4] -> 7[1] [send] via NET/IB/3/Shared
g31n04:688228:688624 [4] NCCL INFO Channel 01/1 : 4[4] -> 7[1] [send] via NET/IB/1/Shared
g31n03:688509:688906 [0] NCCL INFO Channel 00/1 : 2[0] -> 0[4] [send] via NET/IB/0/Shared
g31n03:688509:688906 [0] NCCL INFO Channel 01/1 : 2[0] -> 0[4] [send] via NET/IB/2/Shared
g32n03:753096:753492 [0] NCCL INFO Channel 00/1 : 0[4] -> 2[0] [receive] via NET/IB/0/Shared
g32n03:753096:753492 [0] NCCL INFO Channel 01/1 : 0[4] -> 2[0] [receive] via NET/IB/2/Shared
g32n03:753096:753492 [0] NCCL INFO Channel 00/1 : 2[0] -> 4[2] via P2P/IPC
g31n04:688224:688628 [0] NCCL INFO Channel 01/1 : 0[0] -> 3[3] via P2P/IPC
g31n03:688510:688907 [1] NCCL INFO Channel 00/1 : 3[1] -> 1[5] [send] via NET/IB/2/Shared
g31n03:688510:688907 [1] NCCL INFO Channel 01/1 : 3[1] -> 1[5] [send] via NET/IB/0/Shared
g32n03:753097:753493 [1] NCCL INFO Channel 00/1 : 1[5] -> 3[1] [receive] via NET/IB/2/Shared
g32n03:753097:753493 [1] NCCL INFO Channel 01/1 : 1[5] -> 3[1] [receive] via NET/IB/0/Shared
g32n03:753097:753493 [1] NCCL INFO Channel 00/1 : 3[1] -> 5[3] via P2P/IPC
g27n18:839789:840182 [4] NCCL INFO Channel 00/1 : 7[1] -> 4[4] [receive] via NET/IB/3/Shared
g27n18:839789:840182 [4] NCCL INFO Channel 01/1 : 7[1] -> 4[4] [receive] via NET/IB/1/Shared
g27n18:839789:840182 [4] NCCL INFO Channel 00/1 : 4[4] -> 1[1] via P2P/IPC
g31n03:688514:688903 [5] NCCL INFO Channel 00/1 : 0[4] -> 7[5] [receive] via NET/IB/1/Shared
g31n03:688514:688903 [5] NCCL INFO Channel 01/1 : 0[4] -> 7[5] [receive] via NET/IB/3/Shared
g31n03:688514:688903 [5] NCCL INFO Channel 00/1 : 7[5] -> 6[4] via P2P/IPC
g31n04:688229:688625 [5] NCCL INFO Channel 01/1 : 5[5] -> 0[0] via P2P/IPC
g28n04:841284:841684 [3] NCCL INFO Channel 01/1 : 3[3] -> 2[2] via P2P/IPC
g27n17:842067:842464 [4] NCCL INFO Channel 00/1 : 6[4] -> 5[3] via P2P/IPC
g25n18:898853:899355 [1] NCCL INFO Channel 00/1 : 1[1] -> 7[1] [send] via NET/IB/2/Shared
g25n18:898853:899355 [1] NCCL INFO Channel 01/1 : 1[1] -> 7[1] [send] via NET/IB/0/Shared
g27n17:842063:842460 [2] NCCL INFO Channel 00/1 : 4[2] -> 3[1] via P2P/IPC
g25n18:898852:899357 [0] NCCL INFO Channel 00/1 : 0[0] -> 6[0] [send] via NET/IB/0/Shared
g25n18:898852:899357 [0] NCCL INFO Channel 01/1 : 0[0] -> 6[0] [send] via NET/IB/2/Shared
g27n17:842061:842459 [1] NCCL INFO Channel 00/1 : 3[1] -> 2[0] via P2P/IPC
g25n18:898857:899352 [5] NCCL INFO Channel 00/1 : 6[0] -> 5[5] [receive] via NET/IB/1/Shared
g25n18:898857:899352 [5] NCCL INFO Channel 01/1 : 6[0] -> 5[5] [receive] via NET/IB/3/Shared
g25n18:898857:899352 [5] NCCL INFO Channel 00/1 : 5[5] -> 4[4] via P2P/IPC
g27n17:842064:842462 [3] NCCL INFO Channel 00/1 : 5[3] -> 4[2] via P2P/IPC
g26n02:851593:851993 [4] NCCL INFO Channel 00/1 : 0[4] -> 7[5] [send] via NET/IB/1/Shared
g26n02:851593:851993 [4] NCCL INFO Channel 01/1 : 0[4] -> 7[5] [send] via NET/IB/1/Shared
g27n18:839789:840182 [4] NCCL INFO Channel 01/1 : 4[4] -> 1[1] via P2P/IPC
g31n04:688225:688627 [1] NCCL INFO Channel 00/1 : 1[1] -> 5[5] via P2P/IPC
g31n03:688514:688903 [5] NCCL INFO Channel 01/1 : 7[5] -> 6[4] via P2P/IPC
g27n17:842067:842464 [4] NCCL INFO Channel 01/1 : 6[4] -> 5[3] via P2P/IPC
g27n17:842061:842459 [1] NCCL INFO Channel 01/1 : 3[1] -> 2[0] via P2P/IPC
g32n03:753096:753492 [0] NCCL INFO Channel 01/1 : 2[0] -> 4[2] via P2P/IPC
g31n04:688228:688624 [4] NCCL INFO Channel 00/1 : 4[4] -> 0[0] via P2P/IPC
g27n17:842063:842460 [2] NCCL INFO Channel 01/1 : 4[2] -> 3[1] via P2P/IPC
g32n03:753097:753493 [1] NCCL INFO Channel 01/1 : 3[1] -> 5[3] via P2P/IPC
g27n18:839782:840184 [0] NCCL INFO Channel 00/1 : 0[0] -> 5[5] via P2P/IPC
g27n17:842064:842462 [3] NCCL INFO Channel 01/1 : 5[3] -> 4[2] via P2P/IPC
g31n04:688227:688626 [3] NCCL INFO Channel 00/1 : 3[3] -> 6[0] [send] via NET/IB/1/Shared
g31n04:688227:688626 [3] NCCL INFO Channel 01/1 : 3[3] -> 6[0] [send] via NET/IB/3/Shared
g31n04:688227:688626 [3] NCCL INFO Channel 00/1 : 7[1] -> 3[3] [receive] via NET/IB/1/Shared
g31n04:688227:688626 [3] NCCL INFO Channel 01/1 : 7[1] -> 3[3] [receive] via NET/IB/3/Shared
g31n04:688227:688626 [3] NCCL INFO Channel 00/1 : 3[3] -> 7[1] [send] via NET/IB/1/Shared
g31n04:688227:688626 [3] NCCL INFO Channel 01/1 : 3[3] -> 7[1] [send] via NET/IB/3/Shared
g31n04:688227:688626 [3] NCCL INFO Channel 00/1 : 6[0] -> 3[3] [receive] via NET/IB/1/Shared
g31n04:688227:688626 [3] NCCL INFO Channel 01/1 : 6[0] -> 3[3] [receive] via NET/IB/3/Shared
g31n04:688227:688626 [3] NCCL INFO Channel 00/1 : 3[3] -> 0[0] via P2P/IPC
g27n18:839790:840181 [5] NCCL INFO Channel 00/1 : 5[5] -> 2[2] via P2P/IPC
g31n04:688226:688629 [2] NCCL INFO Channel 00/1 : 6[0] -> 2[2] [receive] via NET/IB/0/Shared
g31n04:688226:688629 [2] NCCL INFO Channel 01/1 : 6[0] -> 2[2] [receive] via NET/IB/2/Shared
g31n04:688226:688629 [2] NCCL INFO Channel 00/1 : 2[2] -> 6[0] [send] via NET/IB/0/Shared
g31n04:688226:688629 [2] NCCL INFO Channel 01/1 : 2[2] -> 6[0] [send] via NET/IB/2/Shared
g31n02:736467:736866 [4] NCCL INFO Channel 00/1 : 0[4] -> 7[5] [send] via NET/IB/1/Shared
g31n02:736467:736866 [4] NCCL INFO Channel 01/1 : 0[4] -> 7[5] [send] via NET/IB/1/Shared
g25n18:898857:899352 [5] NCCL INFO Channel 01/1 : 5[5] -> 4[4] via P2P/IPC
g32n03:753098:753491 [2] NCCL INFO Channel 00/1 : 4[2] -> 6[4] via P2P/IPC
g32n03:753099:753489 [3] NCCL INFO Channel 00/1 : 5[3] -> 7[5] via P2P/IPC
g27n18:839783:840186 [1] NCCL INFO Channel 00/1 : 1[1] -> 6[0] [send] via NET/IB/2/Shared
g27n18:839783:840186 [1] NCCL INFO Channel 01/1 : 1[1] -> 6[0] [send] via NET/IB/0/Shared
g31n03:688510:688907 [1] NCCL INFO Channel 00/1 : 3[1] -> 2[0] via P2P/IPC
g27n18:839789:840182 [4] NCCL INFO Channel 00/1 : 6[0] -> 4[4] [receive] via NET/IB/3/Shared
g27n18:839789:840182 [4] NCCL INFO Channel 01/1 : 6[0] -> 4[4] [receive] via NET/IB/1/Shared
g27n18:839789:840182 [4] NCCL INFO Channel 00/1 : 4[4] -> 2[2] via P2P/IPC
g31n03:688513:688902 [4] NCCL INFO Channel 00/1 : 6[4] -> 5[3] via P2P/IPC
g27n18:839782:840184 [0] NCCL INFO Channel 01/1 : 0[0] -> 5[5] via P2P/IPC
g31n03:688512:688904 [3] NCCL INFO Channel 00/1 : 5[3] -> 4[2] via P2P/IPC
g27n18:839790:840181 [5] NCCL INFO Channel 01/1 : 5[5] -> 2[2] via P2P/IPC
g32n03:753098:753491 [2] NCCL INFO Channel 01/1 : 4[2] -> 6[4] via P2P/IPC
g31n04:688228:688624 [4] NCCL INFO Channel 01/1 : 4[4] -> 0[0] via P2P/IPC
g26n01:811331:811729 [0] NCCL INFO Channel 00/1 : 6[0] -> 5[5] [send] via NET/IB/0/Shared
g26n01:811331:811729 [0] NCCL INFO Channel 01/1 : 6[0] -> 5[5] [send] via NET/IB/0/Shared
g32n03:753099:753489 [3] NCCL INFO Channel 01/1 : 5[3] -> 7[5] via P2P/IPC
g31n04:688225:688627 [1] NCCL INFO Channel 01/1 : 1[1] -> 5[5] via P2P/IPC
g25n18:898853:899355 [1] NCCL INFO Channel 00/1 : 1[1] -> 0[0] via P2P/IPC
g25n18:898855:899353 [3] NCCL INFO Channel 00/1 : 3[3] -> 2[2] via P2P/IPC
g25n18:898854:899356 [2] NCCL INFO Channel 00/1 : 2[2] -> 1[1] via P2P/IPC
g31n03:688511:688905 [2] NCCL INFO Channel 00/1 : 4[2] -> 3[1] via P2P/IPC
g25n18:898856:899354 [4] NCCL INFO Channel 00/1 : 4[4] -> 3[3] via P2P/IPC
g31n03:688510:688907 [1] NCCL INFO Channel 01/1 : 3[1] -> 2[0] via P2P/IPC
g31n04:688229:688625 [5] NCCL INFO Channel 00/1 : 5[5] -> 1[1] via P2P/IPC
g31n03:688513:688902 [4] NCCL INFO Channel 01/1 : 6[4] -> 5[3] via P2P/IPC
g31n04:688224:688628 [0] NCCL INFO Channel 00/1 : 0[0] -> 4[4] via P2P/IPC
g31n03:688512:688904 [3] NCCL INFO Channel 01/1 : 5[3] -> 4[2] via P2P/IPC
g27n18:839789:840182 [4] NCCL INFO Channel 01/1 : 4[4] -> 2[2] via P2P/IPC
g31n04:688227:688626 [3] NCCL INFO Channel 01/1 : 3[3] -> 0[0] via P2P/IPC
g25n18:898853:899355 [1] NCCL INFO Channel 01/1 : 1[1] -> 0[0] via P2P/IPC
g25n18:898855:899353 [3] NCCL INFO Channel 01/1 : 3[3] -> 2[2] via P2P/IPC
g25n18:898856:899354 [4] NCCL INFO Channel 01/1 : 4[4] -> 3[3] via P2P/IPC
g32n03:753100:753490 [4] NCCL INFO Channel 00/1 : 6[4] -> 0[4] [send] via NET/IB/3/Shared
g32n03:753100:753490 [4] NCCL INFO Channel 01/1 : 6[4] -> 0[4] [send] via NET/IB/1/Shared
g25n18:898854:899356 [2] NCCL INFO Channel 01/1 : 2[2] -> 1[1] via P2P/IPC
g32n03:753101:753488 [5] NCCL INFO Channel 00/1 : 7[5] -> 1[5] [send] via NET/IB/1/Shared
g32n03:753101:753488 [5] NCCL INFO Channel 01/1 : 7[5] -> 1[5] [send] via NET/IB/3/Shared
g27n18:839785:840185 [2] NCCL INFO Channel 00/1 : 2[2] -> 7[1] [send] via NET/IB/0/Shared
g27n18:839785:840185 [2] NCCL INFO Channel 01/1 : 2[2] -> 7[1] [send] via NET/IB/2/Shared
g31n04:688229:688625 [5] NCCL INFO Channel 01/1 : 5[5] -> 1[1] via P2P/IPC
g32n03:753097:753493 [1] NCCL INFO Channel 00/1 : 0[4] -> 3[1] [receive] via NET/IB/2/Shared
g32n03:753097:753493 [1] NCCL INFO Channel 01/1 : 0[4] -> 3[1] [receive] via NET/IB/0/Shared
g32n03:753097:753493 [1] NCCL INFO Channel 00/1 : 3[1] -> 6[4] via P2P/IPC
g31n04:688224:688628 [0] NCCL INFO Channel 01/1 : 0[0] -> 4[4] via P2P/IPC
g31n03:688509:688906 [0] NCCL INFO Channel 00/1 : 2[0] -> 1[5] [send] via NET/IB/0/Shared
g31n03:688509:688906 [0] NCCL INFO Channel 01/1 : 2[0] -> 1[5] [send] via NET/IB/2/Shared
g27n18:839790:840181 [5] NCCL INFO Channel 00/1 : 7[1] -> 5[5] [receive] via NET/IB/1/Shared
g27n18:839790:840181 [5] NCCL INFO Channel 01/1 : 7[1] -> 5[5] [receive] via NET/IB/3/Shared
g27n18:839790:840181 [5] NCCL INFO Channel 00/1 : 5[5] -> 3[3] via P2P/IPC
g31n03:688511:688905 [2] NCCL INFO Channel 01/1 : 4[2] -> 3[1] via P2P/IPC
g25n18:898852:899357 [0] NCCL INFO Channel 00/1 : 0[0] -> 7[1] [send] via NET/IB/0/Shared
g25n18:898852:899357 [0] NCCL INFO Channel 01/1 : 0[0] -> 7[1] [send] via NET/IB/2/Shared
g27n17:842060:842461 [0] NCCL INFO Channel 00/1 : 2[0] -> 1[5] [send] via NET/IB/0/Shared
g27n17:842060:842461 [0] NCCL INFO Channel 01/1 : 2[0] -> 1[5] [send] via NET/IB/2/Shared
g27n18:839790:840181 [5] NCCL INFO Channel 01/1 : 5[5] -> 3[3] via P2P/IPC
g31n04:688228:688624 [4] NCCL INFO Channel 00/1 : 7[1] -> 4[4] [receive] via NET/IB/3/Shared
g31n04:688228:688624 [4] NCCL INFO Channel 01/1 : 7[1] -> 4[4] [receive] via NET/IB/1/Shared
g31n04:688228:688624 [4] NCCL INFO Channel 00/1 : 4[4] -> 1[1] via P2P/IPC
g32n03:753098:753491 [2] NCCL INFO Channel 00/1 : 1[5] -> 4[2] [receive] via NET/IB/0/Shared
g32n03:753098:753491 [2] NCCL INFO Channel 01/1 : 1[5] -> 4[2] [receive] via NET/IB/2/Shared
g32n03:753098:753491 [2] NCCL INFO Channel 00/1 : 4[2] -> 7[5] via P2P/IPC
g27n18:839787:840183 [3] NCCL INFO Channel 00/1 : 3[3] -> 1[1] via P2P/IPC
g27n18:839785:840185 [2] NCCL INFO Channel 00/1 : 2[2] -> 0[0] via P2P/IPC
g32n03:753097:753493 [1] NCCL INFO Channel 01/1 : 3[1] -> 6[4] via P2P/IPC
g28n01:770093:770493 [1] NCCL INFO Channel 00/1 : 3[3] -> 7[1] [receive] via NET/IB/2/Shared
g28n01:770093:770493 [1] NCCL INFO Channel 01/1 : 3[3] -> 7[1] [receive] via NET/IB/2/Shared
g28n01:770093:770493 [1] NCCL INFO Channel 00/1 : 7[1] -> 3[3] [send] via NET/IB/2/Shared
g28n01:770093:770493 [1] NCCL INFO Channel 01/1 : 7[1] -> 3[3] [send] via NET/IB/2/Shared
g28n01:770093:770493 [1] NCCL INFO Channel 00/1 : 2[2] -> 7[1] [receive] via NET/IB/2/Shared
g28n01:770093:770493 [1] NCCL INFO Channel 01/1 : 2[2] -> 7[1] [receive] via NET/IB/2/Shared
g28n01:770093:770493 [1] NCCL INFO Channel 00/1 : 7[1] -> 4[4] [send] via NET/IB/2/Shared
g28n01:770093:770493 [1] NCCL INFO Channel 01/1 : 7[1] -> 4[4] [send] via NET/IB/2/Shared
g28n01:770093:770493 [1] NCCL INFO Channel 00/1 : 1[1] -> 7[1] [receive] via NET/IB/2/Shared
g28n01:770093:770493 [1] NCCL INFO Channel 01/1 : 1[1] -> 7[1] [receive] via NET/IB/2/Shared
g28n01:770093:770493 [1] NCCL INFO Channel 00/1 : 7[1] -> 5[5] [send] via NET/IB/2/Shared
g28n01:770093:770493 [1] NCCL INFO Channel 01/1 : 7[1] -> 5[5] [send] via NET/IB/2/Shared
g28n01:770093:770493 [1] NCCL INFO Channel 00/1 : 0[0] -> 7[1] [receive] via NET/IB/2/Shared
g28n01:770093:770493 [1] NCCL INFO Channel 01/1 : 0[0] -> 7[1] [receive] via NET/IB/2/Shared
g28n01:770093:770493 [1] NCCL INFO Channel 00/1 : 7[1] -> 6[0] via P2P/IPC
g28n01:770092:770492 [0] NCCL INFO Channel 00/1 : 2[2] -> 6[0] [receive] via NET/IB/0/Shared
g28n01:770092:770492 [0] NCCL INFO Channel 01/1 : 2[2] -> 6[0] [receive] via NET/IB/0/Shared
g28n01:770092:770492 [0] NCCL INFO Channel 00/1 : 6[0] -> 2[2] [send] via NET/IB/0/Shared
g28n01:770092:770492 [0] NCCL INFO Channel 01/1 : 6[0] -> 2[2] [send] via NET/IB/0/Shared
g28n01:770092:770492 [0] NCCL INFO Channel 00/1 : 1[1] -> 6[0] [receive] via NET/IB/0/Shared
g28n01:770092:770492 [0] NCCL INFO Channel 01/1 : 1[1] -> 6[0] [receive] via NET/IB/0/Shared
g28n01:770092:770492 [0] NCCL INFO Channel 00/1 : 6[0] -> 3[3] [send] via NET/IB/0/Shared
g28n01:770092:770492 [0] NCCL INFO Channel 01/1 : 6[0] -> 3[3] [send] via NET/IB/0/Shared
g28n01:770092:770492 [0] NCCL INFO Channel 00/1 : 0[0] -> 6[0] [receive] via NET/IB/0/Shared
g28n01:770092:770492 [0] NCCL INFO Channel 01/1 : 0[0] -> 6[0] [receive] via NET/IB/0/Shared
g28n01:770092:770492 [0] NCCL INFO Channel 00/1 : 6[0] -> 4[4] [send] via NET/IB/0/Shared
g28n01:770092:770492 [0] NCCL INFO Channel 01/1 : 6[0] -> 4[4] [send] via NET/IB/0/Shared
g27n18:839787:840183 [3] NCCL INFO Channel 01/1 : 3[3] -> 1[1] via P2P/IPC
g27n18:839785:840185 [2] NCCL INFO Channel 01/1 : 2[2] -> 0[0] via P2P/IPC
g31n04:688228:688624 [4] NCCL INFO Channel 01/1 : 4[4] -> 1[1] via P2P/IPC
g28n01:770093:770493 [1] NCCL INFO Channel 01/1 : 7[1] -> 6[0] via P2P/IPC
g32n03:753098:753491 [2] NCCL INFO Channel 01/1 : 4[2] -> 7[5] via P2P/IPC
g31n04:688229:688625 [5] NCCL INFO Channel 00/1 : 5[5] -> 2[2] via P2P/IPC
g32n03:753096:753492 [0] NCCL INFO Channel 00/1 : 2[0] -> 5[3] via P2P/IPC
g32n03:753101:753488 [5] NCCL INFO Channel 00/1 : 7[5] -> 2[0] via P2P/IPC
g32n03:753100:753490 [4] NCCL INFO Channel 00/1 : 6[4] -> 1[5] [send] via NET/IB/3/Shared
g32n03:753100:753490 [4] NCCL INFO Channel 01/1 : 6[4] -> 1[5] [send] via NET/IB/1/Shared
g31n04:688224:688628 [0] NCCL INFO Channel 00/1 : 0[0] -> 5[5] via P2P/IPC
g31n04:688225:688627 [1] NCCL INFO Channel 00/1 : 1[1] -> 6[0] [send] via NET/IB/2/Shared
g31n04:688225:688627 [1] NCCL INFO Channel 01/1 : 1[1] -> 6[0] [send] via NET/IB/0/Shared
g31n04:688228:688624 [4] NCCL INFO Channel 00/1 : 6[0] -> 4[4] [receive] via NET/IB/3/Shared
g31n04:688228:688624 [4] NCCL INFO Channel 01/1 : 6[0] -> 4[4] [receive] via NET/IB/1/Shared
g31n04:688228:688624 [4] NCCL INFO Channel 00/1 : 4[4] -> 2[2] via P2P/IPC
g31n04:688229:688625 [5] NCCL INFO Channel 01/1 : 5[5] -> 2[2] via P2P/IPC
g32n03:753096:753492 [0] NCCL INFO Channel 01/1 : 2[0] -> 5[3] via P2P/IPC
g27n18:839783:840186 [1] NCCL INFO Channel 00/1 : 1[1] -> 7[1] [send] via NET/IB/2/Shared
g27n18:839783:840186 [1] NCCL INFO Channel 01/1 : 1[1] -> 7[1] [send] via NET/IB/0/Shared
g32n03:753101:753488 [5] NCCL INFO Channel 01/1 : 7[5] -> 2[0] via P2P/IPC
g27n18:839782:840184 [0] NCCL INFO Channel 00/1 : 0[0] -> 6[0] [send] via NET/IB/0/Shared
g27n18:839782:840184 [0] NCCL INFO Channel 01/1 : 0[0] -> 6[0] [send] via NET/IB/2/Shared
g27n18:839790:840181 [5] NCCL INFO Channel 00/1 : 6[0] -> 5[5] [receive] via NET/IB/1/Shared
g27n18:839790:840181 [5] NCCL INFO Channel 01/1 : 6[0] -> 5[5] [receive] via NET/IB/3/Shared
g27n18:839790:840181 [5] NCCL INFO Channel 00/1 : 5[5] -> 4[4] via P2P/IPC
g31n04:688224:688628 [0] NCCL INFO Channel 01/1 : 0[0] -> 5[5] via P2P/IPC
g31n04:688228:688624 [4] NCCL INFO Channel 01/1 : 4[4] -> 2[2] via P2P/IPC
g32n03:753097:753493 [1] NCCL INFO Channel 00/1 : 3[1] -> 7[5] via P2P/IPC
g32n03:753097:753493 [1] NCCL INFO Channel 01/1 : 3[1] -> 7[5] via P2P/IPC
g31n04:688226:688629 [2] NCCL INFO Channel 00/1 : 2[2] -> 7[1] [send] via NET/IB/0/Shared
g31n04:688226:688629 [2] NCCL INFO Channel 01/1 : 2[2] -> 7[1] [send] via NET/IB/2/Shared
g31n04:688229:688625 [5] NCCL INFO Channel 00/1 : 7[1] -> 5[5] [receive] via NET/IB/1/Shared
g31n04:688229:688625 [5] NCCL INFO Channel 01/1 : 7[1] -> 5[5] [receive] via NET/IB/3/Shared
g31n04:688229:688625 [5] NCCL INFO Channel 00/1 : 5[5] -> 3[3] via P2P/IPC
g32n03:753098:753491 [2] NCCL INFO Channel 00/1 : 0[4] -> 4[2] [receive] via NET/IB/0/Shared
g32n03:753098:753491 [2] NCCL INFO Channel 01/1 : 0[4] -> 4[2] [receive] via NET/IB/2/Shared
g32n03:753098:753491 [2] NCCL INFO Channel 00/1 : 4[2] -> 0[4] [send] via NET/IB/0/Shared
g32n03:753098:753491 [2] NCCL INFO Channel 01/1 : 4[2] -> 0[4] [send] via NET/IB/2/Shared
g32n03:753100:753490 [4] NCCL INFO Channel 00/1 : 6[4] -> 2[0] via P2P/IPC
g31n04:688229:688625 [5] NCCL INFO Channel 01/1 : 5[5] -> 3[3] via P2P/IPC
g32n03:753099:753489 [3] NCCL INFO Channel 00/1 : 5[3] -> 0[4] [send] via NET/IB/1/Shared
g32n03:753099:753489 [3] NCCL INFO Channel 01/1 : 5[3] -> 0[4] [send] via NET/IB/3/Shared
g32n03:753099:753489 [3] NCCL INFO Channel 00/1 : 1[5] -> 5[3] [receive] via NET/IB/1/Shared
g32n03:753099:753489 [3] NCCL INFO Channel 01/1 : 1[5] -> 5[3] [receive] via NET/IB/3/Shared
g32n03:753099:753489 [3] NCCL INFO Channel 00/1 : 5[3] -> 1[5] [send] via NET/IB/1/Shared
g32n03:753099:753489 [3] NCCL INFO Channel 01/1 : 5[3] -> 1[5] [send] via NET/IB/3/Shared
g32n03:753099:753489 [3] NCCL INFO Channel 00/1 : 0[4] -> 5[3] [receive] via NET/IB/1/Shared
g32n03:753099:753489 [3] NCCL INFO Channel 01/1 : 0[4] -> 5[3] [receive] via NET/IB/3/Shared
g32n03:753099:753489 [3] NCCL INFO Channel 00/1 : 5[3] -> 2[0] via P2P/IPC
g31n04:688227:688626 [3] NCCL INFO Channel 00/1 : 3[3] -> 1[1] via P2P/IPC
g32n03:753100:753490 [4] NCCL INFO Channel 01/1 : 6[4] -> 2[0] via P2P/IPC
g31n04:688226:688629 [2] NCCL INFO Channel 00/1 : 2[2] -> 0[0] via P2P/IPC
g27n18:839789:840182 [4] NCCL INFO Channel 00/1 : 4[4] -> 3[3] via P2P/IPC
g31n05:704344:704743 [0] NCCL INFO Channel 00/1 : 4[4] -> 6[0] [receive] via NET/IB/0/Shared
g31n05:704344:704743 [0] NCCL INFO Channel 01/1 : 4[4] -> 6[0] [receive] via NET/IB/0/Shared
g31n05:704344:704743 [0] NCCL INFO Channel 00/1 : 6[0] -> 0[0] [send] via NET/IB/0/Shared
g31n05:704344:704743 [0] NCCL INFO Channel 01/1 : 6[0] -> 0[0] [send] via NET/IB/0/Shared
g31n05:704344:704743 [0] NCCL INFO Channel 00/1 : 3[3] -> 6[0] [receive] via NET/IB/0/Shared
g31n05:704344:704743 [0] NCCL INFO Channel 01/1 : 3[3] -> 6[0] [receive] via NET/IB/0/Shared
g31n05:704344:704743 [0] NCCL INFO Channel 00/1 : 6[0] -> 1[1] [send] via NET/IB/0/Shared
g31n05:704344:704743 [0] NCCL INFO Channel 01/1 : 6[0] -> 1[1] [send] via NET/IB/0/Shared
g31n05:704344:704743 [0] NCCL INFO Channel 00/1 : 2[2] -> 6[0] [receive] via NET/IB/0/Shared
g31n05:704344:704743 [0] NCCL INFO Channel 01/1 : 2[2] -> 6[0] [receive] via NET/IB/0/Shared
g31n05:704344:704743 [0] NCCL INFO Channel 00/1 : 6[0] -> 2[2] [send] via NET/IB/0/Shared
g31n05:704344:704743 [0] NCCL INFO Channel 01/1 : 6[0] -> 2[2] [send] via NET/IB/0/Shared
g31n05:704344:704743 [0] NCCL INFO Channel 00/1 : 1[1] -> 6[0] [receive] via NET/IB/0/Shared
g31n05:704344:704743 [0] NCCL INFO Channel 01/1 : 1[1] -> 6[0] [receive] via NET/IB/0/Shared
g31n05:704344:704743 [0] NCCL INFO Channel 00/1 : 6[0] -> 3[3] [send] via NET/IB/0/Shared
g31n05:704344:704743 [0] NCCL INFO Channel 01/1 : 6[0] -> 3[3] [send] via NET/IB/0/Shared
g31n05:704344:704743 [0] NCCL INFO Channel 00/1 : 0[0] -> 6[0] [receive] via NET/IB/0/Shared
g31n05:704344:704743 [0] NCCL INFO Channel 01/1 : 0[0] -> 6[0] [receive] via NET/IB/0/Shared
g31n05:704344:704743 [0] NCCL INFO Channel 00/1 : 6[0] -> 4[4] [send] via NET/IB/0/Shared
g31n05:704344:704743 [0] NCCL INFO Channel 01/1 : 6[0] -> 4[4] [send] via NET/IB/0/Shared
g32n03:753099:753489 [3] NCCL INFO Channel 01/1 : 5[3] -> 2[0] via P2P/IPC
g27n18:839790:840181 [5] NCCL INFO Channel 01/1 : 5[5] -> 4[4] via P2P/IPC
g31n05:704345:704742 [1] NCCL INFO Channel 00/1 : 3[3] -> 7[1] [receive] via NET/IB/2/Shared
g31n05:704345:704742 [1] NCCL INFO Channel 01/1 : 3[3] -> 7[1] [receive] via NET/IB/2/Shared
g31n05:704345:704742 [1] NCCL INFO Channel 00/1 : 7[1] -> 3[3] [send] via NET/IB/2/Shared
g31n05:704345:704742 [1] NCCL INFO Channel 01/1 : 7[1] -> 3[3] [send] via NET/IB/2/Shared
g31n05:704345:704742 [1] NCCL INFO Channel 00/1 : 2[2] -> 7[1] [receive] via NET/IB/2/Shared
g31n05:704345:704742 [1] NCCL INFO Channel 01/1 : 2[2] -> 7[1] [receive] via NET/IB/2/Shared
g31n05:704345:704742 [1] NCCL INFO Channel 00/1 : 7[1] -> 4[4] [send] via NET/IB/2/Shared
g31n05:704345:704742 [1] NCCL INFO Channel 01/1 : 7[1] -> 4[4] [send] via NET/IB/2/Shared
g31n05:704345:704742 [1] NCCL INFO Channel 00/1 : 1[1] -> 7[1] [receive] via NET/IB/2/Shared
g31n05:704345:704742 [1] NCCL INFO Channel 01/1 : 1[1] -> 7[1] [receive] via NET/IB/2/Shared
g31n05:704345:704742 [1] NCCL INFO Channel 00/1 : 7[1] -> 5[5] [send] via NET/IB/2/Shared
g31n05:704345:704742 [1] NCCL INFO Channel 01/1 : 7[1] -> 5[5] [send] via NET/IB/2/Shared
g31n05:704345:704742 [1] NCCL INFO Channel 00/1 : 0[0] -> 7[1] [receive] via NET/IB/2/Shared
g31n05:704345:704742 [1] NCCL INFO Channel 01/1 : 0[0] -> 7[1] [receive] via NET/IB/2/Shared
g31n05:704345:704742 [1] NCCL INFO Channel 00/1 : 7[1] -> 6[0] via P2P/IPC
g31n04:688227:688626 [3] NCCL INFO Channel 01/1 : 3[3] -> 1[1] via P2P/IPC
g32n03:753101:753488 [5] NCCL INFO Channel 00/1 : 7[5] -> 3[1] via P2P/IPC
g31n04:688226:688629 [2] NCCL INFO Channel 01/1 : 2[2] -> 0[0] via P2P/IPC
g27n18:839785:840185 [2] NCCL INFO Channel 00/1 : 2[2] -> 1[1] via P2P/IPC
g27n18:839787:840183 [3] NCCL INFO Channel 00/1 : 3[3] -> 2[2] via P2P/IPC
g28n01:770092:770492 [0] NCCL INFO Channel 00/1 : 6[0] -> 5[5] [send] via NET/IB/0/Shared
g28n01:770092:770492 [0] NCCL INFO Channel 01/1 : 6[0] -> 5[5] [send] via NET/IB/0/Shared
g31n05:704345:704742 [1] NCCL INFO Channel 01/1 : 7[1] -> 6[0] via P2P/IPC
g32n03:753101:753488 [5] NCCL INFO Channel 01/1 : 7[5] -> 3[1] via P2P/IPC
g27n18:839789:840182 [4] NCCL INFO Channel 01/1 : 4[4] -> 3[3] via P2P/IPC
g32n03:753096:753492 [0] NCCL INFO Channel 00/1 : 2[0] -> 6[4] via P2P/IPC
g27n18:839785:840185 [2] NCCL INFO Channel 01/1 : 2[2] -> 1[1] via P2P/IPC
g27n18:839787:840183 [3] NCCL INFO Channel 01/1 : 3[3] -> 2[2] via P2P/IPC
g27n18:839783:840186 [1] NCCL INFO Channel 00/1 : 1[1] -> 0[0] via P2P/IPC
g32n03:753096:753492 [0] NCCL INFO Channel 01/1 : 2[0] -> 6[4] via P2P/IPC
g31n04:688225:688627 [1] NCCL INFO Channel 00/1 : 1[1] -> 7[1] [send] via NET/IB/2/Shared
g31n04:688225:688627 [1] NCCL INFO Channel 01/1 : 1[1] -> 7[1] [send] via NET/IB/0/Shared
g31n04:688224:688628 [0] NCCL INFO Channel 00/1 : 0[0] -> 6[0] [send] via NET/IB/0/Shared
g31n04:688224:688628 [0] NCCL INFO Channel 01/1 : 0[0] -> 6[0] [send] via NET/IB/2/Shared
g31n04:688229:688625 [5] NCCL INFO Channel 00/1 : 6[0] -> 5[5] [receive] via NET/IB/1/Shared
g31n04:688229:688625 [5] NCCL INFO Channel 01/1 : 6[0] -> 5[5] [receive] via NET/IB/3/Shared
g31n04:688229:688625 [5] NCCL INFO Channel 00/1 : 5[5] -> 4[4] via P2P/IPC
g32n03:753100:753490 [4] NCCL INFO Channel 00/1 : 1[5] -> 6[4] [receive] via NET/IB/3/Shared
g32n03:753100:753490 [4] NCCL INFO Channel 01/1 : 1[5] -> 6[4] [receive] via NET/IB/1/Shared
g32n03:753100:753490 [4] NCCL INFO Channel 00/1 : 6[4] -> 3[1] via P2P/IPC
g27n18:839783:840186 [1] NCCL INFO Channel 01/1 : 1[1] -> 0[0] via P2P/IPC
g32n03:753101:753488 [5] NCCL INFO Channel 00/1 : 7[5] -> 4[2] via P2P/IPC
g32n03:753100:753490 [4] NCCL INFO Channel 01/1 : 6[4] -> 3[1] via P2P/IPC
g31n04:688229:688625 [5] NCCL INFO Channel 01/1 : 5[5] -> 4[4] via P2P/IPC
g32n03:753101:753488 [5] NCCL INFO Channel 01/1 : 7[5] -> 4[2] via P2P/IPC
g31n05:704344:704743 [0] NCCL INFO Channel 00/1 : 6[0] -> 5[5] [send] via NET/IB/0/Shared
g31n05:704344:704743 [0] NCCL INFO Channel 01/1 : 6[0] -> 5[5] [send] via NET/IB/0/Shared
g32n03:753097:753493 [1] NCCL INFO Channel 00/1 : 3[1] -> 0[4] [send] via NET/IB/2/Shared
g32n03:753097:753493 [1] NCCL INFO Channel 01/1 : 3[1] -> 0[4] [send] via NET/IB/0/Shared
g32n03:753096:753492 [0] NCCL INFO Channel 00/1 : 2[0] -> 7[5] via P2P/IPC
g27n18:839782:840184 [0] NCCL INFO Channel 00/1 : 0[0] -> 7[1] [send] via NET/IB/0/Shared
g27n18:839782:840184 [0] NCCL INFO Channel 01/1 : 0[0] -> 7[1] [send] via NET/IB/2/Shared
g31n04:688228:688624 [4] NCCL INFO Channel 00/1 : 4[4] -> 3[3] via P2P/IPC
g32n03:753100:753490 [4] NCCL INFO Channel 00/1 : 0[4] -> 6[4] [receive] via NET/IB/3/Shared
g32n03:753100:753490 [4] NCCL INFO Channel 01/1 : 0[4] -> 6[4] [receive] via NET/IB/1/Shared
g32n03:753100:753490 [4] NCCL INFO Channel 00/1 : 6[4] -> 4[2] via P2P/IPC
g31n04:688225:688627 [1] NCCL INFO Channel 00/1 : 1[1] -> 0[0] via P2P/IPC
g31n04:688227:688626 [3] NCCL INFO Channel 00/1 : 3[3] -> 2[2] via P2P/IPC
g31n04:688226:688629 [2] NCCL INFO Channel 00/1 : 2[2] -> 1[1] via P2P/IPC
g32n03:753098:753491 [2] NCCL INFO Channel 00/1 : 4[2] -> 1[5] [send] via NET/IB/0/Shared
g32n03:753098:753491 [2] NCCL INFO Channel 01/1 : 4[2] -> 1[5] [send] via NET/IB/2/Shared
g31n04:688225:688627 [1] NCCL INFO Channel 01/1 : 1[1] -> 0[0] via P2P/IPC
g32n03:753096:753492 [0] NCCL INFO Channel 01/1 : 2[0] -> 7[5] via P2P/IPC
g31n04:688228:688624 [4] NCCL INFO Channel 01/1 : 4[4] -> 3[3] via P2P/IPC
g32n03:753100:753490 [4] NCCL INFO Channel 01/1 : 6[4] -> 4[2] via P2P/IPC
g31n04:688227:688626 [3] NCCL INFO Channel 01/1 : 3[3] -> 2[2] via P2P/IPC
g31n04:688226:688629 [2] NCCL INFO Channel 01/1 : 2[2] -> 1[1] via P2P/IPC
g32n03:753101:753488 [5] NCCL INFO Channel 00/1 : 1[5] -> 7[5] [receive] via NET/IB/1/Shared
g32n03:753101:753488 [5] NCCL INFO Channel 01/1 : 1[5] -> 7[5] [receive] via NET/IB/3/Shared
g32n03:753101:753488 [5] NCCL INFO Channel 00/1 : 7[5] -> 5[3] via P2P/IPC
g31n04:688224:688628 [0] NCCL INFO Channel 00/1 : 0[0] -> 7[1] [send] via NET/IB/0/Shared
g31n04:688224:688628 [0] NCCL INFO Channel 01/1 : 0[0] -> 7[1] [send] via NET/IB/2/Shared
g32n03:753098:753491 [2] NCCL INFO Channel 00/1 : 4[2] -> 2[0] via P2P/IPC
g32n03:753101:753488 [5] NCCL INFO Channel 01/1 : 7[5] -> 5[3] via P2P/IPC
g32n03:753098:753491 [2] NCCL INFO Channel 01/1 : 4[2] -> 2[0] via P2P/IPC
g32n03:753099:753489 [3] NCCL INFO Channel 00/1 : 5[3] -> 3[1] via P2P/IPC
g32n02:753687:754088 [4] NCCL INFO Channel 00/1 : 6[4] -> 0[4] [receive] via NET/IB/1/Shared
g32n02:753687:754088 [4] NCCL INFO Channel 01/1 : 6[4] -> 0[4] [receive] via NET/IB/1/Shared
g32n02:753687:754088 [4] NCCL INFO Channel 00/1 : 0[4] -> 2[0] [send] via NET/IB/1/Shared
g32n02:753687:754088 [4] NCCL INFO Channel 01/1 : 0[4] -> 2[0] [send] via NET/IB/1/Shared
g32n02:753687:754088 [4] NCCL INFO Channel 00/1 : 5[3] -> 0[4] [receive] via NET/IB/1/Shared
g32n02:753687:754088 [4] NCCL INFO Channel 01/1 : 5[3] -> 0[4] [receive] via NET/IB/1/Shared
g32n02:753687:754088 [4] NCCL INFO Channel 00/1 : 0[4] -> 3[1] [send] via NET/IB/1/Shared
g32n02:753687:754088 [4] NCCL INFO Channel 01/1 : 0[4] -> 3[1] [send] via NET/IB/1/Shared
g32n02:753687:754088 [4] NCCL INFO Channel 00/1 : 4[2] -> 0[4] [receive] via NET/IB/1/Shared
g32n02:753687:754088 [4] NCCL INFO Channel 01/1 : 4[2] -> 0[4] [receive] via NET/IB/1/Shared
g32n02:753687:754088 [4] NCCL INFO Channel 00/1 : 0[4] -> 4[2] [send] via NET/IB/1/Shared
g32n02:753687:754088 [4] NCCL INFO Channel 01/1 : 0[4] -> 4[2] [send] via NET/IB/1/Shared
g32n02:753687:754088 [4] NCCL INFO Channel 00/1 : 3[1] -> 0[4] [receive] via NET/IB/1/Shared
g32n02:753687:754088 [4] NCCL INFO Channel 01/1 : 3[1] -> 0[4] [receive] via NET/IB/1/Shared
g32n02:753687:754088 [4] NCCL INFO Channel 00/1 : 0[4] -> 5[3] [send] via NET/IB/1/Shared
g32n02:753687:754088 [4] NCCL INFO Channel 01/1 : 0[4] -> 5[3] [send] via NET/IB/1/Shared
g32n02:753687:754088 [4] NCCL INFO Channel 00/1 : 2[0] -> 0[4] [receive] via NET/IB/1/Shared
g32n02:753687:754088 [4] NCCL INFO Channel 01/1 : 2[0] -> 0[4] [receive] via NET/IB/1/Shared
g32n02:753687:754088 [4] NCCL INFO Channel 00/1 : 0[4] -> 6[4] [send] via NET/IB/1/Shared
g32n02:753687:754088 [4] NCCL INFO Channel 01/1 : 0[4] -> 6[4] [send] via NET/IB/1/Shared
g32n02:753688:754089 [5] NCCL INFO Channel 00/1 : 1[5] -> 2[0] [send] via NET/IB/3/Shared
g32n02:753688:754089 [5] NCCL INFO Channel 01/1 : 1[5] -> 2[0] [send] via NET/IB/3/Shared
g32n02:753688:754089 [5] NCCL INFO Channel 00/1 : 7[5] -> 1[5] [receive] via NET/IB/3/Shared
g32n02:753688:754089 [5] NCCL INFO Channel 01/1 : 7[5] -> 1[5] [receive] via NET/IB/3/Shared
g32n02:753688:754089 [5] NCCL INFO Channel 00/1 : 1[5] -> 3[1] [send] via NET/IB/3/Shared
g32n02:753688:754089 [5] NCCL INFO Channel 01/1 : 1[5] -> 3[1] [send] via NET/IB/3/Shared
g32n02:753688:754089 [5] NCCL INFO Channel 00/1 : 6[4] -> 1[5] [receive] via NET/IB/3/Shared
g32n02:753688:754089 [5] NCCL INFO Channel 01/1 : 6[4] -> 1[5] [receive] via NET/IB/3/Shared
g32n02:753688:754089 [5] NCCL INFO Channel 00/1 : 1[5] -> 4[2] [send] via NET/IB/3/Shared
g32n02:753688:754089 [5] NCCL INFO Channel 01/1 : 1[5] -> 4[2] [send] via NET/IB/3/Shared
g32n02:753688:754089 [5] NCCL INFO Channel 00/1 : 5[3] -> 1[5] [receive] via NET/IB/3/Shared
g32n02:753688:754089 [5] NCCL INFO Channel 01/1 : 5[3] -> 1[5] [receive] via NET/IB/3/Shared
g32n02:753688:754089 [5] NCCL INFO Channel 00/1 : 1[5] -> 5[3] [send] via NET/IB/3/Shared
g32n02:753688:754089 [5] NCCL INFO Channel 01/1 : 1[5] -> 5[3] [send] via NET/IB/3/Shared
g32n02:753688:754089 [5] NCCL INFO Channel 00/1 : 4[2] -> 1[5] [receive] via NET/IB/3/Shared
g32n02:753688:754089 [5] NCCL INFO Channel 01/1 : 4[2] -> 1[5] [receive] via NET/IB/3/Shared
g32n02:753688:754089 [5] NCCL INFO Channel 00/1 : 1[5] -> 6[4] [send] via NET/IB/3/Shared
g32n02:753688:754089 [5] NCCL INFO Channel 01/1 : 1[5] -> 6[4] [send] via NET/IB/3/Shared
g32n02:753688:754089 [5] NCCL INFO Channel 00/1 : 3[1] -> 1[5] [receive] via NET/IB/3/Shared
g32n02:753688:754089 [5] NCCL INFO Channel 01/1 : 3[1] -> 1[5] [receive] via NET/IB/3/Shared
g32n02:753688:754089 [5] NCCL INFO Channel 00/1 : 1[5] -> 7[5] [send] via NET/IB/3/Shared
g32n02:753688:754089 [5] NCCL INFO Channel 01/1 : 1[5] -> 7[5] [send] via NET/IB/3/Shared
g32n02:753688:754089 [5] NCCL INFO Channel 00/1 : 2[0] -> 1[5] [receive] via NET/IB/3/Shared
g32n02:753688:754089 [5] NCCL INFO Channel 01/1 : 2[0] -> 1[5] [receive] via NET/IB/3/Shared
g32n02:753688:754089 [5] NCCL INFO Channel 00/1 : 1[5] -> 0[4] via P2P/IPC
g32n03:753099:753489 [3] NCCL INFO Channel 01/1 : 5[3] -> 3[1] via P2P/IPC
g32n03:753096:753492 [0] NCCL INFO Channel 00/1 : 2[0] -> 0[4] [send] via NET/IB/0/Shared
g32n03:753096:753492 [0] NCCL INFO Channel 01/1 : 2[0] -> 0[4] [send] via NET/IB/2/Shared
g32n02:753688:754089 [5] NCCL INFO Channel 01/1 : 1[5] -> 0[4] via P2P/IPC
g32n03:753097:753493 [1] NCCL INFO Channel 00/1 : 3[1] -> 1[5] [send] via NET/IB/2/Shared
g32n03:753097:753493 [1] NCCL INFO Channel 01/1 : 3[1] -> 1[5] [send] via NET/IB/0/Shared
g32n03:753101:753488 [5] NCCL INFO Channel 00/1 : 0[4] -> 7[5] [receive] via NET/IB/1/Shared
g32n03:753101:753488 [5] NCCL INFO Channel 01/1 : 0[4] -> 7[5] [receive] via NET/IB/3/Shared
g32n03:753101:753488 [5] NCCL INFO Channel 00/1 : 7[5] -> 6[4] via P2P/IPC
g32n03:753100:753490 [4] NCCL INFO Channel 00/1 : 6[4] -> 5[3] via P2P/IPC
g32n03:753098:753491 [2] NCCL INFO Channel 00/1 : 4[2] -> 3[1] via P2P/IPC
g32n03:753101:753488 [5] NCCL INFO Channel 01/1 : 7[5] -> 6[4] via P2P/IPC
g32n02:753687:754088 [4] NCCL INFO Channel 00/1 : 0[4] -> 7[5] [send] via NET/IB/1/Shared
g32n02:753687:754088 [4] NCCL INFO Channel 01/1 : 0[4] -> 7[5] [send] via NET/IB/1/Shared
g32n03:753100:753490 [4] NCCL INFO Channel 01/1 : 6[4] -> 5[3] via P2P/IPC
g32n03:753097:753493 [1] NCCL INFO Channel 00/1 : 3[1] -> 2[0] via P2P/IPC
g32n03:753099:753489 [3] NCCL INFO Channel 00/1 : 5[3] -> 4[2] via P2P/IPC
g32n03:753098:753491 [2] NCCL INFO Channel 01/1 : 4[2] -> 3[1] via P2P/IPC
g32n03:753097:753493 [1] NCCL INFO Channel 01/1 : 3[1] -> 2[0] via P2P/IPC
g32n03:753099:753489 [3] NCCL INFO Channel 01/1 : 5[3] -> 4[2] via P2P/IPC
g32n03:753096:753492 [0] NCCL INFO Channel 00/1 : 2[0] -> 1[5] [send] via NET/IB/0/Shared
g32n03:753096:753492 [0] NCCL INFO Channel 01/1 : 2[0] -> 1[5] [send] via NET/IB/2/Shared

------------------------------------------------------------
Sender: LSF System <lsfadmin@batch1>
Subject: Job 3329553: <gpt3-megatron> in cluster <summit> Done

Job <gpt3-megatron> was submitted from host <login1> by user <sajaldash> in cluster <summit> at Mon Mar  4 12:18:08 2024
Job was executed on host(s) <1*batch1>, in queue <debug>, as user <sajaldash> in cluster <summit> at Mon Mar  4 12:18:14 2024
                            <42*g25n18>
                            <42*g26n01>
                            <42*g26n02>
                            <42*g27n17>
                            <42*g27n18>
                            <42*g28n01>
                            <42*g28n02>
                            <42*g28n03>
                            <42*g28n04>
                            <42*g31n01>
                            <42*g31n02>
                            <42*g31n03>
                            <42*g31n04>
                            <42*g31n05>
                            <42*g32n02>
                            <42*g32n03>
</ccs/home/sajaldash> was used as the home directory.
</gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed> was used as the working directory.
Started at Mon Mar  4 12:18:14 2024
Terminated at Mon Mar  4 12:19:19 2024
Results reported at Mon Mar  4 12:19:19 2024

The output (if any) is above this job summary.



PS:

Read file <logs/gpt3-megatron.3329553.e> for stderr output of this job.

