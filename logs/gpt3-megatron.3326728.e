Error: SMPI environment generator script not found at:
Path : /gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/bin/alias.pl
Error: No such file or directory
Warning. Failed to setup the Spectrum MPI environment.
Continuing to launch. Beware, the Spectrum MPI environment may not be completely setup.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
WARNING:root:Cannot import JIT optimized kernels. CUDA extension will be disabled.
/bin/sh: line 0: type: git: not found
/bin/sh: line 0: type: git: not found
/bin/sh: line 0: type: git: not found
/bin/sh: line 0: type: git: not found
/bin/sh: line 0: type: git: not found
/bin/sh: line 0: type: git: not found
/bin/sh: line 0: type: git: not found
/bin/sh: line 0: type: git: not found
/bin/sh: line 0: type: git: not found
/bin/sh: line 0: type: git: not found
/bin/sh: line 0: type: git: not found
/bin/sh: line 0: type: git: not found
/bin/sh: line 0: type: git: not found
/bin/sh: line 0: type: git: not found
/bin/sh: line 0: type: git: not found
/bin/sh: line 0: type: git: not found
/bin/sh: line 0: type: git: not found
/bin/sh: line 0: type: git: not found
/bin/sh: line 0: type: git: not found
/bin/sh: line 0: type: git: not found
/bin/sh: line 0: type: git: not found
/bin/sh: line 0: type: git: not found
/bin/sh: line 0: type: git: not found
/bin/sh: line 0: type: git: not found
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
    set_global_variables(args)
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
    set_global_variables(args)
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
    _ = _build_tokenizer(args)
        ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
    _ = _build_tokenizer(args)
        ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
    _GLOBAL_TOKENIZER = build_tokenizer(args)
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
    _GLOBAL_TOKENIZER = build_tokenizer(args)
                        ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
                        ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
    set_global_variables(args)
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
    _ = _build_tokenizer(args)
    self.encoder = json.load(open(vocab_file))
        ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
    self.encoder = json.load(open(vocab_file))
                             ^^^^^^^^^^^^^^^^
                             ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'
    _GLOBAL_TOKENIZER = build_tokenizer(args)
                        ^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
    self.encoder = json.load(open(vocab_file))
                             ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
    set_global_variables(args)
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
    _ = _build_tokenizer(args)
        ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
    _GLOBAL_TOKENIZER = build_tokenizer(args)
                        ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
    self.encoder = json.load(open(vocab_file))
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
    set_global_variables(args)
                             ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
    set_global_variables(args)
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
    _ = _build_tokenizer(args)
        ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
    set_global_variables(args)
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
    _GLOBAL_TOKENIZER = build_tokenizer(args)
    _ = _build_tokenizer(args)
                        ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
        ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
    _GLOBAL_TOKENIZER = build_tokenizer(args)
                        ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
    self.encoder = json.load(open(vocab_file))
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
                             ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
    self.encoder = json.load(open(vocab_file))
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
                             ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'
    set_global_variables(args)
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
    set_global_variables(args)
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
    _ = _build_tokenizer(args)
        ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
    set_global_variables(args)
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
    _ = _build_tokenizer(args)
        ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
    _GLOBAL_TOKENIZER = build_tokenizer(args)
                        ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
    _GLOBAL_TOKENIZER = build_tokenizer(args)
                        ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
    _ = _build_tokenizer(args)
        ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
    _GLOBAL_TOKENIZER = build_tokenizer(args)
                        ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
    self.encoder = json.load(open(vocab_file))
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
    self.encoder = json.load(open(vocab_file))
                             ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
    set_global_variables(args)
                             ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'
    set_global_variables(args)
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
    self.encoder = json.load(open(vocab_file))
    set_global_variables(args)
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
                             ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
    _ = _build_tokenizer(args)
        ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
    _ = _build_tokenizer(args)
        ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
    _ = _build_tokenizer(args)
        ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
    set_global_variables(args)
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
    _ = _build_tokenizer(args)
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
    _GLOBAL_TOKENIZER = build_tokenizer(args)
                        ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
    _ = _build_tokenizer(args)
        ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
    _GLOBAL_TOKENIZER = build_tokenizer(args)
                        ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
    set_global_variables(args)
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
    _GLOBAL_TOKENIZER = build_tokenizer(args)
                        ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
        ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
    _GLOBAL_TOKENIZER = build_tokenizer(args)
                        ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
    _ = _build_tokenizer(args)
        ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
    _GLOBAL_TOKENIZER = build_tokenizer(args)
                        ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
    _GLOBAL_TOKENIZER = build_tokenizer(args)
                        ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
    self.encoder = json.load(open(vocab_file))
                             ^^^^^^^^^^^^^^^^
    self.encoder = json.load(open(vocab_file))
                             ^^^^^^^^^^^^^^^^
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
    self.encoder = json.load(open(vocab_file))
                             ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
    self.encoder = json.load(open(vocab_file))
                             ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'
    self.encoder = json.load(open(vocab_file))
                             ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
    set_global_variables(args)
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
    _ = _build_tokenizer(args)
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
        ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
    _GLOBAL_TOKENIZER = build_tokenizer(args)
    self.encoder = json.load(open(vocab_file))
                             ^^^^^^^^^^^^^^^^
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
                        ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
    set_global_variables(args)
    set_global_variables(args)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
    set_global_variables(args)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
    self.encoder = json.load(open(vocab_file))
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
    set_global_variables(args)
                             ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'
    _ = _build_tokenizer(args)
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
    set_global_variables(args)
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
        ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
    _ = _build_tokenizer(args)
    _GLOBAL_TOKENIZER = build_tokenizer(args)
    set_global_variables(args)
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
                        ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
    set_global_variables(args)
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
    _ = _build_tokenizer(args)
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
    _ = _build_tokenizer(args)
        ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
    _ = _build_tokenizer(args)
        ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
    _ = _build_tokenizer(args)
        ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
    set_global_variables(args)
        ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
    _GLOBAL_TOKENIZER = build_tokenizer(args)
                        ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
    _GLOBAL_TOKENIZER = build_tokenizer(args)
                        ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
        ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
    _GLOBAL_TOKENIZER = build_tokenizer(args)
                        ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
    _ = _build_tokenizer(args)
    _GLOBAL_TOKENIZER = build_tokenizer(args)
                        ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
    self.encoder = json.load(open(vocab_file))
    _ = _build_tokenizer(args)
        ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
                             ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'
    _GLOBAL_TOKENIZER = build_tokenizer(args)
                        ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
        ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
    _GLOBAL_TOKENIZER = build_tokenizer(args)
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
                        ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
    _GLOBAL_TOKENIZER = build_tokenizer(args)
                        ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
    self.encoder = json.load(open(vocab_file))
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
                             ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
    self.encoder = json.load(open(vocab_file))
    self.encoder = json.load(open(vocab_file))
    self.encoder = json.load(open(vocab_file))
    self.encoder = json.load(open(vocab_file))
                             ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'
    self.encoder = json.load(open(vocab_file))
                             ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'
                             ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'
                             ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
                             ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'
    self.encoder = json.load(open(vocab_file))
                             ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'
/bin/sh: line 0: type: git: not found
/bin/sh: line 0: type: git: not found
/bin/sh: line 0: type: git: not found
/bin/sh: line 0: type: git: not found
/bin/sh: line 0: type: git: not found
/bin/sh: line 0: type: git: not found
/bin/sh: line 0: type: git: not found
/bin/sh: line 0: type: git: not found
/bin/sh: line 0: type: git: not found
/bin/sh: line 0: type: git: not found
/bin/sh: line 0: type: git: not found
/bin/sh: line 0: type: git: not found
/bin/sh: line 0: type: git: not found
/bin/sh: line 0: type: git: not found
/bin/sh: line 0: type: git: not found
/bin/sh: line 0: type: git: not found
/bin/sh: line 0: type: git: not found
/bin/sh: line 0: type: git: not found
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
    set_global_variables(args)
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
    set_global_variables(args)
    set_global_variables(args)
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
    set_global_variables(args)
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
    _ = _build_tokenizer(args)
    set_global_variables(args)
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
    set_global_variables(args)
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
    _ = _build_tokenizer(args)
        ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
        ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
    _ = _build_tokenizer(args)
        ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
    _ = _build_tokenizer(args)
        ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
    _ = _build_tokenizer(args)
        ^^^^^^^^^^^^^^^^^^^^^^
    _ = _build_tokenizer(args)
        ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
    _GLOBAL_TOKENIZER = build_tokenizer(args)
                        ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
    _GLOBAL_TOKENIZER = build_tokenizer(args)
                        ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
    _GLOBAL_TOKENIZER = build_tokenizer(args)
                        ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
    _GLOBAL_TOKENIZER = build_tokenizer(args)
                        ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
    _GLOBAL_TOKENIZER = build_tokenizer(args)
                        ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
    _GLOBAL_TOKENIZER = build_tokenizer(args)
                        ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
    self.encoder = json.load(open(vocab_file))
                             ^^^^^^^^^^^^^^^^
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
    self.encoder = json.load(open(vocab_file))
                             ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
    self.encoder = json.load(open(vocab_file))
    self.encoder = json.load(open(vocab_file))
                             ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'
    self.encoder = json.load(open(vocab_file))
                             ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'
    self.encoder = json.load(open(vocab_file))
                             ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'
                             ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
    set_global_variables(args)
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
    set_global_variables(args)
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
    set_global_variables(args)
    set_global_variables(args)
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
    set_global_variables(args)
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
    _ = _build_tokenizer(args)
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
    _ = _build_tokenizer(args)
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
    _ = _build_tokenizer(args)
        ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
        ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
        ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
    _ = _build_tokenizer(args)
        ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
    _GLOBAL_TOKENIZER = build_tokenizer(args)
                        ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
    _GLOBAL_TOKENIZER = build_tokenizer(args)
                        ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
    _GLOBAL_TOKENIZER = build_tokenizer(args)
                        ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
    _GLOBAL_TOKENIZER = build_tokenizer(args)
                        ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
    _ = _build_tokenizer(args)
        ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
    _GLOBAL_TOKENIZER = build_tokenizer(args)
                        ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
    self.encoder = json.load(open(vocab_file))
                             ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
    self.encoder = json.load(open(vocab_file))
                             ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'
    self.encoder = json.load(open(vocab_file))
                             ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'
    self.encoder = json.load(open(vocab_file))
                             ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
    self.encoder = json.load(open(vocab_file))
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
                             ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
    set_global_variables(args)
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
    _ = _build_tokenizer(args)
        ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
    _GLOBAL_TOKENIZER = build_tokenizer(args)
                        ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
    self.encoder = json.load(open(vocab_file))
                             ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'
/bin/sh: line 0: type: git: not found
/bin/sh: line 0: type: git: not found
/bin/sh: line 0: type: git: not found
/bin/sh: line 0: type: git: not found
/bin/sh: line 0: type: git: not found
/bin/sh: line 0: type: git: not found
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
    set_global_variables(args)
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
    set_global_variables(args)
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
    set_global_variables(args)
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
    set_global_variables(args)
    _ = _build_tokenizer(args)
        ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
    _ = _build_tokenizer(args)
        ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
    _GLOBAL_TOKENIZER = build_tokenizer(args)
    set_global_variables(args)
    _ = _build_tokenizer(args)
        ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
    set_global_variables(args)
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
                        ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
    _ = _build_tokenizer(args)
    _GLOBAL_TOKENIZER = build_tokenizer(args)
        ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
                        ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
    _GLOBAL_TOKENIZER = build_tokenizer(args)
                        ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
    _ = _build_tokenizer(args)
        ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
    _ = _build_tokenizer(args)
        ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
    _GLOBAL_TOKENIZER = build_tokenizer(args)
                        ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
    _GLOBAL_TOKENIZER = build_tokenizer(args)
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
    _GLOBAL_TOKENIZER = build_tokenizer(args)
                        ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
                        ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
    self.encoder = json.load(open(vocab_file))
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
                             ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'
    self.encoder = json.load(open(vocab_file))
                             ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
    self.encoder = json.load(open(vocab_file))
                             ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'
    self.encoder = json.load(open(vocab_file))
                             ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'
    self.encoder = json.load(open(vocab_file))
                             ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'
    self.encoder = json.load(open(vocab_file))
                             ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
    set_global_variables(args)
    set_global_variables(args)
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
    _ = _build_tokenizer(args)
        ^^^^^^^^^^^^^^^^^^^^^^
    _ = _build_tokenizer(args)
        ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
    _GLOBAL_TOKENIZER = build_tokenizer(args)
                        ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
    _GLOBAL_TOKENIZER = build_tokenizer(args)
                        ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
    set_global_variables(args)
    self.encoder = json.load(open(vocab_file))
                             ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
    self.encoder = json.load(open(vocab_file))
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
                             ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'
    _ = _build_tokenizer(args)
        ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
    _GLOBAL_TOKENIZER = build_tokenizer(args)
                        ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
    set_global_variables(args)
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
    set_global_variables(args)
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
    _ = _build_tokenizer(args)
        ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
Traceback (most recent call last):
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
    _ = _build_tokenizer(args)
        ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
    pretrain(train_valid_test_datasets_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/training.py", line 126, in pretrain
    initialize_megatron(extra_args_provider=extra_args_provider,
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/initialize.py", line 80, in initialize_megatron
    _GLOBAL_TOKENIZER = build_tokenizer(args)
                        ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
    self.encoder = json.load(open(vocab_file))
    _GLOBAL_TOKENIZER = build_tokenizer(args)
                        ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
    set_global_variables(args)
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 92, in set_global_variables
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
                             ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
    _ = _build_tokenizer(args)
        ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/global_vars.py", line 125, in _build_tokenizer
    _GLOBAL_TOKENIZER = build_tokenizer(args)
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
                        ^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 31, in build_tokenizer
    tokenizer = _GPT2BPETokenizer(args.vocab_file, args.merge_file)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/tokenizer.py", line 267, in __init__
    self.encoder = json.load(open(vocab_file))
                             ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'
    self.encoder = json.load(open(vocab_file))
                             ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'
    self.tokenizer = GPT2Tokenizer(vocab_file, merge_file, errors='replace',
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/tokenizer/gpt2_tokenization.py", line 159, in __init__
    self.encoder = json.load(open(vocab_file))
                             ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'gpt2-vocab.json'

real	0m8.385s
user	0m0.361s
sys	0m0.088s
