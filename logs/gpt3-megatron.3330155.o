d09n07
10.134.8.171
[2024-03-04 16:42:20,707] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:20,707] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:20,707] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:20,707] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:20,707] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:20,707] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:20,711] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:20,711] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:20,711] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:20,714] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:20,715] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:20,715] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:20,781] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:20,781] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:20,781] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:20,782] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:20,782] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:20,782] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:20,942] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:20,943] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:20,946] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:20,946] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:20,947] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:20,947] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:20,964] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:20,964] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:20,964] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:20,964] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:20,964] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:20,964] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:20,984] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:20,984] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:20,984] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:20,984] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:20,984] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:20,984] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,196] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,196] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,197] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,197] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,197] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,197] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,201] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,201] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,201] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,201] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,201] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,201] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,409] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,409] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,409] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,409] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,409] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,409] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,423] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,423] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,423] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,423] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,423] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,423] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,448] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,448] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,448] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,448] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,448] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,448] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,454] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,454] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,454] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,454] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,454] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,454] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,548] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,548] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,548] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,548] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,548] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,548] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,561] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,561] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,561] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,561] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,561] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:21,561] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 80
MA = 10.134.8.171
World view:  80 96 10.134.8.171
[2024-03-04 16:42:24,219] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,219] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 83
MA = 10.134.8.171
World view:  83 96 10.134.8.171
[2024-03-04 16:42:24,219] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,219] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 78
MA = 10.134.8.171
World view:  78 96 10.134.8.171
[2024-03-04 16:42:24,220] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 79
MA = 10.134.8.171
World view:  79 96 10.134.8.171
[2024-03-04 16:42:24,220] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,220] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 16:42:24,220] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 81
MA = 10.134.8.171
World view:  81 96 10.134.8.171
[2024-03-04 16:42:24,220] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,220] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 82
MA = 10.134.8.171
World view:  82 96 10.134.8.171
[2024-03-04 16:42:24,220] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,220] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 42
MA = 10.134.8.171
World view:  42 96 10.134.8.171
[2024-03-04 16:42:24,316] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,316] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 43
MA = 10.134.8.171
World view:  43 96 10.134.8.171
[2024-03-04 16:42:24,316] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,316] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 45
MA = 10.134.8.171
World view:  45 96 10.134.8.171
[2024-03-04 16:42:24,316] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 46
MA = 10.134.8.171
World view:  46 96 10.134.8.171
[2024-03-04 16:42:24,316] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 47
MA = 10.134.8.171
World view:  47 96 10.134.8.171
[2024-03-04 16:42:24,316] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,316] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 16:42:24,316] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 16:42:24,316] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 44
MA = 10.134.8.171
World view:  44 96 10.134.8.171
[2024-03-04 16:42:24,316] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,316] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 67
MA = 10.134.8.171
World view:  67 96 10.134.8.171
[2024-03-04 16:42:24,450] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,450] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 68
MA = 10.134.8.171
World view:  68 96 10.134.8.171
[2024-03-04 16:42:24,451] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,451] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 66
MA = 10.134.8.171
World view:  66 96 10.134.8.171
[2024-03-04 16:42:24,451] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,451] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 70
MA = 10.134.8.171
World view:  70 96 10.134.8.171
[2024-03-04 16:42:24,451] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,451] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 69
MA = 10.134.8.171
World view:  69 96 10.134.8.171
[2024-03-04 16:42:24,451] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,451] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 71
MA = 10.134.8.171
World view:  71 96 10.134.8.171
[2024-03-04 16:42:24,451] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,452] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 72
MA = 10.134.8.171
World view:  72 96 10.134.8.171
[2024-03-04 16:42:24,465] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,465] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 74
MA = 10.134.8.171
World view:  74 96 10.134.8.171
[2024-03-04 16:42:24,466] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 73
MA = 10.134.8.171
World view:  73 96 10.134.8.171
[2024-03-04 16:42:24,466] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 76
MA = 10.134.8.171
World view:  76 96 10.134.8.171
[2024-03-04 16:42:24,466] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 77
MA = 10.134.8.171
World view:  77 96 10.134.8.171
[2024-03-04 16:42:24,466] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,466] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 16:42:24,466] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 16:42:24,466] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 16:42:24,466] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 84
MA = 10.134.8.171
World view:  84 96 10.134.8.171
[2024-03-04 16:42:24,466] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,466] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 75
MA = 10.134.8.171
World view:  75 96 10.134.8.171
[2024-03-04 16:42:24,466] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,467] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 86
MA = 10.134.8.171
World view:  86 96 10.134.8.171
[2024-03-04 16:42:24,466] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,466] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 87
MA = 10.134.8.171
World view:  87 96 10.134.8.171
[2024-03-04 16:42:24,467] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 89
MA = 10.134.8.171
World view:  89 96 10.134.8.171
[2024-03-04 16:42:24,467] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,467] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 16:42:24,467] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 88
MA = 10.134.8.171
World view:  88 96 10.134.8.171
[2024-03-04 16:42:24,467] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 85
MA = 10.134.8.171
World view:  85 96 10.134.8.171
[2024-03-04 16:42:24,467] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,467] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 16:42:24,467] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 92
MA = 10.134.8.171
World view:  92 96 10.134.8.171
[2024-03-04 16:42:24,705] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,706] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 91
MA = 10.134.8.171
World view:  91 96 10.134.8.171
[2024-03-04 16:42:24,706] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,706] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 93
MA = 10.134.8.171
World view:  93 96 10.134.8.171
[2024-03-04 16:42:24,706] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,706] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 94
MA = 10.134.8.171
World view:  94 96 10.134.8.171
[2024-03-04 16:42:24,706] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,706] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 90
MA = 10.134.8.171
World view:  90 96 10.134.8.171
[2024-03-04 16:42:24,706] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,706] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 55
MA = 10.134.8.171
World view:  55 96 10.134.8.171
[2024-03-04 16:42:24,771] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,772] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 54
MA = 10.134.8.171
World view:  54 96 10.134.8.171
[2024-03-04 16:42:24,771] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,772] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 56
MA = 10.134.8.171
World view:  56 96 10.134.8.171
[2024-03-04 16:42:24,772] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,772] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 58
MA = 10.134.8.171
World view:  58 96 10.134.8.171
[2024-03-04 16:42:24,772] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 59
MA = 10.134.8.171
World view:  59 96 10.134.8.171
[2024-03-04 16:42:24,772] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,772] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 16:42:24,772] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 57
MA = 10.134.8.171
World view:  57 96 10.134.8.171
[2024-03-04 16:42:24,775] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,775] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 32
MA = 10.134.8.171
World view:  32 96 10.134.8.171
[2024-03-04 16:42:24,831] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,831] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 33
MA = 10.134.8.171
World view:  33 96 10.134.8.171
[2024-03-04 16:42:24,831] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 34
MA = 10.134.8.171
World view:  34 96 10.134.8.171
[2024-03-04 16:42:24,831] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,831] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 16:42:24,831] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 31
MA = 10.134.8.171
World view:  31 96 10.134.8.171
[2024-03-04 16:42:24,832] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,832] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 30
MA = 10.134.8.171
World view:  30 96 10.134.8.171
[2024-03-04 16:42:24,832] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,832] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 35
MA = 10.134.8.171
World view:  35 96 10.134.8.171
[2024-03-04 16:42:24,832] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,832] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 60
MA = 10.134.8.171
World view:  60 96 10.134.8.171
[2024-03-04 16:42:24,961] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 62
MA = 10.134.8.171
World view:  62 96 10.134.8.171
[2024-03-04 16:42:24,961] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,962] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 16:42:24,962] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 61
MA = 10.134.8.171
World view:  61 96 10.134.8.171
[2024-03-04 16:42:24,962] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,962] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 64
MA = 10.134.8.171
World view:  64 96 10.134.8.171
[2024-03-04 16:42:24,962] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,962] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 63
MA = 10.134.8.171
World view:  63 96 10.134.8.171
[2024-03-04 16:42:24,964] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,964] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 65
MA = 10.134.8.171
World view:  65 96 10.134.8.171
[2024-03-04 16:42:24,965] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,965] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 26
MA = 10.134.8.171
World view:  26 96 10.134.8.171
[2024-03-04 16:42:24,979] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,979] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 29
MA = 10.134.8.171
World view:  29 96 10.134.8.171
[2024-03-04 16:42:24,979] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 25
MA = 10.134.8.171
World view:  25 96 10.134.8.171
[2024-03-04 16:42:24,979] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,979] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 16:42:24,979] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 28
MA = 10.134.8.171
World view:  28 96 10.134.8.171
[2024-03-04 16:42:24,979] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,979] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 24
MA = 10.134.8.171
World view:  24 96 10.134.8.171
[2024-03-04 16:42:24,980] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,980] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 27
MA = 10.134.8.171
World view:  27 96 10.134.8.171
[2024-03-04 16:42:24,981] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,981] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 49
MA = 10.134.8.171
World view:  49 96 10.134.8.171
[2024-03-04 16:42:24,982] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,982] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 52
MA = 10.134.8.171
World view:  52 96 10.134.8.171
[2024-03-04 16:42:24,982] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 50
MA = 10.134.8.171
World view:  50 96 10.134.8.171
[2024-03-04 16:42:24,982] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,982] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 51
MA = 10.134.8.171
World view:  51 96 10.134.8.171
[2024-03-04 16:42:24,982] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,982] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 16:42:24,982] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 53
MA = 10.134.8.171
World view:  53 96 10.134.8.171
[2024-03-04 16:42:24,983] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,983] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 48
MA = 10.134.8.171
World view:  48 96 10.134.8.171
[2024-03-04 16:42:24,983] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:24,983] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 36
MA = 10.134.8.171
World view:  36 96 10.134.8.171
[2024-03-04 16:42:25,262] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 37
MA = 10.134.8.171
World view:  37 96 10.134.8.171
[2024-03-04 16:42:25,262] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 38
MA = 10.134.8.171
World view:  38 96 10.134.8.171
[2024-03-04 16:42:25,262] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:25,262] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 16:42:25,262] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 40
MA = 10.134.8.171
World view:  40 96 10.134.8.171
[2024-03-04 16:42:25,262] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:25,262] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 16:42:25,262] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 41
MA = 10.134.8.171
World view:  41 96 10.134.8.171
[2024-03-04 16:42:25,262] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:25,262] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 39
MA = 10.134.8.171
World view:  39 96 10.134.8.171
[2024-03-04 16:42:25,263] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:25,263] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 14
MA = 10.134.8.171
World view:  14 96 10.134.8.171
[2024-03-04 16:42:26,831] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 16
MA = 10.134.8.171
World view:  16 96 10.134.8.171
[2024-03-04 16:42:26,831] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 17
MA = 10.134.8.171
World view:  17 96 10.134.8.171
[2024-03-04 16:42:26,831] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:26,831] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 16:42:26,831] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 16:42:26,831] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 15
MA = 10.134.8.171
World view:  15 96 10.134.8.171
[2024-03-04 16:42:26,831] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 19
MA = 10.134.8.171
World view:  19 96 10.134.8.171
[2024-03-04 16:42:26,832] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:26,831] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 12
MA = 10.134.8.171
World view:  12 96 10.134.8.171
[2024-03-04 16:42:26,832] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 20
MA = 10.134.8.171
World view:  20 96 10.134.8.171
[2024-03-04 16:42:26,832] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 22
MA = 10.134.8.171
World view:  22 96 10.134.8.171
[2024-03-04 16:42:26,832] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:26,832] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 16:42:26,832] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 23
MA = 10.134.8.171
World view:  23 96 10.134.8.171
[2024-03-04 16:42:26,832] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:26,832] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 21
MA = 10.134.8.171
World view:  21 96 10.134.8.171
[2024-03-04 16:42:26,832] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 18
MA = 10.134.8.171
World view:  18 96 10.134.8.171
[2024-03-04 16:42:26,832] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:26,832] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 16:42:26,832] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 16:42:26,832] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 16:42:26,832] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 13
MA = 10.134.8.171
World view:  13 96 10.134.8.171
[2024-03-04 16:42:26,832] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:26,832] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 16:42:30,686] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:30,686] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:30,686] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:30,686] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:30,686] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:30,686] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:30,687] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:30,687] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:30,687] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:30,687] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:30,687] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-03-04 16:42:30,687] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 95
MA = 10.134.8.171
World view:  95 96 10.134.8.171
> setting tensorboard ...
[2024-03-04 16:42:35,178] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:35,178] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 0
MA = 10.134.8.171
World view:  0 96 10.134.8.171
using world size: 96, data-parallel-size: 12, sequence-parallel size: 1, tensor-model-parallel size: 8, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  add_bias_linear ................................. True
  add_position_embedding .......................... True
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  aml_data_download_path .......................... None
  apply_layernorm_1p .............................. False
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  async_tensor_model_parallel_allreduce ........... False
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  barrier_with_L1_time ............................ True
  bert_binary_head ................................ True
  bert_embedder_type .............................. megatron
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  checkpoint_activations .......................... True
  checkpoint_in_cpu ............................... False
  checkpoint_num_layers ........................... 1
  classes_fraction ................................ 1.0
  clip_grad ....................................... 1.0
  compression_training ............................ False
  consumed_train_samples .......................... 0
  consumed_train_tokens ........................... 0
  consumed_valid_samples .......................... 0
  contigious_checkpointing ........................ False
  cpu_optimizer ................................... False
  cpu_torch_adam .................................. False
  create_moe_param_group .......................... True
  curriculum_learning_legacy ...................... False
  data_cache_path ................................. None
  data_efficiency_curriculum_learning ............. False
  data_impl ....................................... mmap
  data_parallel_random_init ....................... False
  data_parallel_size .............................. 12
  data_path ....................................... ['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document']
  data_per_class_fraction ......................... 1.0
  data_sharding ................................... True
  dataloader_type ................................. single
  DDP_impl ........................................ local
  decoder_num_layers .............................. None
  decoder_seq_length .............................. None
  deepscale ....................................... False
  deepscale_config ................................ None
  deepspeed ....................................... True
  deepspeed_activation_checkpointing .............. True
  deepspeed_config ................................ ds_config_gpt_gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true.json
  deepspeed_mpi ................................... False
  dino_bottleneck_size ............................ 256
  dino_freeze_last_layer .......................... 1
  dino_head_hidden_size ........................... 2048
  dino_local_crops_number ......................... 10
  dino_local_img_size ............................. 96
  dino_norm_last_layer ............................ False
  dino_teacher_temp ............................... 0.07
  dino_warmup_teacher_temp ........................ 0.04
  dino_warmup_teacher_temp_epochs ................. 30
  distribute_checkpointed_activations ............. False
  distribute_saved_activations .................... False
  distributed_backend ............................. nccl
  distributed_timeout_minutes ..................... 10
  ds_fused_adam ................................... False
  ds_inference .................................... False
  ds_pipeline_enabled ............................. False
  ds_sequence_parallel_size ....................... 1
  embedding_path .................................. None
  embedding_weights_in_fp32 ....................... False
  empty_unused_memory_level ....................... 0
  enable_expert_tensor_parallelism ................ False
  encoder_num_layers .............................. 32
  encoder_seq_length .............................. 2048
  end_weight_decay ................................ 0.1
  eod_mask_loss ................................... False
  eval_interval ................................... 100
  eval_iters ...................................... 10
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... 30000000
  exit_interval ................................... None
  exit_on_missing_checkpoint ...................... False
  exit_signal_handler ............................. False
  expert_interval ................................. 2
  ffn_hidden_size ................................. 16384
  finetune ........................................ False
  force_ds_sequence_parallel ...................... False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  fp8_amax_compute_algo ........................... most_recent
  fp8_amax_history_len ............................ 1
  fp8_e4m3 ........................................ False
  fp8_hybrid ...................................... False
  fp8_interval .................................... 1
  fp8_margin ...................................... 0
  fp8_wgrad ....................................... True
  global_batch_size ............................... 24
  gradient_accumulation_fusion .................... True
  head_lr_mult .................................... 1.0
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 4096
  hidden_size_teacher ............................. None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_h ........................................... 224
  img_w ........................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference ....................................... False
  inference_batch_times_seqlen_threshold .......... 512
  init_method_std ................................. 0.014
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  iter_per_epoch .................................. 1250
  kd .............................................. False
  kd_alpha_ce ..................................... 1
  kd_beta_ce ...................................... 1
  kd_temp ......................................... 1.0
  kv_channels ..................................... 128
  layernorm_epsilon ............................... 1e-05
  lazy_mpu_init ................................... None
  load ............................................ /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true
  load_tag ........................................ None
  load_teacher .................................... None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... True
  log_interval .................................... 10
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... False
  log_optimizer_states_to_tensorboard ............. False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... True
  log_validation_ppl_to_tensorboard ............... True
  log_world_size_to_tensorboard ................... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.00012
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_decay_tokens ................................. 300000000000
  lr_warmup_fraction .............................. None
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_warmup_tokens ................................ 375000000
  make_vocab_size_divisible_by .................... 128
  mask_factor ..................................... 1.0
  mask_prob ....................................... 0.15
  mask_type ....................................... random
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 2048
  max_tokens_to_oom ............................... 12000
  mem_efficient_ln ................................ True
  memory_centric_tiled_linear ..................... False
  merge_file ...................................... gpt2-merges.txt
  micro_batch_size ................................ 1
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-06
  mlp_type ........................................ standard
  mmap_warmup ..................................... False
  moe_eval_capacity_factor ........................ 1.0
  moe_expert_parallel_size ........................ 4
  moe_loss_coeff .................................. 0.01
  moe_min_capacity ................................ 4
  moe_token_dropping .............................. True
  moe_train_capacity_factor ....................... 1.0
  mos ............................................. False
  no_load_lr_state ................................ False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_persist_layer_norm ........................... False
  no_pipeline_parallel ............................ True
  no_save_optim ................................... None
  no_save_rng ..................................... None
  normalization ................................... layernorm
  num_attention_heads ............................. 32
  num_attention_heads_teacher ..................... None
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... [4]
  num_experts_switch .............................. None
  num_experts_teacher ............................. [1]
  num_key_value_heads ............................. 32
  num_layers ...................................... 32
  num_layers_per_virtual_pipeline_stage ........... None
  num_layers_teacher .............................. None
  num_workers ..................................... 0
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  output_bert_embeddings .......................... False
  overlap_p2p_comm ................................ False
  override_opt_param_scheduler .................... True
  params_dtype .................................... torch.float16
  partition_activations ........................... False
  patch_dim ....................................... 16
  perform_initialization .......................... True
  pipeline_model_parallel_size .................... 1
  pipeline_model_parallel_split_rank .............. None
  profile_backward ................................ False
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  random_ltd ...................................... False
  rank ............................................ 0
  recompute_granularity ........................... None
  recompute_method ................................ None
  recompute_num_layers ............................ 1
  remote_device ................................... none
  repeated_dataloader ............................. False
  reset_attention_mask ............................ False
  reset_iteration ................................. False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  retro_add_retriever ............................. False
  retro_cyclic_train_iters ........................ None
  retro_encoder_attention_dropout ................. 0.1
  retro_encoder_hidden_dropout .................... 0.1
  retro_encoder_layers ............................ 2
  retro_num_neighbors ............................. 2
  retro_num_retrieved_chunks ...................... 2
  retro_return_doc_ids ............................ False
  retro_workdir ................................... None
  return_data_index ............................... False
  rope_theta ...................................... 10000
  rotary_percent .................................. 1.0
  sample_rate ..................................... 1.0
  save ............................................ /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true
  save_interval ................................... 10000
  scatter_gather_tensors_in_pipeline .............. True
  scattered_embeddings ............................ False
  seed ............................................ 1234
  seq_length ...................................... 2048
  sequence_parallel ............................... False
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  skip_train ...................................... False
  split ........................................... 98,2,0
  split_transformers .............................. False
  squared_relu .................................... False
  standalone_embedding_stage ...................... False
  start_weight_decay .............................. 0.1
  swiglu .......................................... False
  swin_backbone_type .............................. tiny
  synchronize_each_layer .......................... False
  tensor_model_parallel_size ...................... 8
  tensorboard_dir ................................. /gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1
  test_data_path .................................. None
  tile_factor ..................................... 1
  timing_log_level ................................ 0
  timing_log_option ............................... minmax
  titles_data_path ................................ None
  tokenizer_model ................................. None
  tokenizer_type .................................. GPT2BPETokenizer
  topk ............................................ 1
  train_data_exact_num_epochs ..................... None
  train_data_path ................................. None
  train_desc_path ................................. None
  train_doc_idx_path .............................. None
  train_idx_path .................................. None
  train_iters ..................................... 18310546
  train_sample_idx_path ........................... None
  train_samples ................................... None
  train_shuffle_idx_path .......................... None
  train_tokens .................................... 300000000000
  transformer_impl ................................ local
  transformer_pipeline_model_parallel_size ........ 1
  universal_checkpoint ............................ False
  untie_embeddings_and_output_weights ............. False
  use_checkpoint_args ............................. False
  use_checkpoint_opt_param_scheduler .............. False
  use_contiguous_buffers_in_local_ddp ............. True
  use_cpu_initialization .......................... None
  use_dataset_only ................................ False
  use_distributed_optimizer ....................... False
  use_flash_attn .................................. False
  use_flash_attn_triton ........................... False
  use_flash_attn_v1 ............................... False
  use_flash_attn_v2 ............................... False
  use_one_sent_docs ............................... False
  use_pin_memory .................................. False
  use_ring_exchange_p2p ........................... False
  use_rotary_position_embeddings .................. False
  use_tutel ....................................... False
  valid_data_path ................................. None
  variable_seq_lengths ............................ False
  virtual_pipeline_model_parallel_size ............ None
  vision_backbone_type ............................ vit
  vision_pretraining .............................. False
  vision_pretraining_type ......................... classify
  vocab_extra_ids ................................. 0
  vocab_file ...................................... gpt2-vocab.json
  vocab_size ...................................... None
  weight_decay .................................... 0.1
  weight_decay_incr_style ......................... constant
  world_size ...................................... 96
  zero_allgather_bucket_size ...................... 0.0
  zero_contigious_gradients ....................... False
  zero_reduce_bucket_size ......................... 0.0
  zero_reduce_scatter ............................. False
  zero_stage ...................................... 1.0
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 2
> building GPT2BPETokenizer tokenizer ...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 7
MA = 10.134.8.171
World view:  7 96 10.134.8.171
[2024-03-04 16:42:42,287] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:42,287] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 9
MA = 10.134.8.171
World view:  9 96 10.134.8.171
[2024-03-04 16:42:42,287] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 11
MA = 10.134.8.171
World view:  11 96 10.134.8.171
[2024-03-04 16:42:42,287] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 10
MA = 10.134.8.171
World view:  10 96 10.134.8.171
[2024-03-04 16:42:42,287] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:42,287] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 16:42:42,287] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 16:42:42,287] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 6
MA = 10.134.8.171
World view:  6 96 10.134.8.171
[2024-03-04 16:42:42,288] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:42,288] [INFO] [comm.py:616:init_distributed] cdb=None
 > padded vocab (size: 50257) with 943 dummy tokens (new size: 51200)
> initializing torch distributed ...
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 4
MA = 10.134.8.171
World view:  4 96 10.134.8.171
[2024-03-04 16:42:42,288] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 5
MA = 10.134.8.171
World view:  5 96 10.134.8.171
[2024-03-04 16:42:42,288] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:42,288] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:42,288] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 1
MA = 10.134.8.171
World view:  1 96 10.134.8.171
[2024-03-04 16:42:42,288] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:42,288] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 2
MA = 10.134.8.171
World view:  2 96 10.134.8.171
[2024-03-04 16:42:42,288] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:42,288] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 16:42:42,288] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 16:42:42,288] [INFO] [comm.py:616:init_distributed] cdb=None
[2024-03-04 16:42:42,288] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 3
MA = 10.134.8.171
World view:  3 96 10.134.8.171
[2024-03-04 16:42:42,288] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:42,288] [INFO] [comm.py:616:init_distributed] cdb=None
--------------------------------------------------
DeepSpeed C++/CUDA extension op report
--------------------------------------------------
NOTE: Ops not installed will be just-in-time (JIT) compiled at
      runtime if needed. Op compatibility means that your system
      meet the required dependencies to JIT install the op.
--------------------------------------------------
JIT compiled ops requires ninja
ninja .................. [91m[FAIL][0m
--------------------------------------------------
op name ................ installed .. compatible
--------------------------------------------------
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
async_io ............... [93m[NO][0m ....... [93m[NO][0m
cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
--------------------------------------------------
DeepSpeed general environment info:
torch install path ............... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/torch']
torch version .................... 2.0.1
deepspeed install path ........... ['/gpfs/alpine2/stf218/world-shared/sajal/moe-env-311/lib/python3.11/site-packages/deepspeed']
deepspeed info ................... 0.10.0+f5c834a6e, f5c834a6e, HEAD
torch cuda version ............... 11.8
torch hip version ................ None
nvcc version ..................... 11.8
deepspeed wheel compiled w. ...... torch 2.0, cuda 12.2
**** Git info for Megatron: git_hash=unknown git_branch=unknown ****
world vision: 96 8
MA = 10.134.8.171
World view:  8 96 10.134.8.171
[2024-03-04 16:42:42,289] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2024-03-04 16:42:42,289] [INFO] [comm.py:616:init_distributed] cdb=None
> initialized tensor model parallel with size 8
> initialized pipeline model parallel with size 1
> setting random seeds to 1234 ...
[2024-03-04 16:42:47,654] [INFO] [checkpointing.py:226:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
make: Entering directory '/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/data'
make: Nothing to be done for 'default'.
make: Leaving directory '/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/megatron/data'
> compiling dataset index builder ...
>>> done with dataset index builder. Compilation time: 0.107 seconds
> compiling and loading fused kernels ...
ninja: no work to do.
ninja: no work to do.
ninja: no work to do.
d09n07:1061887:1061887 [0] NCCL INFO Bootstrap : Using ib0:10.41.8.171<0>
d09n07:1061887:1061887 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
d09n07:1061887:1061887 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
d09n07:1061887:1061887 [0] NCCL INFO cudaDriverVersion 12020
NCCL version 2.18.3+cuda11.8
>>> done with compiling and loading fused kernels. Compilation time: 2.997 seconds
time to initialize megatron (seconds): 28.804
[after megatron is initialized] datetime: 2024-03-04 16:42:52 
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=0, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
building GPT model ...
[2024-03-04 16:42:52,660] [INFO] [utils.py:785:see_memory_usage] Before Building Model
[2024-03-04 16:42:52,661] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.13 GB         CA 0.0 GB         Max_CA 0 GB 
[2024-03-04 16:42:52,661] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 49.03 GB, percent = 8.2%
[2024-03-04 16:42:53,094] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 1 | expert_parallel_size: 4
[2024-03-04 16:42:53,136] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 1 | expert_parallel_size: 4
[2024-03-04 16:42:53,168] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 1 | expert_parallel_size: 4
[2024-03-04 16:42:53,205] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 1 | expert_parallel_size: 4
[2024-03-04 16:42:53,226] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 1 | expert_parallel_size: 4
[2024-03-04 16:42:53,265] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 1 | expert_parallel_size: 4
[2024-03-04 16:42:53,286] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 1 | expert_parallel_size: 4
[2024-03-04 16:42:53,337] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 1 | expert_parallel_size: 4
[2024-03-04 16:42:53,369] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 1 | expert_parallel_size: 4
[2024-03-04 16:42:53,425] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 1 | expert_parallel_size: 4
[2024-03-04 16:42:53,446] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 1 | expert_parallel_size: 4
[2024-03-04 16:42:53,524] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 1 | expert_parallel_size: 4
[2024-03-04 16:42:53,546] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 1 | expert_parallel_size: 4
[2024-03-04 16:42:53,559] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 1 | expert_parallel_size: 4
[2024-03-04 16:42:53,570] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 1 | expert_parallel_size: 4
[2024-03-04 16:42:53,583] [INFO] [logging.py:96:log_dist] [Rank 0] Creating MoE layer with num_experts: 4 | num_local_experts: 1 | expert_parallel_size: 4
[2024-03-04 16:42:53,675] [INFO] [utils.py:785:see_memory_usage] After Building Model
[2024-03-04 16:42:53,676] [INFO] [utils.py:786:see_memory_usage] MA 5.05 GB         Max_MA 5.3 GB         CA 5.38 GB         Max_CA 5 GB 
[2024-03-04 16:42:53,676] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 78.56 GB, percent = 13.2%
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2720358400
> learning rate decay style: cosine
DeepSpeed is enabled.
[2024-03-04 16:42:53,687] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0+f5c834a6e, git-hash=f5c834a6e, git-branch=HEAD
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=1, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
 > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 2720358400
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=7, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
 > number of parameters on (tensor, pipeline) model parallel rank (7, 0): 2720358400
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=6, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
 > number of parameters on (tensor, pipeline) model parallel rank (6, 0): 2720358400
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=3, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
 > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 2720358400
No existing process group found, creating a new group named: ep_size_4
[2024-03-04 16:42:53,743] [INFO] [logging.py:96:log_dist] [Rank 0] Creating expert and data parallel groups with size 4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_inArgs= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_inArgs= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=4, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_s_pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=2, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
 > number of parameters on (tensor, pipeline) model parallel rank (4, 0): 2720358400
_pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=5, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
 > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 2720358400
ize=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
 > number of parameters on (tensor, pipeline) model parallel rank (5, 0): 2720358400
[2024-03-04 16:42:53,887] [INFO] [logging.py:96:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_4 with ranks: [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60, 64, 68, 72, 76, 80, 84, 88, 92]
[2024-03-04 16:42:53,898] [INFO] [logging.py:96:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_4 with ranks: [1, 5, 9, 13, 17, 21, 25, 29, 33, 37, 41, 45, 49, 53, 57, 61, 65, 69, 73, 77, 81, 85, 89, 93]
[2024-03-04 16:42:53,909] [INFO] [logging.py:96:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_4 with ranks: [2, 6, 10, 14, 18, 22, 26, 30, 34, 38, 42, 46, 50, 54, 58, 62, 66, 70, 74, 78, 82, 86, 90, 94]
[2024-03-04 16:42:53,919] [INFO] [logging.py:96:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_4 with ranks: [3, 7, 11, 15, 19, 23, 27, 31, 35, 39, 43, 47, 51, 55, 59, 63, 67, 71, 75, 79, 83, 87, 91, 95]
[2024-03-04 16:42:53,930] [INFO] [logging.py:96:log_dist] [Rank 0] creating expert parallel process group named ep_size_4 with ranks: [0, 1, 2, 3]
[2024-03-04 16:42:53,941] [INFO] [logging.py:96:log_dist] [Rank 0] creating expert parallel process group named ep_size_4 with ranks: [4, 5, 6, 7]
[2024-03-04 16:42:53,951] [INFO] [logging.py:96:log_dist] [Rank 0] creating expert parallel process group named ep_size_4 with ranks: [8, 9, 10, 11]
[2024-03-04 16:42:53,962] [INFO] [logging.py:96:log_dist] [Rank 0] creating expert parallel process group named ep_size_4 with ranks: [12, 13, 14, 15]
[2024-03-04 16:42:53,973] [INFO] [logging.py:96:log_dist] [Rank 0] creating expert parallel process group named ep_size_4 with ranks: [16, 17, 18, 19]
[2024-03-04 16:42:53,983] [INFO] [logging.py:96:log_dist] [Rank 0] creating expert parallel process group named ep_size_4 with ranks: [20, 21, 22, 23]
[2024-03-04 16:42:53,994] [INFO] [logging.py:96:log_dist] [Rank 0] creating expert parallel process group named ep_size_4 with ranks: [24, 25, 26, 27]
[2024-03-04 16:42:54,005] [INFO] [logging.py:96:log_dist] [Rank 0] creating expert parallel process group named ep_size_4 with ranks: [28, 29, 30, 31]
[2024-03-04 16:42:54,016] [INFO] [logging.py:96:log_dist] [Rank 0] creating expert parallel process group named ep_size_4 with ranks: [32, 33, 34, 35]
[2024-03-04 16:42:54,026] [INFO] [logging.py:96:log_dist] [Rank 0] creating expert parallel process group named ep_size_4 with ranks: [36, 37, 38, 39]
[2024-03-04 16:42:54,037] [INFO] [logging.py:96:log_dist] [Rank 0] creating expert parallel process group named ep_size_4 with ranks: [40, 41, 42, 43]
[2024-03-04 16:42:54,048] [INFO] [logging.py:96:log_dist] [Rank 0] creating expert parallel process group named ep_size_4 with ranks: [44, 45, 46, 47]
[2024-03-04 16:42:54,058] [INFO] [logging.py:96:log_dist] [Rank 0] creating expert parallel process group named ep_size_4 with ranks: [48, 49, 50, 51]
[2024-03-04 16:42:54,069] [INFO] [logging.py:96:log_dist] [Rank 0] creating expert parallel process group named ep_size_4 with ranks: [52, 53, 54, 55]
[2024-03-04 16:42:54,080] [INFO] [logging.py:96:log_dist] [Rank 0] creating expert parallel process group named ep_size_4 with ranks: [56, 57, 58, 59]
[2024-03-04 16:42:54,090] [INFO] [logging.py:96:log_dist] [Rank 0] creating expert parallel process group named ep_size_4 with ranks: [60, 61, 62, 63]
[2024-03-04 16:42:54,101] [INFO] [logging.py:96:log_dist] [Rank 0] creating expert parallel process group named ep_size_4 with ranks: [64, 65, 66, 67]
[2024-03-04 16:42:54,112] [INFO] [logging.py:96:log_dist] [Rank 0] creating expert parallel process group named ep_size_4 with ranks: [68, 69, 70, 71]
[2024-03-04 16:42:54,123] [INFO] [logging.py:96:log_dist] [Rank 0] creating expert parallel process group named ep_size_4 with ranks: [72, 73, 74, 75]
[2024-03-04 16:42:54,133] [INFO] [logging.py:96:log_dist] [Rank 0] creating expert parallel process group named ep_size_4 with ranks: [76, 77, 78, 79]
[2024-03-04 16:42:54,144] [INFO] [logging.py:96:log_dist] [Rank 0] creating expert parallel process group named ep_size_4 with ranks: [80, 81, 82, 83]
[2024-03-04 16:42:54,155] [INFO] [logging.py:96:log_dist] [Rank 0] creating expert parallel process group named ep_size_4 with ranks: [84, 85, 86, 87]
[2024-03-04 16:42:54,165] [INFO] [logging.py:96:log_dist] [Rank 0] creating expert parallel process group named ep_size_4 with ranks: [88, 89, 90, 91]
d09n07:1061892:1061892 [5] NCCL INFO cudaDriverVersion 12020
d09n07:1061892:1061892 [5] NCCL INFO Bootstrap : Using ib0:10.41.8.171<0>
d09n07:1061892:1061892 [5] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
d09n07:1061892:1061892 [5] NCCL INFO NET/Plugin : No plugin found, using internal implementation
d09n07:1061892:1062189 [5] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.8.171<0>
d09n07:1061892:1062189 [5] NCCL INFO Using network IB
d09n07:1061892:1062189 [5] NCCL INFO comm 0x12fbc38e0 rank 5 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x7b12926388d12ce3 - Init START
d09n07:1061892:1062189 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
d09n07:1061892:1062189 [5] NCCL INFO Trees [0] -1/-1/-1->5->4 [1] 1/-1/-1->5->3 [2] -1/-1/-1->5->4 [3] 1/-1/-1->5->3
d09n07:1061892:1062189 [5] NCCL INFO P2P Chunksize set to 131072
d09n07:1061892:1062189 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[0] [send] via NET/IB/1
d09n07:1061892:1062189 [5] NCCL INFO Channel 02/0 : 5[5] -> 6[0] [send] via NET/IB/1
d09n07:1061892:1062189 [5] NCCL INFO Channel 01/0 : 5[5] -> 2[2] via P2P/IPC
d09n07:1061892:1062189 [5] NCCL INFO Channel 03/0 : 5[5] -> 2[2] via P2P/IPC
d09n07:1061892:1062189 [5] NCCL INFO Connected all rings
d09n07:1061892:1062189 [5] NCCL INFO Channel 01/0 : 5[5] -> 1[1] via P2P/IPC
d09n07:1061892:1062189 [5] NCCL INFO Channel 03/0 : 5[5] -> 1[1] via P2P/IPC
d09n07:1061892:1062189 [5] NCCL INFO Channel 01/0 : 5[5] -> 3[3] via P2P/IPC
d09n07:1061892:1062189 [5] NCCL INFO Channel 03/0 : 5[5] -> 3[3] via P2P/IPC
d09n07:1061892:1062189 [5] NCCL INFO Channel 00/0 : 5[5] -> 4[4] via P2P/IPC
d09n07:1061892:1062189 [5] NCCL INFO Channel 02/0 : 5[5] -> 4[4] via P2P/IPC
d09n07:1061892:1062189 [5] NCCL INFO Connected all trees
d09n07:1061892:1062189 [5] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
d09n07:1061892:1062189 [5] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
d09n07:1061892:1062189 [5] NCCL INFO comm 0x12fbc38e0 rank 5 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x7b12926388d12ce3 - Init COMPLETE
NCCL version 2.18.3+cuda11.8
d09n08:1072933:1072933 [1] NCCL INFO cudaDriverVersion 12020
d09n08:1072933:1072933 [1] NCCL INFO Bootstrap : Using ib0:10.41.8.172<0>
d09n08:1072933:1072933 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
d09n08:1072933:1072933 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
d09n08:1072933:1073163 [1] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.8.172<0>
d09n08:1072933:1073163 [1] NCCL INFO Using network IB
d09n08:1072933:1073163 [1] NCCL INFO comm 0x159a93a20 rank 7 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x7b12926388d12ce3 - Init START
d09n08:1072933:1073163 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
d09n08:1072933:1073163 [1] NCCL INFO Trees [0] 8/-1/-1->7->6 [1] 6/-1/-1->7->11 [2] 8/12/-1->7->6 [3] 6/-1/-1->7->11
d09n08:1072933:1073163 [1] NCCL INFO P2P Chunksize set to 131072
d09n08:1072933:1073163 [1] NCCL INFO Channel 00/0 : 7[1] -> 8[2] via P2P/IPC
d09n08:1072933:1073163 [1] NCCL INFO Channel 02/0 : 7[1] -> 8[2] via P2P/IPC
d09n08:1072933:1073163 [1] NCCL INFO Channel 01/0 : 7[1] -> 6[0] via P2P/IPC
d09n08:1072933:1073163 [1] NCCL INFO Channel 03/0 : 7[1] -> 6[0] via P2P/IPC
d09n08:1072933:1073163 [1] NCCL INFO Connected all rings
d09n08:1072933:1073163 [1] NCCL INFO Channel 01/0 : 7[1] -> 11[5] via P2P/IPC
d09n08:1072933:1073163 [1] NCCL INFO Channel 03/0 : 7[1] -> 11[5] via P2P/IPC
d09n08:1072933:1073163 [1] NCCL INFO Channel 02/0 : 7[1] -> 12[0] [send] via NET/IB/0
d09n08:1072933:1073163 [1] NCCL INFO Channel 02/0 : 12[0] -> 7[1] [receive] via NET/IB/0
d09n08:1072933:1073163 [1] NCCL INFO Channel 00/0 : 7[1] -> 6[0] via P2P/IPC
d09n08:1072933:1073163 [1] NCCL INFO Channel 02/0 : 7[1] -> 6[0] via P2P/IPC
d09n08:1072933:1073163 [1] NCCL INFO Connected all trees
d09n08:1072933:1073163 [1] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
d09n08:1072933:1073163 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
d09n08:1072933:1073163 [1] NCCL INFO comm 0x159a93a20 rank 7 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x7b12926388d12ce3 - Init COMPLETE
NCCL version 2.18.3+cuda11.8
d09n07:1061891:1061891 [4] NCCL INFO cudaDriverVersion 12020
d09n07:1061891:1061891 [4] NCCL INFO Bootstrap : Using ib0:10.41.8.171<0>
d09n07:1061891:1061891 [4] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
d09n07:1061891:1061891 [4] NCCL INFO NET/Plugin : No plugin found, using internal implementation
d09n07:1061891:1062190 [4] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.8.171<0>
d09n07:1061891:1062190 [4] NCCL INFO Using network IB
d09n07:1061891:1062190 [4] NCCL INFO comm 0x121c33b00 rank 4 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x7b12926388d12ce3 - Init START
d09n07:1061891:1062190 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
d09n07:1061891:1062190 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 3/52/-1->4->-1 [2] 5/-1/-1->4->3 [3] 3/-1/-1->4->10
d09n07:1061891:1062190 [4] NCCL INFO P2P Chunksize set to 131072
d09n07:1061891:1062190 [4] NCCL INFO Channel 00/0 : 4[4] -> 5[5] via P2P/IPC
d09n07:1061891:1062190 [4] NCCL INFO Channel 01/0 : 4[4] -> 5[5] via P2P/IPC
d09n07:1061891:1062190 [4] NCCL INFO Channel 02/0 : 4[4] -> 5[5] via P2P/IPC
d09n07:1061891:1062190 [4] NCCL INFO Channel 03/0 : 4[4] -> 5[5] via P2P/IPC
d09n07:1061891:1062190 [4] NCCL INFO Connected all rings
d09n07:1061891:1062190 [4] NCCL INFO Channel 03/0 : 4[4] -> 10[4] [send] via NET/IB/3
d09n07:1061891:1062190 [4] NCCL INFO Channel 01/0 : 52[4] -> 4[4] [receive] via NET/IB/3
d09n07:1061891:1062190 [4] NCCL INFO Channel 01/0 : 4[4] -> 52[4] [send] via NET/IB/3
d09n07:1061891:1062190 [4] NCCL INFO Channel 03/0 : 10[4] -> 4[4] [receive] via NET/IB/3
d09n07:1061891:1062190 [4] NCCL INFO Channel 00/0 : 4[4] -> 3[3] via P2P/IPC
d09n07:1061891:1062190 [4] NCCL INFO Channel 01/0 : 4[4] -> 3[3] via P2P/IPC
d09n07:1061891:1062190 [4] NCCL INFO Channel 02/0 : 4[4] -> 3[3] via P2P/IPC
d09n07:1061891:1062190 [4] NCCL INFO Channel 03/0 : 4[4] -> 3[3] via P2P/IPC
d09n07:1061891:1062190 [4] NCCL INFO Connected all trees
d09n07:1061891:1062190 [4] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
d09n07:1061891:1062190 [4] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
d09n07:1061891:1062190 [4] NCCL INFO comm 0x121c33b00 rank 4 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x7b12926388d12ce3 - Init COMPLETE
NCCL version 2.18.3+cuda11.8
d09n07:1061889:1061889 [2] NCCL INFO cudaDriverVersion 12020
d09n07:1061889:1061889 [2] NCCL INFO Bootstrap : Using ib0:10.41.8.171<0>
d09n07:1061889:1061889 [2] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
d09n07:1061889:1061889 [2] NCCL INFO NET/Plugin : No plugin found, using internal implementation
d09n07:1061889:1062187 [2] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.8.171<0>
d09n07:1061889:1062187 [2] NCCL INFO Using network IB
d09n07:1061889:1062187 [2] NCCL INFO comm 0x12f593f50 rank 2 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x7b12926388d12ce3 - Init START
d09n07:1061889:1062187 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
d09n07:1061889:1062187 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] -1/-1/-1->2->0 [2] 3/-1/-1->2->1 [3] -1/-1/-1->2->0
d09n07:1061889:1062187 [2] NCCL INFO P2P Chunksize set to 131072
d09n07:1061889:1062187 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/IPC
d09n07:1061889:1062187 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/IPC
d09n07:1061889:1062187 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/IPC
d09n07:1061889:1062187 [2] NCCL INFO Channel 03/0 : 2[2] -> 1[1] via P2P/IPC
d09n07:1061889:1062187 [2] NCCL INFO Connected all rings
d09n07:1061889:1062187 [2] NCCL INFO Channel 01/0 : 2[2] -> 0[0] via P2P/IPC
d09n07:1061889:1062187 [2] NCCL INFO Channel 03/0 : 2[2] -> 0[0] via P2P/IPC
d09n07:1061889:1062187 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/IPC
d09n07:1061889:1062187 [2] NCCL INFO Channel 02/0 : 2[2] -> 1[1] via P2P/IPC
d09n07:1061889:1062187 [2] NCCL INFO Connected all trees
d09n07:1061889:1062187 [2] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
d09n07:1061889:1062187 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
d09n07:1061889:1062187 [2] NCCL INFO comm 0x12f593f50 rank 2 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x7b12926388d12ce3 - Init COMPLETE
NCCL version 2.18.3+cuda11.8
d09n08:1072932:1072932 [0] NCCL INFO cudaDriverVersion 12020
d09n08:1072932:1072932 [0] NCCL INFO Bootstrap : Using ib0:10.41.8.172<0>
d09n08:1072932:1072932 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
d09n08:1072932:1072932 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
d09n08:1072932:1073164 [0] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.8.172<0>
d09n08:1072932:1073164 [0] NCCL INFO Using network IB
d09n08:1072932:1073164 [0] NCCL INFO comm 0x14e523960 rank 6 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x7b12926388d12ce3 - Init START
d09n08:1072932:1073164 [0] NCCL INFO Setting affinity for GPU 0 to 0f
d09n08:1072932:1073164 [0] NCCL INFO Trees [0] 7/-1/-1->6->13 [1] 8/-1/-1->6->7 [2] 7/0/-1->6->18 [3] 8/-1/-1->6->7
d09n08:1072932:1073164 [0] NCCL INFO P2P Chunksize set to 131072
d09n08:1072932:1073164 [0] NCCL INFO Channel 00/0 : 5[5] -> 6[0] [receive] via NET/IB/0
d09n08:1072932:1073164 [0] NCCL INFO Channel 02/0 : 5[5] -> 6[0] [receive] via NET/IB/0
d09n08:1072932:1073164 [0] NCCL INFO Channel 00/0 : 6[0] -> 7[1] via P2P/IPC
d09n08:1072932:1073164 [0] NCCL INFO Channel 02/0 : 6[0] -> 7[1] via P2P/IPC
d09n08:1072932:1073164 [0] NCCL INFO Channel 01/0 : 6[0] -> 15[3] [send] via NET/IB/2
d09n08:1072932:1073164 [0] NCCL INFO Channel 03/0 : 6[0] -> 15[3] [send] via NET/IB/2
d09n08:1072932:1073164 [0] NCCL INFO Connected all rings
d09n08:1072932:1073164 [0] NCCL INFO Channel 01/0 : 6[0] -> 7[1] via P2P/IPC
d09n08:1072932:1073164 [0] NCCL INFO Channel 03/0 : 6[0] -> 7[1] via P2P/IPC
d09n08:1072932:1073164 [0] NCCL INFO Channel 01/0 : 6[0] -> 8[2] via P2P/IPC
d09n08:1072932:1073164 [0] NCCL INFO Channel 03/0 : 6[0] -> 8[2] via P2P/IPC
d09n08:1072932:1073164 [0] NCCL INFO Channel 02/0 : 0[0] -> 6[0] [receive] via NET/IB/0
d09n08:1072932:1073164 [0] NCCL INFO Channel 00/0 : 6[0] -> 13[1] [send] via NET/IB/0
d09n08:1072932:1073164 [0] NCCL INFO Channel 02/0 : 6[0] -> 18[0] [send] via NET/IB/0
d09n08:1072932:1073164 [0] NCCL INFO Channel 02/0 : 18[0] -> 6[0] [receive] via NET/IB/0
d09n08:1072932:1073164 [0] NCCL INFO Channel 00/0 : 13[1] -> 6[0] [receive] via NET/IB/0
d09n08:1072932:1073164 [0] NCCL INFO Channel 02/0 : 6[0] -> 0[0] [send] via NET/IB/0
d09n08:1072932:1073164 [0] NCCL INFO Connected all trees
d09n08:1072932:1073164 [0] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
d09n08:1072932:1073164 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
d09n08:1072932:1073164 [0] NCCL INFO comm 0x14e523960 rank 6 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x7b12926388d12ce3 - Init COMPLETE
NCCL version 2.18.3+cuda11.8
d09n07:1061890:1061890 [3] NCCL INFO cudaDriverVersion 12020
d09n07:1061890:1061890 [3] NCCL INFO Bootstrap : Using ib0:10.41.8.171<0>
d09n07:1061890:1061890 [3] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
d09n07:1061890:1061890 [3] NCCL INFO NET/Plugin : No plugin found, using internal implementation
d09n07:1061890:1062188 [3] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.8.171<0>
d09n07:1061890:1062188 [3] NCCL INFO Using network IB
d09n07:1061890:1062188 [3] NCCL INFO comm 0x174aa3d60 rank 3 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x7b12926388d12ce3 - Init START
d09n07:1061890:1062188 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
d09n07:1061890:1062188 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 5/-1/-1->3->4 [2] 4/-1/-1->3->2 [3] 5/-1/-1->3->4
d09n07:1061890:1062188 [3] NCCL INFO P2P Chunksize set to 131072
d09n07:1061890:1062188 [3] NCCL INFO Channel 00/0 : 3[3] -> 4[4] via P2P/IPC
d09n07:1061890:1062188 [3] NCCL INFO Channel 01/0 : 3[3] -> 4[4] via P2P/IPC
d09n07:1061890:1062188 [3] NCCL INFO Channel 02/0 : 3[3] -> 4[4] via P2P/IPC
d09n07:1061890:1062188 [3] NCCL INFO Channel 03/0 : 3[3] -> 4[4] via P2P/IPC
d09n07:1061890:1062188 [3] NCCL INFO Channel 01/0 : 90[0] -> 3[3] [receive] via NET/IB/3
d09n07:1061890:1062188 [3] NCCL INFO Channel 03/0 : 90[0] -> 3[3] [receive] via NET/IB/3
d09n07:1061890:1062188 [3] NCCL INFO Connected all rings
d09n07:1061890:1062188 [3] NCCL INFO Channel 01/0 : 3[3] -> 5[5] via P2P/IPC
d09n07:1061890:1062188 [3] NCCL INFO Channel 03/0 : 3[3] -> 5[5] via P2P/IPC
d09n07:1061890:1062188 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/IPC
d09n07:1061890:1062188 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/IPC
d09n07:1061890:1062188 [3] NCCL INFO Connected all trees
d09n07:1061890:1062188 [3] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
d09n07:1061890:1062188 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
d09n07:1061890:1062188 [3] NCCL INFO comm 0x174aa3d60 rank 3 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x7b12926388d12ce3 - Init COMPLETE
NCCL version 2.18.3+cuda11.8
[2024-03-04 16:42:54,176] [INFO] [logging.py:96:log_dist] [Rank 0] creating expert parallel process group named ep_size_4 with ranks: [92, 93, 94, 95]
d09n07:1061888:1061888 [1] NCCL INFO cudaDriverVersion 12020
d09n07:1061888:1061888 [1] NCCL INFO Bootstrap : Using ib0:10.41.8.171<0>
d09n07:1061888:1061888 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
d09n07:1061888:1061888 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
d09n07:1061888:1062186 [1] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.8.171<0>
d09n07:1061888:1062186 [1] NCCL INFO Using network IB
d09n07:1061888:1062186 [1] NCCL INFO comm 0x128973e90 rank 1 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x7b12926388d12ce3 - Init START
d09n07:1061888:1062186 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
d09n07:1061888:1062186 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 0/-1/-1->1->5 [2] 2/-1/-1->1->0 [3] 0/-1/-1->1->5
d09n07:1061888:1062186 [1] NCCL INFO P2P Chunksize set to 131072
d09n07:1061888:1062186 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/IPC
d09n07:1061888:1062186 [1] NCCL INFO Channel 02/0 : 1[1] -> 2[2] via P2P/IPC
d09n07:1061888:1062186 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/IPC
d09n07:1061888:1062186 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/IPC
d09n07:1061888:1062186 [1] NCCL INFO Connected all rings
d09n07:1061888:1062186 [1] NCCL INFO Channel 01/0 : 1[1] -> 5[5] via P2P/IPC
d09n07:1061888:1062186 [1] NCCL INFO Channel 03/0 : 1[1] -> 5[5] via P2P/IPC
d09n07:1061888:1062186 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/IPC
d09n07:1061888:1062186 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/IPC
d09n07:1061888:1062186 [1] NCCL INFO Connected all trees
d09n07:1061888:1062186 [1] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
d09n07:1061888:1062186 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
d09n07:1061888:1062186 [1] NCCL INFO comm 0x128973e90 rank 1 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x7b12926388d12ce3 - Init COMPLETE
NCCL version 2.18.3+cuda11.8
[2024-03-04 16:42:54,976] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-03-04 16:42:54,981] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2024-03-04 16:42:54,981] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-03-04 16:42:55,042] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2024-03-04 16:42:55,042] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
f18n06:1026681:1026681 [2] NCCL INFO cudaDriverVersion 12020
f18n06:1026681:1026681 [2] NCCL INFO Bootstrap : Using ib0:10.41.14.110<0>
f18n06:1026681:1026681 [2] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
f18n06:1026681:1026681 [2] NCCL INFO NET/Plugin : No plugin found, using internal implementation
f18n06:1026681:1026910 [2] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.14.110<0>
f18n06:1026681:1026910 [2] NCCL INFO Using network IB
f18n06:1026681:1026910 [2] NCCL INFO comm 0x13ded3c60 rank 62 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x7b12926388d12ce3 - Init START
f18n06:1026681:1026910 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
f18n06:1026681:1026910 [2] NCCL INFO Trees [0] 63/-1/-1->62->61 [1] -1/-1/-1->62->60 [2] 63/-1/-1->62->61 [3] -1/-1/-1->62->60
f18n06:1026681:1026910 [2] NCCL INFO P2P Chunksize set to 131072
f18n06:1026681:1026910 [2] NCCL INFO Channel 00/0 : 62[2] -> 63[3] via P2P/IPC
f18n06:1026681:1026910 [2] NCCL INFO Channel 02/0 : 62[2] -> 63[3] via P2P/IPC
f18n06:1026681:1026910 [2] NCCL INFO Channel 01/0 : 62[2] -> 61[1] via P2P/IPC
f18n06:1026681:1026910 [2] NCCL INFO Channel 03/0 : 62[2] -> 61[1] via P2P/IPC
f18n06:1026681:1026910 [2] NCCL INFO Connected all rings
f18n06:1026681:1026910 [2] NCCL INFO Channel 01/0 : 62[2] -> 60[0] via P2P/IPC
f18n06:1026681:1026910 [2] NCCL INFO Channel 03/0 : 62[2] -> 60[0] via P2P/IPC
f18n06:1026681:1026910 [2] NCCL INFO Channel 00/0 : 62[2] -> 61[1] via P2P/IPC
f18n06:1026681:1026910 [2] NCCL INFO Channel 02/0 : 62[2] -> 61[1] via P2P/IPC
f18n06:1026681:1026910 [2] NCCL INFO Connected all trees
f18n06:1026681:1026910 [2] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
f18n06:1026681:1026910 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
f18n06:1026681:1026910 [2] NCCL INFO comm 0x13ded3c60 rank 62 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x7b12926388d12ce3 - Init COMPLETE
f18n06:1026681:1027002 [2] NCCL INFO Using network IB
f18n06:1026681:1027002 [2] NCCL INFO comm 0x140b8c520 rank 7 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0x414883fe5ee4d1ab - Init START
f18n06:1026681:1027002 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
f18n06:1026681:1027002 [2] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] 9/5/-1->7->3
f18n06:1026681:1027002 [2] NCCL INFO P2P Chunksize set to 131072
f18n06:1026681:1027002 [2] NCCL INFO Channel 00/0 : 6[0] -> 7[2] [receive] via NET/IB/0
f18n06:1026681:1027002 [2] NCCL INFO Channel 01/0 : 6[0] -> 7[2] [receive] via NET/IB/0
f18n06:1026681:1027002 [2] NCCL INFO Channel 00/0 : 7[2] -> 8[4] [send] via NET/IB/0
f18n06:1026681:1027002 [2] NCCL INFO Channel 01/0 : 7[2] -> 8[4] [send] via NET/IB/0
f18n06:1026681:1027002 [2] NCCL INFO Connected all rings
f18n06:1026681:1027002 [2] NCCL INFO Channel 01/0 : 5[4] -> 7[2] [receive] via NET/IB/0
f18n06:1026681:1027002 [2] NCCL INFO Channel 01/0 : 7[2] -> 9[0] [send] via NET/IB/0
f18n06:1026681:1027002 [2] NCCL INFO Channel 01/0 : 3[0] -> 7[2] [receive] via NET/IB/0
f18n06:1026681:1027002 [2] NCCL INFO Channel 01/0 : 7[2] -> 3[0] [send] via NET/IB/0
f18n06:1026681:1027002 [2] NCCL INFO Channel 01/0 : 9[0] -> 7[2] [receive] via NET/IB/0
f18n06:1026681:1027002 [2] NCCL INFO Channel 01/0 : 7[2] -> 5[4] [send] via NET/IB/0
f18n06:1026681:1027002 [2] NCCL INFO Channel 00/0 : 7[2] -> 6[0] [send] via NET/IB/0
f18n06:1026681:1027002 [2] NCCL INFO Connected all trees
f18n06:1026681:1027002 [2] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
f18n06:1026681:1027002 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f18n06:1026681:1027002 [2] NCCL INFO comm 0x140b8c520 rank 7 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0x414883fe5ee4d1ab - Init COMPLETE
f18n06:1026681:1027020 [2] NCCL INFO Using network IB
f18n06:1026681:1027020 [2] NCCL INFO comm 0x140bccaf0 rank 15 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0x32e49bb35e6c8ace - Init START
f18n06:1026681:1027020 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
f18n06:1026681:1027020 [2] NCCL INFO Trees [0] 13/16/-1->15->18 [1] -1/-1/-1->15->14
f18n06:1026681:1027020 [2] NCCL INFO P2P Chunksize set to 131072
f18n06:1026681:1027020 [2] NCCL INFO Channel 00/0 : 14[4] -> 15[2] [receive] via NET/IB/0
f18n06:1026681:1027020 [2] NCCL INFO Channel 01/0 : 14[4] -> 15[2] [receive] via NET/IB/0
f18n06:1026681:1027020 [2] NCCL INFO Channel 00/0 : 15[2] -> 16[0] [send] via NET/IB/0
f18n06:1026681:1027020 [2] NCCL INFO Channel 01/0 : 15[2] -> 16[0] [send] via NET/IB/0
f18n06:1026681:1027020 [2] NCCL INFO Connected all rings
f18n06:1026681:1027020 [2] NCCL INFO Channel 00/0 : 13[0] -> 15[2] [receive] via NET/IB/0
f18n06:1026681:1027020 [2] NCCL INFO Channel 00/0 : 15[2] -> 18[2] [send] via NET/IB/0
f18n06:1026681:1027020 [2] NCCL INFO Channel 00/0 : 18[2] -> 15[2] [receive] via NET/IB/0
f18n06:1026681:1027020 [2] NCCL INFO Channel 00/0 : 15[2] -> 13[0] [send] via NET/IB/0
f18n06:1026681:1027020 [2] NCCL INFO Channel 00/0 : 16[0] -> 15[2] [receive] via NET/IB/0
f18n06:1026681:1027020 [2] NCCL INFO Channel 01/0 : 15[2] -> 14[4] [send] via NET/IB/0
f18n06:1026681:1027020 [2] NCCL INFO Connected all trees
f18n06:1026681:1027020 [2] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
f18n06:1026681:1027020 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f18n06:1026681:1027020 [2] NCCL INFO comm 0x140bccaf0 rank 15 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0x32e49bb35e6c8ace - Init COMPLETE
d09n07:1061889:1062289 [2] NCCL INFO Using network IB
d09n07:1061889:1062289 [2] NCCL INFO comm 0x131e41f60 rank 0 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0xe7c8afafd6ec6b2d - Init START
d09n07:1061889:1062289 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
d09n07:1061889:1062289 [2] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7   8   9  10  11
d09n07:1061889:1062289 [2] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7   8   9  10  11
d09n07:1061889:1062289 [2] NCCL INFO Trees [0] 8/-1/-1->0->-1 [1] -1/-1/-1->0->1
d09n07:1061889:1062289 [2] NCCL INFO P2P Chunksize set to 131072
d09n07:1061889:1062289 [2] NCCL INFO Channel 00/0 : 11[0] -> 0[2] [receive] via NET/IB/0
d09n07:1061889:1062289 [2] NCCL INFO Channel 01/0 : 11[0] -> 0[2] [receive] via NET/IB/0
d09n07:1061889:1062289 [2] NCCL INFO Channel 00/0 : 0[2] -> 1[4] [send] via NET/IB/0
d09n07:1061889:1062289 [2] NCCL INFO Channel 01/0 : 0[2] -> 1[4] [send] via NET/IB/0
d09n07:1061889:1062289 [2] NCCL INFO Connected all rings
d09n07:1061889:1062289 [2] NCCL INFO Channel 00/0 : 8[0] -> 0[2] [receive] via NET/IB/0
d09n07:1061889:1062289 [2] NCCL INFO Channel 00/0 : 0[2] -> 8[0] [send] via NET/IB/0
d09n07:1061889:1062289 [2] NCCL INFO Channel 01/0 : 1[4] -> 0[2] [receive] via NET/IB/0
d09n07:1061889:1062289 [2] NCCL INFO Connected all trees
d09n07:1061889:1062289 [2] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
d09n07:1061889:1062289 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d09n07:1061889:1062289 [2] NCCL INFO comm 0x131e41f60 rank 0 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0xe7c8afafd6ec6b2d - Init COMPLETE
d09n07:1061889:1062316 [2] NCCL INFO Using network IB
d09n07:1061889:1062316 [2] NCCL INFO comm 0x131e2b7c0 rank 0 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0x32e49bb35e6c8ace - Init START
d09n07:1061889:1062316 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
d09n07:1061889:1062316 [2] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
d09n07:1061889:1062316 [2] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
d09n07:1061889:1062316 [2] NCCL INFO Trees [0] 12/-1/-1->0->-1 [1] -1/-1/-1->0->1
d09n07:1061889:1062316 [2] NCCL INFO P2P Chunksize set to 131072
d09n07:1061889:1062316 [2] NCCL INFO Channel 00/0 : 23[4] -> 0[2] [receive] via NET/IB/0
d09n07:1061889:1062316 [2] NCCL INFO Channel 01/0 : 23[4] -> 0[2] [receive] via NET/IB/0
d09n07:1061889:1062316 [2] NCCL INFO Channel 00/0 : 0[2] -> 1[0] [send] via NET/IB/0
d09n07:1061889:1062316 [2] NCCL INFO Channel 01/0 : 0[2] -> 1[0] [send] via NET/IB/0
d09n07:1061889:1062316 [2] NCCL INFO Connected all rings
d09n07:1061889:1062316 [2] NCCL INFO Channel 00/0 : 12[2] -> 0[2] [receive] via NET/IB/0
d09n07:1061889:1062316 [2] NCCL INFO Channel 00/0 : 0[2] -> 12[2] [send] via NET/IB/0
d09n07:1061889:1062316 [2] NCCL INFO Channel 01/0 : 1[0] -> 0[2] [receive] via NET/IB/0
d09n07:1061889:1062316 [2] NCCL INFO Connected all trees
d09n07:1061889:1062316 [2] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
d09n07:1061889:1062316 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d09n07:1061889:1062316 [2] NCCL INFO comm 0x131e2b7c0 rank 0 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0x32e49bb35e6c8ace - Init COMPLETE
d11n16:1129247:1129247 [2] NCCL INFO cudaDriverVersion 12020
d11n16:1129247:1129247 [2] NCCL INFO Bootstrap : Using ib0:10.41.8.216<0>
d11n16:1129247:1129247 [2] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
d11n16:1129247:1129247 [2] NCCL INFO NET/Plugin : No plugin found, using internal implementation
d11n16:1129247:1129479 [2] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.8.216<0>
d11n16:1129247:1129479 [2] NCCL INFO Using network IB
d11n16:1129247:1129479 [2] NCCL INFO comm 0x148324850 rank 14 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x7b12926388d12ce3 - Init START
d11n16:1129247:1129479 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
d11n16:1129247:1129479 [2] NCCL INFO Trees [0] 15/-1/-1->14->13 [1] -1/-1/-1->14->12 [2] 15/-1/-1->14->13 [3] -1/-1/-1->14->12
d11n16:1129247:1129479 [2] NCCL INFO P2P Chunksize set to 131072
d11n16:1129247:1129479 [2] NCCL INFO Channel 00/0 : 14[2] -> 15[3] via P2P/IPC
d11n16:1129247:1129479 [2] NCCL INFO Channel 02/0 : 14[2] -> 15[3] via P2P/IPC
d11n16:1129247:1129479 [2] NCCL INFO Channel 01/0 : 14[2] -> 13[1] via P2P/IPC
d11n16:1129247:1129479 [2] NCCL INFO Channel 03/0 : 14[2] -> 13[1] via P2P/IPC
d11n16:1129247:1129479 [2] NCCL INFO Connected all rings
d11n16:1129247:1129479 [2] NCCL INFO Channel 01/0 : 14[2] -> 12[0] via P2P/IPC
d11n16:1129247:1129479 [2] NCCL INFO Channel 03/0 : 14[2] -> 12[0] via P2P/IPC
d11n16:1129247:1129479 [2] NCCL INFO Channel 00/0 : 14[2] -> 13[1] via P2P/IPC
d11n16:1129247:1129479 [2] NCCL INFO Channel 02/0 : 14[2] -> 13[1] via P2P/IPC
d11n16:1129247:1129479 [2] NCCL INFO Connected all trees
d11n16:1129247:1129479 [2] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
d11n16:1129247:1129479 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
d11n16:1129247:1129479 [2] NCCL INFO comm 0x148324850 rank 14 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x7b12926388d12ce3 - Init COMPLETE
d11n16:1129247:1129559 [2] NCCL INFO Using network IB
d11n16:1129247:1129559 [2] NCCL INFO comm 0x14aff6a00 rank 1 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0x414883fe5ee4d1ab - Init START
d11n16:1129247:1129559 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
d11n16:1129247:1129559 [2] NCCL INFO Trees [0] -1/-1/-1->1->2 [1] 2/0/-1->1->3
d11n16:1129247:1129559 [2] NCCL INFO P2P Chunksize set to 131072
d11n16:1129247:1129559 [2] NCCL INFO Channel 00/0 : 0[0] -> 1[2] [receive] via NET/IB/0
d11n16:1129247:1129559 [2] NCCL INFO Channel 01/0 : 0[0] -> 1[2] [receive] via NET/IB/0
d11n16:1129247:1129559 [2] NCCL INFO Channel 00/0 : 1[2] -> 2[4] [send] via NET/IB/0
d11n16:1129247:1129559 [2] NCCL INFO Channel 01/0 : 1[2] -> 2[4] [send] via NET/IB/0
d11n16:1129247:1129559 [2] NCCL INFO Connected all rings
d11n16:1129247:1129559 [2] NCCL INFO Channel 01/0 : 1[2] -> 3[0] [send] via NET/IB/0
d11n16:1129247:1129559 [2] NCCL INFO Channel 01/0 : 3[0] -> 1[2] [receive] via NET/IB/0
d11n16:1129247:1129559 [2] NCCL INFO Channel 00/0 : 2[4] -> 1[2] [receive] via NET/IB/0
d11n16:1129247:1129559 [2] NCCL INFO Channel 01/0 : 2[4] -> 1[2] [receive] via NET/IB/0
d11n16:1129247:1129559 [2] NCCL INFO Channel 01/0 : 1[2] -> 0[0] [send] via NET/IB/0
d11n16:1129247:1129559 [2] NCCL INFO Connected all trees
d11n16:1129247:1129559 [2] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
d11n16:1129247:1129559 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d11n16:1129247:1129559 [2] NCCL INFO comm 0x14aff6a00 rank 1 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0x414883fe5ee4d1ab - Init COMPLETE
d11n16:1129247:1129579 [2] NCCL INFO Using network IB
d11n16:1129247:1129579 [2] NCCL INFO comm 0x14b1c6620 rank 3 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0x32e49bb35e6c8ace - Init START
d11n16:1129247:1129579 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
d11n16:1129247:1129579 [2] NCCL INFO Trees [0] 1/4/-1->3->6 [1] -1/-1/-1->3->2
d11n16:1129247:1129579 [2] NCCL INFO P2P Chunksize set to 131072
d11n16:1129247:1129579 [2] NCCL INFO Channel 00/0 : 2[4] -> 3[2] [receive] via NET/IB/0
d11n16:1129247:1129579 [2] NCCL INFO Channel 01/0 : 2[4] -> 3[2] [receive] via NET/IB/0
d11n16:1129247:1129579 [2] NCCL INFO Channel 00/0 : 3[2] -> 4[0] [send] via NET/IB/0
d11n16:1129247:1129579 [2] NCCL INFO Channel 01/0 : 3[2] -> 4[0] [send] via NET/IB/0
d11n16:1129247:1129579 [2] NCCL INFO Connected all rings
d11n16:1129247:1129579 [2] NCCL INFO Channel 00/0 : 1[0] -> 3[2] [receive] via NET/IB/0
d11n16:1129247:1129579 [2] NCCL INFO Channel 00/0 : 3[2] -> 6[2] [send] via NET/IB/0
d11n16:1129247:1129579 [2] NCCL INFO Channel 00/0 : 6[2] -> 3[2] [receive] via NET/IB/0
d11n16:1129247:1129579 [2] NCCL INFO Channel 00/0 : 3[2] -> 1[0] [send] via NET/IB/0
d11n16:1129247:1129579 [2] NCCL INFO Channel 00/0 : 4[0] -> 3[2] [receive] via NET/IB/0
d11n16:1129247:1129579 [2] NCCL INFO Channel 01/0 : 3[2] -> 2[4] [send] via NET/IB/0
d11n16:1129247:1129579 [2] NCCL INFO Connected all trees
d11n16:1129247:1129579 [2] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
d11n16:1129247:1129579 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d11n16:1129247:1129579 [2] NCCL INFO comm 0x14b1c6620 rank 3 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0x32e49bb35e6c8ace - Init COMPLETE
f16n16:1079195:1079195 [2] NCCL INFO cudaDriverVersion 12020
f16n16:1079195:1079195 [2] NCCL INFO Bootstrap : Using ib0:10.41.14.84<0>
f16n16:1079195:1079195 [2] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
f16n16:1079195:1079195 [2] NCCL INFO NET/Plugin : No plugin found, using internal implementation
f16n16:1079195:1079427 [2] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.14.84<0>
f16n16:1079195:1079427 [2] NCCL INFO Using network IB
f16n16:1079195:1079427 [2] NCCL INFO comm 0x145123560 rank 26 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x7b12926388d12ce3 - Init START
f16n16:1079195:1079427 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
f16n16:1079195:1079427 [2] NCCL INFO Trees [0] 27/-1/-1->26->25 [1] -1/-1/-1->26->24 [2] 27/-1/-1->26->25 [3] -1/-1/-1->26->24
f16n16:1079195:1079427 [2] NCCL INFO P2P Chunksize set to 131072
f16n16:1079195:1079427 [2] NCCL INFO Channel 00/0 : 26[2] -> 27[3] via P2P/IPC
f16n16:1079195:1079427 [2] NCCL INFO Channel 02/0 : 26[2] -> 27[3] via P2P/IPC
f16n16:1079195:1079427 [2] NCCL INFO Channel 01/0 : 26[2] -> 25[1] via P2P/IPC
f16n16:1079195:1079427 [2] NCCL INFO Channel 03/0 : 26[2] -> 25[1] via P2P/IPC
f16n16:1079195:1079427 [2] NCCL INFO Connected all rings
f16n16:1079195:1079427 [2] NCCL INFO Channel 01/0 : 26[2] -> 24[0] via P2P/IPC
f16n16:1079195:1079427 [2] NCCL INFO Channel 03/0 : 26[2] -> 24[0] via P2P/IPC
f16n16:1079195:1079427 [2] NCCL INFO Channel 00/0 : 26[2] -> 25[1] via P2P/IPC
f16n16:1079195:1079427 [2] NCCL INFO Channel 02/0 : 26[2] -> 25[1] via P2P/IPC
f16n16:1079195:1079427 [2] NCCL INFO Connected all trees
f16n16:1079195:1079427 [2] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
f16n16:1079195:1079427 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
f16n16:1079195:1079427 [2] NCCL INFO comm 0x145123560 rank 26 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x7b12926388d12ce3 - Init COMPLETE
f16n16:1079195:1079509 [2] NCCL INFO Using network IB
f16n16:1079195:1079509 [2] NCCL INFO comm 0x147d9c530 rank 3 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0xe7c8afafd6ec6b2d - Init START
f16n16:1079195:1079509 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
f16n16:1079195:1079509 [2] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] 7/1/-1->3->11
f16n16:1079195:1079509 [2] NCCL INFO P2P Chunksize set to 131072
f16n16:1079195:1079509 [2] NCCL INFO Channel 00/0 : 2[0] -> 3[2] [receive] via NET/IB/0
f16n16:1079195:1079509 [2] NCCL INFO Channel 01/0 : 2[0] -> 3[2] [receive] via NET/IB/0
f16n16:1079195:1079509 [2] NCCL INFO Channel 00/0 : 3[2] -> 4[4] [send] via NET/IB/0
f16n16:1079195:1079509 [2] NCCL INFO Channel 01/0 : 3[2] -> 4[4] [send] via NET/IB/0
f16n16:1079195:1079509 [2] NCCL INFO Connected all rings
f16n16:1079195:1079509 [2] NCCL INFO Channel 01/0 : 1[4] -> 3[2] [receive] via NET/IB/0
f16n16:1079195:1079509 [2] NCCL INFO Channel 01/0 : 11[0] -> 3[2] [receive] via NET/IB/0
f16n16:1079195:1079509 [2] NCCL INFO Channel 01/0 : 3[2] -> 7[4] [send] via NET/IB/0
f16n16:1079195:1079509 [2] NCCL INFO Channel 01/0 : 7[4] -> 3[2] [receive] via NET/IB/0
f16n16:1079195:1079509 [2] NCCL INFO Channel 01/0 : 3[2] -> 11[0] [send] via NET/IB/0
f16n16:1079195:1079509 [2] NCCL INFO Channel 01/0 : 3[2] -> 1[4] [send] via NET/IB/0
f16n16:1079195:1079509 [2] NCCL INFO Channel 00/0 : 3[2] -> 2[0] [send] via NET/IB/0
f16n16:1079195:1079509 [2] NCCL INFO Connected all trees
f16n16:1079195:1079509 [2] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
f16n16:1079195:1079509 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f16n16:1079195:1079509 [2] NCCL INFO comm 0x147d9c530 rank 3 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0xe7c8afafd6ec6b2d - Init COMPLETE
f16n16:1079195:1079529 [2] NCCL INFO Using network IB
f16n16:1079195:1079529 [2] NCCL INFO comm 0x147a5cd30 rank 6 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0x32e49bb35e6c8ace - Init START
f16n16:1079195:1079529 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
f16n16:1079195:1079529 [2] NCCL INFO Trees [0] 3/9/-1->6->12 [1] -1/-1/-1->6->7
f16n16:1079195:1079529 [2] NCCL INFO P2P Chunksize set to 131072
f16n16:1079195:1079529 [2] NCCL INFO Channel 00/0 : 5[4] -> 6[2] [receive] via NET/IB/0
f16n16:1079195:1079529 [2] NCCL INFO Channel 01/0 : 5[4] -> 6[2] [receive] via NET/IB/0
f16n16:1079195:1079529 [2] NCCL INFO Channel 00/0 : 6[2] -> 7[0] [send] via NET/IB/0
f16n16:1079195:1079529 [2] NCCL INFO Channel 01/0 : 6[2] -> 7[0] [send] via NET/IB/0
f16n16:1079195:1079529 [2] NCCL INFO Connected all rings
f16n16:1079195:1079529 [2] NCCL INFO Channel 00/0 : 3[2] -> 6[2] [receive] via NET/IB/0
f16n16:1079195:1079529 [2] NCCL INFO Channel 00/0 : 6[2] -> 9[2] [send] via NET/IB/0
f16n16:1079195:1079529 [2] NCCL INFO Channel 00/0 : 6[2] -> 12[2] [send] via NET/IB/0
f16n16:1079195:1079529 [2] NCCL INFO Channel 00/0 : 12[2] -> 6[2] [receive] via NET/IB/0
f16n16:1079195:1079529 [2] NCCL INFO Channel 00/0 : 9[2] -> 6[2] [receive] via NET/IB/0
f16n16:1079195:1079529 [2] NCCL INFO Channel 00/0 : 6[2] -> 3[2] [send] via NET/IB/0
f16n16:1079195:1079529 [2] NCCL INFO Channel 01/0 : 7[0] -> 6[2] [receive] via NET/IB/0
f16n16:1079195:1079529 [2] NCCL INFO Connected all trees
f16n16:1079195:1079529 [2] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
f16n16:1079195:1079529 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f16n16:1079195:1079529 [2] NCCL INFO comm 0x147a5cd30 rank 6 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0x32e49bb35e6c8ace - Init COMPLETE
f16n18:1091133:1091133 [2] NCCL INFO cudaDriverVersion 12020
f16n18:1091133:1091133 [2] NCCL INFO Bootstrap : Using ib0:10.41.14.86<0>
f16n18:1091133:1091133 [2] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
f16n18:1091133:1091133 [2] NCCL INFO NET/Plugin : No plugin found, using internal implementation
f16n18:1091133:1091363 [2] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.14.86<0>
f16n18:1091133:1091363 [2] NCCL INFO Using network IB
f16n18:1091133:1091363 [2] NCCL INFO comm 0x15cb63860 rank 38 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x7b12926388d12ce3 - Init START
f16n18:1091133:1091363 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
f16n18:1091133:1091363 [2] NCCL INFO Trees [0] 39/-1/-1->38->37 [1] -1/-1/-1->38->36 [2] 39/-1/-1->38->37 [3] -1/-1/-1->38->36
f16n18:1091133:1091363 [2] NCCL INFO P2P Chunksize set to 131072
f16n18:1091133:1091363 [2] NCCL INFO Channel 00/0 : 38[2] -> 39[3] via P2P/IPC
f16n18:1091133:1091363 [2] NCCL INFO Channel 02/0 : 38[2] -> 39[3] via P2P/IPC
f16n18:1091133:1091363 [2] NCCL INFO Channel 01/0 : 38[2] -> 37[1] via P2P/IPC
f16n18:1091133:1091363 [2] NCCL INFO Channel 03/0 : 38[2] -> 37[1] via P2P/IPC
f16n18:1091133:1091363 [2] NCCL INFO Connected all rings
f16n18:1091133:1091363 [2] NCCL INFO Channel 01/0 : 38[2] -> 36[0] via P2P/IPC
f16n18:1091133:1091363 [2] NCCL INFO Channel 03/0 : 38[2] -> 36[0] via P2P/IPC
f16n18:1091133:1091363 [2] NCCL INFO Channel 00/0 : 38[2] -> 37[1] via P2P/IPC
f16n18:1091133:1091363 [2] NCCL INFO Channel 02/0 : 38[2] -> 37[1] via P2P/IPC
f16n18:1091133:1091363 [2] NCCL INFO Connected all trees
f16n18:1091133:1091363 [2] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
f16n18:1091133:1091363 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
f16n18:1091133:1091363 [2] NCCL INFO comm 0x15cb63860 rank 38 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x7b12926388d12ce3 - Init COMPLETE
f16n18:1091133:1091464 [2] NCCL INFO Using network IB
f16n18:1091133:1091464 [2] NCCL INFO comm 0x15f7ab310 rank 4 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0x414883fe5ee4d1ab - Init START
f16n18:1091133:1091464 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
f16n18:1091133:1091464 [2] NCCL INFO Trees [0] 2/6/-1->4->8 [1] -1/-1/-1->4->5
f16n18:1091133:1091464 [2] NCCL INFO P2P Chunksize set to 131072
f16n18:1091133:1091464 [2] NCCL INFO Channel 00/0 : 3[0] -> 4[2] [receive] via NET/IB/0
f16n18:1091133:1091464 [2] NCCL INFO Channel 01/0 : 3[0] -> 4[2] [receive] via NET/IB/0
f16n18:1091133:1091464 [2] NCCL INFO Channel 00/0 : 4[2] -> 5[4] [send] via NET/IB/0
f16n18:1091133:1091464 [2] NCCL INFO Channel 01/0 : 4[2] -> 5[4] [send] via NET/IB/0
f16n18:1091133:1091464 [2] NCCL INFO Connected all rings
f16n18:1091133:1091464 [2] NCCL INFO Channel 00/0 : 2[4] -> 4[2] [receive] via NET/IB/0
f16n18:1091133:1091464 [2] NCCL INFO Channel 00/0 : 4[2] -> 6[0] [send] via NET/IB/0
f16n18:1091133:1091464 [2] NCCL INFO Channel 00/0 : 4[2] -> 8[4] [send] via NET/IB/0
f16n18:1091133:1091464 [2] NCCL INFO Channel 00/0 : 8[4] -> 4[2] [receive] via NET/IB/0
f16n18:1091133:1091464 [2] NCCL INFO Channel 00/0 : 6[0] -> 4[2] [receive] via NET/IB/0
f16n18:1091133:1091464 [2] NCCL INFO Channel 00/0 : 4[2] -> 2[4] [send] via NET/IB/0
f16n18:1091133:1091464 [2] NCCL INFO Channel 01/0 : 5[4] -> 4[2] [receive] via NET/IB/0
f16n18:1091133:1091464 [2] NCCL INFO Connected all trees
f16n18:1091133:1091464 [2] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
f16n18:1091133:1091464 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f16n18:1091133:1091464 [2] NCCL INFO comm 0x15f7ab310 rank 4 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0x414883fe5ee4d1ab - Init COMPLETE
f16n18:1091133:1091483 [2] NCCL INFO Using network IB
f16n18:1091133:1091483 [2] NCCL INFO comm 0x15e9982b0 rank 9 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0x32e49bb35e6c8ace - Init START
f16n18:1091133:1091483 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
f16n18:1091133:1091483 [2] NCCL INFO Trees [0] 7/10/-1->9->6 [1] -1/-1/-1->9->8
f16n18:1091133:1091483 [2] NCCL INFO P2P Chunksize set to 131072
f16n18:1091133:1091483 [2] NCCL INFO Channel 00/0 : 8[4] -> 9[2] [receive] via NET/IB/0
f16n18:1091133:1091483 [2] NCCL INFO Channel 01/0 : 8[4] -> 9[2] [receive] via NET/IB/0
f16n18:1091133:1091483 [2] NCCL INFO Channel 00/0 : 9[2] -> 10[0] [send] via NET/IB/0
f16n18:1091133:1091483 [2] NCCL INFO Channel 01/0 : 9[2] -> 10[0] [send] via NET/IB/0
f16n18:1091133:1091483 [2] NCCL INFO Connected all rings
f16n18:1091133:1091483 [2] NCCL INFO Channel 00/0 : 7[0] -> 9[2] [receive] via NET/IB/0
f16n18:1091133:1091483 [2] NCCL INFO Channel 00/0 : 6[2] -> 9[2] [receive] via NET/IB/0
f16n18:1091133:1091483 [2] NCCL INFO Channel 00/0 : 9[2] -> 6[2] [send] via NET/IB/0
f16n18:1091133:1091483 [2] NCCL INFO Channel 00/0 : 9[2] -> 7[0] [send] via NET/IB/0
f16n18:1091133:1091483 [2] NCCL INFO Channel 00/0 : 10[0] -> 9[2] [receive] via NET/IB/0
f16n18:1091133:1091483 [2] NCCL INFO Channel 01/0 : 9[2] -> 8[4] [send] via NET/IB/0
f16n18:1091133:1091483 [2] NCCL INFO Connected all trees
f16n18:1091133:1091483 [2] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
f16n18:1091133:1091483 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f16n18:1091133:1091483 [2] NCCL INFO comm 0x15e9982b0 rank 9 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0x32e49bb35e6c8ace - Init COMPLETE
f17n02:1075533:1075533 [2] NCCL INFO cudaDriverVersion 12020
f17n02:1075533:1075533 [2] NCCL INFO Bootstrap : Using ib0:10.41.14.88<0>
f17n02:1075533:1075533 [2] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
f17n02:1075533:1075533 [2] NCCL INFO NET/Plugin : No plugin found, using internal implementation
f17n02:1075533:1075767 [2] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.14.88<0>
f17n02:1075533:1075767 [2] NCCL INFO Using network IB
f17n02:1075533:1075767 [2] NCCL INFO comm 0x136653eb0 rank 50 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x7b12926388d12ce3 - Init START
f17n02:1075533:1075767 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
f17n02:1075533:1075767 [2] NCCL INFO Trees [0] 51/-1/-1->50->49 [1] -1/-1/-1->50->48 [2] 51/-1/-1->50->49 [3] -1/-1/-1->50->48
f17n02:1075533:1075767 [2] NCCL INFO P2P Chunksize set to 131072
f17n02:1075533:1075767 [2] NCCL INFO Channel 00/0 : 50[2] -> 51[3] via P2P/IPC
f17n02:1075533:1075767 [2] NCCL INFO Channel 02/0 : 50[2] -> 51[3] via P2P/IPC
f17n02:1075533:1075767 [2] NCCL INFO Channel 01/0 : 50[2] -> 49[1] via P2P/IPC
f17n02:1075533:1075767 [2] NCCL INFO Channel 03/0 : 50[2] -> 49[1] via P2P/IPC
f17n02:1075533:1075767 [2] NCCL INFO Connected all rings
f17n02:1075533:1075767 [2] NCCL INFO Channel 01/0 : 50[2] -> 48[0] via P2P/IPC
f17n02:1075533:1075767 [2] NCCL INFO Channel 03/0 : 50[2] -> 48[0] via P2P/IPC
f17n02:1075533:1075767 [2] NCCL INFO Channel 00/0 : 50[2] -> 49[1] via P2P/IPC
f17n02:1075533:1075767 [2] NCCL INFO Channel 02/0 : 50[2] -> 49[1] via P2P/IPC
f17n02:1075533:1075767 [2] NCCL INFO Connected all trees
f17n02:1075533:1075767 [2] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
f17n02:1075533:1075767 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
f17n02:1075533:1075767 [2] NCCL INFO comm 0x136653eb0 rank 50 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x7b12926388d12ce3 - Init COMPLETE
f17n02:1075533:1075883 [2] NCCL INFO Using network IB
f17n02:1075533:1075883 [2] NCCL INFO comm 0x138ff89c0 rank 6 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0xe7c8afafd6ec6b2d - Init START
f17n02:1075533:1075883 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
f17n02:1075533:1075883 [2] NCCL INFO Trees [0] 5/7/-1->6->4 [1] -1/-1/-1->6->5
f17n02:1075533:1075883 [2] NCCL INFO P2P Chunksize set to 131072
f17n02:1075533:1075883 [2] NCCL INFO Channel 00/0 : 5[0] -> 6[2] [receive] via NET/IB/0
f17n02:1075533:1075883 [2] NCCL INFO Channel 01/0 : 5[0] -> 6[2] [receive] via NET/IB/0
f17n02:1075533:1075883 [2] NCCL INFO Channel 00/0 : 6[2] -> 7[4] [send] via NET/IB/0
f17n02:1075533:1075883 [2] NCCL INFO Channel 01/0 : 6[2] -> 7[4] [send] via NET/IB/0
f17n02:1075533:1075883 [2] NCCL INFO Connected all rings
f17n02:1075533:1075883 [2] NCCL INFO Channel 00/0 : 4[4] -> 6[2] [receive] via NET/IB/0
f17n02:1075533:1075883 [2] NCCL INFO Channel 00/0 : 6[2] -> 4[4] [send] via NET/IB/0
f17n02:1075533:1075883 [2] NCCL INFO Channel 00/0 : 7[4] -> 6[2] [receive] via NET/IB/0
f17n02:1075533:1075883 [2] NCCL INFO Channel 00/0 : 6[2] -> 5[0] [send] via NET/IB/0
f17n02:1075533:1075883 [2] NCCL INFO Channel 01/0 : 6[2] -> 5[0] [send] via NET/IB/0
f17n02:1075533:1075883 [2] NCCL INFO Connected all trees
f17n02:1075533:1075883 [2] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
f17n02:1075533:1075883 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f17n02:1075533:1075883 [2] NCCL INFO comm 0x138ff89c0 rank 6 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0xe7c8afafd6ec6b2d - Init COMPLETE
f17n02:1075533:1075902 [2] NCCL INFO Using network IB
f17n02:1075533:1075902 [2] NCCL INFO comm 0x1392fd170 rank 12 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0x32e49bb35e6c8ace - Init START
f17n02:1075533:1075902 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
f17n02:1075533:1075902 [2] NCCL INFO Trees [0] 6/18/-1->12->0 [1] -1/-1/-1->12->13
f17n02:1075533:1075902 [2] NCCL INFO P2P Chunksize set to 131072
f17n02:1075533:1075902 [2] NCCL INFO Channel 00/0 : 11[4] -> 12[2] [receive] via NET/IB/0
f17n02:1075533:1075902 [2] NCCL INFO Channel 01/0 : 11[4] -> 12[2] [receive] via NET/IB/0
f17n02:1075533:1075902 [2] NCCL INFO Channel 00/0 : 12[2] -> 13[0] [send] via NET/IB/0
f17n02:1075533:1075902 [2] NCCL INFO Channel 01/0 : 12[2] -> 13[0] [send] via NET/IB/0
f17n02:1075533:1075902 [2] NCCL INFO Connected all rings
f17n02:1075533:1075902 [2] NCCL INFO Channel 00/0 : 6[2] -> 12[2] [receive] via NET/IB/0
f17n02:1075533:1075902 [2] NCCL INFO Channel 00/0 : 12[2] -> 18[2] [send] via NET/IB/0
f17n02:1075533:1075902 [2] NCCL INFO Channel 00/0 : 0[2] -> 12[2] [receive] via NET/IB/0
f17n02:1075533:1075902 [2] NCCL INFO Channel 00/0 : 12[2] -> 0[2] [send] via NET/IB/0
f17n02:1075533:1075902 [2] NCCL INFO Channel 00/0 : 18[2] -> 12[2] [receive] via NET/IB/0
f17n02:1075533:1075902 [2] NCCL INFO Channel 00/0 : 12[2] -> 6[2] [send] via NET/IB/0
f17n02:1075533:1075902 [2] NCCL INFO Channel 01/0 : 13[0] -> 12[2] [receive] via NET/IB/0
f17n02:1075533:1075902 [2] NCCL INFO Connected all trees
f17n02:1075533:1075902 [2] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
f17n02:1075533:1075902 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f17n02:1075533:1075902 [2] NCCL INFO comm 0x1392fd170 rank 12 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0x32e49bb35e6c8ace - Init COMPLETE
g04n18:916024:916024 [2] NCCL INFO cudaDriverVersion 12020
g04n18:916024:916024 [2] NCCL INFO Bootstrap : Using ib0:10.41.16.12<0>
g04n18:916024:916024 [2] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g04n18:916024:916024 [2] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g04n18:916024:916258 [2] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.16.12<0>
g04n18:916024:916258 [2] NCCL INFO Using network IB
g04n18:916024:916258 [2] NCCL INFO comm 0x1716f3bb0 rank 74 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x7b12926388d12ce3 - Init START
g04n18:916024:916258 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g04n18:916024:916258 [2] NCCL INFO Trees [0] 75/-1/-1->74->73 [1] -1/-1/-1->74->72 [2] 75/-1/-1->74->73 [3] -1/-1/-1->74->72
g04n18:916024:916258 [2] NCCL INFO P2P Chunksize set to 131072
g04n18:916024:916258 [2] NCCL INFO Channel 00/0 : 74[2] -> 75[3] via P2P/IPC
g04n18:916024:916258 [2] NCCL INFO Channel 02/0 : 74[2] -> 75[3] via P2P/IPC
g04n18:916024:916258 [2] NCCL INFO Channel 01/0 : 74[2] -> 73[1] via P2P/IPC
g04n18:916024:916258 [2] NCCL INFO Channel 03/0 : 74[2] -> 73[1] via P2P/IPC
g04n18:916024:916258 [2] NCCL INFO Connected all rings
g04n18:916024:916258 [2] NCCL INFO Channel 01/0 : 74[2] -> 72[0] via P2P/IPC
g04n18:916024:916258 [2] NCCL INFO Channel 03/0 : 74[2] -> 72[0] via P2P/IPC
g04n18:916024:916258 [2] NCCL INFO Channel 00/0 : 74[2] -> 73[1] via P2P/IPC
g04n18:916024:916258 [2] NCCL INFO Channel 02/0 : 74[2] -> 73[1] via P2P/IPC
g04n18:916024:916258 [2] NCCL INFO Connected all trees
g04n18:916024:916258 [2] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g04n18:916024:916258 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g04n18:916024:916258 [2] NCCL INFO comm 0x1716f3bb0 rank 74 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x7b12926388d12ce3 - Init COMPLETE
g04n18:916024:916343 [2] NCCL INFO Using network IB
g04n18:916024:916343 [2] NCCL INFO comm 0x174437430 rank 9 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0xe7c8afafd6ec6b2d - Init START
g04n18:916024:916343 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g04n18:916024:916343 [2] NCCL INFO Trees [0] -1/-1/-1->9->10 [1] 10/8/-1->9->7
g04n18:916024:916343 [2] NCCL INFO P2P Chunksize set to 131072
g04n18:916024:916343 [2] NCCL INFO Channel 00/0 : 8[0] -> 9[2] [receive] via NET/IB/0
g04n18:916024:916343 [2] NCCL INFO Channel 01/0 : 8[0] -> 9[2] [receive] via NET/IB/0
g04n18:916024:916343 [2] NCCL INFO Channel 00/0 : 9[2] -> 10[4] [send] via NET/IB/0
g04n18:916024:916343 [2] NCCL INFO Channel 01/0 : 9[2] -> 10[4] [send] via NET/IB/0
g04n18:916024:916343 [2] NCCL INFO Connected all rings
g04n18:916024:916343 [2] NCCL INFO Channel 01/0 : 7[4] -> 9[2] [receive] via NET/IB/0
g04n18:916024:916343 [2] NCCL INFO Channel 01/0 : 9[2] -> 7[4] [send] via NET/IB/0
g04n18:916024:916343 [2] NCCL INFO Channel 00/0 : 10[4] -> 9[2] [receive] via NET/IB/0
g04n18:916024:916343 [2] NCCL INFO Channel 01/0 : 10[4] -> 9[2] [receive] via NET/IB/0
g04n18:916024:916343 [2] NCCL INFO Channel 01/0 : 9[2] -> 8[0] [send] via NET/IB/0
g04n18:916024:916343 [2] NCCL INFO Connected all trees
g04n18:916024:916343 [2] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g04n18:916024:916343 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g04n18:916024:916343 [2] NCCL INFO comm 0x174437430 rank 9 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0xe7c8afafd6ec6b2d - Init COMPLETE
g04n18:916024:916359 [2] NCCL INFO Using network IB
g04n18:916024:916359 [2] NCCL INFO comm 0x174445820 rank 18 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0x32e49bb35e6c8ace - Init START
g04n18:916024:916359 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g04n18:916024:916359 [2] NCCL INFO Trees [0] 15/21/-1->18->12 [1] -1/-1/-1->18->19
g04n18:916024:916359 [2] NCCL INFO P2P Chunksize set to 131072
g04n18:916024:916359 [2] NCCL INFO Channel 00/0 : 17[4] -> 18[2] [receive] via NET/IB/0
g04n18:916024:916359 [2] NCCL INFO Channel 01/0 : 17[4] -> 18[2] [receive] via NET/IB/0
g04n18:916024:916359 [2] NCCL INFO Channel 00/0 : 18[2] -> 19[0] [send] via NET/IB/0
g04n18:916024:916359 [2] NCCL INFO Channel 01/0 : 18[2] -> 19[0] [send] via NET/IB/0
g04n18:916024:916359 [2] NCCL INFO Connected all rings
g04n18:916024:916359 [2] NCCL INFO Channel 00/0 : 15[2] -> 18[2] [receive] via NET/IB/0
g04n18:916024:916359 [2] NCCL INFO Channel 00/0 : 18[2] -> 21[2] [send] via NET/IB/0
g04n18:916024:916359 [2] NCCL INFO Channel 00/0 : 12[2] -> 18[2] [receive] via NET/IB/0
g04n18:916024:916359 [2] NCCL INFO Channel 00/0 : 18[2] -> 12[2] [send] via NET/IB/0
g04n18:916024:916359 [2] NCCL INFO Channel 00/0 : 21[2] -> 18[2] [receive] via NET/IB/0
g04n18:916024:916359 [2] NCCL INFO Channel 00/0 : 18[2] -> 15[2] [send] via NET/IB/0
g04n18:916024:916359 [2] NCCL INFO Channel 01/0 : 19[0] -> 18[2] [receive] via NET/IB/0
g04n18:916024:916359 [2] NCCL INFO Connected all trees
g04n18:916024:916359 [2] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g04n18:916024:916359 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g04n18:916024:916359 [2] NCCL INFO comm 0x174445820 rank 18 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0x32e49bb35e6c8ace - Init COMPLETE
g05n09:221464:221464 [2] NCCL INFO cudaDriverVersion 12020
g05n09:221464:221464 [2] NCCL INFO Bootstrap : Using ib0:10.41.16.21<0>
g05n09:221464:221464 [2] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g05n09:221464:221464 [2] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g05n09:221464:221692 [2] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.16.21<0>
g05n09:221464:221692 [2] NCCL INFO Using network IB
g05n09:221464:221692 [2] NCCL INFO comm 0x147913a90 rank 86 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x7b12926388d12ce3 - Init START
g05n09:221464:221692 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g05n09:221464:221692 [2] NCCL INFO Trees [0] 87/-1/-1->86->85 [1] -1/-1/-1->86->84 [2] 87/-1/-1->86->85 [3] -1/-1/-1->86->84
g05n09:221464:221692 [2] NCCL INFO P2P Chunksize set to 131072
g05n09:221464:221692 [2] NCCL INFO Channel 00/0 : 86[2] -> 87[3] via P2P/IPC
g05n09:221464:221692 [2] NCCL INFO Channel 02/0 : 86[2] -> 87[3] via P2P/IPC
g05n09:221464:221692 [2] NCCL INFO Channel 01/0 : 86[2] -> 85[1] via P2P/IPC
g05n09:221464:221692 [2] NCCL INFO Channel 03/0 : 86[2] -> 85[1] via P2P/IPC
g05n09:221464:221692 [2] NCCL INFO Connected all rings
g05n09:221464:221692 [2] NCCL INFO Channel 01/0 : 86[2] -> 84[0] via P2P/IPC
g05n09:221464:221692 [2] NCCL INFO Channel 03/0 : 86[2] -> 84[0] via P2P/IPC
g05n09:221464:221692 [2] NCCL INFO Channel 00/0 : 86[2] -> 85[1] via P2P/IPC
g05n09:221464:221692 [2] NCCL INFO Channel 02/0 : 86[2] -> 85[1] via P2P/IPC
g05n09:221464:221692 [2] NCCL INFO Connected all trees
g05n09:221464:221692 [2] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g05n09:221464:221692 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g05n09:221464:221692 [2] NCCL INFO comm 0x147913a90 rank 86 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x7b12926388d12ce3 - Init COMPLETE
g05n09:221464:221782 [2] NCCL INFO Using network IB
g05n09:221464:221782 [2] NCCL INFO comm 0x14a5d9ea0 rank 10 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0x414883fe5ee4d1ab - Init START
g05n09:221464:221782 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g05n09:221464:221782 [2] NCCL INFO Trees [0] 9/11/-1->10->8 [1] -1/-1/-1->10->9
g05n09:221464:221782 [2] NCCL INFO P2P Chunksize set to 131072
g05n09:221464:221782 [2] NCCL INFO Channel 00/0 : 9[0] -> 10[2] [receive] via NET/IB/0
g05n09:221464:221782 [2] NCCL INFO Channel 01/0 : 9[0] -> 10[2] [receive] via NET/IB/0
g05n09:221464:221782 [2] NCCL INFO Channel 00/0 : 10[2] -> 11[4] [send] via NET/IB/0
g05n09:221464:221782 [2] NCCL INFO Channel 01/0 : 10[2] -> 11[4] [send] via NET/IB/0
g05n09:221464:221782 [2] NCCL INFO Connected all rings
g05n09:221464:221782 [2] NCCL INFO Channel 00/0 : 8[4] -> 10[2] [receive] via NET/IB/0
g05n09:221464:221782 [2] NCCL INFO Channel 00/0 : 10[2] -> 8[4] [send] via NET/IB/0
g05n09:221464:221782 [2] NCCL INFO Channel 00/0 : 11[4] -> 10[2] [receive] via NET/IB/0
g05n09:221464:221782 [2] NCCL INFO Channel 00/0 : 10[2] -> 9[0] [send] via NET/IB/0
g05n09:221464:221782 [2] NCCL INFO Channel 01/0 : 10[2] -> 9[0] [send] via NET/IB/0
g05n09:221464:221782 [2] NCCL INFO Connected all trees
g05n09:221464:221782 [2] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g05n09:221464:221782 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g05n09:221464:221782 [2] NCCL INFO comm 0x14a5d9ea0 rank 10 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0x414883fe5ee4d1ab - Init COMPLETE
g05n09:221464:221801 [2] NCCL INFO Using network IB
g05n09:221464:221801 [2] NCCL INFO comm 0x14a246120 rank 21 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0x32e49bb35e6c8ace - Init START
g05n09:221464:221801 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g05n09:221464:221801 [2] NCCL INFO Trees [0] 19/22/-1->21->18 [1] -1/-1/-1->21->20
g05n09:221464:221801 [2] NCCL INFO P2P Chunksize set to 131072
g05n09:221464:221801 [2] NCCL INFO Channel 00/0 : 20[4] -> 21[2] [receive] via NET/IB/0
g05n09:221464:221801 [2] NCCL INFO Channel 01/0 : 20[4] -> 21[2] [receive] via NET/IB/0
g05n09:221464:221801 [2] NCCL INFO Channel 00/0 : 21[2] -> 22[0] [send] via NET/IB/0
g05n09:221464:221801 [2] NCCL INFO Channel 01/0 : 21[2] -> 22[0] [send] via NET/IB/0
g05n09:221464:221801 [2] NCCL INFO Connected all rings
g05n09:221464:221801 [2] NCCL INFO Channel 00/0 : 19[0] -> 21[2] [receive] via NET/IB/0
g05n09:221464:221801 [2] NCCL INFO Channel 00/0 : 18[2] -> 21[2] [receive] via NET/IB/0
g05n09:221464:221801 [2] NCCL INFO Channel 00/0 : 21[2] -> 18[2] [send] via NET/IB/0
g05n09:221464:221801 [2] NCCL INFO Channel 00/0 : 21[2] -> 19[0] [send] via NET/IB/0
g05n09:221464:221801 [2] NCCL INFO Channel 00/0 : 22[0] -> 21[2] [receive] via NET/IB/0
g05n09:221464:221801 [2] NCCL INFO Channel 01/0 : 21[2] -> 20[4] [send] via NET/IB/0
g05n09:221464:221801 [2] NCCL INFO Connected all trees
g05n09:221464:221801 [2] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g05n09:221464:221801 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g05n09:221464:221801 [2] NCCL INFO comm 0x14a246120 rank 21 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0x32e49bb35e6c8ace - Init COMPLETE
d11n17:1141836:1141836 [4] NCCL INFO cudaDriverVersion 12020
d11n17:1141836:1141836 [4] NCCL INFO Bootstrap : Using ib0:10.41.8.217<0>
d11n17:1141836:1141836 [4] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
d11n17:1141836:1141836 [4] NCCL INFO NET/Plugin : No plugin found, using internal implementation
d11n17:1141836:1142065 [4] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.8.217<0>
d11n17:1141836:1142065 [4] NCCL INFO Using network IB
d11n17:1141836:1142065 [4] NCCL INFO comm 0x161ec37e0 rank 22 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x7b12926388d12ce3 - Init START
d11n17:1141836:1142065 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
d11n17:1141836:1142065 [4] NCCL INFO Trees [0] 23/-1/-1->22->21 [1] 21/-1/-1->22->16 [2] 23/-1/-1->22->21 [3] 21/10/-1->22->46
d11n17:1141836:1142065 [4] NCCL INFO P2P Chunksize set to 131072
d11n17:1141836:1142065 [4] NCCL INFO Channel 00/0 : 22[4] -> 23[5] via P2P/IPC
d11n17:1141836:1142065 [4] NCCL INFO Channel 01/0 : 22[4] -> 23[5] via P2P/IPC
d11n17:1141836:1142065 [4] NCCL INFO Channel 02/0 : 22[4] -> 23[5] via P2P/IPC
d11n17:1141836:1142065 [4] NCCL INFO Channel 03/0 : 22[4] -> 23[5] via P2P/IPC
d11n17:1141836:1142065 [4] NCCL INFO Connected all rings
d11n17:1141836:1142065 [4] NCCL INFO Channel 01/0 : 16[4] -> 22[4] [receive] via NET/IB/3
d11n17:1141836:1142065 [4] NCCL INFO Channel 03/0 : 10[4] -> 22[4] [receive] via NET/IB/3
d11n17:1141836:1142065 [4] NCCL INFO Channel 03/0 : 22[4] -> 46[4] [send] via NET/IB/3
d11n17:1141836:1142065 [4] NCCL INFO Channel 03/0 : 46[4] -> 22[4] [receive] via NET/IB/3
d11n17:1141836:1142065 [4] NCCL INFO Channel 03/0 : 22[4] -> 10[4] [send] via NET/IB/3
d11n17:1141836:1142065 [4] NCCL INFO Channel 01/0 : 22[4] -> 16[4] [send] via NET/IB/3
d11n17:1141836:1142065 [4] NCCL INFO Channel 00/0 : 22[4] -> 21[3] via P2P/IPC
d11n17:1141836:1142065 [4] NCCL INFO Channel 01/0 : 22[4] -> 21[3] via P2P/IPC
d11n17:1141836:1142065 [4] NCCL INFO Channel 02/0 : 22[4] -> 21[3] via P2P/IPC
d11n17:1141836:1142065 [4] NCCL INFO Channel 03/0 : 22[4] -> 21[3] via P2P/IPC
d11n17:1141836:1142065 [4] NCCL INFO Connected all trees
d11n17:1141836:1142065 [4] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
d11n17:1141836:1142065 [4] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
d11n17:1141836:1142065 [4] NCCL INFO comm 0x161ec37e0 rank 22 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x7b12926388d12ce3 - Init COMPLETE
d11n17:1141836:1142149 [4] NCCL INFO Using network IB
d11n17:1141836:1142149 [4] NCCL INFO comm 0x1647c7080 rank 2 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x414883fe5ee4d1ab - Init START
d11n17:1141836:1142149 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
d11n17:1141836:1142149 [4] NCCL INFO Trees [0] 1/3/-1->2->4 [1] -1/-1/-1->2->1
d11n17:1141836:1142149 [4] NCCL INFO P2P Chunksize set to 131072
d11n17:1141836:1142149 [4] NCCL INFO Channel 00/0 : 1[2] -> 2[4] [receive] via NET/IB/1
d11n17:1141836:1142149 [4] NCCL INFO Channel 01/0 : 1[2] -> 2[4] [receive] via NET/IB/1
d11n17:1141836:1142149 [4] NCCL INFO Channel 00/0 : 2[4] -> 3[0] [send] via NET/IB/1
d11n17:1141836:1142149 [4] NCCL INFO Channel 01/0 : 2[4] -> 3[0] [send] via NET/IB/1
d11n17:1141836:1142149 [4] NCCL INFO Connected all rings
d11n17:1141836:1142149 [4] NCCL INFO Channel 00/0 : 2[4] -> 4[2] [send] via NET/IB/1
d11n17:1141836:1142149 [4] NCCL INFO Channel 00/0 : 4[2] -> 2[4] [receive] via NET/IB/1
d11n17:1141836:1142149 [4] NCCL INFO Channel 00/0 : 3[0] -> 2[4] [receive] via NET/IB/1
d11n17:1141836:1142149 [4] NCCL INFO Channel 00/0 : 2[4] -> 1[2] [send] via NET/IB/1
d11n17:1141836:1142149 [4] NCCL INFO Channel 01/0 : 2[4] -> 1[2] [send] via NET/IB/1
d11n17:1141836:1142149 [4] NCCL INFO Connected all trees
d11n17:1141836:1142149 [4] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
d11n17:1141836:1142149 [4] NCCL d11n17:1141832:1141832 [0] NCCL INFO cudaDriverVersion 12020
d11n17:1141832:1141832 [0] NCCL INFO Bootstrap : Using ib0:10.41.8.217<0>
d11n17:1141832:1141832 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
d11n17:1141832:1141832 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
d11n17:1141832:1142063 [0] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.8.217<0>
d11n17:1141832:1142063 [0] NCCL INFO Using network IB
d11n17:1141832:1142063 [0] NCCL INFO comm 0x15faf3f20 rank 18 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x7b12926388d12ce3 - Init START
d11n17:1141832:1142063 [0] NCCL INFO Setting affinity for GPU 0 to 0f
d11n17:1141832:1142063 [0] NCCL INFO Trees [0] 19/-1/-1->18->12 [1] 20/-1/-1->18->19 [2] 19/6/-1->18->42 [3] 20/-1/-1->18->19
d11n17:1141832:1142063 [0] NCCL INFO P2P Chunksize set to 131072
d11n17:1141832:1142063 [0] NCCL INFO Channel 00/0 : 17[5] -> 18[0] [receive] via NET/IB/0
d11n17:1141832:1142063 [0] NCCL INFO Channel 02/0 : 17[5] -> 18[0] [receive] via NET/IB/0
d11n17:1141832:1142063 [0] NCCL INFO Channel 00/0 : 18[0] -> 19[1] via P2P/IPC
d11n17:1141832:1142063 [0] NCCL INFO Channel 02/0 : 18[0] -> 19[1] via P2P/IPC
d11n17:1141832:1142063 [0] NCCL INFO Channel 01/0 : 18[0] -> 27[3] [send] via NET/IB/2
d11n17:1141832:1142063 [0] NCCL INFO Channel 03/0 : 18[0] -> 27[3] [send] via NET/IB/2
d11n17:1141832:1142063 [0] NCCL INFO Connected all rings
d11n17:1141832:1142063 [0] NCCL INFO Channel 01/0 : 18[0] -> 19[1] via P2P/IPC
d11n17:1141832:1142063 [0] NCCL INFO Channel 03/0 : 18[0] -> 19[1] via P2P/IPC
d11n17:1141832:1142063 [0] NCCL INFO Channel 01/0 : 18[0] -> 20[2] via P2P/IPC
d11n17:1141832:1142063 [0] NCCL INFO Channel 03/0 : 18[0] -> 20[2] via P2P/IPC
d11n17:1141832:1142063 [0] NCCL INFO Channel 00/0 : 12[0] -> 18[0] [receive] via NET/IB/0
d11n17:1141832:1142063 [0] NCCL INFO Channel 02/0 : 6[0] -> 18[0] [receive] via NET/IB/0
d11n17:1141832:1142063 [0] NCCL INFO Channel 02/0 : 18[0] -> 42[0] [send] via NET/IB/0
d11n17:1141832:1142063 [0] NCCL INFO Channel 02/0 : 42[0] -> 18[0] [receive] via NET/IB/0
d11n17:1141832:1142063 [0] NCCL INFO Channel 02/0 : 18[0] -> 6[0] [send] via NET/IB/0
d11n17:1141832:1142063 [0] NCCL INFO Channel 00/0 : 18[0] -> 12[0] [send] via NET/IB/0
d11n17:1141832:1142063 [0] NCCL INFO Connected all trees
d11n17:1141832:1142063 [0] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
d11n17:1141832:1142063 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
d11n17:1141832:1142063 [0] NCCL INFO comm 0x15faf3f20 rank 18 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x7b12926388d12ce3 - Init COMPLETE
d11n17:1141832:1142147 [0] NCCL INFO Using network IB
d11n17:1141832:1142147 [0] NCCL INFO comm 0x16283adc0 rank 2 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0xe7c8afafd6ec6b2d - Init START
d11n17:1141832:1142147 [0] NCCL INFO Setting affinity for GPU 0 to 0f
d11n17:1141832:1142147 [0] NCCL INFO Trees [0] 1/3/-1->2->4 [1] -1/-1/-1->2->1
d11n17:1141832:1142147 [0] NCCL INFO P2P Chunksize set to 131072
d11n17:1141832:1142147 [0] NCCL INFO Channel 00/0 : 1[4] -> 2[0] [receive] via NET/IB/0
d11n17:1141832:1142147 [0] NCCL INFO Channel 01/0 : 1[4] -> 2[0] [receive] via NET/IB/0
d11n17:1141832:1142147 [0] NCCL INFO Channel 00/0 : 2[0] -> 3[2] [send] via NET/IB/0
d11n17:1141832:1142147 [0] NCCL INFO Channel 01/0 : 2[0] -> 3[2] [send] via NET/IB/0
d11n17:1141832:1142147 [0] NCCL INFO Connected all rings
d11n17:1141832:1142147 [0] NCCL INFO Channel 00/0 : 2[0] -> 4[4] [send] via NET/IB/0
d11n17:1141832:1142147 [0] NCCL INFO Channel 00/0 : 4[4] -> 2[0] [receive] via NET/IB/0
d11n17:1141832:1142147 [0] NCCL INFO Channel 00/0 : 3[2] -> 2[0] [receive] via NET/IB/0
d11n17:1141832:1142147 [0] NCCL INFO Channel 00/0 : 2[0] -> 1[4] [send] via NET/IB/0
d11n17:1141832:1142147 [0] NCCL INFO Channel 01/0 : 2[0] -> 1[4] [send] via NET/IB/0
d11n17:1141832:1142147 [0] NCCL INFO Connected all trees
d11n17:1141832:1142147 [0] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
d11n17:1141832:1142147 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d11n17:1141832:1142147 [0] NCCL INFO comm 0x16283adc0 rank 2 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0xe7c8afafd6ec6b2d - Init COMPLETE
d11n17:1141832:1142171 [0] NCCL INFO Using network IB
d11n17:1141832:1142171 [0] NCCL INFO comm 0x16279cab0 rank 4 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0x32e49bb35e6c8ace - Init START
d11n17:1141832:1142171 [0] NCCL INFO Setting affinity for GPU 0 to 0f
d11n17:1141832:1142171 [0] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/1/-1->4->10
d11n17:1141832:1142171 [0] NCCL INFO P2P Chunksize set to 131072
d11n17:1141832:1142171 [0] NCCL INFO Channel 00/0 : 3[2] -> 4[0] [receive] via NET/IB/0
d11n17:1141832:1142171 [0] NCCL INFO Channel 01/0 : 3[2] -> 4[0] [receive] via NET/IB/0
d11n17:1141832:1142171 [0] NCCL INFO Channel 00/0 : 4[0] -> 5[4] via P2P/IPC
d11n17:1141832:1142171 [0] NCCL INFO Channel 01/0 : 4[0] -> 5[4] via P2P/IPC
d11n17:1141832:1142171 [0] NCCL INFO Connected all rings
d11n17:1141832:1142171 [0] NCCL INFO Channel 01/0 : 1[0] -> 4[0] [receive] via NET/IB/0
d11n17:1141832:1142171 [0] NCCL INFO Channel 01/0 : 4[0] -> 10[0] [send] via NET/IB/0
d11n17:1141832:1142171 [0] NCCL INFO Channel 01/0 : 10[0] -> 4[0] [receive] via NET/IB/0
d11n17:1141832:1142171 [0] NCCL INFO Channel 01/0 : 4[0] -> 1[0] [send] via NET/IB/0
d11n17:1141832:1142171 [0] NCCL INFO Channel 00/0 : 4[0] -> 3[2] [send] via NET/IB/0
d11n17:1141832:1142171 [0] NCCL INFO Connected all trees
d11n17:1141832:1142171 [0] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
d11n17:1141832:1142171 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d11n17:1141832:1142171 [0] NCCL INFO comm 0x16279cab0 rank 4 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0x32e49bb35e6c8ace - Init COMPLETE
d11n16:1129250:1129250 [5] NCCL INFO cudaDriverVersion 12020
d11n16:1129250:1129250 [5] NCCL INFO Bootstrap : Using ib0:10.41.8.216<0>
d11n16:1129250:1129250 [5] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
d11n16:1129250:1129250 [5] NCCL INFO NET/Plugin : No plugin found, using internal implementation
d11n16:1129250:1129478 [5] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.8.216<0>
d11n16:1129250:1129478 [5] NCCL INFO Using network IB
d11n16:1129250:1129478 [5] NCCL INFO comm 0x151c73990 rank 17 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x7b12926388d12ce3 - Init START
d11n16:1129250:1129478 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
d11n16:1129250:1129478 [5] NCCL INFO Trees [0] -1/-1/-1->17->16 [1] 13/-1/-1->17->15 [2] -1/-1/-1->17->16 [3] 13/-1/-1->17->15
d11n16:1129250:1129478 [5] NCCL INFO P2P Chunksize set to 131072
d11n16:1129250:1129478 [5] NCCL INFO Channel 00/0 : 17[5] -> 18[0] [send] via NET/IB/1
d11n16:1129250:1129478 [5] NCCL INFO Channel 02/0 : 17[5] -> 18[0] [send] via NET/IB/1
d11n16:1129250:1129478 [5] NCCL INFO Channel 01/0 : 17[5] -> 14[2] via P2P/IPC
d11n16:1129250:1129478 [5] NCCL INFO Channel 03/0 : 17[5] -> 14[2] via P2P/IPC
d11n16:1129250:1129478 [5] NCCL INFO Connected all rings
d11n16:1129250:1129478 [5] NCCL INFO Channel 01/0 : 17[5] -> 13[1] via P2P/IPC
d11n16:1129250:1129478 [5] NCCL INFO Channel 03/0 : 17[5] -> 13[1] via P2P/IPC
d11n16:1129250:1129478 [5] NCCL INFO Channel 01/0 : 17[5] -> 15[3] via P2P/IPC
d11n16:1129250:1129478 [5] NCCL INFO Channel 03/0 : 17[5] -> 15[3] via P2P/IPC
d11n16:1129250:1129478 [5] NCCL INFO Channel 00/0 : 17[5] -> 16[4] via P2P/IPC
d11n16:1129250:1129478 [5] NCCL INFO Channel 02/0 : 17[5] -> 16[4] via P2P/IPC
d11n16:1129250:1129478 [5] NCCL INFO Connected all trees
d11n16:1129250:1129478 [5] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
d11n16:1129250:1129478 [5] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
d11n16:1129250:1129478 [5] NCCL INFO comm 0x151c73990 rank 17 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x7b12926388d12ce3 - Init COMPLETE
d11n16:1129250:1129561 [5] NCCL INFO Using network IB
d11n16:1129250:1129561 [5] NCCL INFO comm 0x15490d120 rank 2 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x86d434c9d289d59a - Init START
d11n16:1129250:1129561 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
d11n16:1129250:1129561 [5] NCCL INFO Trees [0] 1/3/-1->2->4 [1] -1/-1/-1->2->1
d11n16:1129250:1129561 [5] NCCL INFO P2P Chunksize set to 131072
d11n16:1129250:1129561 [5] NCCL INFO Channel 00/0 : 1[3] -> 2[5] [receive] via NET/IB/3
d11n16:1129250:1129561 [5] NCCL INFO Channel 01/0 : 1[3] -> 2[5] [receive] via NET/IB/3
d11n16:1129250:1129561 [5] NCCL INFO Channel 00/0 : 2[5] -> 3[1] [send] via NET/IB/3
d11n16:1129250:1129561 [5] NCCL INFO Channel 01/0 : 2[5] -> 3[1] [send] via NET/IB/3
d11n16:1129250:1129561 [5] NCCL INFO Connected all rings
d11n16:1129250:1129561 [5] NCCL INFO Channel 00/0 : 2[5] -> 4[3] [send] via NET/IB/3
d11n16:1129250:1129561 [5] NCCL INFO Channel 00/0 : 4[3] -> 2[5] [receive] via NET/IB/3
d11n16:1129250:1129561 [5] NCCL INFO Channel 00/0 : 3[1] -> 2[5] [receive] via NET/IB/3
d11n16:1129250:1129561 [5] NCCL INFO Channel 00/0 : 2[5] -> 1[3] [send] via NET/IB/3
d11n16:1129250:1129561 [5] NCCL INFO Channel 01/0 : 2[5] -> 1[3] [send] via NET/IB/3
d11n16:1129250:1129561 [5] NCCL INFO Connected all trees
d11n16:1129250:1129561 [5] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
d11n16:1129250:1129561 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d11n16:1129250:1129561 [5] NCCL INFO comm 0x15490d120 rank 2 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x86d434c9d289d59a - Init COMPLETE
d11n16:1129250:1129581 [5] NCCL INFO Using network IB
d11n16:1129250:1129581 [5] NCCL INFO comm 0x153c184f0 rank INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d11n17:1141836:1142149 [4] NCCL INFO comm 0x1647c7080 rank 2 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x414883fe5ee4d1ab - Init COMPLETE
d11n17:1141836:1142166 [4] NCCL INFO Using network IB
d11n17:1141836:1142166 [4] NCCL INFO comm 0x164b8b260 rank 5 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x32e49bb35e6c8ace - Init START
d11n17:1141836:1142166 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
d11n17:1141836:1142166 [4] NCCL INFO Trees [0] -1/-1/-1->5->4 [1] 7/-1/-1->5->4
d11n17:1141836:1142166 [4] NCCL INFO P2P Chunksize set to 131072
d11n17:1141836:1142166 [4] NCCL INFO Channel 00/0 : 5[4] -> 6[2] [send] via NET/IB/1
d11n17:1141836:1142166 [4] NCCL INFO Channel 01/0 : 5[4] -> 6[2] [send] via NET/IB/1
d11n17:1141836:1142166 [4] NCCL INFO Connected all rings
d11n17:1141836:1142166 [4] NCCL INFO Channel 01/0 : 5[4] -> 7[0] [send] via NET/IB/0
d11n17:1141836:1142166 [4] NCCL INFO Channel 01/0 : 7[0] -> 5[4] [receive] via NET/IB/0
d11n17:1141836:1142166 [4] NCCL INFO Channel 00/0 : 5[4] -> 4[0] via P2P/IPC
d11n17:1141836:1142166 [4] NCCL INFO Channel 01/0 : 5[4] -> 4[0] via P2P/IPC
d11n17:1141836:1142166 [4] NCCL INFO Connected all trees
d11n17:1141836:1142166 [4] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
d11n17:1141836:1142166 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d11n17:1141836:1142166 [4] NCCL INFO comm 0x164b8b260 rank 5 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x32e49bb35e6c8ace - Init COMPLETE
4 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xa49a27aa8b15034e - Init START
d11n16:1129250:1129581 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
d11n16:1129250:1129581 [5] NCCL INFO Trees [0] 2/-1/-1->4->3 [1] -1/-1/-1->4->3
d11n16:1129250:1129581 [5] NCCL INFO P2P Chunksize set to 131072
d11n16:1129250:1129581 [5] NCCL INFO Channel 00/0 : 4[5] -> 5[3] [send] via NET/IB/3
d11n16:1129250:1129581 [5] NCCL INFO Channel 01/0 : 4[5] -> 5[3] [send] via NET/IB/3
d11n16:1129250:1129581 [5] NCCL INFO Connected all rings
d11n16:1129250:1129581 [5] NCCL INFO Channel 00/0 : 2[3] -> 4[5] [receive] via NET/IB/2
d11n16:1129250:1129581 [5] NCCL INFO Channel 00/0 : 4[5] -> 2[3] [send] via NET/IB/2
d11n16:1129250:1129581 [5] NCCL INFO Channel 00/0 : 4[5] -> 3[1] via P2P/IPC
d11n16:1129250:1129581 [5] NCCL INFO Channel 01/0 : 4[5] -> 3[1] via P2P/IPC
d11n16:1129250:1129581 [5] NCCL INFO Connected all trees
d11n16:1129250:1129581 [5] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
d11n16:1129250:1129581 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d11n16:1129250:1129581 [5] NCCL INFO comm 0x153c184f0 rank 4 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xa49a27aa8b15034e - Init COMPLETE
f16n17:1090241:1090241 [4] NCCL INFO cudaDriverVersion 12020
f16n17:1090241:1090241 [4] NCCL INFO Bootstrap : Using ib0:10.41.14.85<0>
f16n17:1090241:1090241 [4] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
f16n17:1090241:1090241 [4] NCCL INFO NET/Plugin : No plugin found, using internal implementation
f16n17:1090241:1090470 [4] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.14.85<0>
f16n17:1090241:1090470 [4] NCCL INFO Using network IB
f16n17:1090241:1090470 [4] NCCL INFO comm 0x1533b3d90 rank 34 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x7b12926388d12ce3 - Init START
f16n17:1090241:1090470 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
f16n17:1090241:1090470 [4] NCCL INFO Trees [0] 35/-1/-1->34->33 [1] 33/-1/-1->34->39 [2] 35/-1/-1->34->33 [3] 33/28/-1->34->21
f16n17:1090241:1090470 [4] NCCL INFO P2P Chunksize set to 131072
f16n17:1090241:1090470 [4] NCCL INFO Channel 00/0 : 34[4] -> 35[5] via P2P/IPC
f16n17:1090241:1090470 [4] NCCL INFO Channel 01/0 : 34[4] -> 35[5] via P2P/IPC
f16n17:1090241:1090470 [4] NCCL INFO Channel 02/0 : 34[4] -> 35[5] via P2P/IPC
f16n17:1090241:1090470 [4] NCCL INFO Channel 03/0 : 34[4] -> 35[5] via P2P/IPC
f16n17:1090241:1090470 [4] NCCL INFO Connected all rings
f16n17:1090241:1090470 [4] NCCL INFO Channel 01/0 : 34[4] -> 39[3] [send] via NET/IB/3
f16n17:1090241:1090470 [4] NCCL INFO Channel 03/0 : 28[4] -> 34[4] [receive] via NET/IB/3
f16n17:1090241:1090470 [4] NCCL INFO Channel 03/0 : 21[3] -> 34[4] [receive] via NET/IB/3
f16n17:1090241:1090470 [4] NCCL INFO Channel 03/0 : 34[4] -> 21[3] [send] via NET/IB/3
f16n17:1090241:1090470 [4] NCCL INFO Channel 03/0 : 34[4] -> 28[4] [send] via NET/IB/3
f16n17:1090241:1090470 [4] NCCL INFO Channel 01/0 : 39[3] -> 34[4] [receive] via NET/IB/3
f16n17:1090241:1090470 [4] NCCL INFO Channel 00/0 : 34[4] -> 33[3] via P2P/IPC
f16n17:1090241:1090470 [4] NCCL INFO Channel 01/0 : 34[4] -> 33[3] via P2P/IPC
f16n17:1090241:1090470 [4] NCCL INFO Channel 02/0 : 34[4] -> 33[3] via P2P/IPC
f16n17:1090241:1090470 [4] NCCL INFO Channel 03/0 : 34[4] -> 33[3] via P2P/IPC
f16n17:1090241:1090470 [4] NCCL INFO Connected all trees
f16n17:1090241:1090470 [4] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
f16n17:1090241:1090470 [4] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
f16n17:1090241:1090470 [4] NCCL INFO comm 0x1533b3d90 rank 34 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x7b12926388d12ce3 - Init COMPLETE
f16n17:1090241:1090558 [4] NCCL INFO Using network IB
f16n17:1090241:1090558 [4] NCCL INFO comm 0x154fdc740 rank 4 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xe7c8afafd6ec6b2d - Init START
f16n17:1090241:1090558 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
f16n17:1090241:1090558 [4] NCCL INFO Trees [0] 2/6/-1->4->8 [1] -1/-1/-1->4->5
f16n17:1090241:1090558 [4] NCCL INFO P2P Chunksize set to 131072
f16n17:1090241:1090558 [4] NCCL INFO Channel 00/0 : 3[2] -> 4[4] [receive] via NET/IB/1
f16n17:1090241:1090558 [4] NCCL INFO Channel 01/0 : 3[2] -> 4[4] [receive] via NET/IB/1
f16n17:1090241:1090558 [4] NCCL INFO Channel 00/0 : 4[4] -> 5[0] [send] via NET/IB/1
f16n17:1090241:1090558 [4] NCCL INFO Channel 01/0 : 4[4] -> 5[0] [send] via NET/IB/1
f16n17:1090241:1090558 [4] NCCL INFO Connected all rings
f16n17:1090241:1090558 [4] NCCL INFO Channel 00/0 : 2[0] -> 4[4] [receive] via NET/IB/1
f16n17:1090241:1090558 [4] NCCL INFO Channel 00/0 : 4[4] -> 6[2] [send] via NET/IB/1
f16n17:1090241:1090558 [4] NCCL INFO Channel 00/0 : 4[4] -> 8[0] [send] via NET/IB/1
f16n17:1090241:1090558 [4] NCCL INFO Channel 00/0 : 8[0] -> 4[4] [receive] via NET/IB/1
f16n17:1090241:1090558 [4] NCCL INFO Channel 00/0 : 6[2] -> 4[4] [receive] via NET/IB/1
f16n17:1090241:1090558 [4] NCCL INFO Channel 00/0 : 4[4] -> 2[0] [send] via NET/IB/1
f16n17:1090241:1090558 [4] NCCL INFO Channel 01/0 : 5[0] -> 4[4] [receive] via NET/IB/1
f16n17:1090241:1090558 [4] NCCL INFO Connected all trees
f16n17:1090241:1090558 [4] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
f16n17:1090241:1090558 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f16n17:1090241:1090558 [4] NCCL INFO comm 0x154fdc740 rank 4 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xe7c8afafd6ec6b2d - Init COMPLETE
f16n17:1090241:1090580 [4] NCCL INFO Using network IB
f16n17:1090241:1090580 [4] NCCL INFO comm 0x156117690 rank 8 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x32e49bb35e6c8ace - Init START
f16n17:1090241:1090580 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
f16n17:1090241:1090580 [4] NCCL INFO Trees [0] -1/-1/-1->8->7 [1] 9/-1/-1->8->7
f16n17:1090241:1090580 [4] NCCL INFO P2P Chunksize set to 131072
f16n17:1090241:1090580 [4] NCCL INFO Channel 00/0 : 8[4] -> 9[2] [send] via NET/IB/1
f16n17:1090241:1090580 [4] NCCL INFO Channel 01/0 : 8[4] -> 9[2] [send] via NET/IB/1
f16n17:1090241:1090580 [4] NCCL INFO Connected all rings
f16n17:1090241:1090580 [4] NCCL INFO Channel 01/0 : 9[2] -> 8[4] [receive] via NET/IB/0
f16n17:1090241:1090580 [4] NCCL INFO Channel 00/0 : 8[4] -> 7[0] via P2P/IPC
f16n17:1090241:1090580 [4] NCCL INFO Channel 01/0 : 8[4] -> 7[0] via P2P/IPC
f16n17:1090241:1090580 [4] NCCL INFO Connected all trees
f16n17:1090241:1090580 [4] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
f16n17:1090241:1090580 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f16n17:1090241:1090580 [4] NCCL INFO comm 0x156117690 rank 8 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x32e49bb35e6c8ace - Init COMPLETE
f17n01:969557:969557 [0] NCCL INFO cudaDriverVersion 12020
f17n01:969557:969557 [0] NCCL INFO Bootstrap : Using ib0:10.41.14.87<0>
f17n01:969557:969557 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
f17n01:969557:969557 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
f17n01:969557:969794 [0] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.14.87<0>
f17n01:969557:969794 [0] NCCL INFO Using network IB
f17n01:969557:969794 [0] NCCL INFO comm 0x141f737c0 rank 42 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x7b12926388d12ce3 - Init START
f17n01:969557:969794 [0] NCCL INFO Setting affinity for GPU 0 to 0f
f17n01:969557:969794 [0] NCCL INFO Trees [0] 43/-1/-1->42->36 [1] 44/-1/-1->42->43 [2] 43/18/-1->42->90 [3] 44/-1/-1->42->43
f17n01:969557:969794 [0] NCCL INFO P2P Chunksize set to 131072
f17n01:969557:969794 [0] NCCL INFO Channel 00/0 : 41[5] -> 42[0] [receive] via NET/IB/0
f17n01:969557:969794 [0] NCCL INFO Channel 02/0 : 41[5] -> 42[0] [receive] via NET/IB/0
f17n01:969557:969794 [0] NCCL INFO Channel 00/0 : 42[0] -> 43[1] via P2P/IPC
f17n01:969557:969794 [0] NCCL INFO Channel 02/0 : 42[0] -> 43[1] via P2P/IPC
f17n01:969557:969794 [0] NCCL INFO Channel 01/0 : 42[0] -> 51[3] [send] via NET/IB/2
f17n01:969557:969794 [0] NCCL INFO Channel 03/0 : 42[0] -> 51[3] [send] via NET/IB/2
f17n01:969557:969794 [0] NCCL INFO Connected all rings
f17n01:969557:969794 [0] NCCL INFO Channel 01/0 : 42[0] -> 43[1] via P2P/IPC
f17n01:969557:969794 [0] NCCL INFO Channel 03/0 : 42[0] -> 43[1] via P2P/IPC
f17n01:969557:969794 [0] NCCL INFO Channel 01/0 : 42[0] -> 44[2] via P2P/IPC
f17n01:969557:969794 [0] NCCL INFO Channel 03/0 : 42[0] -> 44[2] via P2P/IPC
f17n01:969557:969794 [0] NCCL INFO Channel 00/0 : 36[0] -> 42[0] [receive] via NET/IB/0
f17n01:969557:969794 [0] NCCL INFO Channel 02/0 : 18[0] -> 42[0] [receive] via NET/IB/0
f17n01:969557:969794 [0] NCCL INFO Channel 02/0 : 90[0] -> 42[0] [receive] via NET/IB/0
f17n01:969557:969794 [0] NCCL INFO Channel 02/0 : 42[0] -> 90[0] [send] via NET/IB/0
f17n01:969557:969794 [0] NCCL INFO Channel 02/0 : 42[0] -> 18[0] [send] via NET/IB/0
f17n01:969557:969794 [0] NCCL INFO Channel 00/0 : 42[0] -> 36[0] [send] via NET/IB/0
f17n01:969557:969794 [0] NCCL INFO Connected all trees
f17n01:969557:969794 [0] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
f17n01:969557:969794 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
f17n01:969557:969794 [0] NCCL INFO comm 0x141f737c0 rank 42 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x7b12926388d12ce3 - Init COMPLETE
f17n01:969557:969878 [0] NCCL INFO Using network IB
f17n01:969557:969878 [0] NCCL INFO comm 0x144868520 rank 5 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0xe7c8afafd6ec6b2d - Init START
f17n01:969557:969878 [0] NCCL INFO Setting affinity for GPU 0 to 0f
f17n01:969557:969878 [0] NCCL INFO Trees [0] -1/-1/-1->5->6 [1] 6/4/-1->5->7
f17n01:969557:969878 [0] NCCL INFO P2P Chunksize set to 131072
f17n01:969557:969878 [0] NCCL INFO Channel 00/0 : 4[4] -> 5[0] [receive] via NET/IB/0
f17n01:969557:969878 [0] NCCL INFO Channel 01/0 : 4[4] -> 5[0] [receive] via NET/IB/0
f17n01:969557:969878 [0] NCCL INFO Channel 00/0 : 5[0] -> 6[2] [send] via NET/IB/0
f17n01:969557:969878 [0] NCCL INFO Channel 01/0 : 5[0] -> 6[2] [send] via NET/IB/0
f17n01:969557:969878 [0] NCCL INFO Connected all rings
f17n01:969557:969878 [0] NCCL INFO Channel 01/0 : 5[0] -> 7[4] [send] via NET/IB/0
f17n01:969557:969878 [0] NCCL INFO Channel 01/0 : 7[4] -> 5[0] [receive] via NET/IB/0
f17n01:969557:969878 [0] NCCL INFO Channel 00/0 : 6[2] -> 5[0] [receive] via NET/IB/0
f17n01:969557:969878 [0] NCCL INFO Channel 01/0 : 6[2] -> 5[0] [receive] via NET/IB/0
f17n01:969557:969878 [0] NCCL INFO Channel 01/0 : 5[0] -> 4[4] [send] via NET/IB/0
f17n01:969557:969878 [0] NCCL INFO Connected all trees
f17n01:969557:969878 [0] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 f17n01:969564:969564 [4] NCCL INFO cudaDriverVersion 12020
f17n01:969564:969564 [4] NCCL INFO Bootstrap : Using ib0:10.41.14.87<0>
f17n01:969564:969564 [4] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
f17n01:969564:969564 [4] NCCL INFO NET/Plugin : No plugin found, using internal implementation
f17n01:969564:969791 [4] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.14.87<0>
f17n01:969564:969791 [4] NCCL INFO Using network IB
f17n01:969564:969791 [4] NCCL INFO comm 0x146b93ec0 rank 46 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x7b12926388d12ce3 - Init START
f17n01:969564:969791 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
f17n01:969564:969791 [4] NCCL INFO Trees [0] 47/-1/-1->46->45 [1] 45/-1/-1->46->40 [2] 47/-1/-1->46->45 [3] 45/22/-1->46->94
f17n01:969564:969791 [4] NCCL INFO P2P Chunksize set to 131072
f17n01:969564:969791 [4] NCCL INFO Channel 00/0 : 46[4] -> 47[5] via P2P/IPC
f17n01:969564:969791 [4] NCCL INFO Channel 01/0 : 46[4] -> 47[5] via P2P/IPC
f17n01:969564:969791 [4] NCCL INFO Channel 02/0 : 46[4] -> 47[5] via P2P/IPC
f17n01:969564:969791 [4] NCCL INFO Channel 03/0 : 46[4] -> 47[5] via P2P/IPC
f17n01:969564:969791 [4] NCCL INFO Connected all rings
f17n01:969564:969791 [4] NCCL INFO Channel 01/0 : 40[4] -> 46[4] [receive] via NET/IB/3
f17n01:969564:969791 [4] NCCL INFO Channel 03/0 : 22[4] -> 46[4] [receive] via NET/IB/3
f17n01:969564:969791 [4] NCCL INFO Channel 03/0 : 94[4] -> 46[4] [receive] via NET/IB/3
f17n01:969564:969791 [4] NCCL INFO Channel 03/0 : 46[4] -> 94[4] [send] via NET/IB/3
f17n01:969564:969791 [4] NCCL INFO Channel 03/0 : 46[4] -> 22[4] [send] via NET/IB/3
f17n01:969564:969791 [4] NCCL INFO Channel 01/0 : 46[4] -> 40[4] [send] via NET/IB/3
f17n01:969564:969791 [4] NCCL INFO Channel 00/0 : 46[4] -> 45[3] via P2P/IPC
f17n01:969564:969791 [4] NCCL INFO Channel 01/0 : 46[4] -> 45[3] via P2P/IPC
f17n01:969564:969791 [4] NCCL INFO Channel 02/0 : 46[4] -> 45[3] via P2P/IPC
f17n01:969564:969791 [4] NCCL INFO Channel 03/0 : 46[4] -> 45[3] via P2P/IPC
f17n01:969564:969791 [4] NCCL INFO Connected all trees
f17n01:969564:969791 [4] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
f17n01:969564:969791 [4] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
f17n01:969564:969791 [4] NCCL INFO comm 0x146b93ec0 rank 46 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x7b12926388d12ce3 - Init COMPLETE
f17n01:969564:969879 [4] NCCL INFO Using network IB
f17n01:969564:969879 [4] NCCL INFO comm 0x14922b8b0 rank 5 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x414883fe5ee4d1ab - Init START
f17n01:969564:969879 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
f17n01:969564:969879 [4] NCCL INFO Trees [0] -1/-1/-1->5->6 [1] 6/4/-1->5->7
f17n01:969564:969879 [4] NCCL INFO P2P Chunksize set to 131072
f17n01:969564:969879 [4] NCCL INFO Channel 00/0 : 4[2] -> 5[4] [receive] via NET/IB/1
f17n01:969564:969879 [4] NCCL INFO Channel 01/0 : 4[2] -> 5[4] [receive] via NET/IB/1
f17n01:969564:969879 [4] NCCL INFO Channel 00/0 : 5[4] -> 6[0] [send] via NET/IB/1
f17n01:969564:969879 [4] NCCL INFO Channel 01/0 : 5[4] -> 6[0] [send] via NET/IB/1
f17n01:969564:969879 [4] NCCL INFO Connected all rings
f17n01:969564:969879 [4] NCCL INFO Channel 01/0 : 5[4] -> 7[2] [send] via NET/IB/1
f17n01:969564:969879 [4] NCCL INFO Channel 01/0 : 7[2] -> 5[4] [receive] via NET/IB/1
f17n01:969564:969879 [4] NCCL INFO Channel 00/0 : 6[0] -> 5[4] [receive] via NET/IB/1
f17n01:969564:969879 [4] NCCL INFO Channel 01/0 : 6[0] -> 5[4] [receive] via NET/IB/1
f17n01:969564:969879 [4] NCCL INFO Channel 01/0 : 5[4] -> 4[2] [send] via NET/IB/1
f17n01:969564:969879 [4] NCCL INFO Connected all trees
f17n01:969564:969879 [4] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
f17n01:969564:969879 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f17n01:96956| 512
f17n01:969557:969878 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f17n01:969557:969878 [0] NCCL INFO comm 0x144868520 rank 5 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0xe7c8afafd6ec6b2d - Init COMPLETE
f17n01:969557:969898 [0] NCCL INFO Using network IB
f17n01:969557:969898 [0] NCCL INFO comm 0x144c13920 rank 10 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0x32e49bb35e6c8ace - Init START
f17n01:969557:969898 [0] NCCL INFO Setting affinity for GPU 0 to 0f
f17n01:969557:969898 [0] NCCL INFO Trees [0] 11/-1/-1->10->9 [1] 11/4/-1->10->22
f17n01:969557:969898 [0] NCCL INFO P2P Chunksize set to 131072
f17n01:969557:969898 [0] NCCL INFO Channel 00/0 : 9[2] -> 10[0] [receive] via NET/IB/0
f17n01:969557:969898 [0] NCCL INFO Channel 01/0 : 9[2] -> 10[0] [receive] via NET/IB/0
f17n01:969557:969898 [0] NCCL INFO Channel 00/0 : 10[0] -> 11[4] via P2P/IPC
f17n01:969557:969898 [0] NCCL INFO Channel 01/0 : 10[0] -> 11[4] via P2P/IPC
f17n01:969557:969898 [0] NCCL INFO Connected all rings
f17n01:969557:969898 [0] NCCL INFO Channel 01/0 : 4[0] -> 10[0] [receive] via NET/IB/0
f17n01:969557:969898 [0] NCCL INFO Channel 01/0 : 22[0] -> 10[0] [receive] via NET/IB/0
f17n01:969557:969898 [0] NCCL INFO Channel 01/0 : 10[0] -> 22[0] [send] via NET/IB/0
f17n01:969557:969898 [0] NCCL INFO Channel 01/0 : 10[0] -> 4[0] [send] via NET/IB/0
f17n01:969557:969898 [0] NCCL INFO Channel 00/0 : 10[0] -> 9[2] [send] via NET/IB/0
f17n01:969557:969898 [0] NCCL INFO Connected all trees
f17n01:969557:969898 [0] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
f17n01:969557:969898 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f17n01:969557:969898 [0] NCCL INFO comm 0x144c13920 rank 10 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0x32e49bb35e6c8ace - Init COMPLETE
4:969879 [4] NCCL INFO comm 0x14922b8b0 rank 5 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x414883fe5ee4d1ab - Init COMPLETE
f17n01:969564:969896 [4] NCCL INFO Using network IB
f17n01:969564:969896 [4] NCCL INFO comm 0x1494a1360 rank 11 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x32e49bb35e6c8ace - Init START
f17n01:969564:969896 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
f17n01:969564:969896 [4] NCCL INFO Trees [0] -1/-1/-1->11->10 [1] 16/-1/-1->11->10
f17n01:969564:969896 [4] NCCL INFO P2P Chunksize set to 131072
f17n01:969564:969896 [4] NCCL INFO Channel 00/0 : 11[4] -> 12[2] [send] via NET/IB/1
f17n01:969564:969896 [4] NCCL INFO Channel 01/0 : 11[4] -> 12[2] [send] via NET/IB/1
f17n01:969564:969896 [4] NCCL INFO Connected all rings
f17n01:969564:969896 [4] NCCL INFO Channel 01/0 : 11[4] -> 16[0] [send] via NET/IB/0
f17n01:969564:969896 [4] NCCL INFO Channel 01/0 : 16[0] -> 11[4] [receive] via NET/IB/0
f17n01:969564:969896 [4] NCCL INFO Channel 00/0 : 11[4] -> 10[0] via P2P/IPC
f17n01:969564:969896 [4] NCCL INFO Channel 01/0 : 11[4] -> 10[0] via P2P/IPC
f17n01:969564:969896 [4] NCCL INFO Connected all trees
f17n01:969564:969896 [4] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
f17n01:969564:969896 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f17n01:969564:969896 [4] NCCL INFO comm 0x1494a1360 rank 11 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x32e49bb35e6c8ace - Init COMPLETE
f18n05:1007497:1007497 [0] NCCL INFO cudaDriverVersion 12020
f18n05:1007497:1007497 [0] NCCL INFO Bootstrap : Using ib0:10.41.14.109<0>
f18n05:1007497:1007497 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
f18n05:1007497:1007497 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
f18n05:1007497:1007736 [0] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.14.109<0>
f18n05:1007497:1007736 [0] NCCL INFO Using network IB
f18n05:1007497:1007736 [0] NCCL INFO comm 0x14c5b36c0 rank 54 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x7b12926388d12ce3 - Init START
f18n05:1007497:1007736 [0] NCCL INFO Setting affinity for GPU 0 to 0f
f18n05:1007497:1007736 [0] NCCL INFO Trees [0] 55/-1/-1->54->61 [1] 56/-1/-1->54->55 [2] 55/48/-1->54->66 [3] 56/-1/-1->54->55
f18n05:1007497:1007736 [0] NCCL INFO P2P Chunksize set to 131072
f18n05:1007497:1007736 [0] NCCL INFO Channel 00/0 : 53[5] -> 54[0] [receive] via NET/IB/0
f18n05:1007497:1007736 [0] NCCL INFO Channel 02/0 : 53[5] -> 54[0] [receive] via NET/IB/0
f18n05:1007497:1007736 [0] NCCL INFO Channel 00/0 : 54[0] -> 55[1] via P2P/IPC
f18n05:1007497:1007736 [0] NCCL INFO Channel 02/0 : 54[0] -> 55[1] via P2P/IPC
f18n05:1007497:1007736 [0] NCCL INFO Channel 01/0 : 54[0] -> 63[3] [send] via NET/IB/2
f18n05:1007497:1007736 [0] NCCL INFO Channel 03/0 : 54[0] -> 63[3] [send] via NET/IB/2
f18n05:1007497:1007736 [0] NCCL INFO Connected all rings
f18n05:1007497:1007736 [0] NCCL INFO Channel 01/0 : 54[0] -> 55[1] via P2P/IPC
f18n05:1007497:1007736 [0] NCCL INFO Channel 03/0 : 54[0] -> 55[1] via P2P/IPC
f18n05:1007497:1007736 [0] NCCL INFO Channel 01/0 : 54[0] -> 56[2] via P2P/IPC
f18n05:1007497:1007736 [0] NCCL INFO Channel 03/0 : 54[0] -> 56[2] via P2P/IPC
f18n05:1007497:1007736 [0] NCCL INFO Channel 02/0 : 48[0] -> 54[0] [receive] via NET/IB/0
f18n05:1007497:1007736 [0] NCCL INFO Channel 00/0 : 54[0] -> 61[1] [send] via NET/IB/0
f18n05:1007497:1007736 [0] NCCL INFO Channel 02/0 : 54[0] -> 66[0] [send] via NET/IB/0
f18n05:1007497:1007736 [0] NCCL INFO Channel 02/0 : 66[0] -> 54[0] [receive] via NET/IB/0
f18n05:1007497:1007736 [0] NCCL INFO Channel 00/0 : 61[1] -> 54[0] [receive] via NET/IB/0
f18n05:1007497:1007736 [0] NCCL INFO Channel 02/0 : 54[0] -> 48[0] [send] via NET/IB/0
f18n05:1007497:1007736 [0] NCCL INFO Connected all trees
f18n05:1007497:1007736 [0] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
f18n05:1007497:1007736 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
f18n05:1007497:1007736 [0] NCCL INFO comm 0x14c5b36c0 rank 54 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x7b12926388d12ce3 - Init COMPLETE
f18n05:1007497:1007816 [0] NCCL INFO Using network IB
f18n05:1007497:1007816 [0] NCCL INFO comm 0x14fa003b0 rank 6 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x414883fe5ee4d1ab - Init START
f18n05:1007497:1007816 [0] NCCL INFO Setting affinity for GPU 0 to 0f
f18n05:1007497:1007816 [0] NCCL INFO Trees [0] 5/7/-1->6->4 [1] -1/-1/-1->6->5
f18n05:1007497:1007816 [0] NCCL INFO P2P Chunksize set to 131072
f18n05:1007497:1007816 [0] NCCL INFO Channel 00/0 : 5[4] -> 6[0] [receive] via NET/IB/0
f18n05:1007497:1007816 [0] NCCL INFO Channel 01/0 : 5[4] -> 6[0] [receive] via NET/IB/0
f18n05:1007497:1007816 [0] NCCL INFO Channel 00/0 : 6[0] -> 7[2] [send] via NET/IB/0
f18n05:1007497:1007816 [0] NCCL INFO Channel 01/0 : 6[0] -> 7[2] [send] via NET/IB/0
f18n05:1007497:1007816 [0] NCCL INFO Connected all rings
f18n05:1007497:1007816 [0] NCCL INFO Channel 00/0 : 4[2] -> 6[0] [receive] via NET/IB/0
f18n05:1007497:1007816 [0] NCCL INFO Channel 00/0 : 6[0] -> 4[2] [send] via NET/IB/0
f18n05:1007497:1007816 [0] NCCL INFO Channel 00/0 : 7[2] -> 6[0] [receive] via NET/IB/0
f18n05:1007497:1007816 [0] NCCL INFO Channel 00/0 : 6[0] -> 5[4] [send] via NET/IB/0
f18n05:1007497:1007816 [0] NCCL INFO Channel 01/0 : 6[0] -> 5[4] [send] via NET/IB/0
f18n05:1007497:1007816 [0] NCCL INFO Cof18n05:1007504:1007504 [4] NCCL INFO cudaDriverVersion 12020
f18n05:1007504:1007504 [4] NCCL INFO Bootstrap : Using ib0:10.41.14.109<0>
f18n05:1007504:1007504 [4] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
f18n05:1007504:1007504 [4] NCCL INFO NET/Plugin : No plugin found, using internal implementation
f18n05:1007504:1007733 [4] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.14.109<0>
f18n05:1007504:1007733 [4] NCCL INFO Using network IB
f18n05:1007504:1007733 [4] NCCL INFO comm 0x131933830 rank 58 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x7b12926388d12ce3 - Init START
f18n05:1007504:1007733 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
f18n05:1007504:1007733 [4] NCCL INFO Trees [0] 59/-1/-1->58->57 [1] 57/-1/-1->58->63 [2] 59/-1/-1->58->57 [3] 57/52/-1->58->70
f18n05:1007504:1007733 [4] NCCL INFO P2P Chunksize set to 131072
f18n05:1007504:1007733 [4] NCCL INFO Channel 00/0 : 58[4] -> 59[5] via P2P/IPC
f18n05:1007504:1007733 [4] NCCL INFO Channel 01/0 : 58[4] -> 59[5] via P2P/IPC
f18n05:1007504:1007733 [4] NCCL INFO Channel 02/0 : 58[4] -> 59[5] via P2P/IPC
f18n05:1007504:1007733 [4] NCCL INFO Channel 03/0 : 58[4] -> 59[5] via P2P/IPC
f18n05:1007504:1007733 [4] NCCL INFO Connected all rings
f18n05:1007504:1007733 [4] NCCL INFO Channel 01/0 : 58[4] -> 63[3] [send] via NET/IB/3
f18n05:1007504:1007733 [4] NCCL INFO Channel 03/0 : 52[4] -> 58[4] [receive] via NET/IB/3
f18n05:1007504:1007733 [4] NCCL INFO Channel 03/0 : 58[4] -> 70[4] [send] via NET/IB/3
f18n05:1007504:1007733 [4] NCCL INFO Channel 03/0 : 70[4] -> 58[4] [receive] via NET/IB/3
f18n05:1007504:1007733 [4] NCCL INFO Channel 03/0 : 58[4] -> 52[4] [send] via NET/IB/3
f18n05:1007504:1007733 [4] NCCL INFO Channel 01/0 : 63[3] -> 58[4] [receive] via NET/IB/3
f18n05:1007504:1007733 [4] NCCL INFO Channel 00/0 : 58[4] -> 57[3] via P2P/IPC
f18n05:1007504:1007733 [4] NCCL INFO Channel 01/0 : 58[4] -> 57[3] via P2P/IPC
f18n05:1007504:1007733 [4] NCCL INFO Channel 02/0 : 58[4] -> 57[3] via P2P/IPC
f18n05:1007504:1007733 [4] NCCL INFO Channel 03/0 : 58[4] -> 57[3] via P2P/IPC
f18n05:1007504:1007733 [4] NCCL INFO Connected all trees
f18n05:1007504:1007733 [4] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
f18n05:1007504:1007733 [4] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
f18n05:1007504:1007733 [4] NCCL INFO comm 0x131933830 rank 58 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x7b12926388d12ce3 - Init COMPLETE
f18n05:1007504:1007815 [4] NCCL INFO Using network IB
f18n05:1007504:1007815 [4] NCCL INFO comm 0x13476afb0 rank 7 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xe7c8afafd6ec6b2d - Init START
f18n05:1007504:1007815 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
f18n05:1007504:1007815 [4] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] 9/5/-1->7->3
f18n05:1007504:1007815 [4] NCCL INFO P2P Chunksize set to 131072
f18n05:1007504:1007815 [4] NCCL INFO Channel 00/0 : 6[2] -> 7[4] [receive] via NET/IB/1
f18n05:1007504:1007815 [4] NCCL INFO Channel 01/0 : 6[2] -> 7[4] [receive] via NET/IB/1
f18n05:1007504:1007815 [4] NCCL INFO Channel 00/0 : 7[4] -> 8[0] [send] via NET/IB/1
f18n05:1007504:1007815 [4] NCCL INFO Channel 01/0 : 7[4] -> 8[0] [send] via NET/IB/1
f18n05:1007504:1007815 [4] NCCL INFO Connected all rings
f18n05:1007504:1007815 [4] NCCL INFO Channel 01/0 : 5[0] -> 7[4] [receive] via NET/IB/1
f18n05:1007504:1007815 [4] NCCL INFO Channel 01/0 : 7[4] -> 9[2] [send] via NET/IB/1
f18n05:1007504:1007815 [4] NCCL INFO Channel 01/0 : 3[2] -> 7[4] [receive] via NET/IB/1
f18n05:1007504:1007815 [4] NCCL INFO Channel 01/0 : 7[4] -> 3[2] [send] via NET/IB/1
f18n05:1007504:1007815 [4] NCCL INFO Channel 01/0 : 9[2] -> 7[4] [receive] via NET/IB/1
f18n05:1007504:1007815 [4] NCCL INFO Channel 01/0 : 7[4] -> 5[0] [send] via NET/IB/1
f18n05:1007504:1007815 [4] NCCL INFO Channel 00/0 : 7[4] -> 6[2] [send] via NET/IBnnected all trees
f18n05:1007497:1007816 [0] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
f18n05:1007497:1007816 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f18n05:1007497:1007816 [0] NCCL INFO comm 0x14fa003b0 rank 6 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x414883fe5ee4d1ab - Init COMPLETE
f18n05:1007497:1007834 [0] NCCL INFO Using network IB
f18n05:1007497:1007834 [0] NCCL INFO comm 0x14f29bd60 rank 13 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0x32e49bb35e6c8ace - Init START
f18n05:1007497:1007834 [0] NCCL INFO Setting affinity for GPU 0 to 0f
f18n05:1007497:1007834 [0] NCCL INFO Trees [0] 14/-1/-1->13->15 [1] 14/12/-1->13->16
f18n05:1007497:1007834 [0] NCCL INFO P2P Chunksize set to 131072
f18n05:1007497:1007834 [0] NCCL INFO Channel 00/0 : 12[2] -> 13[0] [receive] via NET/IB/0
f18n05:1007497:1007834 [0] NCCL INFO Channel 01/0 : 12[2] -> 13[0] [receive] via NET/IB/0
f18n05:1007497:1007834 [0] NCCL INFO Channel 00/0 : 13[0] -> 14[4] via P2P/IPC
f18n05:1007497:1007834 [0] NCCL INFO Channel 01/0 : 13[0] -> 14[4] via P2P/IPC
f18n05:1007497:1007834 [0] NCCL INFO Connected all rings
f18n05:1007497:1007834 [0] NCCL INFO Channel 00/0 : 13[0] -> 15[2] [send] via NET/IB/0
f18n05:1007497:1007834 [0] NCCL INFO Channel 01/0 : 13[0] -> 16[0] [send] via NET/IB/0
f18n05:1007497:1007834 [0] NCCL INFO Channel 01/0 : 16[0] -> 13[0] [receive] via NET/IB/0
f18n05:1007497:1007834 [0] NCCL INFO Channel 00/0 : 15[2] -> 13[0] [receive] via NET/IB/0
f18n05:1007497:1007834 [0] NCCL INFO Channel 01/0 : 13[0] -> 12[2] [send] via NET/IB/0
f18n05:1007497:1007834 [0] NCCL INFO Connected all trees
f18n05:1007497:1007834 [0] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
f18n05:1007497:1007834 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f18n05:1007497:1007834 [0] NCCL INFO comm 0x14f29bd60 rank 13 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0x32e49bb35e6c8ace - Init COMPLETE
/1
f18n05:1007504:1007815 [4] NCCL INFO Connected all trees
f18n05:1007504:1007815 [4] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
f18n05:1007504:1007815 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f18n05:1007504:1007815 [4] NCCL INFO comm 0x13476afb0 rank 7 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xe7c8afafd6ec6b2d - Init COMPLETE
f18n05:1007504:1007837 [4] NCCL INFO Using network IB
f18n05:1007504:1007837 [4] NCCL INFO comm 0x1345cb160 rank 14 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x32e49bb35e6c8ace - Init START
f18n05:1007504:1007837 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
f18n05:1007504:1007837 [4] NCCL INFO Trees [0] -1/-1/-1->14->13 [1] 15/-1/-1->14->13
f18n05:1007504:1007837 [4] NCCL INFO P2P Chunksize set to 131072
f18n05:1007504:1007837 [4] NCCL INFO Channel 00/0 : 14[4] -> 15[2] [send] via NET/IB/1
f18n05:1007504:1007837 [4] NCCL INFO Channel 01/0 : 14[4] -> 15[2] [send] via NET/IB/1
f18n05:1007504:1007837 [4] NCCL INFO Connected all rings
f18n05:1007504:1007837 [4] NCCL INFO Channel 01/0 : 15[2] -> 14[4] [receive] via NET/IB/0
f18n05:1007504:1007837 [4] NCCL INFO Channel 00/0 : 14[4] -> 13[0] via P2P/IPC
f18n05:1007504:1007837 [4] NCCL INFO Channel 01/0 : 14[4] -> 13[0] via P2P/IPC
f18n05:1007504:1007837 [4] NCCL INFO Connected all trees
f18n05:1007504:1007837 [4] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
f18n05:1007504:1007837 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f18n05:1007504:1007837 [4] NCCL INFO comm 0x1345cb160 rank 14 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x32e49bb35e6c8ace - Init COMPLETE
f16n18:1091131:1091131 [1] NCCL INFO cudaDriverVersion 12020
f16n18:1091131:1091131 [1] NCCL INFO Bootstrap : Using ib0:10.41.14.86<0>
f16n18:1091131:1091131 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
f16n18:1091131:1091131 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
f16n18:1091131:1091365 [1] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.14.86<0>
f16n18:1091131:1091365 [1] NCCL INFO Using network IB
f16n18:1091131:1091365 [1] NCCL INFO comm 0x1440c37e0 rank 37 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x7b12926388d12ce3 - Init START
f16n18:1091131:1091365 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
f16n18:1091131:1091365 [1] NCCL INFO Trees [0] 38/30/-1->37->36 [1] 36/-1/-1->37->41 [2] 38/-1/-1->37->36 [3] 36/-1/-1->37->41
f16n18:1091131:1091365 [1] NCCL INFO P2P Chunksize set to 131072
f16n18:1091131:1091365 [1] NCCL INFO Channel 00/0 : 37[1] -> 38[2] via P2P/IPC
f16n18:1091131:1091365 [1] NCCL INFO Channel 02/0 : 37[1] -> 38[2] via P2P/IPC
f16n18:1091131:1091365 [1] NCCL INFO Channel 01/0 : 37[1] -> 36[0] via P2P/IPC
f16n18:1091131:1091365 [1] NCCL INFO Channel 03/0 : 37[1] -> 36[0] via P2P/IPC
f16n18:1091131:1091365 [1] NCCL INFO Connected all rings
f16n18:1091131:1091365 [1] NCCL INFO Channel 01/0 : 37[1] -> 41[5] via P2P/IPC
f16n18:1091131:1091365 [1] NCCL INFO Channel 03/0 : 37[1] -> 41[5] via P2P/IPC
f16n18:1091131:1091365 [1] NCCL INFO Channel 00/0 : 30[0] -> 37[1] [receive] via NET/IB/0
f16n18:1091131:1091365 [1] NCCL INFO Channel 00/0 : 37[1] -> 30[0] [send] via NET/IB/0
f16n18:1091131:1091365 [1] NCCL INFO Channel 00/0 : 37[1] -> 36[0] via P2P/IPC
f16n18:1091131:1091365 [1] NCCL INFO Channel 02/0 : 37[1] -> 36[0] via P2P/IPC
f16n18:1091131:1091365 [1] NCCL INFO Connected all trees
f16n18:1091131:1091365 [1] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
f16n18:1091131:1091365 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
f16n18:1091131:1091365 [1] NCCL INFO comm 0x1440c37e0 rank 37 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x7b12926388d12ce3 - Init COMPLETE
f16n18:1091131:1091463 [1] NCCL INFO Using network IB
f16n18:1091131:1091463 [1] NCCL INFO comm 0x146ec7ac0 rank 4 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0xa77518dab25ce26 - Init START
f16n18:1091131:1091463 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
f16n18:1091131:1091463 [1] NCCL INFO Trees [0] 2/6/-1->4->8 [1] -1/-1/-1->4->5
f16n18:1091131:1091463 [1] NCCL INFO P2P Chunksize set to 131072
f16n18:1091131:1091463 [1] NCCL INFO Channel 00/0 : 3[5] -> 4[1] [receive] via NET/IB/2
f16n18:1091131:1091463 [1] NCCL INFO Channel 01/0 : 3[5] -> 4[1] [receive] via NET/IB/2
f16n18:1091131:1091463 [1] NCCL INFO Channel 00/0 : 4[1] -> 5[3] [send] via NET/IB/2
f16n18:1091131:1091463 [1] NCCL INFO Channel 01/0 : 4[1] -> 5[3] [send] via NET/IB/2
f16n18:1091131:1091463 [1] NCCL INFO Connected all rings
f16n18:1091131:1091463 [1] NCCL INFO Channel 00/0 : 2[3] -> 4[1] [receive] via NET/IB/2
f16n18:1091131:1091463 [1] NCCL INFO Channel 00/0 : 4[1] -> 6[5] [send] via NET/IB/2
f16n18:1091131:1091463 [1] NCCL INFO Channel 00/0 : 4[1] -> 8[3] [send] via NET/IB/2
f16n18:1091131:1091463 [1] NCCL INFO Channel 00/0 : 8[3] -> 4[1] [receive] via NET/IB/2
f16n18:1091131:1091463 [1] NCCL INFO Channel 00/0 : 6[5] -> 4[1] [receive] via NET/IB/2
f16n18:1091131:1091463 [1] NCCL INFO Channel 00/0 : 4[1] -> 2[3] [send] via NET/IB/2
f16n18:1091131:1091463 [1] NCCL INFO Channel 01/0 : 5[3] -> 4[1] [receive] via NET/IB/2
f16n18:1091131:1091463 [1] NCCL INFO Connected all trees
f16n18:1091131:1091463 [1] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
f16n18:1091131:1091463 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f16n18:1091131:1091463 [1] NCCL INFO comm 0x146ec7ac0 rank 4 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0xa77518dab25ce26 - Init COMPLETE
f16n18:1091131:1091480 [1] NCCL INFO Using network IB
f16n18:1091131:1091480 [1] NCCL INFO comm 0x146d3a8c0 rank 9 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0xa49a27aa8b15034e - Init START
f16n18:1091131:1091480 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
f16n18:1091131:1091480 [1] NCCL INFO Trees [0] 10/11/-1->9->6 [1] 10/-1/-1->9->8
f16n18:1091131:1091480 [1] NCCL INFO P2P Chunksize set to 131072
f16n18:1091131:1091480 [1] NCCL INFO Channel 00/0 : 8[3] -> 9[1] [receive] via NET/IB/2
f16n18:1091131:1091480 [1] NCCL INFO Channel 01/0 : 8[3] -> 9[1] [receive] via NET/IB/2
f16n18:1091131:1091480 [1] NCCL INFO Channel 00/0 : 9[1] -> 10[5] via P2P/IPC
f16n18:1091131:1091480 [1] NCCL INFO Channel 01/0 : 9[1] -> 10[5] via P2P/IPC
f16n18:1091131:1091480 [1] NCCL INFO Connected all rings
f16n18:1091131:1091480 [1] NCCL INFO Channel 00/0 : 9[1] -> 11[3] [send] via NET/IB/2
f16n18:1091131:1091480 [1] NCCL INFO Channel 00/0 : 6[1] -> 9[1] [receive] via NET/IB/2
f16n18:1091131:1091480 [1] NCCL INFO Channel 00/0 : 9[1] -> 6[1] [send] via NET/IB/2
f16n18:1091131:1091480 [1] NCCL INFO Channel 00/0 : 11[3] -> 9[1] [receive] via NET/IB/2
f16n18:1091131:1091480 [1] NCCL INFO Channel 01/0 : 9[1] -> 8[3] [send] via NET/IB/2
f16n18:1091131:1091480 [1] NCCL INFO Connected all trees
f16n18:1091131:1091480 [1] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
f16n18:1091131:1091480 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f16n18:1091131:1091480 [1] NCCL INFO comm 0x146d3a8c0 rank 9 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0xa49a27aa8b15034e - Init COMPLETE
f16n18:1091138:1091138 [5] NCCL INFO cudaDriverVersion 12020
f16n18:1091138:1091138 [5] NCCL INFO Bootstrap : Using ib0:10.41.14.86<0>
f16n18:1091138:1091138 [5] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
f16n18:1091138:1091138 [5] NCCL INFO NET/Plugin : No plugin found, using internal implementation
f16n18:1091138:1091367 [5] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.14.86<0>
f16n18:1091138:1091367 [5] NCCL INFO Using network IB
f16n18:1091138:1091367 [5] NCCL INFO comm 0x1476640b0 rank 41 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x7b12926388d12ce3 - Init START
f16n18:1091138:1091367 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
f16n18:1091138:1091367 [5] NCCL INFO Trees [0] -1/-1/-1->41->40 [1] 37/-1/-1->41->39 [2] -1/-1/-1->41->40 [3] 37/-1/-1->41->39
f16n18:1091138:1091367 [5] NCCL INFO P2P Chunksize set to 131072
f16n18:1091138:1091367 [5] NCCL INFO Channel 00/0 : 41[5] -> 42[0] [send] via NET/IB/1
f16n18:1091138:1091367 [5] NCCL INFO Channel 02/0 : 41[5] -> 42[0] [send] via NET/IB/1
f16n18:1091138:1091367 [5] NCCL INFO Channel 01/0 : 41[5] -> 38[2] via P2P/IPC
f16n18:1091138:1091367 [5] NCCL INFO Channel 03/0 : 41[5] -> 38[2] via P2P/IPC
f16n18:1091138:1091367 [5] NCCL INFO Connected all rings
f16n18:1091138:1091367 [5] NCCL INFO Channel 01/0 : 41[5] -> 37[1] via P2P/IPC
f16n18:1091138:1091367 [5] NCCL INFO Channel 03/0 : 41[5] -> 37[1] via P2P/IPC
f16n18:1091138:1091367 [5] NCCL INFO Channel 01/0 : 41[5] -> 39[3] via P2P/IPC
f16n18:1091138:1091367 [5] NCCL INFO Channel 03/0 : 41[5] -> 39[3] via P2P/IPC
f16n18:1091138:1091367 [5] NCCL INFO Channel 00/0 : 41[5] -> 40[4] via P2P/IPC
f16n18:1091138:1091367 [5] NCCL INFO Channel 02/0 : 41[5] -> 40[4] via P2P/IPC
f16n18:1091138:1091367 [5] NCCL INFO Connected all trees
f16n18:1091138:1091367 [5] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
f16n18:1091138:1091367 [5] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
f16n18:1091138:1091367 [5] NCCL INFO comm 0x1476640b0 rank 41 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x7b12926388d12ce3 - Init COMPLETE
f16n18:1091138:1091466 [5] NCCL INFO Using network IB
f16n18:1091138:1091466 [5] NCCL INFO comm 0x149f40fb0 rank 5 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x86d434c9d289d59a - Init START
f16n18:1091138:1091466 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
f16n18:1091138:1091466 [5] NCCL INFO Trees [0] -1/-1/-1->5->6 [1] 6/4/-1->5->7
f16n18:1091138:1091466 [5] NCCL INFO P2P Chunksize set to 131072
f16n18:1091138:1091466 [5] NCCL INFO Channel 00/0 : 4[3] -> 5[5] [receive] via NET/IB/3
f16n18:1091138:1091466 [5] NCCL INFO Channel 01/0 : 4[3] -> 5[5] [receive] via NET/IB/3
f16n18:1091138:1091466 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[1] [send] via NET/IB/3
f16n18:1091138:1091466 [5] NCCL INFO Channel 01/0 : 5[5] -> 6[1] [send] via NET/IB/3
f16n18:1091138:1091466 [5] NCCL INFO Connected all rings
f16n18:1091138:1091466 [5] NCCL INFO Channel 01/0 : 5[5] -> 7[3] [send] via NET/IB/3
f16n18:1091138:1091466 [5] NCCL INFO Channel 01/0 : 7[3] -> 5[5] [receive] via NET/IB/3
f16n18:1091138:1091466 [5] NCCL INFO Channel 00/0 : 6[1] -> 5[5] [receive] via NET/IB/3
f16n18:1091138:1091466 [5] NCCL INFO Channel 01/0 : 6[1] -> 5[5] [receive] via NET/IB/3
f16n18:1091138:1091466 [5] NCCL INFO Channel 01/0 : 5[5] -> 4[3] [send] via NET/IB/3
f16n18:1091138:1091466 [5] NCCL INFO Connected all trees
f16n18:1091138:1091466 [5] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
f16n18:1091138:1091466 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f16n18:1091138:1091466 [5] NCCL INFO comm 0x149f40fb0 rank 5 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x86d434c9d289d59a - Init COMPLETE
f16n18:1091138:1091485 [5] NCCL INFO Using network IB
f16n18:1091138:1091485 [5] NCCL INFO comm 0x14b389c00 rank 10 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xa49a27aa8b15034e - Init START
f16n18:1091138:1091485 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
f16n18:1091138:1091485 [5] NCCL INFO Trees [0] 8/-1/-1->10->9 [1] -1/-1/-1->10->9
f16n18:1091138:1091485 [5] NCCL INFO P2P Chunksize set to 131072
f16n18:1091138:1091485 [5] NCCL INFO Channel 00/0 : 10[5] -> 11[3] [send] via NET/IB/3
f16n18:1091138:1091485 [5] NCCL INFO Channel 01/0 : 10[5] -> 11[3] [send] via NET/IB/3
f16n18:1091138:1091485 [5] NCCL INFO Connected all rings
f16n18:1091138:1091485 [5] NCCL INFO Channel 00/0 : 8[3] -> 10[5] [receive] via NET/IB/2
f16n18:1091138:1091485 [5] NCCL INFO Channel 00/0 : 10[5] -> 8[3] [send] via NET/IB/2
f16n18:1091138:1091485 [5] NCCL INFO Channel 00/0 : 10[5] -> 9[1] via P2P/IPC
f16n18:1091138:1091485 [5] NCCL INFO Channel 01/0 : 10[5] -> 9[1] via P2P/IPC
f16n18:1091138:1091485 [5] NCCL INFO Connected all trees
f16n18:1091138:1091485 [5] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
f16n18:1091138:1091485 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f16n18:1091138:1091485 [5] NCCL INFO comm 0x14b389c00 rank 10 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xa49a27aa8b15034e - Init COMPLETE
g05n08:178243:178243 [4] NCCL INFO cudaDriverVersion 12020
g05n08:178243:178243 [4] NCCL INFO Bootstrap : Using ib0:10.41.16.20<0>
g05n08:178243:178243 [4] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g05n08:178243:178243 [4] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g05n08:178243:178475 [4] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.16.20<0>
g05n08:178243:178475 [4] NCCL INFO Using network IB
g05n08:178243:178475 [4] NCCL INFO comm 0x143c83cb0 rank 82 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x7b12926388d12ce3 - Init START
g05n08:178243:178475 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g05n08:178243:178475 [4] NCCL INFO Trees [0] 83/-1/-1->82->81 [1] 81/-1/-1->82->87 [2] 83/-1/-1->82->81 [3] 81/76/-1->82->69
g05n08:178243:178475 [4] NCCL INFO P2P Chunksize set to 131072
g05n08:178243:178475 [4] NCCL INFO Channel 00/0 : 82[4] -> 83[5] via P2P/IPC
g05n08:178243:178475 [4] NCCL INFO Channel 01/0 : 82[4] -> 83[5] via P2P/IPC
g05n08:178243:178475 [4] NCCL INFO Channel 02/0 : 82[4] -> 83[5] via P2P/IPC
g05n08:178243:178475 [4] NCCL INFO Channel 03/0 : 82[4] -> 83[5] via P2P/IPC
g05n08:178243:178475 [4] NCCL INFO Connected all rings
g05n08:178243:178475 [4] NCCL INFO Channel 01/0 : 82[4] -> 87[3] [send] via NET/IB/3
g05n08:178243:178475 [4] NCCL INFO Channel 03/0 : 76[4] -> 82[4] [receive] via NET/IB/3
g05n08:178243:178475 [4] NCCL INFO Channel 03/0 : 69[3] -> 82[4] [receive] via NET/IB/3
g05n08:178243:178475 [4] NCCL INFO Channel 03/0 : 82[4] -> 69[3] [send] via NET/IB/3
g05n08:178243:178475 [4] NCCL INFO Channel 03/0 : 82[4] -> 76[4] [send] via NET/IB/3
g05n08:178243:178475 [4] NCCL INFO Channel 01/0 : 87[3] -> 82[4] [receive] via NET/IB/3
g05n08:178243:178475 [4] NCCL INFO Channel 00/0 : 82[4] -> 81[3] via P2P/IPC
g05n08:178243:178475 [4] NCCL INFO Channel 01/0 : 82[4] -> 81[3] via P2P/IPC
g05n08:178243:178475 [4] NCCL INFO Channel 02/0 : 82[4] -> 81[3] via P2P/IPC
g05n08:178243:178475 [4] NCCL INFO Channel 03/0 : 82[4] -> 81[3] via P2P/IPC
g05n08:178243:178475 [4] NCCL INFO Connected all trees
g05n08:178243:178475 [4] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g05n08:178243:178475 [4] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g05n08:178243:178475 [4] NCCL INFO comm 0x143c83cb0 rank 82 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x7b12926388d12ce3 - Init COMPLETE
g05n08:178243:178560 [4] NCCL INFO Using network IB
g05n08:178243:178560 [4] NCCL INFO comm 0x147a1e7a0 rank 10 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xe7c8afafd6ec6b2d - Init START
g05n08:178243:178560 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g05n08:178243:178560 [4] NCCL INFO Trees [0] 9/11/-1->10->8 [1] -1/-1/-1->10->9
g05n08:178243:178560 [4] NCCL INFO P2P Chunksize set to 131072
g05n08:178243:178560 [4] NCCL INFO Channel 00/0 : 9[2] -> 10[4] [receive] via NET/IB/1
g05n08:178243:178560 [4] NCCL INFO Channel 01/0 : 9[2] -> 10[4] [receive] via NET/IB/1
g05n08:178243:178560 [4] NCCL INFO Channel 00/0 : 10[4] -> 11[0] [send] via NET/IB/1
g05n08:178243:178560 [4] NCCL INFO Channel 01/0 : 10[4] -> 11[0] [send] via NET/IB/1
g05n08:178243:178560 [4] NCCL INFO Connected all rings
g05n08:178243:178560 [4] NCCL INFO Channel 00/0 : 8[0] -> 10[4] [receive] via NET/IB/1
g05n08:178243:178560 [4] NCCL INFO Channel 00/0 : 10[4] -> 8[0] [send] via NET/IB/1
g05n08:178243:178560 [4] NCCL INFO Channel 00/0 : 11[0] -> 10[4] [receive] via NET/IB/1
g05n08:178243:178560 [4] NCCL INFO Channel 00/0 : 10[4] -> 9[2] [send] via NET/IB/1
g05n08:178243:178560 [4] NCCL INFO Channel 01/0 : 10[4] -> 9[2] [send] via NET/IB/1
g05n08:178243:178560 [4] NCCL INFO Connected all trees
g05n08:178243:178560 [4] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g05n08:178243:178560 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peerd09n08:1072936:1072936 [4] NCCL INFO cudaDriverVersion 12020
d09n08:1072936:1072936 [4] NCCL INFO Bootstrap : Using ib0:10.41.8.172<0>
d09n08:1072936:1072936 [4] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
d09n08:1072936:1072936 [4] NCCL INFO NET/Plugin : No plugin found, using internal implementation
d09n08:1072936:1073165 [4] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.8.172<0>
d09n08:1072936:1073165 [4] NCCL INFO Using network IB
d09n08:1072936:1073165 [4] NCCL INFO comm 0x142443b80 rank 10 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x7b12926388d12ce3 - Init START
d09n08:1072936:1073165 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
d09n08:1072936:1073165 [4] NCCL INFO Trees [0] 11/-1/-1->10->9 [1] 9/-1/-1->10->15 [2] 11/-1/-1->10->9 [3] 9/4/-1->10->22
d09n08:1072936:1073165 [4] NCCL INFO P2P Chunksize set to 131072
d09n08:1072936:1073165 [4] NCCL INFO Channel 00/0 : 10[4] -> 11[5] via P2P/IPC
d09n08:1072936:1073165 [4] NCCL INFO Channel 01/0 : 10[4] -> 11[5] via P2P/IPC
d09n08:1072936:1073165 [4] NCCL INFO Channel 02/0 : 10[4] -> 11[5] via P2P/IPC
d09n08:1072936:1073165 [4] NCCL INFO Channel 03/0 : 10[4] -> 11[5] via P2P/IPC
d09n08:1072936:1073165 [4] NCCL INFO Connected all rings
d09n08:1072936:1073165 [4] NCCL INFO Channel 01/0 : 10[4] -> 15[3] [send] via NET/IB/3
d09n08:1072936:1073165 [4] NCCL INFO Channel 03/0 : 4[4] -> 10[4] [receive] via NET/IB/3
d09n08:1072936:1073165 [4] NCCL INFO Channel 03/0 : 10[4] -> 22[4] [send] via NET/IB/3
d09n08:1072936:1073165 [4] NCCL INFO Channel 03/0 : 22[4] -> 10[4] [receive] via NET/IB/3
d09n08:1072936:1073165 [4] NCCL INFO Channel 03/0 : 10[4] -> 4[4] [send] via NET/IB/3
d09n08:1072936:1073165 [4] NCCL INFO Channel 01/0 : 15[3] -> 10[4] [receive] via NET/IB/3
d09n08:1072936:1073165 [4] NCCL INFO Channel 00/0 : 10[4] -> 9[3] via P2P/IPC
d09n08:1072936:1073165 [4] NCCL INFO Channel 01/0 : 10[4] -> 9[3] via P2P/IPC
d09n08:1072936:1073165 [4] NCCL INFO Channel 02/0 : 10[4] -> 9[3] via P2P/IPC
d09n08:1072936:1073165 [4] NCCL INFO Channel 03/0 : 10[4] -> 9[3] via P2P/IPC
d09n08:1072936:1073165 [4] NCCL INFO Connected all trees
d09n08:1072936:1073165 [4] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
d09n08:1072936:1073165 [4] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
d09n08:1072936:1073165 [4] NCCL INFO comm 0x142443b80 rank 10 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x7b12926388d12ce3 - Init COMPLETE
d09n08:1072936:1073248 [4] NCCL INFO Using network IB
d09n08:1072936:1073248 [4] NCCL INFO comm 0x144d2ae50 rank 1 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xe7c8afafd6ec6b2d - Init START
d09n08:1072936:1073248 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
d09n08:1072936:1073248 [4] NCCL INFO Trees [0] -1/-1/-1->1->2 [1] 2/0/-1->1->3
d09n08:1072936:1073248 [4] NCCL INFO P2P Chunksize set to 131072
d09n08:1072936:1073248 [4] NCCL INFO Channel 00/0 : 0[2] -> 1[4] [receive] via NET/IB/1
d09n08:1072936:1073248 [4] NCCL INFO Channel 01/0 : 0[2] -> 1[4] [receive] via NET/IB/1
d09n08:1072936:1073248 [4] NCCL INFO Channel 00/0 : 1[4] -> 2[0] [send] via NET/IB/1
d09n08:1072936:1073248 [4] NCCL INFO Channel 01/0 : 1[4] -> 2[0] [send] via NET/IB/1
d09n08:1072936:1073248 [4] NCCL INFO Connected all rings
d09n08:1072936:1073248 [4] NCCL INFO Channel 01/0 : 1[4] -> 3[2] [send] via NET/IB/1
d09n08:1072936:1073248 [4] NCCL INFO Channel 01/0 : 3[2] -> 1[4] [receive] via NET/IB/1
d09n08:1072936:1073248 [4] NCCL INFO Channel 00/0 : 2[0] -> 1[4] [receive] via NET/IB/1
d09n08:1072936:1073248 [4] NCCL INFO Channel 01/0 : 2[0] -> 1[4] [receive] via NET/IB/1
d09n08:1072936:1073248 [4] NCCL INFO Channel 01/0 : 1[4] -> 0[2] [send] via NET/IB/1
d09n08:1072936:1073248 [4] NCCL INFO Connected all trees
d09n08:1072936:1073248 [4] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
d09n08:1072936:1073248 [4] NCCL INFO 2 cg05n08:178236:178236 [0] NCCL INFO cudaDriverVersion 12020
g05n08:178236:178236 [0] NCCL INFO Bootstrap : Using ib0:10.41.16.20<0>
g05n08:178236:178236 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g05n08:178236:178236 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g05n08:178236:178471 [0] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.16.20<0>
g05n08:178236:178471 [0] NCCL INFO Using network IB
g05n08:178236:178471 [0] NCCL INFO comm 0x135ac3f30 rank 78 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x7b12926388d12ce3 - Init START
g05n08:178236:178471 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g05n08:178236:178471 [0] NCCL INFO Trees [0] 79/-1/-1->78->85 [1] 80/-1/-1->78->79 [2] 79/72/-1->78->67 [3] 80/-1/-1->78->79
g05n08:178236:178471 [0] NCCL INFO P2P Chunksize set to 131072
g05n08:178236:178471 [0] NCCL INFO Channel 00/0 : 77[5] -> 78[0] [receive] via NET/IB/0
g05n08:178236:178471 [0] NCCL INFO Channel 02/0 : 77[5] -> 78[0] [receive] via NET/IB/0
g05n08:178236:178471 [0] NCCL INFO Channel 00/0 : 78[0] -> 79[1] via P2P/IPC
g05n08:178236:178471 [0] NCCL INFO Channel 02/0 : 78[0] -> 79[1] via P2P/IPC
g05n08:178236:178471 [0] NCCL INFO Channel 01/0 : 78[0] -> 87[3] [send] via NET/IB/2
g05n08:178236:178471 [0] NCCL INFO Channel 03/0 : 78[0] -> 87[3] [send] via NET/IB/2
g05n08:178236:178471 [0] NCCL INFO Connected all rings
g05n08:178236:178471 [0] NCCL INFO Channel 01/0 : 78[0] -> 79[1] via P2P/IPC
g05n08:178236:178471 [0] NCCL INFO Channel 03/0 : 78[0] -> 79[1] via P2P/IPC
g05n08:178236:178471 [0] NCCL INFO Channel 01/0 : 78[0] -> 80[2] via P2P/IPC
g05n08:178236:178471 [0] NCCL INFO Channel 03/0 : 78[0] -> 80[2] via P2P/IPC
g05n08:178236:178471 [0] NCCL INFO Channel 02/0 : 72[0] -> 78[0] [receive] via NET/IB/0
g05n08:178236:178471 [0] NCCL INFO Channel 00/0 : 78[0] -> 85[1] [send] via NET/IB/0
g05n08:178236:178471 [0] NCCL INFO Channel 02/0 : 67[1] -> 78[0] [receive] via NET/IB/0
g05n08:178236:178471 [0] NCCL INFO Channel 02/0 : 78[0] -> 67[1] [send] via NET/IB/0
g05n08:178236:178471 [0] NCCL INFO Channel 00/0 : 85[1] -> 78[0] [receive] via NET/IB/0
g05n08:178236:178471 [0] NCCL INFO Channel 02/0 : 78[0] -> 72[0] [send] via NET/IB/0
g05n08:178236:178471 [0] NCCL INFO Connected all trees
g05n08:178236:178471 [0] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g05n08:178236:178471 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g05n08:178236:178471 [0] NCCL INFO comm 0x135ac3f30 rank 78 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x7b12926388d12ce3 - Init COMPLETE
g05n08:178236:178558 [0] NCCL INFO Using network IB
g05n08:178236:178558 [0] NCCL INFO comm 0x1383ad730 rank 9 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x414883fe5ee4d1ab - Init START
g05n08:178236:178558 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g05n08:178236:178558 [0] NCCL INFO Trees [0] -1/-1/-1->9->10 [1] 10/8/-1->9->7
g05n08:178236:178558 [0] NCCL INFO P2P Chunksize set to 131072
g05n08:178236:178558 [0] NCCL INFO Channel 00/0 : 8[4] -> 9[0] [receive] via NET/IB/0
g05n08:178236:178558 [0] NCCL INFO Channel 01/0 : 8[4] -> 9[0] [receive] via NET/IB/0
g05n08:178236:178558 [0] NCCL INFO Channel 00/0 : 9[0] -> 10[2] [send] via NET/IB/0
g05n08:178236:178558 [0] NCCL INFO Channel 01/0 : 9[0] -> 10[2] [send] via NET/IB/0
g05n08:178236:178558 [0] NCCL INFO Connected all rings
g05n08:178236:178558 [0] NCCL INFO Channel 01/0 : 7[2] -> 9[0] [receive] via NET/IB/0
g05n08:178236:178558 [0] NCCL INFO Channel 01/0 : 9[0] -> 7[2] [send] via NET/IB/0
g05n08:178236:178558 [0] NCCL INFO Channel 00/0 : 10[2] -> 9[0] [receive] via NET/IB/0
g05n08:178236:178558 [0] NCCL INFO Channel 01/0 : 10[2] -> 9[0] [receive] via NET/IB/0
g05n08:178236:178558 [0] NCCL INFO Channel 01/0 : 9[0] -> 8[4] [send] via NET/IB/0
g05n08:178236:178558 [0] NCCL INFO Connected all trees
g05n08:178236:178558 [0] NCCL INFO threadThresholds 8/8/64 | 96/8/64 d09n08:1072932:1073247 [0] NCCL INFO Using network IB
d09n08:1072932:1073247 [0] NCCL INFO comm 0x1522f5a70 rank 0 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x414883fe5ee4d1ab - Init START
d09n08:1072932:1073247 [0] NCCL INFO Setting affinity for GPU 0 to 0f
d09n08:1072932:1073247 [0] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7   8   9  10  11
d09n08:1072932:1073247 [0] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7   8   9  10  11
d09n08:1072932:1073247 [0] NCCL INFO Trees [0] 8/-1/-1->0->-1 [1] -1/-1/-1->0->1
d09n08:1072932:1073247 [0] NCCL INFO P2P Chunksize set to 131072
d09n08:1072932:1073247 [0] NCCL INFO Channel 00/0 : 11[4] -> 0[0] [receive] via NET/IB/0
d09n08:1072932:1073247 [0] NCCL INFO Channel 01/0 : 11[4] -> 0[0] [receive] via NET/IB/0
d09n08:1072932:1073247 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[2] [send] via NET/IB/0
d09n08:1072932:1073247 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[2] [send] via NET/IB/0
d09n08:1072932:1073247 [0] NCCL INFO Connected all rings
d09n08:1072932:1073247 [0] NCCL INFO Channel 00/0 : 8[4] -> 0[0] [receive] via NET/IB/0
d09n08:1072932:1073247 [0] NCCL INFO Channel 00/0 : 0[0] -> 8[4] [send] via NET/IB/0
d09n08:1072932:1073247 [0] NCCL INFO Channel 01/0 : 1[2] -> 0[0] [receive] via NET/IB/0
d09n08:1072932:1073247 [0] NCCL INFO Connected all trees
d09n08:1072932:1073247 [0] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
d09n08:1072932:1073247 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d09n08:1072932:1073247 [0] NCCL INFO comm 0x1522f5a70 rank 0 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x414883fe5ee4d1ab - Init COMPLETE
d09n08:1072932:1073266 [0] NCCL INFO Using network IB
d09n08:1072932:1073266 [0] NCCL INFO comm 0x15117fa00 rank 1 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0x32e49bb35e6c8ace - Init START
d09n08:1072932:1073266 [0] NCCL INFO Setting affinity for GPU 0 to 0f
d09n08:1072932:1073266 [0] NCCL INFO Trees [0] 2/-1/-1->1->3 [1] 2/0/-1->1->4
d09n08:1072932:1073266 [0] NCCL INFO P2P Chunksize set to 131072
d09n08:1072932:1073266 [0] NCCL INFO Channel 00/0 : 0[2] -> 1[0] [receive] via NET/IB/0
d09n08:1072932:1073266 [0] NCCL INFO Channel 01/0 : 0[2] -> 1[0] [receive] via NET/IB/0
d09n08:1072932:1073266 [0] NCCL INFO Channel 00/0 : 1[0] -> 2[4] via P2P/IPC
d09n08:1072932:1073266 [0] NCCL INFO Channel 01/0 : 1[0] -> 2[4] via P2P/IPC
d09n08:1072932:1073266 [0] NCCL INFO Connected all rings
d09n08:1072932:1073266 [0] NCCL INFO Channel 00/0 : 1[0] -> 3[2] [send] via NET/IB/0
d09n08:1072932:1073266 [0] NCCL INFO Channel 01/0 : 1[0] -> 4[0] [send] via NET/IB/0
d09n08:1072932:1073266 [0] NCCL INFO Channel 01/0 : 4[0] -> 1[0] [receive] via NET/IB/0
d09n08:1072932:1073266 [0] NCCL INFO Channel 00/0 : 3[2] -> 1[0] [receive] via NET/IB/0
d09n08:1072932:1073266 [0] NCCL INFO Channel 01/0 : 1[0] -> 0[2] [send] via NET/IB/0
d09n08:1072932:1073266 [0] NCCL INFO Connected all trees
d09n08:1072932:1073266 [0] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
d09n08:1072932:1073266 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d09n08:1072932:1073266 [0] NCCL INFO comm 0x15117fa00 rank 1 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0x32e49bb35e6c8ace - Init COMPLETE
oll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d09n08:1072936:1073248 [4] NCCL INFO comm 0x144d2ae50 rank 1 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xe7c8afafd6ec6b2d - Init COMPLETE
d09n08:1072936:1073270 [4] NCCL INFO Using network IB
d09n08:1072936:1073270 [4] NCCL INFO comm 0x1461df130 rank 2 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x32e49bb35e6c8ace - Init START
d09n08:1072936:1073270 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
d09n08:1072936:1073270 [4] NCCL INFO Trees [0] -1/-1/-1->2->1 [1] 3/-1/-1->2->1
d09n08:1072936:1073270 [4] NCCL INFO P2P Chunksize set to 131072
d09n08:1072936:1073270 [4] NCCL INFO Channel 00/0 : 2[4] -> 3[2] [send] via NET/IB/1
d09n08:1072936:1073270 [4] NCCL INFO Channel 01/0 : 2[4] -> 3[2] [send] via NET/IB/1
d09n08:1072936:1073270 [4] NCCL INFO Connected all rings
d09n08:1072936:1073270 [4] NCCL INFO Channel 01/0 : 3[2] -> 2[4] [receive] via NET/IB/0
d09n08:1072936:1073270 [4] NCCL INFO Channel 00/0 : 2[4] -> 1[0] via P2P/IPC
d09n08:1072936:1073270 [4] NCCL INFO Channel 01/0 : 2[4] -> 1[0] via P2P/IPC
d09n08:1072936:1073270 [4] NCCL INFO Connected all trees
d09n08:1072936:1073270 [4] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
d09n08:1072936:1073270 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d09n08:1072936:1073270 [4] NCCL INFO comm 0x1461df130 rank 2 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x32e49bb35e6c8ace - Init COMPLETE

g05n08:178243:178560 [4] NCCL INFO comm 0x147a1e7a0 rank 10 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xe7c8afafd6ec6b2d - Init COMPLETE
g05n08:178243:178576 [4] NCCL INFO Using network IB
g05n08:178243:178576 [4] NCCL INFO comm 0x146b54830 rank 20 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x32e49bb35e6c8ace - Init START
g05n08:178243:178576 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g05n08:178243:178576 [4] NCCL INFO Trees [0] -1/-1/-1->20->19 [1] 21/-1/-1->20->19
g05n08:178243:178576 [4] NCCL INFO P2P Chunksize set to 131072
g05n08:178243:178576 [4] NCCL INFO Channel 00/0 : 20[4] -> 21[2] [send] via NET/IB/1
g05n08:178243:178576 [4] NCCL INFO Channel 01/0 : 20[4] -> 21[2] [send] via NET/IB/1
g05n08:178243:178576 [4] NCCL INFO Connected all rings
g05n08:178243:178576 [4] NCCL INFO Channel 01/0 : 21[2] -> 20[4] [receive] via NET/IB/0
g05n08:178243:178576 [4] NCCL INFO Channel 00/0 : 20[4] -> 19[0] via P2P/IPC
g05n08:178243:178576 [4] NCCL INFO Channel 01/0 : 20[4] -> 19[0] via P2P/IPC
g05n08:178243:178576 [4] NCCL INFO Connected all trees
g05n08:178243:178576 [4] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g05n08:178243:178576 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g05n08:178243:178576 [4] NCCL INFO comm 0x146b54830 rank 20 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x32e49bb35e6c8ace - Init COMPLETE
| 512 | 512
g05n08:178236:178558 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g05n08:178236:178558 [0] NCCL INFO comm 0x1383ad730 rank 9 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x414883fe5ee4d1ab - Init COMPLETE
g05n08:178236:178580 [0] NCCL INFO Using network IB
g05n08:178236:178580 [0] NCCL INFO comm 0x1386ed330 rank 19 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0x32e49bb35e6c8ace - Init START
g05n08:178236:178580 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g05n08:178236:178580 [0] NCCL INFO Trees [0] 20/-1/-1->19->21 [1] 20/18/-1->19->17
g05n08:178236:178580 [0] NCCL INFO P2P Chunksize set to 131072
g05n08:178236:178580 [0] NCCL INFO Channel 00/0 : 18[2] -> 19[0] [receive] via NET/IB/0
g05n08:178236:178580 [0] NCCL INFO Channel 01/0 : 18[2] -> 19[0] [receive] via NET/IB/0
g05n08:178236:178580 [0] NCCL INFO Channel 00/0 : 19[0] -> 20[4] via P2P/IPC
g05n08:178236:178580 [0] NCCL INFO Channel 01/0 : 19[0] -> 20[4] via P2P/IPC
g05n08:178236:178580 [0] NCCL INFO Connected all rings
g05n08:178236:178580 [0] NCCL INFO Channel 01/0 : 17[4] -> 19[0] [receive] via NET/IB/0
g05n08:178236:178580 [0] NCCL INFO Channel 00/0 : 19[0] -> 21[2] [send] via NET/IB/0
g05n08:178236:178580 [0] NCCL INFO Channel 00/0 : 21[2] -> 19[0] [receive] via NET/IB/0
g05n08:178236:178580 [0] NCCL INFO Channel 01/0 : 19[0] -> 17[4] [send] via NET/IB/0
g05n08:178236:178580 [0] NCCL INFO Channel 01/0 : 19[0] -> 18[2] [send] via NET/IB/0
g05n08:178236:178580 [0] NCCL INFO Connected all trees
g05n08:178236:178580 [0] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g05n08:178236:178580 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g05n08:178236:178580 [0] NCCL INFO comm 0x1386ed330 rank 19 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0x32e49bb35e6c8ace - Init COMPLETE
g05n10:696650:696650 [4] NCCL INFO cudaDriverVersion 12020
g05n10:696650:696650 [4] NCCL INFO Bootstrap : Using ib0:10.41.16.22<0>
g05n10:696650:696650 [4] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g05n10:696650:696650 [4] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g05n10:696650:696880 [4] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.16.22<0>
g05n10:696650:696880 [4] NCCL INFO Using network IB
g05n10:696650:696880 [4] NCCL INFO comm 0x1512e4060 rank 94 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x7b12926388d12ce3 - Init START
g05n10:696650:696880 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g05n10:696650:696880 [4] NCCL INFO Trees [0] 95/-1/-1->94->93 [1] 93/-1/-1->94->88 [2] 95/-1/-1->94->93 [3] 93/46/-1->94->-1
g05n10:696650:696880 [4] NCCL INFO P2P Chunksize set to 131072
g05n10:696650:696880 [4] NCCL INFO Channel 00/0 : 94[4] -> 95[5] via P2P/IPC
g05n10:696650:696880 [4] NCCL INFO Channel 01/0 : 94[4] -> 95[5] via P2P/IPC
g05n10:696650:696880 [4] NCCL INFO Channel 02/0 : 94[4] -> 95[5] via P2P/IPC
g05n10:696650:696880 [4] NCCL INFO Channel 03/0 : 94[4] -> 95[5] via P2P/IPC
g05n10:696650:696880 [4] NCCL INFO Connected all rings
g05n10:696650:696880 [4] NCCL INFO Channel 01/0 : 88[4] -> 94[4] [receive] via NET/IB/3
g05n10:696650:696880 [4] NCCL INFO Channel 03/0 : 46[4] -> 94[4] [receive] via NET/IB/3
g05n10:696650:696880 [4] NCCL INFO Channel 03/0 : 94[4] -> 46[4] [send] via NET/IB/3
g05n10:696650:696880 [4] NCCL INFO Channel 01/0 : 94[4] -> 88[4] [send] via NET/IB/3
g05n10:696650:696880 [4] NCCL INFO Channel 00/0 : 94[4] -> 93[3] via P2P/IPC
g05n10:696650:696880 [4] NCCL INFO Channel 01/0 : 94[4] -> 93[3] via P2P/IPC
g05n10:696650:696880 [4] NCCL INFO Channel 02/0 : 94[4] -> 93[3] via P2P/IPC
g05n10:696650:696880 [4] NCCL INFO Channel 03/0 : 94[4] -> 93[3] via P2P/IPC
g05n10:696650:696880 [4] NCCL INFO Connected all trees
g05n10:696650:696880 [4] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g05n10:696650:696880 [4] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g05n10:696650:696880 [4] NCCL INFO comm 0x1512e4060 rank 94 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x7b12926388d12ce3 - Init COMPLETE
g05n10:696650:696961 [4] NCCL INFO Using network IB
g05n10:696650:696961 [4] NCCL INFO comm 0x153bf5b00 rank 11 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x414883fe5ee4d1ab - Init START
g05n10:696650:696961 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g05n10:696650:696961 [4] NCCL INFO Trees [0] -1/-1/-1->11->10 [1] 3/-1/-1->11->-1
g05n10:696650:696961 [4] NCCL INFO P2P Chunksize set to 131072
g05n10:696650:696961 [4] NCCL INFO Channel 00/0 : 10[2] -> 11[4] [receive] via NET/IB/1
g05n10:696650:696961 [4] NCCL INFO Channel 01/0 : 10[2] -> 11[4] [receive] via NET/IB/1
g05n10:696650:696961 [4] NCCL INFO Channel 00/0 : 11[4] -> 0[0] [send] via NET/IB/1
g05n10:696650:696961 [4] NCCL INFO Channel 01/0 : 11[4] -> 0[0] [send] via NET/IB/1
g05n10:696650:696961 [4] NCCL INFO Connected all rings
g05n10:696650:696961 [4] NCCL INFO Channel 01/0 : 11[4] -> 3[0] [send] via NET/IB/1
g05n10:696650:696961 [4] NCCL INFO Channel 01/0 : 3[0] -> 11[4] [receive] via NET/IB/1
g05n10:696650:696961 [4] NCCL INFO Channel 00/0 : 11[4] -> 10[2] [send] via NET/IB/1
g05n10:696650:696961 [4] NCCL INFO Connected all trees
g05n10:696650:696961 [4] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g05n10:696650:696961 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g05n10:696650:696961 [4] NCCL INFO comm 0x153bf5b00 rank 11 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x414883fe5ee4d1ab - Init COMPLETE
g05n10:696650:696979 [4] NCCL INFO Using network IB
g05n10:696650:696979 [4] NCCL INFO comm 0x153babd60 rank 23 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x32e49bb35e6c8ace - Init START
g05n10:696650:696979 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g05n10:696650:696979 [4] NCCL INFO Trees [0] -1/-1/-1->23->22 [1] -1/-1/-1->23->22
g05n10:696650:696979 [4] NCCL INFO P2P Chunksize set to 131072
g05n10:696650:696979 [4] NCCL INFO Channel 00/0 : 23[4] -> 0[2] [send] via NET/IB/1
g05n10:696650:696979 [4] NCCL INFO Channel 01/0 : 23[4] -> 0[2] [send] via NET/IB/1
g05n10:696650:696979 [4] NCCL INFO Connected all rings
g05n10:696650:696979 [4] NCCL INFO Channel 00/0 : 23[4] -> 22[0] via P2P/IPC
g05n10:696650:696979 [4] NCCL INFO Channel 01/0 : 23[4] -> 22[0] via P2P/IPC
g05n10:696650:696979 [4] NCCL INFO Connected all trees
g05n10:696650:696979 [4] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g05n10:696650:696979 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g05n10:696650:696979 [4] NCCL INFO comm 0x153babd60 rank 23 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x32e49bb35e6c8ace - Init COMPLETE
g04n18:916029:916029 [5] NCCL INFO cudaDriverVersion 12020
g04n18:916029:916029 [5] NCCL INFO Bootstrap : Using ib0:10.41.16.12<0>
g04n18:916029:916029 [5] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g04n18:916029:916029 [5] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g04n18:916029:916260 [5] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.16.12<0>
g04n18:916029:916260 [5] NCCL INFO Using network IB
g04n18:916029:916260 [5] NCCL INFO comm 0x154074840 rank 77 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x7b12926388d12ce3 - Init START
g04n18:916029:916260 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g04n18:916029:916260 [5] NCCL INFO Trees [0] -1/-1/-1->77->76 [1] 73/-1/-1->77->75 [2] -1/-1/-1->77->76 [3] 73/-1/-1->77->75
g04n18:916029:916260 [5] NCCL INFO P2P Chunksize set to 131072
g04n18:916029:916260 [5] NCCL INFO Channel 00/0 : 77[5] -> 78[0] [send] via NET/IB/1
g04n18:916029:916260 [5] NCCL INFO Channel 02/0 : 77[5] -> 78[0] [send] via NET/IB/1
g04n18:916029:916260 [5] NCCL INFO Channel 01/0 : 77[5] -> 74[2] via P2P/IPC
g04n18:916029:916260 [5] NCCL INFO Channel 03/0 : 77[5] -> 74[2] via P2P/IPC
g04n18:916029:916260 [5] NCCL INFO Connected all rings
g04n18:916029:916260 [5] NCCL INFO Channel 01/0 : 77[5] -> 73[1] via P2P/IPC
g04n18:916029:916260 [5] NCCL INFO Channel 03/0 : 77[5] -> 73[1] via P2P/IPC
g04n18:916029:916260 [5] NCCL INFO Channel 01/0 : 77[5] -> 75[3] via P2P/IPC
g04n18:916029:916260 [5] NCCL INFO Channel 03/0 : 77[5] -> 75[3] via P2P/IPC
g04n18:916029:916260 [5] NCCL INFO Channel 00/0 : 77[5] -> 76[4] via P2P/IPC
g04n18:916029:916260 [5] NCCL INFO Channel 02/0 : 77[5] -> 76[4] via P2P/IPC
g04n18:916029:916260 [5] NCCL INFO Connected all trees
g04n18:916029:916260 [5] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g04n18:916029:916260 [5] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g04n18:916029:916260 [5] NCCL INFO comm 0x154074840 rank 77 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x7b12926388d12ce3 - Init COMPLETE
g04n18:916029:916341 [5] NCCL INFO Using network IB
g04n18:916029:916341 [5] NCCL INFO comm 0x156f108e0 rank 9 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xa77518dab25ce26 - Init START
g04n18:916029:916341 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g04n18:916029:916341 [5] NCCL INFO Trees [0] -1/-1/-1->9->10 [1] 10/8/-1->9->7
g04n18:916029:916341 [5] NCCL INFO P2P Chunksize set to 131072
g04n18:916029:916341 [5] NCCL INFO Channel 00/0 : 8[3] -> 9[5] [receive] via NET/IB/3
g04n18:916029:916341 [5] NCCL INFO Channel 01/0 : 8[3] -> 9[5] [receive] via NET/IB/3
g04n18:916029:916341 [5] NCCL INFO Channel 00/0 : 9[5] -> 10[1] [send] via NET/IB/3
g04n18:916029:916341 [5] NCCL INFO Channel 01/0 : 9[5] -> 10[1] [send] via NET/IB/3
g04n18:916029:916341 [5] NCCL INFO Connected all rings
g04n18:916029:916341 [5] NCCL INFO Channel 01/0 : 7[1] -> 9[5] [receive] via NET/IB/3
g04n18:916029:916341 [5] NCCL INFO Channel 01/0 : 9[5] -> 7[1] [send] via NET/IB/3
g04n18:916029:916341 [5] NCCL INFO Channel 00/0 : 10[1] -> 9[5] [receive] via NET/IB/3
g04n18:916029:916341 [5] NCCL INFO Channel 01/0 : 10[1] -> 9[5] [receive] via NET/IB/3
g04n18:916029:916341 [5] NCCL INFO Channel 01/0 : 9[5] -> 8[3] [send] via NET/IB/3
g04n18:916029:916341 [5] NCCL INFO Connected all trees
g04n18:916029:916341 [5] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g04n18:916029:916341 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g04n18:916029:916341 [5] NCCL INFO comm 0x156f108e0 rank 9 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xa77518dab25ce26 - Init COMPLETE
g04n18:916029:916364 [5] NCCL INFO Using network IB
g04n18:916029:916364 [5] NCCL INFO comm 0x156dc1c00 rank 19 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xa49a27aa8b15034e - Init START
g04n18:916029:916364 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g04n18:916029:916364 [5] NCCL INFO Trees [0] 15/-1/-1->19->18 [1] -1/-1/-1->19->18
g04n18:916029:916364 [5] NCCL INFO P2P Chunksize set to 131072
g04n18:916029:916364 [5] NCCL INFO Channel 00/0 : 19[5] -> 20[3] [send] via NET/IB/3
g04n18:916029:916364 [5] NCCL INFO Channel 01/0 : 19[5] -> 20[3] [send] via NET/IB/3
g04n18:916029:916364 [5] NCCL INFO Connected all rings
g04n18:916029:916364 [5] NCCL INFO Channel 00/0 : 15[1] -> 19[5] [receive] via NET/IB/2
g04n18:916029:916364 [5] NCCL INFO Channel 00/0 : 19[5] -> 15[1] [send] via NET/IB/2
g04n18:916029:916364 [5] NCCL INFO Channel 00/0 : 19[5] -> 18[1] via P2P/IPC
g04n18:916029:916364 [5] NCCL INFO Channel 01/0 : 19[5] -> 18[1] via P2P/IPC
g04n18:916029:916364 [5] NCCL INFO Connected all trees
g04n18:916029:916364 [5] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g04n18:916029:916364 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g04n18:916029:916364 [5] NCCL INFO comm 0x156dc1c00 rank 19 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xa49a27aa8b15034e - Init COMPLETE
d09n07:1061890:1062291 [3] NCCL INFO Using network IB
d09n07:1061890:1062291 [3] NCCL INFO comm 0x1779ade60 rank 0 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x1defb2708b178c98 - Init START
d09n07:1061890:1062291 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
d09n07:1061890:1062291 [3] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7   8   9  10  11
d09n07:1061890:1062291 [3] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7   8   9  10  11
d09n07:1061890:1062291 [3] NCCL INFO Trees [0] 8/-1/-1->0->-1 [1] -1/-1/-1->0->1
d09n07:1061890:1062291 [3] NCCL INFO P2P Chunksize set to 131072
d09n07:1061890:1062291 [3] NCCL INFO Channel 00/0 : 11[1] -> 0[3] [receive] via NET/IB/3
d09n07:1061890:1062291 [3] NCCL INFO Channel 01/0 : 11[1] -> 0[3] [receive] via NET/IB/3
d09n07:1061890:1062291 [3] NCCL INFO Channel 00/0 : 0[3] -> 1[5] [send] via NET/IB/3
d09n07:1061890:1062291 [3] NCCL INFO Channel 01/0 : 0[3] -> 1[5] [send] via NET/IB/3
d09n07:1061890:1062291 [3] NCCL INFO Connected all rings
d09n07:1061890:1062291 [3] NCCL INFO Channel 00/0 : 8[1] -> 0[3] [receive] via NET/IB/3
d09n07:1061890:1062291 [3] NCCL INFO Channel 00/0 : 0[3] -> 8[1] [send] via NET/IB/3
d09n07:1061890:1062291 [3] NCCL INFO Channel 01/0 : 1[5] -> 0[3] [receive] via NET/IB/3
d09n07:1061890:1062291 [3] NCCL INFO Connected all trees
d09n07:1061890:1062291 [3] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
d09n07:1061890:1062291 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d09n07:1061890:1062291 [3] NCCL INFO comm 0x1779ade60 rank 0 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x1defb2708b178c98 - Init COMPLETE
d09n07:1061890:1062314 [3] NCCL INFO Using network IB
d09n07:1061890:1062314 [3] NCCL INFO comm 0x1777333f0 rank 0 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x821cecfd780eaba1 - Init START
d09n07:1061890:1062314 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
d09n07:1061890:1062314 [3] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
d09n07:1061890:1062314 [3] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
d09n07:1061890:1062314 [3] NCCL INFO Trees [0] 12/-1/-1->0->-1 [1] -1/-1/-1->0->1
d09n07:1061890:1062314 [3] NCCL INFO P2P Chunksize set to 131072
d09n07:1061890:1062314 [3] NCCL INFO Channel 00/0 : 23[5] -> 0[3] [receive] via NET/IB/3
d09n07:1061890:1062314 [3] NCCL INFO Channel 01/0 : 23[5] -> 0[3] [receive] via NET/IB/3
d09n07:1061890:1062314 [3] NCCL INFO Channel 00/0 : 0[3] -> 1[1] [send] via NET/IB/3
d09n07:1061890:1062314 [3] NCCL INFO Channel 01/0 : 0[3] -> 1[1] [send] via NET/IB/3
d09n07:1061890:1062314 [3] NCCL INFO Connected all rings
d09n07:1061890:1062314 [3] NCCL INFO Channel 00/0 : 12[3] -> 0[3] [receive] via NET/IB/3
d09n07:1061890:1062314 [3] NCCL INFO Channel 00/0 : 0[3] -> 12[3] [send] via NET/IB/3
d09n07:1061890:1062314 [3] NCCL INFO Channel 01/0 : 1[1] -> 0[3] [receive] via NET/IB/3
d09n07:1061890:1062314 [3] NCCL INFO Connected all trees
d09n07:1061890:1062314 [3] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
d09n07:1061890:1062314 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d09n07:1061890:1062314 [3] NCCL INFO comm 0x1777333f0 rank 0 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x821cecfd780eaba1 - Init COMPLETE
d09n07:1061892:1062285 [5] NCCL INFO Using network IB
d09n07:1061892:1062285 [5] NCCL INFO comm 0x13280c630 rank 0 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xa77518dab25ce26 - Init START
d09n07:1061892:1062285 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
d09n07:1061892:1062285 [5] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7   8   9  10  11
d09n07:1061892:1062285 [5] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7   8   9  10  11
d09n07:1061892:1062285 [5] NCCL INFO Trees [0] 8/-1/-1->0->-1 [1] -1/-1/-1->0->1
d09n07:1061892:1062285 [5] NCCL INFO P2P Chunksize set to 131072
d09n07:1061892:1062285 [5] NCCL INFO Channel 00/0 : 11[3] -> 0[5] [receive] via NET/IB/3
d09n07:1061892:1062285 [5] NCCL INFO Channel 01/0 : 11[3] -> 0[5] [receive] via NET/IB/3
d09n07:1061892:1062285 [5] NCCL INFO Channel 00/0 : 0[5] -> 1[1] [send] via NET/IB/3
d09n07:1061892:1062285 [5] NCCL INFO Channel 01/0 : 0[5] -> 1[1] [send] via NET/IB/3
d09n07:1061892:1062285 [5] NCCL INFO Connected all rings
d09n07:1061892:1062285 [5] NCCL INFO Channel 00/0 : 8[3] -> 0[5] [receive] via NET/IB/3
d09n07:1061892:1062285 [5] NCCL INFO Channel 00/0 : 0[5] -> 8[3] [send] via NET/IB/3
d09n07:1061892:1062285 [5] NCCL INFO Channel 01/0 : 1[1] -> 0[5] [receive] via NET/IB/3
d09n07:1061892:1062285 [5] NCCL INFO Connected all trees
d09n07:1061892:1062285 [5] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
d09n07:1061892:1062285 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d09n07:1061892:1062285 [5] NCCL INFO comm 0x13280c630 rank 0 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xa77518dab25ce26 - Init COMPLETE
d09n07:1061892:1062320 [5] NCCL INFO Using network IB
d09n07:1061892:1062320 [5] NCCL INFO comm 0x132808a80 rank 1 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xa49a27aa8b15034e - Init START
d09n07:1061892:1062320 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
d09n07:1061892:1062320 [5] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
d09n07:1061892:1062320 [5] NCCL INFO P2P Chunksize set to 131072
d09n07:1061892:1062320 [5] NCCL INFO Channel 00/0 : 1[5] -> 2[3] [send] via NET/IB/3
d09n07:1061892:1062320 [5] NCCL INFO Channel 01/0 : 1[5] -> 2[3] [send] via NET/IB/3
d09n07:1061892:1062320 [5] NCCL INFO Connected all rings
d09n07:1061892:1062320 [5] NCCL INFO Channel 00/0 : 1[5] -> 0[1] via P2P/IPC
d09n07:1061892:1062320 [5] NCCL INFO Channel 01/0 : 1[5] -> 0[1] via P2P/IPC
d09n07:1061892:1062320 [5] NCCL INFO Connected all trees
d09n07:1061892:1062320 [5] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
d09n07:1061892:1062320 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d09n07:1061892:1062320 [5] NCCL INFO comm 0x132808a80 rank 1 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xa49a27aa8b15034e - Init COMPLETE
d09n07:1061888:1062293 [1] NCCL INFO Using network IB
d09n07:1061888:1062293 [1] NCCL INFO comm 0x12b6e2960 rank 0 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x86d434c9d289d59a - Init START
d09n07:1061888:1062293 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
d09n07:1061888:1062293 [1] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7   8   9  10  11
d09n07:1061888:1062293 [1] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7   8   9  10  11
d09n07:1061888:1062293 [1] NCCL INFO Trees [0] 8/-1/-1->0->-1 [1] -1/-1/-1->0->1
d09n07:1061888:1062293 [1] NCCL INFO P2P Chunksize set to 131072
d09n07:1061888:1062293 [1] NCCL INFO Channel 00/0 : 11[5] -> 0[1] [receive] via NET/IB/2
d09n07:1061888:1062293 [1] NCCL INFO Channel 01/0 : 11[5] -> 0[1] [receive] via NET/IB/2
d09n07:1061888:1062293 [1] NCCL INFO Channel 00/0 : 0[1] -> 1[3] [send] via NET/IB/2
d09n07:1061888:1062293 [1] NCCL INFO Channel 01/0 : 0[1] -> 1[3] [send] via NET/IB/2
d09n07:1061888:1062293 [1] NCCL INFO Connected all rings
d09n07:1061888:1062293 [1] NCCL INFO Channel 00/0 : 8[5] -> 0[1] [receive] via NET/IB/2
d09n07:1061888:1062293 [1] NCCL INFO Channel 00/0 : 0[1] -> 8[5] [send] via NET/IB/2
d09n07:1061888:1062293 [1] NCCL INFO Channel 01/0 : 1[3] -> 0[1] [receive] via NET/IB/2
d09n07:1061888:1062293 [1] NCCL INFO Connected all trees
d09n07:1061888:1062293 [1] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
d09n07:1061888:1062293 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d09n07:1061888:1062293 [1] NCCL INFO comm 0x12b6e2960 rank 0 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x86d434c9d289d59a - Init COMPLETE
d09n07:1061888:1062312 [1] NCCL INFO Using network IB
d09n07:1061888:1062312 [1] NCCL INFO comm 0x12b6d0ba0 rank 0 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0xa49a27aa8b15034e - Init START
d09n07:1061888:1062312 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
d09n07:1061888:1062312 [1] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
d09n07:1061888:1062312 [1] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
d09n07:1061888:1062312 [1] NCCL INFO Trees [0] 1/12/-1->0->-1 [1] 1/-1/-1->0->2
d09n07:1061888:1062312 [1] NCCL INFO P2P Chunksize set to 131072
d09n07:1061888:1062312 [1] NCCL INFO Channel 00/0 : 23[3] -> 0[1] [receive] via NET/IB/2
d09n07:1061888:1062312 [1] NCCL INFO Channel 01/0 : 23[3] -> 0[1] [receive] via NET/IB/2
d09n07:1061888:1062312 [1] NCCL INFO Channel 00/0 : 0[1] -> 1[5] via P2P/IPC
d09n07:1061888:1062312 [1] NCCL INFO Channel 01/0 : 0[1] -> 1[5] via P2P/IPC
d09n07:1061888:1062312 [1] NCCL INFO Connected all rings
d09n07:1061888:1062312 [1] NCCL INFO Channel 01/0 : 0[1] -> 2[3] [send] via NET/IB/2
d09n07:1061888:1062312 [1] NCCL INFO Channel 00/0 : 12[1] -> 0[1] [receive] via NET/IB/2
d09n07:1061888:1062312 [1] NCCL INFO Channel 00/0 : 0[1] -> 12[1] [send] via NET/IB/2
d09n07:1061888:1062312 [1] NCCL INFO Channel 01/0 : 2[3] -> 0[1] [receive] via NET/IB/2
d09n07:1061888:1062312 [1] NCCL INFO Connected all trees
d09n07:1061888:1062312 [1] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
d09n07:1061888:1062312 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d09n07:1061888:1062312 [1] NCCL INFO comm 0x12b6d0ba0 rank 0 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0xa49a27aa8b15034e - Init COMPLETE
f16n16:1079193:1079193 [1] NCCL INFO cudaDriverVersion 12020
f16n16:1079193:1079193 [1] NCCL INFO Bootstrap : Using ib0:10.41.14.84<0>
f16n16:1079193:1079193 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
f16n16:1079193:1079193 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
f16n16:1079193:1079428 [1] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.14.84<0>
f16n16:1079193:1079428 [1] NCCL INFO Using network IB
f16n16:1079193:1079428 [1] NCCL INFO comm 0x14a403b50 rank 25 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x7b12926388d12ce3 - Init START
f16n16:1079193:1079428 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
f16n16:1079193:1079428 [1] NCCL INFO Trees [0] 26/12/-1->25->24 [1] 24/-1/-1->25->29 [2] 26/-1/-1->25->24 [3] 24/-1/-1->25->29
f16n16:1079193:1079428 [1] NCCL INFO P2P Chunksize set to 131072
f16n16:1079193:1079428 [1] NCCL INFO Channel 00/0 : 25[1] -> 26[2] via P2P/IPC
f16n16:1079193:1079428 [1] NCCL INFO Channel 02/0 : 25[1] -> 26[2] via P2P/IPC
f16n16:1079193:1079428 [1] NCCL INFO Channel 01/0 : 25[1] -> 24[0] via P2P/IPC
f16n16:1079193:1079428 [1] NCCL INFO Channel 03/0 : 25[1] -> 24[0] via P2P/IPC
f16n16:1079193:1079428 [1] NCCL INFO Connected all rings
f16n16:1079193:1079428 [1] NCCL INFO Channel 01/0 : 25[1] -> 29[5] via P2P/IPC
f16n16:1079193:1079428 [1] NCCL INFO Channel 03/0 : 25[1] -> 29[5] via P2P/IPC
f16n16:1079193:1079428 [1] NCCL INFO Channel 00/0 : 12[0] -> 25[1] [receive] via NET/IB/0
f16n16:1079193:1079428 [1] NCCL INFO Channel 00/0 : 25[1] -> 12[0] [send] via NET/IB/0
f16n16:1079193:1079428 [1] NCCL INFO Channel 00/0 : 25[1] -> 24[0] via P2P/IPC
f16n16:1079193:1079428 [1] NCCL INFO Channel 02/0 : 25[1] -> 24[0] via P2P/IPC
f16n16:1079193:1079428 [1] NCCL INFO Connected all trees
f16n16:1079193:1079428 [1] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
f16n16:1079193:1079428 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
f16n16:1079193:1079428 [1] NCCL INFO comm 0x14a403b50 rank 25 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x7b12926388d12ce3 - Init COMPLETE
f16n16:1079193:1079508 [1] NCCL INFO Using network IB
f16n16:1079193:1079508 [1] NCCL INFO comm 0x14d0db560 rank 3 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x86d434c9d289d59a - Init START
f16n16:1079193:1079508 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
f16n16:1079193:1079508 [1] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] 7/1/-1->3->11
f16n16:1079193:1079508 [1] NCCL INFO P2P Chunksize set to 131072
f16n16:1079193:1079508 [1] NCCL INFO Channel 00/0 : 2[5] -> 3[1] [receive] via NET/IB/2
f16n16:1079193:1079508 [1] NCCL INFO Channel 01/0 : 2[5] -> 3[1] [receive] via NET/IB/2
f16n16:1079193:1079508 [1] NCCL INFO Channel 00/0 : 3[1] -> 4[3] [send] via NET/IB/2
f16n16:1079193:1079508 [1] NCCL INFO Channel 01/0 : 3[1] -> 4[3] [send] via NET/IB/2
f16n16:1079193:1079508 [1] NCCL INFO Connected all rings
f16n16:1079193:1079508 [1] NCCL INFO Channel 01/0 : 1[3] -> 3[1] [receive] via NET/IB/2
f16n16:1079193:1079508 [1] NCCL INFO Channel 01/0 : 11[5] -> 3[1] [receive] via NET/IB/2
f16n16:1079193:1079508 [1] NCCL INFO Channel 01/0 : 3[1] -> 7[3] [send] via NET/IB/2
f16n16:1079193:1079508 [1] NCCL INFO Channel 01/0 : 7[3] -> 3[1] [receive] via NET/IB/2
f16n16:1079193:1079508 [1] NCCL INFO Channel 01/0 : 3[1] -> 11[5] [send] via NET/IB/2
f16n16:1079193:1079508 [1] NCCL INFO Channel 01/0 : 3[1] -> 1[3] [send] via NET/IB/2
f16n16:1079193:1079508 [1] NCCL INFO Channel 00/0 : 3[1] -> 2[5] [send] via NET/IB/2
f16n16:1079193:1079508 [1] NCCL INFO Connected all trees
f16n16:1079193:1079508 [1] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
f16n16:1079193:1079508 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f16n16:1079193:1079508 [1] NCCL INFO comm 0x14d0db560 rank 3 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x86d434c9d289d59a - Init COMPLETE
f16n16:1079193:1079526 [1] NCCL INFO Using network IB
f16n16:1079193:1079526 [1] NCCL INFO comm 0x14becfd60 rank 6 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0xa49a27aa8b15034e - Init START
f16n16:1079193:1079526 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
f16n16:1079193:1079526 [1] NCCL INFO Trees [0] 7/9/-1->6->13 [1] 7/-1/-1->6->8
f16n16:1079193:1079526 [1] NCCL INFO P2P Chunksize set to 131072
f16n16:1079193:1079526 [1] NCCL INFO Channel 00/0 : 5[3] -> 6[1] [receive] via NET/IB/2
f16n16:1079193:1079526 [1] NCCL INFO Channel 01/0 : 5[3] -> 6[1] [receive] via NET/IB/2
f16n16:1079193:1079526 [1] NCCL INFO Channel 00/0 : 6[1] -> 7[5] via P2P/IPC
f16n16:1079193:1079526 [1] NCCL INFO Channel 01/0 : 6[1] -> 7[5] via P2P/IPC
f16n16:1079193:1079526 [1] NCCL INFO Connected all rings
f16n16:1079193:1079526 [1] NCCL INFO Channel 01/0 : 6[1] -> 8[3] [send] via NET/IB/2
f16n16:1079193:1079526 [1] NCCL INFO Channel 00/0 : 6[1] -> 9[1] [send] via NET/IB/2
f16n16:1079193:1079526 [1] NCCL INFO Channel 00/0 : 6[1] -> 13[5] [send] via NET/IB/2
f16n16:1079193:1079526 [1] NCCL INFO Channel 00/0 : 13[5] -> 6[1] [receive] via NET/IB/2
f16n16:1079193:1079526 [1] NCCL INFO Channel 00/0 : 9[1] -> 6[1] [receive] via NET/IB/2
f16n16:1079193:1079526 [1] NCCL INFO Channel 01/0 : 8[3] -> 6[1] [receive] via NET/IB/2
f16n16:1079193:1079526 [1] NCCL INFO Connected all trees
f16n16:1079193:1079526 [1] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
f16n16:1079193:1079526 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f16n16:1079193:1079526 [1] NCCL INFO comm 0x14becfd60 rank 6 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0xa49a27aa8b15034e - Init COMPLETE
f17n02:1075538:1075538 [5] NCCL INFO cudaDriverVersion 12020
f17n02:1075538:1075538 [5] NCCL INFO Bootstrap : Using ib0:10.41.14.88<0>
f17n02:1075538:1075538 [5] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
f17n02:1075538:1075538 [5] NCCL INFO NET/Plugin : No plugin found, using internal implementation
f17n02:1075538:1075769 [5] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.14.88<0>
f17n02:1075538:1075769 [5] NCCL INFO Using network IB
f17n02:1075538:1075769 [5] NCCL INFO comm 0x174a841c0 rank 53 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x7b12926388d12ce3 - Init START
f17n02:1075538:1075769 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
f17n02:1075538:1075769 [5] NCCL INFO Trees [0] -1/-1/-1->53->52 [1] 49/-1/-1->53->51 [2] -1/-1/-1->53->52 [3] 49/-1/-1->53->51
f17n02:1075538:1075769 [5] NCCL INFO P2P Chunksize set to 131072
f17n02:1075538:1075769 [5] NCCL INFO Channel 00/0 : 53[5] -> 54[0] [send] via NET/IB/1
f17n02:1075538:1075769 [5] NCCL INFO Channel 02/0 : 53[5] -> 54[0] [send] via NET/IB/1
f17n02:1075538:1075769 [5] NCCL INFO Channel 01/0 : 53[5] -> 50[2] via P2P/IPC
f17n02:1075538:1075769 [5] NCCL INFO Channel 03/0 : 53[5] -> 50[2] via P2P/IPC
f17n02:1075538:1075769 [5] NCCL INFO Connected all rings
f17n02:1075538:1075769 [5] NCCL INFO Channel 01/0 : 53[5] -> 49[1] via P2P/IPC
f17n02:1075538:1075769 [5] NCCL INFO Channel 03/0 : 53[5] -> 49[1] via P2P/IPC
f17n02:1075538:1075769 [5] NCCL INFO Channel 01/0 : 53[5] -> 51[3] via P2P/IPC
f17n02:1075538:1075769 [5] NCCL INFO Channel 03/0 : 53[5] -> 51[3] via P2P/IPC
f17n02:1075538:1075769 [5] NCCL INFO Channel 00/0 : 53[5] -> 52[4] via P2P/IPC
f17n02:1075538:1075769 [5] NCCL INFO Channel 02/0 : 53[5] -> 52[4] via P2P/IPC
f17n02:1075538:1075769 [5] NCCL INFO Connected all trees
f17n02:1075538:1075769 [5] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
f17n02:1075538:1075769 [5] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
f17n02:1075538:1075769 [5] NCCL INFO comm 0x174a841c0 rank 53 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x7b12926388d12ce3 - Init COMPLETE
f17n02:1075538:1075879 [5] NCCL INFO Using network IB
f17n02:1075538:1075879 [5] NCCL INFO comm 0x1773e8640 rank 6 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xa77518dab25ce26 - Init START
f17n02:1075538:1075879 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
f17n02:1075538:1075879 [5] NCCL INFO Trees [0] 5/7/-1->6->4 [1] -1/-1/-1->6->5
f17n02:1075538:1075879 [5] NCCL INFO P2P Chunksize set to 131072
f17n02:1075538:1075879 [5] NCCL INFO Channel 00/0 : 5[3] -> 6[5] [receive] via NET/IB/3
f17n02:1075538:1075879 [5] NCCL INFO Channel 01/0 : 5[3] -> 6[5] [receive] via NET/IB/3
f17n02:1075538:1075879 [5] NCCL INFO Channel 00/0 : 6[5] -> 7[1] [send] via NET/IB/3
f17n02:1075538:1075879 [5] NCCL INFO Channel 01/0 : 6[5] -> 7[1] [send] via NET/IB/3
f17n02:1075538:1075879 [5] NCCL INFO Connected all rings
f17n02:1075538:1075879 [5] NCCL INFO Channel 00/0 : 4[1] -> 6[5] [receive] via NET/IB/3
f17n02:1075538:1075879 [5] NCCL INFO Channel 00/0 : 6[5] -> 4[1] [send] via NET/IB/3
f17n02:1075538:1075879 [5] NCCL INFO Channel 00/0 : 7[1] -> 6[5] [receive] via NET/IB/3
f17n02:1075538:1075879 [5] NCCL INFO Channel 00/0 : 6[5] -> 5[3] [send] via NET/IB/3
f17n02:1075538:1075879 [5] NCCL INFO Channel 01/0 : 6[5] -> 5[3] [send] via NET/IB/3
f17n02:1075538:1075879 [5] NCCL INFO Connected all trees
f17n02:1075538:1075879 [5] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
f17n02:1075538:1075879 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f17n02:1075538:1075879 [5] NCCL INFO comm 0x1773e8640 rank 6 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xa77518dab25ce26 - Init COMPLETE
f17n02:1075538:1075900 [5] NCCL INFO Using network IB
f17n02:1075538:1075900 [5] NCCL INFO comm 0x1776b1040 rank 13f17n02:1075531:1075531 [1] NCCL INFO cudaDriverVersion 12020
f17n02:1075531:1075531 [1] NCCL INFO Bootstrap : Using ib0:10.41.14.88<0>
f17n02:1075531:1075531 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
f17n02:1075531:1075531 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
f17n02:1075531:1075766 [1] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.14.88<0>
f17n02:1075531:1075766 [1] NCCL INFO Using network IB
f17n02:1075531:1075766 [1] NCCL INFO comm 0x16ce53700 rank 49 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x7b12926388d12ce3 - Init START
f17n02:1075531:1075766 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
f17n02:1075531:1075766 [1] NCCL INFO Trees [0] 50/24/-1->49->48 [1] 48/-1/-1->49->53 [2] 50/-1/-1->49->48 [3] 48/-1/-1->49->53
f17n02:1075531:1075766 [1] NCCL INFO P2P Chunksize set to 131072
f17n02:1075531:1075766 [1] NCCL INFO Channel 00/0 : 49[1] -> 50[2] via P2P/IPC
f17n02:1075531:1075766 [1] NCCL INFO Channel 02/0 : 49[1] -> 50[2] via P2P/IPC
f17n02:1075531:1075766 [1] NCCL INFO Channel 01/0 : 49[1] -> 48[0] via P2P/IPC
f17n02:1075531:1075766 [1] NCCL INFO Channel 03/0 : 49[1] -> 48[0] via P2P/IPC
f17n02:1075531:1075766 [1] NCCL INFO Connected all rings
f17n02:1075531:1075766 [1] NCCL INFO Channel 01/0 : 49[1] -> 53[5] via P2P/IPC
f17n02:1075531:1075766 [1] NCCL INFO Channel 03/0 : 49[1] -> 53[5] via P2P/IPC
f17n02:1075531:1075766 [1] NCCL INFO Channel 00/0 : 24[0] -> 49[1] [receive] via NET/IB/0
f17n02:1075531:1075766 [1] NCCL INFO Channel 00/0 : 49[1] -> 24[0] [send] via NET/IB/0
f17n02:1075531:1075766 [1] NCCL INFO Channel 00/0 : 49[1] -> 48[0] via P2P/IPC
f17n02:1075531:1075766 [1] NCCL INFO Channel 02/0 : 49[1] -> 48[0] via P2P/IPC
f17n02:1075531:1075766 [1] NCCL INFO Connected all trees
f17n02:1075531:1075766 [1] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
f17n02:1075531:1075766 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
f17n02:1075531:1075766 [1] NCCL INFO comm 0x16ce53700 rank 49 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x7b12926388d12ce3 - Init COMPLETE
f17n02:1075531:1075882 [1] NCCL INFO Using network IB
f17n02:1075531:1075882 [1] NCCL INFO comm 0x16eafcef0 rank 6 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x86d434c9d289d59a - Init START
f17n02:1075531:1075882 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
f17n02:1075531:1075882 [1] NCCL INFO Trees [0] 5/7/-1->6->4 [1] -1/-1/-1->6->5
f17n02:1075531:1075882 [1] NCCL INFO P2P Chunksize set to 131072
f17n02:1075531:1075882 [1] NCCL INFO Channel 00/0 : 5[5] -> 6[1] [receive] via NET/IB/2
f17n02:1075531:1075882 [1] NCCL INFO Channel 01/0 : 5[5] -> 6[1] [receive] via NET/IB/2
f17n02:1075531:1075882 [1] NCCL INFO Channel 00/0 : 6[1] -> 7[3] [send] via NET/IB/2
f17n02:1075531:1075882 [1] NCCL INFO Channel 01/0 : 6[1] -> 7[3] [send] via NET/IB/2
f17n02:1075531:1075882 [1] NCCL INFO Connected all rings
f17n02:1075531:1075882 [1] NCCL INFO Channel 00/0 : 4[3] -> 6[1] [receive] via NET/IB/2
f17n02:1075531:1075882 [1] NCCL INFO Channel 00/0 : 6[1] -> 4[3] [send] via NET/IB/2
f17n02:1075531:1075882 [1] NCCL INFO Channel 00/0 : 7[3] -> 6[1] [receive] via NET/IB/2
f17n02:1075531:1075882 [1] NCCL INFO Channel 00/0 : 6[1] -> 5[5] [send] via NET/IB/2
f17n02:1075531:1075882 [1] NCCL INFO Channel 01/0 : 6[1] -> 5[5] [send] via NET/IB/2
f17n02:1075531:1075882 [1] NCCL INFO Connected all trees
f17n02:1075531:1075882 [1] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
f17n02:1075531:1075882 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f17n02:1075531:1075882 [1] NCCL INFO comm 0x16eafcef0 rank 6 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x86d434c9d289d59a - Init COMPLETE
f17n02:1075531:1075901 [1] NCCL INFO Using network IB
f17n02:1075531:1075901 [1] NCCL INFO comm 0x16ec68420 rank 12 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0xa49a27aa8b1503 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xa49a27aa8b15034e - Init START
f17n02:1075538:1075900 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
f17n02:1075538:1075900 [5] NCCL INFO Trees [0] 6/-1/-1->13->12 [1] -1/-1/-1->13->12
f17n02:1075538:1075900 [5] NCCL INFO P2P Chunksize set to 131072
f17n02:1075538:1075900 [5] NCCL INFO Channel 00/0 : 13[5] -> 14[3] [send] via NET/IB/3
f17n02:1075538:1075900 [5] NCCL INFO Channel 01/0 : 13[5] -> 14[3] [send] via NET/IB/3
f17n02:1075538:1075900 [5] NCCL INFO Connected all rings
f17n02:1075538:1075900 [5] NCCL INFO Channel 00/0 : 6[1] -> 13[5] [receive] via NET/IB/2
f17n02:1075538:1075900 [5] NCCL INFO Channel 00/0 : 13[5] -> 6[1] [send] via NET/IB/2
f17n02:1075538:1075900 [5] NCCL INFO Channel 00/0 : 13[5] -> 12[1] via P2P/IPC
f17n02:1075538:1075900 [5] NCCL INFO Channel 01/0 : 13[5] -> 12[1] via P2P/IPC
f17n02:1075538:1075900 [5] NCCL INFO Connected all trees
f17n02:1075538:1075900 [5] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
f17n02:1075538:1075900 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f17n02:1075538:1075900 [5] NCCL INFO comm 0x1776b1040 rank 13 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xa49a27aa8b15034e - Init COMPLETE
4e - Init START
f17n02:1075531:1075901 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
f17n02:1075531:1075901 [1] NCCL INFO Trees [0] 13/18/-1->12->0 [1] 13/-1/-1->12->14
f17n02:1075531:1075901 [1] NCCL INFO P2P Chunksize set to 131072
f17n02:1075531:1075901 [1] NCCL INFO Channel 00/0 : 11[3] -> 12[1] [receive] via NET/IB/2
f17n02:1075531:1075901 [1] NCCL INFO Channel 01/0 : 11[3] -> 12[1] [receive] via NET/IB/2
f17n02:1075531:1075901 [1] NCCL INFO Channel 00/0 : 12[1] -> 13[5] via P2P/IPC
f17n02:1075531:1075901 [1] NCCL INFO Channel 01/0 : 12[1] -> 13[5] via P2P/IPC
f17n02:1075531:1075901 [1] NCCL INFO Connected all rings
f17n02:1075531:1075901 [1] NCCL INFO Channel 01/0 : 12[1] -> 14[3] [send] via NET/IB/2
f17n02:1075531:1075901 [1] NCCL INFO Channel 00/0 : 12[1] -> 18[1] [send] via NET/IB/2
f17n02:1075531:1075901 [1] NCCL INFO Channel 00/0 : 0[1] -> 12[1] [receive] via NET/IB/2
f17n02:1075531:1075901 [1] NCCL INFO Channel 00/0 : 12[1] -> 0[1] [send] via NET/IB/2
f17n02:1075531:1075901 [1] NCCL INFO Channel 00/0 : 18[1] -> 12[1] [receive] via NET/IB/2
f17n02:1075531:1075901 [1] NCCL INFO Channel 01/0 : 14[3] -> 12[1] [receive] via NET/IB/2
f17n02:1075531:1075901 [1] NCCL INFO Connected all trees
f17n02:1075531:1075901 [1] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
f17n02:1075531:1075901 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f17n02:1075531:1075901 [1] NCCL INFO comm 0x16ec68420 rank 12 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0xa49a27aa8b15034e - Init COMPLETE
f17n02:1075534:1075534 [3] NCCL INFO cudaDriverVersion 12020
f17n02:1075534:1075534 [3] NCCL INFO Bootstrap : Using ib0:10.41.14.88<0>
f17n02:1075534:1075534 [3] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
f17n02:1075534:1075534 [3] NCCL INFO NET/Plugin : No plugin found, using internal implementation
f17n02:1075534:1075771 [3] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.14.88<0>
f17n02:1075534:1075771 [3] NCCL INFO Using network IB
f17n02:1075534:1075771 [3] NCCL INFO comm 0x14f1f3e40 rank 51 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x7b12926388d12ce3 - Init START
f17n02:1075534:1075771 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
f17n02:1075534:1075771 [3] NCCL INFO Trees [0] 52/-1/-1->51->50 [1] 53/28/-1->51->52 [2] 52/-1/-1->51->50 [3] 53/-1/-1->51->52
f17n02:1075534:1075771 [3] NCCL INFO P2P Chunksize set to 131072
f17n02:1075534:1075771 [3] NCCL INFO Channel 00/0 : 51[3] -> 52[4] via P2P/IPC
f17n02:1075534:1075771 [3] NCCL INFO Channel 01/0 : 51[3] -> 52[4] via P2P/IPC
f17n02:1075534:1075771 [3] NCCL INFO Channel 02/0 : 51[3] -> 52[4] via P2P/IPC
f17n02:1075534:1075771 [3] NCCL INFO Channel 03/0 : 51[3] -> 52[4] via P2P/IPC
f17n02:1075534:1075771 [3] NCCL INFO Channel 01/0 : 42[0] -> 51[3] [receive] via NET/IB/3
f17n02:1075534:1075771 [3] NCCL INFO Channel 03/0 : 42[0] -> 51[3] [receive] via NET/IB/3
f17n02:1075534:1075771 [3] NCCL INFO Connected all rings
f17n02:1075534:1075771 [3] NCCL INFO Channel 01/0 : 51[3] -> 53[5] via P2P/IPC
f17n02:1075534:1075771 [3] NCCL INFO Channel 03/0 : 51[3] -> 53[5] via P2P/IPC
f17n02:1075534:1075771 [3] NCCL INFO Channel 01/0 : 28[4] -> 51[3] [receive] via NET/IB/3
f17n02:1075534:1075771 [3] NCCL INFO Channel 01/0 : 51[3] -> 28[4] [send] via NET/IB/3
f17n02:1075534:1075771 [3] NCCL INFO Channel 00/0 : 51[3] -> 50[2] via P2P/IPC
f17n02:1075534:1075771 [3] NCCL INFO Channel 02/0 : 51[3] -> 50[2] via P2P/IPC
f17n02:1075534:1075771 [3] NCCL INFO Connected all trees
f17n02:1075534:1075771 [3] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
f17n02:1075534:1075771 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
f17n02:1075534:1075771 [3] NCCL INFO comm 0x14f1f3e40 rank 51 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x7b12926388d12ce3 - Init COMPLETE
f17n02:1075534:1075881 [3] NCCL INFO Using network IB
f17n02:1075534:1075881 [3] NCCL INFO comm 0x152087bb0 rank 6 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x1defb2708b178c98 - Init START
f17n02:1075534:1075881 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
f17n02:1075534:1075881 [3] NCCL INFO Trees [0] 5/7/-1->6->4 [1] -1/-1/-1->6->5
f17n02:1075534:1075881 [3] NCCL INFO P2P Chunksize set to 131072
f17n02:1075534:1075881 [3] NCCL INFO Channel 00/0 : 5[1] -> 6[3] [receive] via NET/IB/3
f17n02:1075534:1075881 [3] NCCL INFO Channel 01/0 : 5[1] -> 6[3] [receive] via NET/IB/3
f17n02:1075534:1075881 [3] NCCL INFO Channel 00/0 : 6[3] -> 7[5] [send] via NET/IB/3
f17n02:1075534:1075881 [3] NCCL INFO Channel 01/0 : 6[3] -> 7[5] [send] via NET/IB/3
f17n02:1075534:1075881 [3] NCCL INFO Connected all rings
f17n02:1075534:1075881 [3] NCCL INFO Channel 00/0 : 4[5] -> 6[3] [receive] via NET/IB/3
f17n02:1075534:1075881 [3] NCCL INFO Channel 00/0 : 6[3] -> 4[5] [send] via NET/IB/3
f17n02:1075534:1075881 [3] NCCL INFO Channel 00/0 : 7[5] -> 6[3] [receive] via NET/IB/3
f17n02:1075534:1075881 [3] NCCL INFO Channel 00/0 : 6[3] -> 5[1] [send] via NET/IB/3
f17n02:1075534:1075881 [3] NCCL INFO Channel 01/0 : 6[3] -> 5[1] [send] via NET/IB/3
f17n02:1075534:1075881 [3] NCCL INFO Connected all trees
f17n02:1075534:1075881 [3] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
f17n02:1075534:1075881 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f17n02:1075534:1075881 [3] NCCL INFO comm 0x152087bb0 rank 6 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x1defb2708b178c98 - Init COMPLETE
f17n02:1075534:1075903 [3] NCCL INFO Using network IB
f17n02:1075534:1075903 [3] NCCL INFO comm 0x151ee0020 rank 12 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x821cecfd780eaba1 - Init START
f17n02:1075534:1075903 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
f17n02:1075534:1075903 [3] NCCL INFO Trees [0] 6/18/-1->12->0 [1] -1/-1/-1->12->13
f17n02:1075534:1075903 [3] NCCL INFO P2P Chunksize set to 131072
f17n02:1075534:1075903 [3] NCCL INFO Channel 00/0 : 11[5] -> 12[3] [receive] via NET/IB/3
f17n02:1075534:1075903 [3] NCCL INFO Channel 01/0 : 11[5] -> 12[3] [receive] via NET/IB/3
f17n02:1075534:1075903 [3] NCCL INFO Channel 00/0 : 12[3] -> 13[1] [send] via NET/IB/3
f17n02:1075534:1075903 [3] NCCL INFO Channel 01/0 : 12[3] -> 13[1] [send] via NET/IB/3
f17n02:1075534:1075903 [3] NCCL INFO Connected all rings
f17n02:1075534:1075903 [3] NCCL INFO Channel 00/0 : 6[3] -> 12[3] [receive] via NET/IB/3
f17n02:1075534:1075903 [3] NCCL INFO Channel 00/0 : 12[3] -> 18[3] [send] via NET/IB/3
f17n02:1075534:1075903 [3] NCCL INFO Channel 00/0 : 0[3] -> 12[3] [receive] via NET/IB/3
f17n02:1075534:1075903 [3] NCCL INFO Channel 00/0 : 12[3] -> 0[3] [send] via NET/IB/3
f17n02:1075534:1075903 [3] NCCL INFO Channel 00/0 : 18[3] -> 12[3] [receive] via NET/IB/3
f17n02:1075534:1075903 [3] NCCL INFO Channel 00/0 : 12[3] -> 6[3] [send] via NET/IB/3
f17n02:1075534:1075903 [3] NCCL INFO Channel 01/0 : 13[1] -> 12[3] [receive] via NET/IB/3
f17n02:1075534:1075903 [3] NCCL INFO Connected all trees
f17n02:1075534:1075903 [3] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
f17n02:1075534:1075903 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f17n02:1075534:1075903 [3] NCCL INFO comm 0x151ee0020 rank 12 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x821cecfd780eaba1 - Init COMPLETE
g04n17:914200:914200 [4] NCCL INFO cudaDriverVersion 12020
g04n17:914200:914200 [4] NCCL INFO Bootstrap : Using ib0:10.41.16.11<0>
g04n17:914200:914200 [4] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g04n17:914200:914200 [4] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g04n17:914200:914430 [4] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.16.11<0>
g04n17:914200:914430 [4] NCCL INFO Using network IB
g04n17:914200:914430 [4] NCCL INFO comm 0x14da43d80 rank 70 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x7b12926388d12ce3 - Init START
g04n17:914200:914430 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g04n17:914200:914430 [4] NCCL INFO Trees [0] 71/-1/-1->70->69 [1] 69/-1/-1->70->64 [2] 71/-1/-1->70->69 [3] 69/58/-1->70->45
g04n17:914200:914430 [4] NCCL INFO P2P Chunksize set to 131072
g04n17:914200:914430 [4] NCCL INFO Channel 00/0 : 70[4] -> 71[5] via P2P/IPC
g04n17:914200:914430 [4] NCCL INFO Channel 01/0 : 70[4] -> 71[5] via P2P/IPC
g04n17:914200:914430 [4] NCCL INFO Channel 02/0 : 70[4] -> 71[5] via P2P/IPC
g04n17:914200:914430 [4] NCCL INFO Channel 03/0 : 70[4] -> 71[5] via P2P/IPC
g04n17:914200:914430 [4] NCCL INFO Connected all rings
g04n17:914200:914430 [4] NCCL INFO Channel 01/0 : 64[4] -> 70[4] [receive] via NET/IB/3
g04n17:914200:914430 [4] NCCL INFO Channel 03/0 : 58[4] -> 70[4] [receive] via NET/IB/3
g04n17:914200:914430 [4] NCCL INFO Channel 03/0 : 45[3] -> 70[4] [receive] via NET/IB/3
g04n17:914200:914430 [4] NCCL INFO Channel 03/0 : 70[4] -> 45[3] [send] via NET/IB/3
g04n17:914200:914430 [4] NCCL INFO Channel 03/0 : 70[4] -> 58[4] [send] via NET/IB/3
g04n17:914200:914430 [4] NCCL INFO Channel 01/0 : 70[4] -> 64[4] [send] via NET/IB/3
g04n17:914200:914430 [4] NCCL INFO Channel 00/0 : 70[4] -> 69[3] via P2P/IPC
g04n17:914200:914430 [4] NCCL INFO Channel 01/0 : 70[4] -> 69[3] via P2P/IPC
g04n17:914200:914430 [4] NCCL INFO Channel 02/0 : 70[4] -> 69[3] via P2P/IPC
g04n17:914200:914430 [4] NCCL INFO Channel 03/0 : 70[4] -> 69[3] via P2P/IPC
g04n17:914200:914430 [4] NCCL INFO Connected all trees
g04n17:914200:914430 [4] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g04n17:914200:914430 [4] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g04n17:914200:914430 [4] NCCL INFO comm 0x14da43d80 rank 70 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x7b12926388d12ce3 - Init COMPLETE
g04n17:914200:914514 [4] NCCL INFO Using network IB
g04n17:914200:914514 [4] NCCL INFO comm 0x150914dc0 rank 8 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x414883fe5ee4d1ab - Init START
g04n17:914200:914514 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g04n17:914200:914514 [4] NCCL INFO Trees [0] 4/10/-1->8->0 [1] -1/-1/-1->8->9
g04n17:914200:914514 [4] NCCL INFO P2P Chunksize set to 131072
g04n17:914200:914514 [4] NCCL INFO Channel 00/0 : 7[2] -> 8[4] [receive] via NET/IB/1
g04n17:914200:914514 [4] NCCL INFO Channel 01/0 : 7[2] -> 8[4] [receive] via NET/IB/1
g04n17:914200:914514 [4] NCCL INFO Channel 00/0 : 8[4] -> 9[0] [send] via NET/IB/1
g04n17:914200:914514 [4] NCCL INFO Channel 01/0 : 8[4] -> 9[0] [send] via NET/IB/1
g04n17:914200:914514 [4] NCCL INFO Connected all rings
g04n17:914200:914514 [4] NCCL INFO Channel 00/0 : 8[4] -> 10[2] [send] via NET/IB/1
g04n17:914200:914514 [4] NCCL INFO Channel 00/0 : 4[2] -> 8[4] [receive] via NET/IB/1
g04n17:914200:914514 [4] NCCL INFO Channel 00/0 : 8[4] -> 0[0] [send] via NET/IB/1
g04n17:914200:914514 [4] NCCL INFO Channel 00/0 : 0[0] -> 8[4] [receive] via NET/IB/1
g04n17:914200:914514 [4] NCCL INFO Channel 00/0 : 8[4] -> 4[2] [send] via NET/IB/1
g04n17:914200:914514 [4] NCCL INFO Channel 00/0 : 10[2] -> 8[4] [receive] via NET/IB/1
g04n17:914200:914514 [4] NCCL INFO Channel 01/0 : 9[0] -> 8[4] [receive] via NET/IB/1
g04n17:914200:914514 [4] NCCL INFO Connected all trees
g04n17:914200:914514 [4] NCCL d11n16:1129246:1129246 [1] NCCL INFO cudaDriverVersion 12020
d11n16:1129246:1129246 [1] NCCL INFO Bootstrap : Using ib0:10.41.8.216<0>
d11n16:1129246:1129246 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
d11n16:1129246:1129246 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
d11n16:1129246:1129475 [1] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.8.216<0>
d11n16:1129246:1129475 [1] NCCL INFO Using network IB
d11n16:1129246:1129475 [1] NCCL INFO comm 0x142c648e0 rank 13 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x7b12926388d12ce3 - Init START
d11n16:1129246:1129475 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
d11n16:1129246:1129475 [1] NCCL INFO Trees [0] 14/6/-1->13->12 [1] 12/-1/-1->13->17 [2] 14/-1/-1->13->12 [3] 12/-1/-1->13->17
d11n16:1129246:1129475 [1] NCCL INFO P2P Chunksize set to 131072
d11n16:1129246:1129475 [1] NCCL INFO Channel 00/0 : 13[1] -> 14[2] via P2P/IPC
d11n16:1129246:1129475 [1] NCCL INFO Channel 02/0 : 13[1] -> 14[2] via P2P/IPC
d11n16:1129246:1129475 [1] NCCL INFO Channel 01/0 : 13[1] -> 12[0] via P2P/IPC
d11n16:1129246:1129475 [1] NCCL INFO Channel 03/0 : 13[1] -> 12[0] via P2P/IPC
d11n16:1129246:1129475 [1] NCCL INFO Connected all rings
d11n16:1129246:1129475 [1] NCCL INFO Channel 01/0 : 13[1] -> 17[5] via P2P/IPC
d11n16:1129246:1129475 [1] NCCL INFO Channel 03/0 : 13[1] -> 17[5] via P2P/IPC
d11n16:1129246:1129475 [1] NCCL INFO Channel 00/0 : 6[0] -> 13[1] [receive] via NET/IB/0
d11n16:1129246:1129475 [1] NCCL INFO Channel 00/0 : 13[1] -> 6[0] [send] via NET/IB/0
d11n16:1129246:1129475 [1] NCCL INFO Channel 00/0 : 13[1] -> 12[0] via P2P/IPC
d11n16:1129246:1129475 [1] NCCL INFO Channel 02/0 : 13[1] -> 12[0] via P2P/IPC
d11n16:1129246:1129475 [1] NCCL INFO Connected all trees
d11n16:1129246:1129475 [1] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
d11n16:1129246:1129475 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
d11n16:1129246:1129475 [1] NCCL INFO comm 0x142c648e0 rank 13 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x7b12926388d12ce3 - Init COMPLETE
d11n16:1129246:1129557 [1] NCCL INFO Using network IB
d11n16:1129246:1129557 [1] NCCL INFO comm 0x14491e670 rank 1 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0xa77518dab25ce26 - Init START
d11n16:1129246:1129557 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
d11n16:1129246:1129557 [1] NCCL INFO Trees [0] -1/-1/-1->1->2 [1] 2/0/-1->1->3
d11n16:1129246:1129557 [1] NCCL INFO P2P Chunksize set to 131072
d11n16:1129246:1129557 [1] NCCL INFO Channel 00/0 : 0[5] -> 1[1] [receive] via NET/IB/2
d11n16:1129246:1129557 [1] NCCL INFO Channel 01/0 : 0[5] -> 1[1] [receive] via NET/IB/2
d11n16:1129246:1129557 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[3] [send] via NET/IB/2
d11n16:1129246:1129557 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[3] [send] via NET/IB/2
d11n16:1129246:1129557 [1] NCCL INFO Connected all rings
d11n16:1129246:1129557 [1] NCCL INFO Channel 01/0 : 1[1] -> 3[5] [send] via NET/IB/2
d11n16:1129246:1129557 [1] NCCL INFO Channel 01/0 : 3[5] -> 1[1] [receive] via NET/IB/2
d11n16:1129246:1129557 [1] NCCL INFO Channel 00/0 : 2[3] -> 1[1] [receive] via NET/IB/2
d11n16:1129246:1129557 [1] NCCL INFO Channel 01/0 : 2[3] -> 1[1] [receive] via NET/IB/2
d11n16:1129246:1129557 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[5] [send] via NET/IB/2
d11n16:1129246:1129557 [1] NCCL INFO Connected all trees
d11n16:1129246:1129557 [1] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
d11n16:1129246:1129557 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d11n16:1129246:1129557 [1] NCCL INFO comm 0x14491e670 rank 1 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0xa77518dab25ce26 - Init COMPLETE
d11n16:1129246:1129580 [1] NCCL INFO Using network IB
d11n16:1129246:1129580 [1] NCCL INFO comm 0x145b57760 rank 3 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0xa49a27aa8b15034e - Init START
d11n16:1129246:1129580 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
d11n16:1129246:1129580 [1] NCCL INFO Trees [0] 4/5/-1->3->7 [1] 4/-1/-1->3->2
d11n16:1129246:1129580 [1] NCCL INFO P2P Chunksize set to 131072
d11n16:1129246:1129580 [1] NCCL INFO Channel 00/0 : 2[3] -> 3[1] [receive] via NET/IB/2
d11n16:1129246:1129580 [1] NCCL INFO Channel 01/0 : 2[3] -> 3[1] [receive] via NET/IB/2
d11n16:1129246:1129580 [1] NCCL INFO Channel 00/0 : 3[1] -> 4[5] via P2P/IPC
d11n16:1129246:1129580 [1] NCCL INFO Channel 01/0 : 3[1] -> 4[5] via P2P/IPC
d11n16:1129246:1129580 [1] NCCL INFO Connected all rings
d11n16:1129246:1129580 [1] NCCL INFO Channel 00/0 : 3[1] -> 5[3] [send] via NET/IB/2
d11n16:1129246:1129580 [1] NCCL INFO Channel 00/0 : 3[1] -> 7[5] [send] via NET/IB/2
d11n16:1129246:1129580 [1] NCCL INFO Channel 00/0 : 7[5] -> 3[1] [receive] via NET/IB/2
d11n16:1129246:1129580 [1] NCCL INFO Channel 00/0 : 5[3] -> 3[1] [receive] via NET/IB/2
d11n16:1129246:1129580 [1] NCCL INFO Channel 01/0 : 3[1] -> 2[3] [send] via NET/IB/2
d11n16:1129246:1129580 [1] NCCL INFO Connected all trees
d11n16:1129246:1129580 [1] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
d11n16:1129246:1129580 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d11n16:1129246:1129580 [1] NCCL INFO comm 0x145b57760 rank 3 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0xa49a27aa8b15034e - Init COMPLETE
INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g04n17:914200:914514 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g04n17:914200:914514 [4] NCCL INFO comm 0x150914dc0 rank 8 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x414883fe5ee4d1ab - Init COMPLETE
g04n17:914200:914534 [4] NCCL INFO Using network IB
g04n17:914200:914534 [4] NCCL INFO comm 0x1506f1440 rank 17 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x32e49bb35e6c8ace - Init START
g04n17:914200:914534 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g04n17:914200:914534 [4] NCCL INFO Trees [0] -1/-1/-1->17->16 [1] 19/-1/-1->17->16
g04n17:914200:914534 [4] NCCL INFO P2P Chunksize set to 131072
g04n17:914200:914534 [4] NCCL INFO Channel 00/0 : 17[4] -> 18[2] [send] via NET/IB/1
g04n17:914200:914534 [4] NCCL INFO Channel 01/0 : 17[4] -> 18[2] [send] via NET/IB/1
g04n17:914200:914534 [4] NCCL INFO Connected all rings
g04n17:914200:914534 [4] NCCL INFO Channel 01/0 : 17[4] -> 19[0] [send] via NET/IB/0
g04n17:914200:914534 [4] NCCL INFO Channel 01/0 : 19[0] -> 17[4] [receive] via NET/IB/0
g04n17:914200:914534 [4] NCCL INFO Channel 00/0 : 17[4] -> 16[0] via P2P/IPC
g04n17:914200:914534 [4] NCCL INFO Channel 01/0 : 17[4] -> 16[0] via P2P/IPC
g04n17:914200:914534 [4] NCCL INFO Connected all trees
g04n17:914200:914534 [4] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g04n17:914200:914534 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g04n17:914200:914534 [4] NCCL INFO comm 0x1506f1440 rank 17 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x32e49bb35e6c8ace - Init COMPLETE
d11n16:1129248:1129248 [3] NCCL INFO cudaDriverVersion 12020
d11n16:1129248:1129248 [3] NCCL INFO Bootstrap : Using ib0:10.41.8.216<0>
d11n16:1129248:1129248 [3] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
d11n16:1129248:1129248 [3] NCCL INFO NET/Plugin : No plugin found, using internal implementation
d11n16:1129248:1129477 [3] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.8.216<0>
d11n16:1129248:1129477 [3] NCCL INFO Using network IB
d11n16:1129248:1129477 [3] NCCL INFO comm 0x133fc4120 rank 15 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x7b12926388d12ce3 - Init START
d11n16:1129248:1129477 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
d11n16:1129248:1129477 [3] NCCL INFO Trees [0] 16/-1/-1->15->14 [1] 17/10/-1->15->16 [2] 16/-1/-1->15->14 [3] 17/-1/-1->15->16
d11n16:1129248:1129477 [3] NCCL INFO P2P Chunksize set to 131072
d11n16:1129248:1129477 [3] NCCL INFO Channel 00/0 : 15[3] -> 16[4] via P2P/IPC
d11n16:1129248:1129477 [3] NCCL INFO Channel 01/0 : 15[3] -> 16[4] via P2P/IPC
d11n16:1129248:1129477 [3] NCCL INFO Channel 02/0 : 15[3] -> 16[4] via P2P/IPC
d11n16:1129248:1129477 [3] NCCL INFO Channel 03/0 : 15[3] -> 16[4] via P2P/IPC
d11n16:1129248:1129477 [3] NCCL INFO Channel 01/0 : 6[0] -> 15[3] [receive] via NET/IB/3
d11n16:1129248:1129477 [3] NCCL INFO Channel 03/0 : 6[0] -> 15[3] [receive] via NET/IB/3
d11n16:1129248:1129477 [3] NCCL INFO Connected all rings
d11n16:1129248:1129477 [3] NCCL INFO Channel 01/0 : 15[3] -> 17[5] via P2P/IPC
d11n16:1129248:1129477 [3] NCCL INFO Channel 03/0 : 15[3] -> 17[5] via P2P/IPC
d11n16:1129248:1129477 [3] NCCL INFO Channel 01/0 : 10[4] -> 15[3] [receive] via NET/IB/3
d11n16:1129248:1129477 [3] NCCL INFO Channel 01/0 : 15[3] -> 10[4] [send] via NET/IB/3
d11n16:1129248:1129477 [3] NCCL INFO Channel 00/0 : 15[3] -> 14[2] via P2P/IPC
d11n16:1129248:1129477 [3] NCCL INFO Channel 02/0 : 15[3] -> 14[2] via P2P/IPC
d11n16:1129248:1129477 [3] NCCL INFO Connected all trees
d11n16:1129248:1129477 [3] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
d11n16:1129248:1129477 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
d11n16:1129248:1129477 [3] NCCL INFO comm 0x133fc4120 rank 15 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x7b12926388d12ce3 - Init COMPLETE
d11n16:1129248:1129560 [3] NCCL INFO Using network IB
d11n16:1129248:1129560 [3] NCCL INFO comm 0x136c42520 rank 1 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x54676b30da0675d6 - Init START
d11n16:1129248:1129560 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
d11n16:1129248:1129560 [3] NCCL INFO Trees [0] -1/-1/-1->1->2 [1] 2/0/-1->1->3
d11n16:1129248:1129560 [3] NCCL INFO P2P Chunksize set to 131072
d11n16:1129248:1129560 [3] NCCL INFO Channel 00/0 : 0[1] -> 1[3] [receive] via NET/IB/3
d11n16:1129248:1129560 [3] NCCL INFO Channel 01/0 : 0[1] -> 1[3] [receive] via NET/IB/3
d11n16:1129248:1129560 [3] NCCL INFO Channel 00/0 : 1[3] -> 2[5] [send] via NET/IB/3
d11n16:1129248:1129560 [3] NCCL INFO Channel 01/0 : 1[3] -> 2[5] [send] via NET/IB/3
d11n16:1129248:1129560 [3] NCCL INFO Connected all rings
d11n16:1129248:1129560 [3] NCCL INFO Channel 01/0 : 1[3] -> 3[1] [send] via NET/IB/3
d11n16:1129248:1129560 [3] NCCL INFO Channel 01/0 : 3[1] -> 1[3] [receive] via NET/IB/3
d11n16:1129248:1129560 [3] NCCL INFO Channel 00/0 : 2[5] -> 1[3] [receive] via NET/IB/3
d11n16:1129248:1129560 [3] NCCL INFO Channel 01/0 : 2[5] -> 1[3] [receive] via NET/IB/3
d11n16:1129248:1129560 [3] NCCL INFO Channel 01/0 : 1[3] -> 0[1] [send] via NET/IB/3
d11n16:1129248:1129560 [3] NCCL INFO Connected all trees
d11n16:1129248:1129560 [3] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
d11n16:1129248:1129560 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d11n16:1129248:1129560 [3] NCCL INFO comm 0x136c42520 rank 1 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 comg04n17:914193:914193 [0] NCCL INFO cudaDriverVersion 12020
g04n17:914193:914193 [0] NCCL INFO Bootstrap : Using ib0:10.41.16.11<0>
g04n17:914193:914193 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g04n17:914193:914193 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g04n17:914193:914428 [0] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.16.11<0>
g04n17:914193:914428 [0] NCCL INFO Using network IB
g04n17:914193:914428 [0] NCCL INFO comm 0x129a63bc0 rank 66 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x7b12926388d12ce3 - Init START
g04n17:914193:914428 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g04n17:914193:914428 [0] NCCL INFO Trees [0] 67/-1/-1->66->60 [1] 68/-1/-1->66->67 [2] 67/54/-1->66->43 [3] 68/-1/-1->66->67
g04n17:914193:914428 [0] NCCL INFO P2P Chunksize set to 131072
g04n17:914193:914428 [0] NCCL INFO Channel 00/0 : 65[5] -> 66[0] [receive] via NET/IB/0
g04n17:914193:914428 [0] NCCL INFO Channel 02/0 : 65[5] -> 66[0] [receive] via NET/IB/0
g04n17:914193:914428 [0] NCCL INFO Channel 00/0 : 66[0] -> 67[1] via P2P/IPC
g04n17:914193:914428 [0] NCCL INFO Channel 02/0 : 66[0] -> 67[1] via P2P/IPC
g04n17:914193:914428 [0] NCCL INFO Channel 01/0 : 66[0] -> 75[3] [send] via NET/IB/2
g04n17:914193:914428 [0] NCCL INFO Channel 03/0 : 66[0] -> 75[3] [send] via NET/IB/2
g04n17:914193:914428 [0] NCCL INFO Connected all rings
g04n17:914193:914428 [0] NCCL INFO Channel 01/0 : 66[0] -> 67[1] via P2P/IPC
g04n17:914193:914428 [0] NCCL INFO Channel 03/0 : 66[0] -> 67[1] via P2P/IPC
g04n17:914193:914428 [0] NCCL INFO Channel 01/0 : 66[0] -> 68[2] via P2P/IPC
g04n17:914193:914428 [0] NCCL INFO Channel 03/0 : 66[0] -> 68[2] via P2P/IPC
g04n17:914193:914428 [0] NCCL INFO Channel 00/0 : 60[0] -> 66[0] [receive] via NET/IB/0
g04n17:914193:914428 [0] NCCL INFO Channel 02/0 : 54[0] -> 66[0] [receive] via NET/IB/0
g04n17:914193:914428 [0] NCCL INFO Channel 02/0 : 43[1] -> 66[0] [receive] via NET/IB/0
g04n17:914193:914428 [0] NCCL INFO Channel 02/0 : 66[0] -> 43[1] [send] via NET/IB/0
g04n17:914193:914428 [0] NCCL INFO Channel 02/0 : 66[0] -> 54[0] [send] via NET/IB/0
g04n17:914193:914428 [0] NCCL INFO Channel 00/0 : 66[0] -> 60[0] [send] via NET/IB/0
g04n17:914193:914428 [0] NCCL INFO Connected all trees
g04n17:914193:914428 [0] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g04n17:914193:914428 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g04n17:914193:914428 [0] NCCL INFO comm 0x129a63bc0 rank 66 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x7b12926388d12ce3 - Init COMPLETE
g04n17:914193:914518 [0] NCCL INFO Using network IB
g04n17:914193:914518 [0] NCCL INFO comm 0x12c6f7a80 rank 8 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0xe7c8afafd6ec6b2d - Init START
g04n17:914193:914518 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g04n17:914193:914518 [0] NCCL INFO Trees [0] 4/10/-1->8->0 [1] -1/-1/-1->8->9
g04n17:914193:914518 [0] NCCL INFO P2P Chunksize set to 131072
g04n17:914193:914518 [0] NCCL INFO Channel 00/0 : 7[4] -> 8[0] [receive] via NET/IB/0
g04n17:914193:914518 [0] NCCL INFO Channel 01/0 : 7[4] -> 8[0] [receive] via NET/IB/0
g04n17:914193:914518 [0] NCCL INFO Channel 00/0 : 8[0] -> 9[2] [send] via NET/IB/0
g04n17:914193:914518 [0] NCCL INFO Channel 01/0 : 8[0] -> 9[2] [send] via NET/IB/0
g04n17:914193:914518 [0] NCCL INFO Connected all rings
g04n17:914193:914518 [0] NCCL INFO Channel 00/0 : 8[0] -> 10[4] [send] via NET/IB/0
g04n17:914193:914518 [0] NCCL INFO Channel 00/0 : 4[4] -> 8[0] [receive] via NET/IB/0
g04n17:914193:914518 [0] NCCL INFO Channel 00/0 : 8[0] -> 0[2] [send] via NET/IB/0
g04n17:914193:914518 [0] NCCL INFO Channel 00/0 : 0[2] -> 8[0] [receive] via NET/IB/0
g04n17:914193:914518 [0] NCCL INFO Channel 00/0 : 8[0] -> 4[4] [send] via NET/IB/0
g04n17:914193:914518 [0] NCCL INFO Channel 00/0 : 10[4] -> 8[0] [receive] via NET/IB/0
g04n17:914193:914518 [0] NCCL INFO Channel 01/0 : 9[2] -> 8[0] [receive] via NET/IB/0
g04n17:914193:914518 [0] NCCL INFO Connected all trees
g04n17:914193:914518 [0] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g04n17:914193:914518 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g04n17:914193:914518 [0] NCCL INFO comm 0x12c6f7a80 rank 8 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0xe7c8afafd6ec6b2d - Init COMPLETE
g04n17:914193:914532 [0] NCCL INFO Using network IB
g04n17:914193:914532 [0] NCCL INFO comm 0x12c394e70 rank 16 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0x32e49bb35e6c8ace - Init START
g04n17:914193:914532 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g04n17:914193:914532 [0] NCCL INFO Trees [0] 17/-1/-1->16->15 [1] 17/13/-1->16->11
g04n17:914193:914532 [0] NCCL INFO P2P Chunksize set to 131072
g04n17:914193:914532 [0] NCCL INFO Channel 00/0 : 15[2] -> 16[0] [receive] via NET/IB/0
g04n17:914193:914532 [0] NCCL INFO Channel 01/0 : 15[2] -> 16[0] [receive] via NET/IB/0
g04n17:914193:914532 [0] NCCL INFO Channel 00/0 : 16[0] -> 17[4] via P2P/IPC
g04n17:914193:914532 [0] NCCL INFO Channel 01/0 : 16[0] -> 17[4] via P2P/IPC
g04n17:914193:914532 [0] NCCL INFO Connected all rings
g04n17:914193:914532 [0] NCCL INFO Channel 01/0 : 13[0] -> 16[0] [receive] via NET/IB/0
g04n17:914193:914532 [0] NCCL INFO Channel 01/0 : 11[4] -> 16[0] [receive] via NET/IB/0
g04n17:914193:914532 [0] NCCL INFO Channel 01/0 : 16[0] -> 11[4] [send] via NET/IB/0
g04n17:914193:914532 [0] NCCL INFO Channel 01/0 : 16[0] -> 13[0] [send] via NET/IB/0
g04n17:914193:914532 [0] NCCL INFO Channel 00/0 : 16[0] -> 15[2] [send] via NET/IB/0
g04n17:914193:914532 [0] NCCL INFO Connected all trees
g04n17:914193:914532 [0] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g04n17:914193:914532 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g04n17:914193:914532 [0] NCCL INFO comm 0x12c394e70 rank 16 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0x32e49bb35e6c8ace - Init COMPLETE
mId 0x54676b30da0675d6 - Init COMPLETE
d11n16:1129248:1129578 [3] NCCL INFO Using network IB
d11n16:1129248:1129578 [3] NCCL INFO comm 0x136ceb2f0 rank 3 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x821cecfd780eaba1 - Init START
d11n16:1129248:1129578 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
d11n16:1129248:1129578 [3] NCCL INFO Trees [0] 1/4/-1->3->6 [1] -1/-1/-1->3->2
d11n16:1129248:1129578 [3] NCCL INFO P2P Chunksize set to 131072
d11n16:1129248:1129578 [3] NCCL INFO Channel 00/0 : 2[5] -> 3[3] [receive] via NET/IB/3
d11n16:1129248:1129578 [3] NCCL INFO Channel 01/0 : 2[5] -> 3[3] [receive] via NET/IB/3
d11n16:1129248:1129578 [3] NCCL INFO Channel 00/0 : 3[3] -> 4[1] [send] via NET/IB/3
d11n16:1129248:1129578 [3] NCCL INFO Channel 01/0 : 3[3] -> 4[1] [send] via NET/IB/3
d11n16:1129248:1129578 [3] NCCL INFO Connected all rings
d11n16:1129248:1129578 [3] NCCL INFO Channel 00/0 : 1[1] -> 3[3] [receive] via NET/IB/3
d11n16:1129248:1129578 [3] NCCL INFO Channel 00/0 : 3[3] -> 6[3] [send] via NET/IB/3
d11n16:1129248:1129578 [3] NCCL INFO Channel 00/0 : 6[3] -> 3[3] [receive] via NET/IB/3
d11n16:1129248:1129578 [3] NCCL INFO Channel 00/0 : 3[3] -> 1[1] [send] via NET/IB/3
d11n16:1129248:1129578 [3] NCCL INFO Channel 00/0 : 4[1] -> 3[3] [receive] via NET/IB/3
d11n16:1129248:1129578 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[5] [send] via NET/IB/3
d11n16:1129248:1129578 [3] NCCL INFO Connected all trees
d11n16:1129248:1129578 [3] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
d11n16:1129248:1129578 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d11n16:1129248:1129578 [3] NCCL INFO comm 0x136ceb2f0 rank 3 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x821cecfd780eaba1 - Init COMPLETE
f16n16:1079200:1079200 [5] NCCL INFO cudaDriverVersion 12020
f16n16:1079200:1079200 [5] NCCL INFO Bootstrap : Using ib0:10.41.14.84<0>
f16n16:1079200:1079200 [5] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
f16n16:1079200:1079200 [5] NCCL INFO NET/Plugin : No plugin found, using internal implementation
f16n16:1079200:1079426 [5] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.14.84<0>
f16n16:1079200:1079426 [5] NCCL INFO Using network IB
f16n16:1079200:1079426 [5] NCCL INFO comm 0x12fef38b0 rank 29 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x7b12926388d12ce3 - Init START
f16n16:1079200:1079426 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
f16n16:1079200:1079426 [5] NCCL INFO Trees [0] -1/-1/-1->29->28 [1] 25/-1/-1->29->27 [2] -1/-1/-1->29->28 [3] 25/-1/-1->29->27
f16n16:1079200:1079426 [5] NCCL INFO P2P Chunksize set to 131072
f16n16:1079200:1079426 [5] NCCL INFO Channel 00/0 : 29[5] -> 30[0] [send] via NET/IB/1
f16n16:1079200:1079426 [5] NCCL INFO Channel 02/0 : 29[5] -> 30[0] [send] via NET/IB/1
f16n16:1079200:1079426 [5] NCCL INFO Channel 01/0 : 29[5] -> 26[2] via P2P/IPC
f16n16:1079200:1079426 [5] NCCL INFO Channel 03/0 : 29[5] -> 26[2] via P2P/IPC
f16n16:1079200:1079426 [5] NCCL INFO Connected all rings
f16n16:1079200:1079426 [5] NCCL INFO Channel 01/0 : 29[5] -> 25[1] via P2P/IPC
f16n16:1079200:1079426 [5] NCCL INFO Channel 03/0 : 29[5] -> 25[1] via P2P/IPC
f16n16:1079200:1079426 [5] NCCL INFO Channel 01/0 : 29[5] -> 27[3] via P2P/IPC
f16n16:1079200:1079426 [5] NCCL INFO Channel 03/0 : 29[5] -> 27[3] via P2P/IPC
f16n16:1079200:1079426 [5] NCCL INFO Channel 00/0 : 29[5] -> 28[4] via P2P/IPC
f16n16:1079200:1079426 [5] NCCL INFO Channel 02/0 : 29[5] -> 28[4] via P2P/IPC
f16n16:1079200:1079426 [5] NCCL INFO Connected all trees
f16n16:1079200:1079426 [5] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
f16n16:1079200:1079426 [5] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
f16n16:1079200:1079426 [5] NCCL INFO comm 0x12fef38b0 rank 29 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x7b12926388d12ce3 - Init COMPLETE
f16n16:1079200:1079511 [5] NCCL INFO Using network IB
f16n16:1079200:1079511 [5] NCCL INFO comm 0x132be7ac0 rank 3 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xa77518dab25ce26 - Init START
f16n16:1079200:1079511 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
f16n16:1079200:1079511 [5] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] 7/1/-1->3->11
f16n16:1079200:1079511 [5] NCCL INFO P2P Chunksize set to 131072
f16n16:1079200:1079511 [5] NCCL INFO Channel 00/0 : 2[3] -> 3[5] [receive] via NET/IB/3
f16n16:1079200:1079511 [5] NCCL INFO Channel 01/0 : 2[3] -> 3[5] [receive] via NET/IB/3
f16n16:1079200:1079511 [5] NCCL INFO Channel 00/0 : 3[5] -> 4[1] [send] via NET/IB/3
f16n16:1079200:1079511 [5] NCCL INFO Channel 01/0 : 3[5] -> 4[1] [send] via NET/IB/3
f16n16:1079200:1079511 [5] NCCL INFO Connected all rings
f16n16:1079200:1079511 [5] NCCL INFO Channel 01/0 : 1[1] -> 3[5] [receive] via NET/IB/3
f16n16:1079200:1079511 [5] NCCL INFO Channel 01/0 : 11[3] -> 3[5] [receive] via NET/IB/3
f16n16:1079200:1079511 [5] NCCL INFO Channel 01/0 : 3[5] -> 7[1] [send] via NET/IB/3
f16n16:1079200:1079511 [5] NCCL INFO Channel 01/0 : 7[1] -> 3[5] [receive] via NET/IB/3
f16n16:1079200:1079511 [5] NCCL INFO Channel 01/0 : 3[5] -> 11[3] [send] via NET/IB/3
f16n16:1079200:1079511 [5] NCCL INFO Channel 01/0 : 3[5] -> 1[1] [send] via NET/IB/3
f16n16:1079200:1079511 [5] NCCL INFO Channel 00/0 : 3[5] -> 2[3] [send] via NET/IB/3
f16n16:1079200:1079511 [5] NCCL INFO Connected all trees
f16n16:1079200:1079511 [5] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
f16n16:1079200:1079511 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f16n16:1079200:1079511 [5] NCCL INFO comm 0x132be7ac0 rank 3 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xa77518dab25ce26 - Init COMPLETE
f16n16:1079200:1079528 [5] NCCL INFO Using network IB
f16n16:1079200:1079528 [5] NCCL INFO comm 0x132853180 rank 7 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xa49a27aa8b15034e - Init START
f16n16:1079200:1079528 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
f16n16:1079200:1079528 [5] NCCL INFO Trees [0] 3/-1/-1->7->6 [1] -1/-1/-1->7->6
f16n16:1079200:1079528 [5] NCCL INFO P2P Chunksize set to 131072
f16n16:1079200:1079528 [5] NCCL INFO Channel 00/0 : 7[5] -> 8[3] [send] via NET/IB/3
f16n16:1079200:1079528 [5] NCCL INFO Channel 01/0 : 7[5] -> 8[3] [send] via NET/IB/3
f16n16:1079200:1079528 [5] NCCL INFO Connected all rings
f16n16:1079200:1079528 [5] NCCL INFO Channel 00/0 : 3[1] -> 7[5] [receive] via NET/IB/2
f16n16:1079200:1079528 [5] NCCL INFO Channel 00/0 : 7[5] -> 3[1] [send] via NET/IB/2
f16n16:1079200:1079528 [5] NCCL INFO Channel 00/0 : 7[5] -> 6[1] via P2P/IPC
f16n16:1079200:1079528 [5] NCCL INFO Channel 01/0 : 7[5] -> 6[1] via P2P/IPC
f16n16:1079200:1079528 [5] NCCL INFO Connected all trees
f16n16:1079200:1079528 [5] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
f16n16:1079200:1079528 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f16n16:1079200:1079528 [5] NCCL INFO comm 0x132853180 rank 7 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xa49a27aa8b15034e - Init COMPLETE
f16n16:1079197:1079197 [3] NCCL INFO cudaDriverVersion 12020
f16n16:1079197:1079197 [3] NCCL INFO Bootstrap : Using ib0:10.41.14.84<0>
f16n16:1079197:1079197 [3] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
f16n16:1079197:1079197 [3] NCCL INFO NET/Plugin : No plugin found, using internal implementation
f16n16:1079197:1079429 [3] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.14.84<0>
f16n16:1079197:1079429 [3] NCCL INFO Using network IB
f16n16:1079197:1079429 [3] NCCL INFO comm 0x123644450 rank 27 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x7b12926388d12ce3 - Init START
f16n16:1079197:1079429 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
f16n16:1079197:1079429 [3] NCCL INFO Trees [0] 28/-1/-1->27->26 [1] 29/16/-1->27->28 [2] 28/-1/-1->27->26 [3] 29/-1/-1->27->28
f16n16:1079197:1079429 [3] NCCL INFO P2P Chunksize set to 131072
f16n16:1079197:1079429 [3] NCCL INFO Channel 00/0 : 27[3] -> 28[4] via P2P/IPC
f16n16:1079197:1079429 [3] NCCL INFO Channel 01/0 : 27[3] -> 28[4] via P2P/IPC
f16n16:1079197:1079429 [3] NCCL INFO Channel 02/0 : 27[3] -> 28[4] via P2P/IPC
f16n16:1079197:1079429 [3] NCCL INFO Channel 03/0 : 27[3] -> 28[4] via P2P/IPC
f16n16:1079197:1079429 [3] NCCL INFO Channel 01/0 : 18[0] -> 27[3] [receive] via NET/IB/3
f16n16:1079197:1079429 [3] NCCL INFO Channel 03/0 : 18[0] -> 27[3] [receive] via NET/IB/3
f16n16:1079197:1079429 [3] NCCL INFO Connected all rings
f16n16:1079197:1079429 [3] NCCL INFO Channel 01/0 : 27[3] -> 29[5] via P2P/IPC
f16n16:1079197:1079429 [3] NCCL INFO Channel 03/0 : 27[3] -> 29[5] via P2P/IPC
f16n16:1079197:1079429 [3] NCCL INFO Channel 01/0 : 16[4] -> 27[3] [receive] via NET/IB/3
f16n16:1079197:1079429 [3] NCCL INFO Channel 01/0 : 27[3] -> 16[4] [send] via NET/IB/3
f16n16:1079197:1079429 [3] NCCL INFO Channel 00/0 : 27[3] -> 26[2] via P2P/IPC
f16n16:1079197:1079429 [3] NCCL INFO Channel 02/0 : 27[3] -> 26[2] via P2P/IPC
f16n16:1079197:1079429 [3] NCCL INFO Connected all trees
f16n16:1079197:1079429 [3] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
f16n16:1079197:1079429 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
f16n16:1079197:1079429 [3] NCCL INFO comm 0x123644450 rank 27 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x7b12926388d12ce3 - Init COMPLETE
f16n16:1079197:1079510 [3] NCCL INFO Using network IB
f16n16:1079197:1079510 [3] NCCL INFO comm 0x125f98660 rank 3 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x1defb2708b178c98 - Init START
f16n16:1079197:1079510 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
f16n16:1079197:1079510 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] 7/1/-1->3->11
f16n16:1079197:1079510 [3] NCCL INFO P2P Chunksize set to 131072
f16n16:1079197:1079510 [3] NCCL INFO Channel 00/0 : 2[1] -> 3[3] [receive] via NET/IB/3
f16n16:1079197:1079510 [3] NCCL INFO Channel 01/0 : 2[1] -> 3[3] [receive] via NET/IB/3
f16n16:1079197:1079510 [3] NCCL INFO Channel 00/0 : 3[3] -> 4[5] [send] via NET/IB/3
f16n16:1079197:1079510 [3] NCCL INFO Channel 01/0 : 3[3] -> 4[5] [send] via NET/IB/3
f16n16:1079197:1079510 [3] NCCL INFO Connected all rings
f16n16:1079197:1079510 [3] NCCL INFO Channel 01/0 : 1[5] -> 3[3] [receive] via NET/IB/3
f16n16:1079197:1079510 [3] NCCL INFO Channel 01/0 : 11[1] -> 3[3] [receive] via NET/IB/3
f16n16:1079197:1079510 [3] NCCL INFO Channel 01/0 : 3[3] -> 7[5] [send] via NET/IB/3
f16n16:1079197:1079510 [3] NCCL INFO Channel 01/0 : 7[5] -> 3[3] [receive] via NET/IB/3
f16n16:1079197:1079510 [3] NCCL INFO Channel 01/0 : 3[3] -> 11[1] [send] via NET/IB/3
f16n16:1079197:1079510 [3] NCCL INFO Channel 01/0 : 3[3] -> 1[5] [send] via NET/IB/3
f16n16:1079197:1079510 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[1] [send] via NET/IB/3
f16n16:1079197:1079510 [3] NCCL INFO Connected all trees
f16n16:1079197:1079510 [3] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
f16n16:1079197:1079510 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f16n16:1079197:1079510 [3] NCCL INFO comm 0x125f98660 rank 3 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x1defb2708b178c98 - Init COMPLETE
f16n16:1079197:1079531 [3] NCCL INFO Using network IB
f16n16:1079197:1079531 [3] NCCL INFO comm 0x1273cd070 rank 6 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x821cecfd780eaba1 - Init START
f16n16:1079197:1079531 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
f16n16:1079197:1079531 [3] NCCL INFO Trees [0] 3/9/-1->6->12 [1] -1/-1/-1->6->7
f16n16:1079197:1079531 [3] NCCL INFO P2P Chunksize set to 131072
f16n16:1079197:1079531 [3] NCCL INFO Channel 00/0 : 5[5] -> 6[3] [receive] via NET/IB/3
f16n16:1079197:1079531 [3] NCCL INFO Channel 01/0 : 5[5] -> 6[3] [receive] via NET/IB/3
f16n16:1079197:1079531 [3] NCCL INFO Channel 00/0 : 6[3] -> 7[1] [send] via NET/IB/3
f16n16:1079197:1079531 [3] NCCL INFO Channel 01/0 : 6[3] -> 7[1] [send] via NET/IB/3
f16n16:1079197:1079531 [3] NCCL INFO Connected all rings
f16n16:1079197:1079531 [3] NCCL INFO Channel 00/0 : 3[3] -> 6[3] [receive] via NET/IB/3
f16n16:1079197:1079531 [3] NCCL INFO Channel 00/0 : 6[3] -> 9[3] [send] via NET/IB/3
f16n16:1079197:1079531 [3] NCCL INFO Channel 00/0 : 6[3] -> 12[3] [send] via NET/IB/3
f16n16:1079197:1079531 [3] NCCL INFO Channel 00/0 : 12[3] -> 6[3] [receive] via NET/IB/3
f16n16:1079197:1079531 [3] NCCL INFO Channel 00/0 : 9[3] -> 6[3] [receive] via NET/IB/3
f16n16:1079197:1079531 [3] NCCL INFO Channel 00/0 : 6[3] -> 3[3] [send] via NET/IB/3
f16n16:1079197:1079531 [3] NCCL INFO Channel 01/0 : 7[1] -> 6[3] [receive] via NET/IB/3
f16n16:1079197:1079531 [3] NCCL INFO Connected all trees
f16n16:1079197:1079531 [3] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
f16n16:1079197:1079531 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f16n16:1079197:1079531 [3] NCCL INFO comm 0x1273cd070 rank 6 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x821cecfd780eaba1 - Init COMPLETE
f16n18:1091134:1091134 [3] NCCL INFO cudaDriverVersion 12020
f16n18:1091134:1091134 [3] NCCL INFO Bootstrap : Using ib0:10.41.14.86<0>
f16n18:1091134:1091134 [3] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
f16n18:1091134:1091134 [3] NCCL INFO NET/Plugin : No plugin found, using internal implementation
f16n18:1091134:1091364 [3] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.14.86<0>
f16n18:1091134:1091364 [3] NCCL INFO Using network IB
f16n18:1091134:1091364 [3] NCCL INFO comm 0x154bf4060 rank 39 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x7b12926388d12ce3 - Init START
f16n18:1091134:1091364 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
f16n18:1091134:1091364 [3] NCCL INFO Trees [0] 40/-1/-1->39->38 [1] 41/34/-1->39->40 [2] 40/-1/-1->39->38 [3] 41/-1/-1->39->40
f16n18:1091134:1091364 [3] NCCL INFO P2P Chunksize set to 131072
f16n18:1091134:1091364 [3] NCCL INFO Channel 00/0 : 39[3] -> 40[4] via P2P/IPC
f16n18:1091134:1091364 [3] NCCL INFO Channel 01/0 : 39[3] -> 40[4] via P2P/IPC
f16n18:1091134:1091364 [3] NCCL INFO Channel 02/0 : 39[3] -> 40[4] via P2P/IPC
f16n18:1091134:1091364 [3] NCCL INFO Channel 03/0 : 39[3] -> 40[4] via P2P/IPC
f16n18:1091134:1091364 [3] NCCL INFO Channel 01/0 : 30[0] -> 39[3] [receive] via NET/IB/3
f16n18:1091134:1091364 [3] NCCL INFO Channel 03/0 : 30[0] -> 39[3] [receive] via NET/IB/3
f16n18:1091134:1091364 [3] NCCL INFO Connected all rings
f16n18:1091134:1091364 [3] NCCL INFO Channel 01/0 : 39[3] -> 41[5] via P2P/IPC
f16n18:1091134:1091364 [3] NCCL INFO Channel 03/0 : 39[3] -> 41[5] via P2P/IPC
f16n18:1091134:1091364 [3] NCCL INFO Channel 01/0 : 34[4] -> 39[3] [receive] via NET/IB/3
f16n18:1091134:1091364 [3] NCCL INFO Channel 01/0 : 39[3] -> 34[4] [send] via NET/IB/3
f16n18:1091134:1091364 [3] NCCL INFO Channel 00/0 : 39[3] -> 38[2] via P2P/IPC
f16n18:1091134:1091364 [3] NCCL INFO Channel 02/0 : 39[3] -> 38[2] via P2P/IPC
f16n18:1091134:1091364 [3] NCCL INFO Connected all trees
f16n18:1091134:1091364 [3] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
f16n18:1091134:1091364 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
f16n18:1091134:1091364 [3] NCCL INFO comm 0x154bf4060 rank 39 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x7b12926388d12ce3 - Init COMPLETE
f16n18:1091134:1091465 [3] NCCL INFO Using network IB
f16n18:1091134:1091465 [3] NCCL INFO comm 0x1589f3680 rank 4 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x54676b30da0675d6 - Init START
f16n18:1091134:1091465 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
f16n18:1091134:1091465 [3] NCCL INFO Trees [0] 2/6/-1->4->8 [1] -1/-1/-1->4->5
f16n18:1091134:1091465 [3] NCCL INFO P2P Chunksize set to 131072
f16n18:1091134:1091465 [3] NCCL INFO Channel 00/0 : 3[1] -> 4[3] [receive] via NET/IB/3
f16n18:1091134:1091465 [3] NCCL INFO Channel 01/0 : 3[1] -> 4[3] [receive] via NET/IB/3
f16n18:1091134:1091465 [3] NCCL INFO Channel 00/0 : 4[3] -> 5[5] [send] via NET/IB/3
f16n18:1091134:1091465 [3] NCCL INFO Channel 01/0 : 4[3] -> 5[5] [send] via NET/IB/3
f16n18:1091134:1091465 [3] NCCL INFO Connected all rings
f16n18:1091134:1091465 [3] NCCL INFO Channel 00/0 : 2[5] -> 4[3] [receive] via NET/IB/3
f16n18:1091134:1091465 [3] NCCL INFO Channel 00/0 : 4[3] -> 6[1] [send] via NET/IB/3
f16n18:1091134:1091465 [3] NCCL INFO Channel 00/0 : 4[3] -> 8[5] [send] via NET/IB/3
f16n18:1091134:1091465 [3] NCCL INFO Channel 00/0 : 8[5] -> 4[3] [receive] via NET/IB/3
f16n18:1091134:1091465 [3] NCCL INFO Channel 00/0 : 6[1] -> 4[3] [receive] via NET/IB/3
f16n18:1091134:1091465 [3] NCCL INFO Channel 00/0 : 4[3] -> 2[5] [send] via NET/IB/3
f16n18:1091134:1091465 [3] NCCL INFO Channel 01/0 : 5[5] -> 4[3] [receive] via NET/IB/3
f16n18:1091134:1091465 [3] NCCL INFO Connected all trees
f16n18:1091134:1091465 [3] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
f16n18:1091134:1091465 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f16n18:1091134:1091465 [3] NCCL INFO comm 0x1589f3680 rank 4 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x54676b30da0675d6 - Init COMPLETE
f16n18:1091134:1091484 [3] NCCL INFO Using network IB
f16n18:1091134:1091484 [3] NCCL INFO comm 0x1574b79c0 rank 9 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x821cecfd780eaba1 - Init START
f16n18:1091134:1091484 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
f16n18:1091134:1091484 [3] NCCL INFO Trees [0] 7/10/-1->9->6 [1] -1/-1/-1->9->8
f16n18:1091134:1091484 [3] NCCL INFO P2P Chunksize set to 131072
f16n18:1091134:1091484 [3] NCCL INFO Channel 00/0 : 8[5] -> 9[3] [receive] via NET/IB/3
f16n18:1091134:1091484 [3] NCCL INFO Channel 01/0 : 8[5] -> 9[3] [receive] via NET/IB/3
f16n18:1091134:1091484 [3] NCCL INFO Channel 00/0 : 9[3] -> 10[1] [send] via NET/IB/3
f16n18:1091134:1091484 [3] NCCL INFO Channel 01/0 : 9[3] -> 10[1] [send] via NET/IB/3
f16n18:1091134:1091484 [3] NCCL INFO Connected all rings
f16n18:1091134:1091484 [3] NCCL INFO Channel 00/0 : 7[1] -> 9[3] [receive] via NET/IB/3
f16n18:1091134:1091484 [3] NCCL INFO Channel 00/0 : 6[3] -> 9[3] [receive] via NET/IB/3
f16n18:1091134:1091484 [3] NCCL INFO Channel 00/0 : 9[3] -> 6[3] [send] via NET/IB/3
f16n18:1091134:1091484 [3] NCCL INFO Channel 00/0 : 9[3] -> 7[1] [send] via NET/IB/3
f16n18:1091134:1091484 [3] NCCL INFO Channel 00/0 : 10[1] -> 9[3] [receive] via NET/IB/3
f16n18:1091134:1091484 [3] NCCL INFO Channel 01/0 : 9[3] -> 8[5] [send] via NET/IB/3
f16n18:1091134:1091484 [3] NCCL INFO Connected all trees
f16n18:1091134:1091484 [3] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
f16n18:1091134:1091484 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f16n18:1091134:1091484 [3] NCCL INFO comm 0x1574b79c0 rank 9 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x821cecfd780eaba1 - Init COMPLETE
g05n10:696643:696643 [0] NCCL INFO cudaDriverVersion 12020
g05n10:696643:696643 [0] NCCL INFO Bootstrap : Using ib0:10.41.16.22<0>
g05n10:696643:696643 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g05n10:696643:696643 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g05n10:696643:696878 [0] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.16.22<0>
g05n10:696643:696878 [0] NCCL INFO Using network IB
g05n10:696643:696878 [0] NCCL INFO comm 0x165ce3d10 rank 90 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x7b12926388d12ce3 - Init START
g05n10:696643:696878 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g05n10:696643:696878 [0] NCCL INFO Trees [0] 91/-1/-1->90->84 [1] 92/-1/-1->90->91 [2] 91/42/-1->90->-1 [3] 92/-1/-1->90->91
g05n10:696643:696878 [0] NCCL INFO P2P Chunksize set to 131072
g05n10:696643:696878 [0] NCCL INFO Channel 00/0 : 89[5] -> 90[0] [receive] via NET/IB/0
g05n10:696643:696878 [0] NCCL INFO Channel 02/0 : 89[5] -> 90[0] [receive] via NET/IB/0
g05n10:696643:696878 [0] NCCL INFO Channel 00/0 : 90[0] -> 91[1] via P2P/IPC
g05n10:696643:696878 [0] NCCL INFO Channel 02/0 : 90[0] -> 91[1] via P2P/IPC
g05n10:696643:696878 [0] NCCL INFO Channel 01/0 : 90[0] -> 3[3] [send] via NET/IB/2
g05n10:696643:696878 [0] NCCL INFO Channel 03/0 : 90[0] -> 3[3] [send] via NET/IB/2
g05n10:696643:696878 [0] NCCL INFO Connected all rings
g05n10:696643:696878 [0] NCCL INFO Channel 01/0 : 90[0] -> 91[1] via P2P/IPC
g05n10:696643:696878 [0] NCCL INFO Channel 03/0 : 90[0] -> 91[1] via P2P/IPC
g05n10:696643:696878 [0] NCCL INFO Channel 01/0 : 90[0] -> 92[2] via P2P/IPC
g05n10:696643:696878 [0] NCCL INFO Channel 03/0 : 90[0] -> 92[2] via P2P/IPC
g05n10:696643:696878 [0] NCCL INFO Channel 00/0 : 84[0] -> 90[0] [receive] via NET/IB/0
g05n10:696643:696878 [0] NCCL INFO Channel 02/0 : 42[0] -> 90[0] [receive] via NET/IB/0
g05n10:696643:696878 [0] NCCL INFO Channel 02/0 : 90[0] -> 42[0] [send] via NET/IB/0
g05n10:696643:696878 [0] NCCL INFO Channel 00/0 : 90[0] -> 84[0] [send] via NET/IB/0
g05n10:696643:696878 [0] NCCL INFO Connected all trees
g05n10:696643:696878 [0] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g05n10:696643:696878 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g05n10:696643:696878 [0] NCCL INFO comm 0x165ce3d10 rank 90 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x7b12926388d12ce3 - Init COMPLETE
g05n10:696643:696963 [0] NCCL INFO Using network IB
g05n10:696643:696963 [0] NCCL INFO comm 0x168625130 rank 11 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0xe7c8afafd6ec6b2d - Init START
g05n10:696643:696963 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g05n10:696643:696963 [0] NCCL INFO Trees [0] -1/-1/-1->11->10 [1] 3/-1/-1->11->-1
g05n10:696643:696963 [0] NCCL INFO P2P Chunksize set to 131072
g05n10:696643:696963 [0] NCCL INFO Channel 00/0 : 10[4] -> 11[0] [receive] via NET/IB/0
g05n10:696643:696963 [0] NCCL INFO Channel 01/0 : 10[4] -> 11[0] [receive] via NET/IB/0
g05n10:696643:696963 [0] NCCL INFO Channel 00/0 : 11[0] -> 0[2] [send] via NET/IB/0
g05n10:696643:696963 [0] NCCL INFO Channel 01/0 : 11[0] -> 0[2] [send] via NET/IB/0
g05n10:696643:696963 [0] NCCL INFO Connected all rings
g05n10:696643:696963 [0] NCCL INFO Channel 01/0 : 11[0] -> 3[2] [send] via NET/IB/0
g05n10:696643:696963 [0] NCCL INFO Channel 01/0 : 3[2] -> 11[0] [receive] via NET/IB/0
g05n10:696643:696963 [0] NCCL INFO Channel 00/0 : 11[0] -> 10[4] [send] via NET/IB/0
g05n10:696643:696963 [0] NCCL INFO Connected all trees
g05n10:696643:696963 [0] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g05n10:696643:696963 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g05n10:696643:696963 [0] NCCL INFO comm 0x168625130 rank 11 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0xe7c8afafd6ec6b2d - Init COMPLETE
g05n10:696643:696984 [0] NCCL INFO Using network IB
g05n10:696643:696984 [0] NCCL INFO comm 0x168920770 rank 22 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0x32e49bb35e6c8ace - Init START
g05n10:696643:696984 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g05n10:696643:696984 [0] NCCL INFO Trees [0] 23/-1/-1->22->21 [1] 23/10/-1->22->-1
g05n10:696643:696984 [0] NCCL INFO P2P Chunksize set to 131072
g05n10:696643:696984 [0] NCCL INFO Channel 00/0 : 21[2] -> 22[0] [receive] via NET/IB/0
g05n10:696643:696984 [0] NCCL INFO Channel 01/0 : 21[2] -> 22[0] [receive] via NET/IB/0
g05n10:696643:696984 [0] NCCL INFO Channel 00/0 : 22[0] -> 23[4] via P2P/IPC
g05n10:696643:696984 [0] NCCL INFO Channel 01/0 : 22[0] -> 23[4] via P2P/IPC
g05n10:696643:696984 [0] NCCL INFO Connected all rings
g05n10:696643:696984 [0] NCCL INFO Channel 01/0 : 10[0] -> 22[0] [receive] via NET/IB/0
g05n10:696643:696984 [0] NCCL INFO Channel 01/0 : 22[0] -> 10[0] [send] via NET/IB/0
g05n10:696643:696984 [0] NCCL INFO Channel 00/0 : 22[0] -> 21[2] [send] via NET/IB/0
g05n10:696643:696984 [0] NCCL INFO Connected all trees
g05n10:696643:696984 [0] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g05n10:696643:696984 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g05n10:696643:696984 [0] NCCL INFO comm 0x168920770 rank 22 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0x32e49bb35e6c8ace - Init COMPLETE
g05n09:221469:221469 [5] NCCL INFO cudaDriverVersion 12020
g05n09:221469:221469 [5] NCCL INFO Bootstrap : Using ib0:10.41.16.21<0>
g05n09:221469:221469 [5] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g05n09:221469:221469 [5] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g05n09:221469:221697 [5] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.16.21<0>
g05n09:221469:221697 [5] NCCL INFO Using network IB
g05n09:221469:221697 [5] NCCL INFO comm 0x16b654220 rank 89 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x7b12926388d12ce3 - Init START
g05n09:221469:221697 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g05n09:221469:221697 [5] NCCL INFO Trees [0] -1/-1/-1->89->88 [1] 85/-1/-1->89->87 [2] -1/-1/-1->89->88 [3] 85/-1/-1->89->87
g05n09:221469:221697 [5] NCCL INFO P2P Chunksize set to 131072
g05n09:221469:221697 [5] NCCL INFO Channel 00/0 : 89[5] -> 90[0] [send] via NET/IB/1
g05n09:221469:221697 [5] NCCL INFO Channel 02/0 : 89[5] -> 90[0] [send] via NET/IB/1
g05n09:221469:221697 [5] NCCL INFO Channel 01/0 : 89[5] -> 86[2] via P2P/IPC
g05n09:221469:221697 [5] NCCL INFO Channel 03/0 : 89[5] -> 86[2] via P2P/IPC
g05n09:221469:221697 [5] NCCL INFO Connected all rings
g05n09:221469:221697 [5] NCCL INFO Channel 01/0 : 89[5] -> 85[1] via P2P/IPC
g05n09:221469:221697 [5] NCCL INFO Channel 03/0 : 89[5] -> 85[1] via P2P/IPC
g05n09:221469:221697 [5] NCCL INFO Channel 01/0 : 89[5] -> 87[3] via P2P/IPC
g05n09:221469:221697 [5] NCCL INFO Channel 03/0 : 89[5] -> 87[3] via P2P/IPC
g05n09:221469:221697 [5] NCCL INFO Channel 00/0 : 89[5] -> 88[4] via P2P/IPC
g05n09:221469:221697 [5] NCCL INFO Channel 02/0 : 89[5] -> 88[4] via P2P/IPC
g05n09:221469:221697 [5] NCCL INFO Connected all trees
g05n09:221469:221697 [5] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g05n09:221469:221697 [5] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g05n09:221469:221697 [5] NCCL INFO comm 0x16b654220 rank 89 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x7b12926388d12ce3 - Init COMPLETE
g05n09:221469:221783 [5] NCCL INFO Using network IB
g05n09:221469:221783 [5] NCCL INFO comm 0x16e2827e0 rank 11 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x86d434c9d289d59a - Init START
g05n09:221469:221783 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g05n09:221469:221783 [5] NCCL INFO Trees [0] -1/-1/-1->11->10 [1] 3/-1/-1->11->-1
g05n09:221469:221783 [5] NCCL INFO P2P Chunksize set to 131072
g05n09:221469:221783 [5] NCCL INFO Channel 00/0 : 10[3] -> 11[5] [receive] via NET/IB/3
g05n09:221469:221783 [5] NCCL INFO Channel 01/0 : 10[3] -> 11[5] [receive] via NET/IB/3
g05n09:221469:221783 [5] NCCL INFO Channel 00/0 : 11[5] -> 0[1] [send] via NET/IB/3
g05n09:221469:221783 [5] NCCL INFO Channel 01/0 : 11[5] -> 0[1] [send] via NET/IB/3
g05n09:221469:221783 [5] NCCL INFO Connected all rings
g05n09:221469:221783 [5] NCCL INFO Channel 01/0 : 11[5] -> 3[1] [send] via NET/IB/3
g05n09:221469:221783 [5] NCCL INFO Channel 01/0 : 3[1] -> 11[5] [receive] via NET/IB/3
g05n09:221469:221783 [5] NCCL INFO Channel 00/0 : 11[5] -> 10[3] [send] via NET/IB/3
g05n09:221469:221783 [5] NCCL INFO Connected all trees
g05n09:221469:221783 [5] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g05n09:221469:221783 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g05n09:221469:221783 [5] NCCL INFO comm 0x16e2827e0 rank 11 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x86d434c9d289d59a - Init COMPLETE
g05n09:221469:221798 [5] NCCL INFO Using network IB
g05n09:221469:221798 [5] NCCL INFO comm 0x16d5622b0 rank 22 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xa49a27aa8b15034e - Init START
g05n09:221469:221798 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g05n09:221469:221798 [5] NCCL INFO Trees [0] 20/-1g05n09:221467:221467 [3] NCCL INFO cudaDriverVersion 12020
g05n09:221467:221467 [3] NCCL INFO Bootstrap : Using ib0:10.41.16.21<0>
g05n09:221467:221467 [3] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g05n09:221467:221467 [3] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g05n09:221467:221696 [3] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.16.21<0>
g05n09:221467:221696 [3] NCCL INFO Using network IB
g05n09:221467:221696 [3] NCCL INFO comm 0x14dd73d40 rank 87 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x7b12926388d12ce3 - Init START
g05n09:221467:221696 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g05n09:221467:221696 [3] NCCL INFO Trees [0] 88/-1/-1->87->86 [1] 89/82/-1->87->88 [2] 88/-1/-1->87->86 [3] 89/-1/-1->87->88
g05n09:221467:221696 [3] NCCL INFO P2P Chunksize set to 131072
g05n09:221467:221696 [3] NCCL INFO Channel 00/0 : 87[3] -> 88[4] via P2P/IPC
g05n09:221467:221696 [3] NCCL INFO Channel 01/0 : 87[3] -> 88[4] via P2P/IPC
g05n09:221467:221696 [3] NCCL INFO Channel 02/0 : 87[3] -> 88[4] via P2P/IPC
g05n09:221467:221696 [3] NCCL INFO Channel 03/0 : 87[3] -> 88[4] via P2P/IPC
g05n09:221467:221696 [3] NCCL INFO Channel 01/0 : 78[0] -> 87[3] [receive] via NET/IB/3
g05n09:221467:221696 [3] NCCL INFO Channel 03/0 : 78[0] -> 87[3] [receive] via NET/IB/3
g05n09:221467:221696 [3] NCCL INFO Connected all rings
g05n09:221467:221696 [3] NCCL INFO Channel 01/0 : 87[3] -> 89[5] via P2P/IPC
g05n09:221467:221696 [3] NCCL INFO Channel 03/0 : 87[3] -> 89[5] via P2P/IPC
g05n09:221467:221696 [3] NCCL INFO Channel 01/0 : 82[4] -> 87[3] [receive] via NET/IB/3
g05n09:221467:221696 [3] NCCL INFO Channel 01/0 : 87[3] -> 82[4] [send] via NET/IB/3
g05n09:221467:221696 [3] NCCL INFO Channel 00/0 : 87[3] -> 86[2] via P2P/IPC
g05n09:221467:221696 [3] NCCL INFO Channel 02/0 : 87[3] -> 86[2] via P2P/IPC
g05n09:221467:221696 [3] NCCL INFO Connected all trees
g05n09:221467:221696 [3] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g05n09:221467:221696 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g05n09:221467:221696 [3] NCCL INFO comm 0x14dd73d40 rank 87 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x7b12926388d12ce3 - Init COMPLETE
g05n09:221467:221781 [3] NCCL INFO Using network IB
g05n09:221467:221781 [3] NCCL INFO comm 0x1509e35b0 rank 10 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x54676b30da0675d6 - Init START
g05n09:221467:221781 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g05n09:221467:221781 [3] NCCL INFO Trees [0] 9/11/-1->10->8 [1] -1/-1/-1->10->9
g05n09:221467:221781 [3] NCCL INFO P2P Chunksize set to 131072
g05n09:221467:221781 [3] NCCL INFO Channel 00/0 : 9[1] -> 10[3] [receive] via NET/IB/3
g05n09:221467:221781 [3] NCCL INFO Channel 01/0 : 9[1] -> 10[3] [receive] via NET/IB/3
g05n09:221467:221781 [3] NCCL INFO Channel 00/0 : 10[3] -> 11[5] [send] via NET/IB/3
g05n09:221467:221781 [3] NCCL INFO Channel 01/0 : 10[3] -> 11[5] [send] via NET/IB/3
g05n09:221467:221781 [3] NCCL INFO Connected all rings
g05n09:221467:221781 [3] NCCL INFO Channel 00/0 : 8[5] -> 10[3] [receive] via NET/IB/3
g05n09:221467:221781 [3] NCCL INFO Channel 00/0 : 10[3] -> 8[5] [send] via NET/IB/3
g05n09:221467:221781 [3] NCCL INFO Channel 00/0 : 11[5] -> 10[3] [receive] via NET/IB/3
g05n09:221467:221781 [3] NCCL INFO Channel 00/0 : 10[3] -> 9[1] [send] via NET/IB/3
g05n09:221467:221781 [3] NCCL INFO Channel 01/0 : 10[3] -> 9[1] [send] via NET/IB/3
g05n09:221467:221781 [3] NCCL INFO Connected all trees
g05n09:221467:221781 [3] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g05n09:221467:221781 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g05n09:221467:221781 [3] NCCL INFO comm 0x1509e35b0 rank 10 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x54676b30da0675d6 - Init COMPLETE
g05n09:221467:221803 [3] NCCL INFO Us/-1->22->21 [1] -1/-1/-1->22->21
g05n09:221469:221798 [5] NCCL INFO P2P Chunksize set to 131072
g05n09:221469:221798 [5] NCCL INFO Channel 00/0 : 22[5] -> 23[3] [send] via NET/IB/3
g05n09:221469:221798 [5] NCCL INFO Channel 01/0 : 22[5] -> 23[3] [send] via NET/IB/3
g05n09:221469:221798 [5] NCCL INFO Connected all rings
g05n09:221469:221798 [5] NCCL INFO Channel 00/0 : 20[3] -> 22[5] [receive] via NET/IB/2
g05n09:221469:221798 [5] NCCL INFO Channel 00/0 : 22[5] -> 20[3] [send] via NET/IB/2
g05n09:221469:221798 [5] NCCL INFO Channel 00/0 : 22[5] -> 21[1] via P2P/IPC
g05n09:221469:221798 [5] NCCL INFO Channel 01/0 : 22[5] -> 21[1] via P2P/IPC
g05n09:221469:221798 [5] NCCL INFO Connected all trees
g05n09:221469:221798 [5] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g05n09:221469:221798 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g05n09:221469:221798 [5] NCCL INFO comm 0x16d5622b0 rank 22 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xa49a27aa8b15034e - Init COMPLETE
g05n09:221462:221462 [1] NCCL INFO cudaDriverVersion 12020
g05n09:221462:221462 [1] NCCL INFO Bootstrap : Using ib0:10.41.16.21<0>
g05n09:221462:221462 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g05n09:221462:221462 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g05n09:221462:221693 [1] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.16.21<0>
g05n09:221462:221693 [1] NCCL INFO Using network IB
g05n09:221462:221693 [1] NCCL INFO comm 0x122c74170 rank 85 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x7b12926388d12ce3 - Init START
g05n09:221462:221693 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g05n09:221462:221693 [1] NCCL INFO Trees [0] 86/78/-1->85->84 [1] 84/-1/-1->85->89 [2] 86/-1/-1->85->84 [3] 84/-1/-1->85->89
g05n09:221462:221693 [1] NCCL INFO P2P Chunksize set to 131072
g05n09:221462:221693 [1] NCCL INFO Channel 00/0 : 85[1] -> 86[2] via P2P/IPC
g05n09:221462:221693 [1] NCCL INFO Channel 02/0 : 85[1] -> 86[2] via P2P/IPC
g05n09:221462:221693 [1] NCCL INFO Channel 01/0 : 85[1] -> 84[0] via P2P/IPC
g05n09:221462:221693 [1] NCCL INFO Channel 03/0 : 85[1] -> 84[0] via P2P/IPC
g05n09:221462:221693 [1] NCCL INFO Connected all rings
g05n09:221462:221693 [1] NCCL INFO Channel 01/0 : 85[1] -> 89[5] via P2P/IPC
g05n09:221462:221693 [1] NCCL INFO Channel 03/0 : 85[1] -> 89[5] via P2P/IPC
g05n09:221462:221693 [1] NCCL INFO Channel 00/0 : 78[0] -> 85[1] [receive] via NET/IB/0
g05n09:221462:221693 [1] NCCL INFO Channel 00/0 : 85[1] -> 78[0] [send] via NET/IB/0
g05n09:221462:221693 [1] NCCL INFO Channel 00/0 : 85[1] -> 84[0] via P2P/IPC
g05n09:221462:221693 [1] NCCL INFO Channel 02/0 : 85[1] -> 84[0] via P2P/IPC
g05n09:221462:221693 [1] NCCL INFO Connected all trees
g05n09:221462:221693 [1] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g05n09:221462:221693 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g05n09:221462:221693 [1] NCCL INFO comm 0x122c74170 rank 85 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x7b12926388d12ce3 - Init COMPLETE
g05n09:221462:221779 [1] NCCL INFO Using network IB
g05n09:221462:221779 [1] NCCL INFO comm 0x1259116f0 rank 10 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0xa77518dab25ce26 - Init START
g05n09:221462:221779 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g05n09:221462:221779 [1] NCCL INFO Trees [0] 9/11/-1->10->8 [1] -1/-1/-1->10->9
g05n09:221462:221779 [1] NCCL INFO P2P Chunksize set to 131072
g05n09:221462:221779 [1] NCCL INFO Channel 00/0 : 9[5] -> 10[1] [receive] via NET/IB/2
g05n09:221462:221779 [1] NCCL INFO Channel 01/0 : 9[5] -> 10[1] [receive] via NET/IB/2
g05n09:221462:221779 [1] NCCL INFO Channel 00/0 : 10[1] -> 11[3] [send] via NET/IB/2
g05n09:221462:221779 [1] NCCL INFO Channel 01/0 : 10[1] -> 11[3] [send] via NET/IB/2
g05n09:221462:221779 [1] NCCL INFO Connected all rings
g05n09:221462:221779 [1] NCCL INFO Channel 00/0 : 8[3] -> 10[1] [receive] via NET/IB/2
g05n09:221462:221779 [1] NCCL INFO Channel 00/0 : 10[1] -> 8[3] [send] via NET/IB/2
g05n09:221462:221779 [1] NCCL INFO Channel 00/0 : 11[3] -> 10[1] [receive] via NET/IB/2
g05n09:221462:221779 [1] NCCL INFO Channel 00/0 : 10[1] -> 9[5] [send] via NET/IB/2
g05n09:221462:221779 [1] NCCL INFO Channel 01/0 : 10[1] -> 9[5] [send] via NET/IB/2
g05n09:221462:221779 [1] NCCL INFO Connected all trees
g05n09:221462:221779 [1] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g05n09:221462:221779 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g05n09:221462:221779 [1] NCCL INFO comm 0x1259116f0 rank 10 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0xa77518dab25ce26 - Init COMPLETE
g05n09:221462:221800 [1] NCCL INFO Using network IB
g05n09:221462:221800 [1] NCCL INFO comm 0x1255b8f20 rank 21 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0xa49a27aa8b15034e - Init START
g05n09:221462:221800 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g05n09:221462:221800 [1] NCCL INFO Trees [0] 22/23/-1->21->18 [1] 22/-1/-1->21->20
g05n09:221462:221800 [1] NCCL INFO P2P Chunksize set to 131072
g05n09:221462:221800 [1] NCCL INFO Channel 00/0 : 20[3] -> 21[1] [receive] via NET/IB/2
g05n09:221462:221800 [1] NCCL INFO Channel 01/0 : 20[3] -> 21[1] [receive] via NET/IB/2
g05n09:221462:221800 [1] NCCL INFO Channel 00/0 : 21[1] -> 22[5] via P2P/IPC
g05n09:221462:221800 [1] NCCL INFO Channel 01/0 : 21[1] -> 22[5] via P2P/IPC
g05n09:221462:221800 [1] NCCL INFO Connected all rings
g05n09:221462:221800 [1] NCCL INFO Channel 00/0 : 21[1] -> 23[3] [send] via NET/IB/2
g05n09:221462:221800 [1] NCCL INFO Channel 00/0 : 18[1] -> 21[1] [receive] via NET/IB/2
g05n09:221462:221800 [1] NCCL INFO Channel 00/0 : 21[1] -> 18[1] [send] via NET/IB/2
g05n09:221462:221800 [1] NCCL INFO Channel 00/0 : 23[3] -> 21[1] [receive] via NET/IB/2
g05n09:221462:221800 [1] NCCL INFO Channel 01/0 : 21[1] -> 20[3] [send] via NET/IB/2
g05n09:221462:221800 [1] NCCL INFO Connected all trees
g05n09:221462:221800 [1] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g05n09:221462:221800 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g05n09:221462:221800 [1] NCCL INFO comm 0x1255b8f20 rank 21 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0xa49a27aa8b15034e - Init COMPLETE
ing network IB
g05n09:221467:221803 [3] NCCL INFO comm 0x15063e300 rank 21 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x821cecfd780eaba1 - Init START
g05n09:221467:221803 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g05n09:221467:221803 [3] NCCL INFO Trees [0] 19/22/-1->21->18 [1] -1/-1/-1->21->20
g05n09:221467:221803 [3] NCCL INFO P2P Chunksize set to 131072
g05n09:221467:221803 [3] NCCL INFO Channel 00/0 : 20[5] -> 21[3] [receive] via NET/IB/3
g05n09:221467:221803 [3] NCCL INFO Channel 01/0 : 20[5] -> 21[3] [receive] via NET/IB/3
g05n09:221467:221803 [3] NCCL INFO Channel 00/0 : 21[3] -> 22[1] [send] via NET/IB/3
g05n09:221467:221803 [3] NCCL INFO Channel 01/0 : 21[3] -> 22[1] [send] via NET/IB/3
g05n09:221467:221803 [3] NCCL INFO Connected all rings
g05n09:221467:221803 [3] NCCL INFO Channel 00/0 : 19[1] -> 21[3] [receive] via NET/IB/3
g05n09:221467:221803 [3] NCCL INFO Channel 00/0 : 18[3] -> 21[3] [receive] via NET/IB/3
g05n09:221467:221803 [3] NCCL INFO Channel 00/0 : 21[3] -> 18[3] [send] via NET/IB/3
g05n09:221467:221803 [3] NCCL INFO Channel 00/0 : 21[3] -> 19[1] [send] via NET/IB/3
g05n09:221467:221803 [3] NCCL INFO Channel 00/0 : 22[1] -> 21[3] [receive] via NET/IB/3
g05n09:221467:221803 [3] NCCL INFO Channel 01/0 : 21[3] -> 20[5] [send] via NET/IB/3
g05n09:221467:221803 [3] NCCL INFO Connected all trees
g05n09:221467:221803 [3] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g05n09:221467:221803 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g05n09:221467:221803 [3] NCCL INFO comm 0x15063e300 rank 21 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x821cecfd780eaba1 - Init COMPLETE
f18n06:1026679:1026679 [1] NCCL INFO cudaDriverVersion 12020
f18n06:1026679:1026679 [1] NCCL INFO Bootstrap : Using ib0:10.41.14.110<0>
f18n06:1026679:1026679 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
f18n06:1026679:1026679 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
f18n06:1026679:1026911 [1] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.14.110<0>
f18n06:1026679:1026911 [1] NCCL INFO Using network IB
f18n06:1026679:1026911 [1] NCCL INFO comm 0x149f339d0 rank 61 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x7b12926388d12ce3 - Init START
f18n06:1026679:1026911 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
f18n06:1026679:1026911 [1] NCCL INFO Trees [0] 62/54/-1->61->60 [1] 60/-1/-1->61->65 [2] 62/-1/-1->61->60 [3] 60/-1/-1->61->65
f18n06:1026679:1026911 [1] NCCL INFO P2P Chunksize set to 131072
f18n06:1026679:1026911 [1] NCCL INFO Channel 00/0 : 61[1] -> 62[2] via P2P/IPC
f18n06:1026679:1026911 [1] NCCL INFO Channel 02/0 : 61[1] -> 62[2] via P2P/IPC
f18n06:1026679:1026911 [1] NCCL INFO Channel 01/0 : 61[1] -> 60[0] via P2P/IPC
f18n06:1026679:1026911 [1] NCCL INFO Channel 03/0 : 61[1] -> 60[0] via P2P/IPC
f18n06:1026679:1026911 [1] NCCL INFO Connected all rings
f18n06:1026679:1026911 [1] NCCL INFO Channel 01/0 : 61[1] -> 65[5] via P2P/IPC
f18n06:1026679:1026911 [1] NCCL INFO Channel 03/0 : 61[1] -> 65[5] via P2P/IPC
f18n06:1026679:1026911 [1] NCCL INFO Channel 00/0 : 54[0] -> 61[1] [receive] via NET/IB/0
f18n06:1026679:1026911 [1] NCCL INFO Channel 00/0 : 61[1] -> 54[0] [send] via NET/IB/0
f18n06:1026679:1026911 [1] NCCL INFO Channel 00/0 : 61[1] -> 60[0] via P2P/IPC
f18n06:1026679:1026911 [1] NCCL INFO Channel 02/0 : 61[1] -> 60[0] via P2P/IPC
f18n06:1026679:1026911 [1] NCCL INFO Connected all trees
f18n06:1026679:1026911 [1] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
f18n06:1026679:1026911 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
f18n06:1026679:1026911 [1] NCCL INFO comm 0x149f339d0 rank 61 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x7b12926388d12ce3 - Init COMPLETE
f18n06:1026679:1027001 [1] NCCL INFO Using network IB
f18n06:1026679:1027001 [1] NCCL INFO comm 0x14bd74a90 rank 7 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0xa77518dab25ce26 - Init START
f18n06:1026679:1027001 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
f18n06:1026679:1027001 [1] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] 9/5/-1->7->3
f18n06:1026679:1027001 [1] NCCL INFO P2P Chunksize set to 131072
f18n06:1026679:1027001 [1] NCCL INFO Channel 00/0 : 6[5] -> 7[1] [receive] via NET/IB/2
f18n06:1026679:1027001 [1] NCCL INFO Channel 01/0 : 6[5] -> 7[1] [receive] via NET/IB/2
f18n06:1026679:1027001 [1] NCCL INFO Channel 00/0 : 7[1] -> 8[3] [send] via NET/IB/2
f18n06:1026679:1027001 [1] NCCL INFO Channel 01/0 : 7[1] -> 8[3] [send] via NET/IB/2
f18n06:1026679:1027001 [1] NCCL INFO Connected all rings
f18n06:1026679:1027001 [1] NCCL INFO Channel 01/0 : 5[3] -> 7[1] [receive] via NET/IB/2
f18n06:1026679:1027001 [1] NCCL INFO Channel 01/0 : 7[1] -> 9[5] [send] via NET/IB/2
f18n06:1026679:1027001 [1] NCCL INFO Channel 01/0 : 3[5] -> 7[1] [receive] via NET/IB/2
f18n06:1026679:1027001 [1] NCCL INFO Channel 01/0 : 7[1] -> 3[5] [send] via NET/IB/2
f18n06:1026679:1027001 [1] NCCL INFO Channel 01/0 : 9[5] -> 7[1] [receive] via NET/IB/2
f18n06:1026679:1027001 [1] NCCL INFO Channel 01/0 : 7[1] -> 5[3] [send] via NET/IB/2
f18n06:1026679:1027001 [1] NCCL INFO Channel 00/0 : 7[1] -> 6[5] [send] via NET/IB/2
f18n06:1026679:1027001 [1] NCCL INFO Connected all trees
f18n06:1026679:1027001 [1] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
f18n06:1026679:1027001 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f18n06:1026679:1027001 [1] NCCL INFO comm 0x14bd74a90 rank 7 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0xa77518dab25ce26 - Init COMPLETE
f18n06:1026679:1027019 [1] NCCL INFO Using network IB
f18n06:1026679:1027019 [1] NCCL INFO comm 0x14cbbd660 rank 15 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0xa49a27aa8b15034e - Init START
f18n06:1026679:1027019 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
f18n06:1026679:1027019 [1] NCCL INFO Trees [0] 16/17/-1->15->19 [1] 16/-1/-1->15->14
f18n06:1026679:1027019 [1] NCCL INFO P2P Chunksize set to 131072
f18n06:1026679:1027019 [1] NCCL INFO Channel 00/0 : 14[3] -> 15[1] [receive] via NET/IB/2
f18n06:1026679:1027019 [1] NCCL INFO Channel 01/0 : 14[3] -> 15[1] [receive] via NET/IB/2
f18n06:1026679:1027019 [1] NCCL INFO Channel 00/0 : 15[1] -> 16[5] via P2P/IPC
f18n06:1026679:1027019 [1] NCCL INFO Channel 01/0 : 15[1] -> 16[5] via P2P/IPC
f18n06:1026679:1027019 [1] NCCL INFO Connected all rings
f18n06:1026679:1027019 [1] NCCL INFO Channel 00/0 : 15[1] -> 17[3] [send] via NET/IB/2
f18n06:1026679:1027019 [1] NCCL INFO Channel 00/0 : 15[1] -> 19[5] [send] via NET/IB/2
f18n06:1026679:1027019 [1] NCCL INFO Channel 00/0 : 19[5] -> 15[1] [receive] via NET/IB/2
f18n06:1026679:1027019 [1] NCCL INFO Channel 00/0 : 17[3] -> 15[1] [receive] via NET/IB/2
f18n06:1026679:1027019 [1] NCCL INFO Channel 01/0 : 15[1] -> 14[3] [send] via NET/IB/2
f18n06:1026679:1027019 [1] NCCL INFO Connected all trees
f18n06:1026679:1027019 [1] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
f18n06:1026679:1027019 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f18n06:1026679:1027019 [1] NCCL INFO comm 0x14cbbd660 rank 15 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0xa49a27aa8b15034e - Init COMPLETE
f18n06:1026682:1026682 [3] NCCL INFO cudaDriverVersion 12020
f18n06:1026682:1026682 [3] NCCL INFO Bootstrap : Using ib0:10.41.14.110<0>
f18n06:1026682:1026682 [3] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
f18n06:1026682:1026682 [3] NCCL INFO NET/Plugin : No plugin found, using internal implementation
f18n06:1026682:1026914 [3] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.14.110<0>
f18n06:1026682:1026914 [3] NCCL INFO Using network IB
f18n06:1026682:1026914 [3] NCCL INFO comm 0x1592f4080 rank 63 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x7b12926388d12ce3 - Init START
f18n06:1026682:1026914 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
f18n06:1026682:1026914 [3] NCCL INFO Trees [0] 64/-1/-1->63->62 [1] 65/58/-1->63->64 [2] 64/-1/-1->63->62 [3] 65/-1/-1->63->64
f18n06:1026682:1026914 [3] NCCL INFO P2P Chunksize set to 131072
f18n06:1026682:1026914 [3] NCCL INFO Channel 00/0 : 63[3] -> 64[4] via P2P/IPC
f18n06:1026682:1026914 [3] NCCL INFO Channel 01/0 : 63[3] -> 64[4] via P2P/IPC
f18n06:1026682:1026914 [3] NCCL INFO Channel 02/0 : 63[3] -> 64[4] via P2P/IPC
f18n06:1026682:1026914 [3] NCCL INFO Channel 03/0 : 63[3] -> 64[4] via P2P/IPC
f18n06:1026682:1026914 [3] NCCL INFO Channel 01/0 : 54[0] -> 63[3] [receive] via NET/IB/3
f18n06:1026682:1026914 [3] NCCL INFO Channel 03/0 : 54[0] -> 63[3] [receive] via NET/IB/3
f18n06:1026682:1026914 [3] NCCL INFO Connected all rings
f18n06:1026682:1026914 [3] NCCL INFO Channel 01/0 : 63[3] -> 65[5] via P2P/IPC
f18n06:1026682:1026914 [3] NCCL INFO Channel 03/0 : 63[3] -> 65[5] via P2P/IPC
f18n06:1026682:1026914 [3] NCCL INFO Channel 01/0 : 58[4] -> 63[3] [receive] via NET/IB/3
f18n06:1026682:1026914 [3] NCCL INFO Channel 01/0 : 63[3] -> 58[4] [send] via NET/IB/3
f18n06:1026682:1026914 [3] NCCL INFO Channel 00/0 : 63[3] -> 62[2] via P2P/IPC
f18n06:1026682:1026914 [3] NCCL INFO Channel 02/0 : 63[3] -> 62[2] via P2P/IPC
f18n06:1026682:1026914 [3] NCCL INFO Connected all trees
f18n06:1026682:1026914 [3] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
f18n06:1026682:1026914 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
f18n06:1026682:1026914 [3] NCCL INFO comm 0x1592f4080 rank 63 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x7b12926388d12ce3 - Init COMPLETE
f18n06:1026682:1027003 [3] NCCL INFO Using network IB
f18n06:1026682:1027003 [3] NCCL INFO comm 0x15bc52420 rank 7 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x54676b30da0675d6 - Init START
f18n06:1026682:1027003 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
f18n06:1026682:1027003 [3] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] 9/5/-1->7->3
f18n06:1026682:1027003 [3] NCCL INFO P2P Chunksize set to 131072
f18n06:1026682:1027003 [3] NCCL INFO Channel 00/0 : 6[1] -> 7[3] [receive] via NET/IB/3
f18n06:1026682:1027003 [3] NCCL INFO Channel 01/0 : 6[1] -> 7[3] [receive] via NET/IB/3
f18n06:1026682:1027003 [3] NCCL INFO Channel 00/0 : 7[3] -> 8[5] [send] via NET/IB/3
f18n06:1026682:1027003 [3] NCCL INFO Channel 01/0 : 7[3] -> 8[5] [send] via NET/IB/3
f18n06:1026682:1027003 [3] NCCL INFO Connected all rings
f18n06:1026682:1027003 [3] NCCL INFO Channel 01/0 : 5[5] -> 7[3] [receive] via NET/IB/3
f18n06:1026682:1027003 [3] NCCL INFO Channel 01/0 : 7[3] -> 9[1] [send] via NET/IB/3
f18n06:1026682:1027003 [3] NCCL INFO Channel 01/0 : 3[1] -> 7[3] [receive] via NET/IB/3
f18n06:1026682:1027003 [3] NCCL INFO Channel 01/0 : 7[3] -> 3[1] [send] via NET/IB/3
f18n06:1026682:1027003 [3] NCCL INFO Channel 01/0 : 9[1] -> 7[3] [receive] via NET/IB/3
f18n06:1026682:1027003 [3] NCCL INFO Channel 01/0 : 7[3] -> 5[5] [send] via NET/IB/3
f18n06:1026682:1027003 [3] NCCL INFO Channel 00/0 : 7[3] -> 6[1] [send] via NET/IB/3
f18n06:1026682:1027003 [3] NCCL INFO Connected all trees
f18n06:1026682:1027003 [3] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
f18n06:1026682:1027003 [3] NCCL INFO 2 coll cf18n06:1026686:1026686 [5] NCCL INFO cudaDriverVersion 12020
f18n06:1026686:1026686 [5] NCCL INFO Bootstrap : Using ib0:10.41.14.110<0>
f18n06:1026686:1026686 [5] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
f18n06:1026686:1026686 [5] NCCL INFO NET/Plugin : No plugin found, using internal implementation
f18n06:1026686:1026915 [5] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.14.110<0>
f18n06:1026686:1026915 [5] NCCL INFO Using network IB
f18n06:1026686:1026915 [5] NCCL INFO comm 0x135983d80 rank 65 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x7b12926388d12ce3 - Init START
f18n06:1026686:1026915 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
f18n06:1026686:1026915 [5] NCCL INFO Trees [0] -1/-1/-1->65->64 [1] 61/-1/-1->65->63 [2] -1/-1/-1->65->64 [3] 61/-1/-1->65->63
f18n06:1026686:1026915 [5] NCCL INFO P2P Chunksize set to 131072
f18n06:1026686:1026915 [5] NCCL INFO Channel 00/0 : 65[5] -> 66[0] [send] via NET/IB/1
f18n06:1026686:1026915 [5] NCCL INFO Channel 02/0 : 65[5] -> 66[0] [send] via NET/IB/1
f18n06:1026686:1026915 [5] NCCL INFO Channel 01/0 : 65[5] -> 62[2] via P2P/IPC
f18n06:1026686:1026915 [5] NCCL INFO Channel 03/0 : 65[5] -> 62[2] via P2P/IPC
f18n06:1026686:1026915 [5] NCCL INFO Connected all rings
f18n06:1026686:1026915 [5] NCCL INFO Channel 01/0 : 65[5] -> 61[1] via P2P/IPC
f18n06:1026686:1026915 [5] NCCL INFO Channel 03/0 : 65[5] -> 61[1] via P2P/IPC
f18n06:1026686:1026915 [5] NCCL INFO Channel 01/0 : 65[5] -> 63[3] via P2P/IPC
f18n06:1026686:1026915 [5] NCCL INFO Channel 03/0 : 65[5] -> 63[3] via P2P/IPC
f18n06:1026686:1026915 [5] NCCL INFO Channel 00/0 : 65[5] -> 64[4] via P2P/IPC
f18n06:1026686:1026915 [5] NCCL INFO Channel 02/0 : 65[5] -> 64[4] via P2P/IPC
f18n06:1026686:1026915 [5] NCCL INFO Connected all trees
f18n06:1026686:1026915 [5] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
f18n06:1026686:1026915 [5] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
f18n06:1026686:1026915 [5] NCCL INFO comm 0x135983d80 rank 65 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x7b12926388d12ce3 - Init COMPLETE
f18n06:1026686:1026999 [5] NCCL INFO Using network IB
f18n06:1026686:1026999 [5] NCCL INFO comm 0x1382caa60 rank 8 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x86d434c9d289d59a - Init START
f18n06:1026686:1026999 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
f18n06:1026686:1026999 [5] NCCL INFO Trees [0] 4/10/-1->8->0 [1] -1/-1/-1->8->9
f18n06:1026686:1026999 [5] NCCL INFO P2P Chunksize set to 131072
f18n06:1026686:1026999 [5] NCCL INFO Channel 00/0 : 7[3] -> 8[5] [receive] via NET/IB/3
f18n06:1026686:1026999 [5] NCCL INFO Channel 01/0 : 7[3] -> 8[5] [receive] via NET/IB/3
f18n06:1026686:1026999 [5] NCCL INFO Channel 00/0 : 8[5] -> 9[1] [send] via NET/IB/3
f18n06:1026686:1026999 [5] NCCL INFO Channel 01/0 : 8[5] -> 9[1] [send] via NET/IB/3
f18n06:1026686:1026999 [5] NCCL INFO Connected all rings
f18n06:1026686:1026999 [5] NCCL INFO Channel 00/0 : 8[5] -> 10[3] [send] via NET/IB/3
f18n06:1026686:1026999 [5] NCCL INFO Channel 00/0 : 4[3] -> 8[5] [receive] via NET/IB/3
f18n06:1026686:1026999 [5] NCCL INFO Channel 00/0 : 8[5] -> 0[1] [send] via NET/IB/3
f18n06:1026686:1026999 [5] NCCL INFO Channel 00/0 : 0[1] -> 8[5] [receive] via NET/IB/3
f18n06:1026686:1026999 [5] NCCL INFO Channel 00/0 : 8[5] -> 4[3] [send] via NET/IB/3
f18n06:1026686:1026999 [5] NCCL INFO Channel 00/0 : 10[3] -> 8[5] [receive] via NET/IB/3
f18n06:1026686:1026999 [5] NCCL INFO Channel 01/0 : 9[1] -> 8[5] [receive] via NET/IB/3
f18n06:1026686:1026999 [5] NCCL INFO Connected all trees
f18n06:1026686:1026999 [5] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
f18n06:1026686:1026999 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f18n06:1026686:1026999 [5] NCCL INFO comm 0x1382caa60 rank 8 nranks 12 cudaDev hannels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f18n06:1026682:1027003 [3] NCCL INFO comm 0x15bc52420 rank 7 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x54676b30da0675d6 - Init COMPLETE
f18n06:1026682:1027022 [3] NCCL INFO Using network IB
f18n06:1026682:1027022 [3] NCCL INFO comm 0x15bf87660 rank 15 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x821cecfd780eaba1 - Init START
f18n06:1026682:1027022 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
f18n06:1026682:1027022 [3] NCCL INFO Trees [0] 13/16/-1->15->18 [1] -1/-1/-1->15->14
f18n06:1026682:1027022 [3] NCCL INFO P2P Chunksize set to 131072
f18n06:1026682:1027022 [3] NCCL INFO Channel 00/0 : 14[5] -> 15[3] [receive] via NET/IB/3
f18n06:1026682:1027022 [3] NCCL INFO Channel 01/0 : 14[5] -> 15[3] [receive] via NET/IB/3
f18n06:1026682:1027022 [3] NCCL INFO Channel 00/0 : 15[3] -> 16[1] [send] via NET/IB/3
f18n06:1026682:1027022 [3] NCCL INFO Channel 01/0 : 15[3] -> 16[1] [send] via NET/IB/3
f18n06:1026682:1027022 [3] NCCL INFO Connected all rings
f18n06:1026682:1027022 [3] NCCL INFO Channel 00/0 : 13[1] -> 15[3] [receive] via NET/IB/3
f18n06:1026682:1027022 [3] NCCL INFO Channel 00/0 : 15[3] -> 18[3] [send] via NET/IB/3
f18n06:1026682:1027022 [3] NCCL INFO Channel 00/0 : 18[3] -> 15[3] [receive] via NET/IB/3
f18n06:1026682:1027022 [3] NCCL INFO Channel 00/0 : 15[3] -> 13[1] [send] via NET/IB/3
f18n06:1026682:1027022 [3] NCCL INFO Channel 00/0 : 16[1] -> 15[3] [receive] via NET/IB/3
f18n06:1026682:1027022 [3] NCCL INFO Channel 01/0 : 15[3] -> 14[5] [send] via NET/IB/3
f18n06:1026682:1027022 [3] NCCL INFO Connected all trees
f18n06:1026682:1027022 [3] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
f18n06:1026682:1027022 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f18n06:1026682:1027022 [3] NCCL INFO comm 0x15bf87660 rank 15 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x821cecfd780eaba1 - Init COMPLETE
5 nvmlDev 5 busId 3505000 commId 0x86d434c9d289d59a - Init COMPLETE
f18n06:1026686:1027023 [5] NCCL INFO Using network IB
f18n06:1026686:1027023 [5] NCCL INFO comm 0x1385eecb0 rank 16 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xa49a27aa8b15034e - Init START
f18n06:1026686:1027023 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
f18n06:1026686:1027023 [5] NCCL INFO Trees [0] 14/-1/-1->16->15 [1] -1/-1/-1->16->15
f18n06:1026686:1027023 [5] NCCL INFO P2P Chunksize set to 131072
f18n06:1026686:1027023 [5] NCCL INFO Channel 00/0 : 16[5] -> 17[3] [send] via NET/IB/3
f18n06:1026686:1027023 [5] NCCL INFO Channel 01/0 : 16[5] -> 17[3] [send] via NET/IB/3
f18n06:1026686:1027023 [5] NCCL INFO Connected all rings
f18n06:1026686:1027023 [5] NCCL INFO Channel 00/0 : 14[3] -> 16[5] [receive] via NET/IB/2
f18n06:1026686:1027023 [5] NCCL INFO Channel 00/0 : 16[5] -> 14[3] [send] via NET/IB/2
f18n06:1026686:1027023 [5] NCCL INFO Channel 00/0 : 16[5] -> 15[1] via P2P/IPC
f18n06:1026686:1027023 [5] NCCL INFO Channel 01/0 : 16[5] -> 15[1] via P2P/IPC
f18n06:1026686:1027023 [5] NCCL INFO Connected all trees
f18n06:1026686:1027023 [5] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
f18n06:1026686:1027023 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f18n06:1026686:1027023 [5] NCCL INFO comm 0x1385eecb0 rank 16 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0xa49a27aa8b15034e - Init COMPLETE
g04n18:916027:916027 [3] NCCL INFO cudaDriverVersion 12020
g04n18:916027:916027 [3] NCCL INFO Bootstrap : Using ib0:10.41.16.12<0>
g04n18:916027:916027 [3] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g04n18:916027:916027 [3] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g04n18:916027:916257 [3] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.16.12<0>
g04n18:916027:916257 [3] NCCL INFO Using network IB
g04n18:916027:916257 [3] NCCL INFO comm 0x1671d4140 rank 75 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x7b12926388d12ce3 - Init START
g04n18:916027:916257 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g04n18:916027:916257 [3] NCCL INFO Trees [0] 76/-1/-1->75->74 [1] 77/64/-1->75->76 [2] 76/-1/-1->75->74 [3] 77/-1/-1->75->76
g04n18:916027:916257 [3] NCCL INFO P2P Chunksize set to 131072
g04n18:916027:916257 [3] NCCL INFO Channel 00/0 : 75[3] -> 76[4] via P2P/IPC
g04n18:916027:916257 [3] NCCL INFO Channel 01/0 : 75[3] -> 76[4] via P2P/IPC
g04n18:916027:916257 [3] NCCL INFO Channel 02/0 : 75[3] -> 76[4] via P2P/IPC
g04n18:916027:916257 [3] NCCL INFO Channel 03/0 : 75[3] -> 76[4] via P2P/IPC
g04n18:916027:916257 [3] NCCL INFO Channel 01/0 : 66[0] -> 75[3] [receive] via NET/IB/3
g04n18:916027:916257 [3] NCCL INFO Channel 03/0 : 66[0] -> 75[3] [receive] via NET/IB/3
g04n18:916027:916257 [3] NCCL INFO Connected all rings
g04n18:916027:916257 [3] NCCL INFO Channel 01/0 : 75[3] -> 77[5] via P2P/IPC
g04n18:916027:916257 [3] NCCL INFO Channel 03/0 : 75[3] -> 77[5] via P2P/IPC
g04n18:916027:916257 [3] NCCL INFO Channel 01/0 : 64[4] -> 75[3] [receive] via NET/IB/3
g04n18:916027:916257 [3] NCCL INFO Channel 01/0 : 75[3] -> 64[4] [send] via NET/IB/3
g04n18:916027:916257 [3] NCCL INFO Channel 00/0 : 75[3] -> 74[2] via P2P/IPC
g04n18:916027:916257 [3] NCCL INFO Channel 02/0 : 75[3] -> 74[2] via P2P/IPC
g04n18:916027:916257 [3] NCCL INFO Connected all trees
g04n18:916027:916257 [3] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g04n18:916027:916257 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g04n18:916027:916257 [3] NCCL INFO comm 0x1671d4140 rank 75 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x7b12926388d12ce3 - Init COMPLETE
g04n18:916027:916340 [3] NCCL INFO Using network IB
g04n18:916027:916340 [3] NCCL INFO comm 0x16a4f59a0 rank 9 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x1defb2708b178c98 - Init START
g04n18:916027:916340 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g04n18:916027:916340 [3] NCCL INFO Trees [0] -1/-1/-1->9->10 [1] 10/8/-1->9->7
g04n18:916027:916340 [3] NCCL INFO P2P Chunksize set to 131072
g04n18:916027:916340 [3] NCCL INFO Channel 00/0 : 8[1] -> 9[3] [receive] via NET/IB/3
g04n18:916027:916340 [3] NCCL INFO Channel 01/0 : 8[1] -> 9[3] [receive] via NET/IB/3
g04n18:916027:916340 [3] NCCL INFO Channel 00/0 : 9[3] -> 10[5] [send] via NET/IB/3
g04n18:916027:916340 [3] NCCL INFO Channel 01/0 : 9[3] -> 10[5] [send] via NET/IB/3
g04n18:916027:916340 [3] NCCL INFO Connected all rings
g04n18:916027:916340 [3] NCCL INFO Channel 01/0 : 7[5] -> 9[3] [receive] via NET/IB/3
g04n18:916027:916340 [3] NCCL INFO Channel 01/0 : 9[3] -> 7[5] [send] via NET/IB/3
g04n18:916027:916340 [3] NCCL INFO Channel 00/0 : 10[5] -> 9[3] [receive] via NET/IB/3
g04n18:916027:916340 [3] NCCL INFO Channel 01/0 : 10[5] -> 9[3] [receive] via NET/IB/3
g04n18:916027:916340 [3] NCCL INFO Channel 01/0 : 9[3] -> 8[1] [send] via NET/IB/3
g04n18:916027:916340 [3] NCCL INFO Connected all trees
g04n18:916027:916340 [3] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g04n18:916027:916340 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g04n18:916027:916340 [3] NCCL INFO comm 0x16a4f59a0 rank 9 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x1defb2708b178c98 - Init COMPLETE
g04n18:916027:916363 [3] NCCL INFO Using network IB
g04n18:916027:916363 [3] NCCL INFO comm 0x1696c1550 rank 18 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x821cecfd780eaba1 - Init START
g04n18:916027:916363 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g04n18:916027:916363 [3] NCCL INFO Trees [0] 15/21/-1->18->12 [1] -1/-1/-1->18->19
g04n18:916027:916363 [3] NCCL INFO P2P Chunksize set to 131072
g04n18:916027:916363 [3] NCCL INFO Channel 00/0 : 17[5] -> 18[3] [receive] via NET/IB/3
g04n18:916027:916363 [3] NCCL INFO Channel 01/0 : 17[5] -> 18[3] [receive] via NET/IB/3
g04n18:916027:916363 [3] NCCL INFO Channel 00/0 : 18[3] -> 19[1] [send] via NET/IB/3
g04n18:916027:916363 [3] NCCL INFO Channel 01/0 : 18[3] -> 19[1] [send] via NET/IB/3
g04n18:916027:916363 [3] NCCL INFO Connected all rings
g04n18:916027:916363 [3] NCCL INFO Channel 00/0 : 15[3] -> 18[3] [receive] via NET/IB/3
g04n18:916027:916363 [3] NCCL INFO Channel 00/0 : 18[3] -> 21[3] [send] via NET/IB/3
g04n18:916027:916363 [3] NCCL INFO Channel 00/0 : 12[3] -> 18[3] [receive] via NET/IB/3
g04n18:916027:916363 [3] NCCL INFO Channel 00/0 : 18[3] -> 12[3] [send] via NET/IB/3
g04n18:916027:916363 [3] NCCL INFO Channel 00/0 : 21[3] -> 18[3] [receive] via NET/IB/3
g04n18:916027:916363 [3] NCCL INFO Channel 00/0 : 18[3] -> 15[3] [send] via NET/IB/3
g04n18:916027:916363 [3] NCCL INFO Channel 01/0 : 19[1] -> 18[3] [receive] via NET/IB/3
g04n18:916027:916363 [3] NCCL INFO Connected all trees
g04n18:916027:916363 [3] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g04n18:916027:916363 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g04n18:916027:916363 [3] NCCL INFO comm 0x1696c1550 rank 18 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x821cecfd780eaba1 - Init COMPLETE
g04n18:916022:916022 [1] NCCL INFO cudaDriverVersion 12020
g04n18:916022:916022 [1] NCCL INFO Bootstrap : Using ib0:10.41.16.12<0>
g04n18:916022:916022 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g04n18:916022:916022 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g04n18:916022:916255 [1] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.16.12<0>
g04n18:916022:916255 [1] NCCL INFO Using network IB
g04n18:916022:916255 [1] NCCL INFO comm 0x1238b3ec0 rank 73 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x7b12926388d12ce3 - Init START
g04n18:916022:916255 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g04n18:916022:916255 [1] NCCL INFO Trees [0] 74/60/-1->73->72 [1] 72/-1/-1->73->77 [2] 74/-1/-1->73->72 [3] 72/-1/-1->73->77
g04n18:916022:916255 [1] NCCL INFO P2P Chunksize set to 131072
g04n18:916022:916255 [1] NCCL INFO Channel 00/0 : 73[1] -> 74[2] via P2P/IPC
g04n18:916022:916255 [1] NCCL INFO Channel 02/0 : 73[1] -> 74[2] via P2P/IPC
g04n18:916022:916255 [1] NCCL INFO Channel 01/0 : 73[1] -> 72[0] via P2P/IPC
g04n18:916022:916255 [1] NCCL INFO Channel 03/0 : 73[1] -> 72[0] via P2P/IPC
g04n18:916022:916255 [1] NCCL INFO Connected all rings
g04n18:916022:916255 [1] NCCL INFO Channel 01/0 : 73[1] -> 77[5] via P2P/IPC
g04n18:916022:916255 [1] NCCL INFO Channel 03/0 : 73[1] -> 77[5] via P2P/IPC
g04n18:916022:916255 [1] NCCL INFO Channel 00/0 : 60[0] -> 73[1] [receive] via NET/IB/0
g04n18:916022:916255 [1] NCCL INFO Channel 00/0 : 73[1] -> 60[0] [send] via NET/IB/0
g04n18:916022:916255 [1] NCCL INFO Channel 00/0 : 73[1] -> 72[0] via P2P/IPC
g04n18:916022:916255 [1] NCCL INFO Channel 02/0 : 73[1] -> 72[0] via P2P/IPC
g04n18:916022:916255 [1] NCCL INFO Connected all trees
g04n18:916022:916255 [1] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g04n18:916022:916255 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g04n18:916022:916255 [1] NCCL INFO comm 0x1238b3ec0 rank 73 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x7b12926388d12ce3 - Init COMPLETE
g04n18:916022:916344 [1] NCCL INFO Using network IB
g04n18:916022:916344 [1] NCCL INFO comm 0x125f47ff0 rank 9 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x86d434c9d289d59a - Init START
g04n18:916022:916344 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g04n18:916022:916344 [1] NCCL INFO Trees [0] -1/-1/-1->9->10 [1] 10/8/-1->9->7
g04n18:916022:916344 [1] NCCL INFO P2P Chunksize set to 131072
g04n18:916022:916344 [1] NCCL INFO Channel 00/0 : 8[5] -> 9[1] [receive] via NET/IB/2
g04n18:916022:916344 [1] NCCL INFO Channel 01/0 : 8[5] -> 9[1] [receive] via NET/IB/2
g04n18:916022:916344 [1] NCCL INFO Channel 00/0 : 9[1] -> 10[3] [send] via NET/IB/2
g04n18:916022:916344 [1] NCCL INFO Channel 01/0 : 9[1] -> 10[3] [send] via NET/IB/2
g04n18:916022:916344 [1] NCCL INFO Connected all rings
g04n18:916022:916344 [1] NCCL INFO Channel 01/0 : 7[3] -> 9[1] [receive] via NET/IB/2
g04n18:916022:916344 [1] NCCL INFO Channel 01/0 : 9[1] -> 7[3] [send] via NET/IB/2
g04n18:916022:916344 [1] NCCL INFO Channel 00/0 : 10[3] -> 9[1] [receive] via NET/IB/2
g04n18:916022:916344 [1] NCCL INFO Channel 01/0 : 10[3] -> 9[1] [receive] via NET/IB/2
g04n18:916022:916344 [1] NCCL INFO Channel 01/0 : 9[1] -> 8[5] [send] via NET/IB/2
g04n18:916022:916344 [1] NCCL INFO Connected all trees
g04n18:916022:916344 [1] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g04n18:916022:916344 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g04n18:916022:916344 [1] NCCL INFO comm 0x125f47ff0 rank 9 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x86d434c9d289d59a - Init COMPLETE
g04n18:916022:916362 [1] NCCL INFO Using network IB
g04n18:916022:916362 [1] NCCL INFO comm 0x1267895e0 rank 18 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0xa49a27aa8b15034e - Init START
g04n18:916022:916362 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g04n18:916022:916362 [1] NCCL INFO Trees [0] 19/21/-1->18->12 [1] 19/-1/-1->18->20
g04n18:916022:916362 [1] NCCL INFO P2P Chunksize set to 131072
g04n18:916022:916362 [1] NCCL INFO Channel 00/0 : 17[3] -> 18[1] [receive] via NET/IB/2
g04n18:916022:916362 [1] NCCL INFO Channel 01/0 : 17[3] -> 18[1] [receive] via NET/IB/2
g04n18:916022:916362 [1] NCCL INFO Channel 00/0 : 18[1] -> 19[5] via P2P/IPC
g04n18:916022:916362 [1] NCCL INFO Channel 01/0 : 18[1] -> 19[5] via P2P/IPC
g04n18:916022:916362 [1] NCCL INFO Connected all rings
g04n18:916022:916362 [1] NCCL INFO Channel 01/0 : 18[1] -> 20[3] [send] via NET/IB/2
g04n18:916022:916362 [1] NCCL INFO Channel 00/0 : 18[1] -> 21[1] [send] via NET/IB/2
g04n18:916022:916362 [1] NCCL INFO Channel 00/0 : 12[1] -> 18[1] [receive] via NET/IB/2
g04n18:916022:916362 [1] NCCL INFO Channel 00/0 : 18[1] -> 12[1] [send] via NET/IB/2
g04n18:916022:916362 [1] NCCL INFO Channel 00/0 : 21[1] -> 18[1] [receive] via NET/IB/2
g04n18:916022:916362 [1] NCCL INFO Channel 01/0 : 20[3] -> 18[1] [receive] via NET/IB/2
g04n18:916022:916362 [1] NCCL INFO Connected all trees
g04n18:916022:916362 [1] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g04n18:916022:916362 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g04n18:916022:916362 [1] NCCL INFO comm 0x1267895e0 rank 18 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0xa49a27aa8b15034e - Init COMPLETE
g05n09:221461:221461 [0] NCCL INFO cudaDriverVersion 12020
g05n09:221461:221461 [0] NCCL INFO Bootstrap : Using ib0:10.41.16.21<0>
g05n09:221461:221461 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g05n09:221461:221461 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g05n09:221461:221694 [0] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.16.21<0>
g05n09:221461:221694 [0] NCCL INFO Using network IB
g05n09:221461:221694 [0] NCCL INFO comm 0x11ae83a50 rank 84 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x7b12926388d12ce3 - Init START
g05n09:221461:221694 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g05n09:221461:221694 [0] NCCL INFO Trees [0] 85/90/-1->84->72 [1] 86/-1/-1->84->85 [2] 85/-1/-1->84->79 [3] 86/-1/-1->84->85
g05n09:221461:221694 [0] NCCL INFO P2P Chunksize set to 131072
g05n09:221461:221694 [0] NCCL INFO Channel 00/0 : 83[5] -> 84[0] [receive] via NET/IB/0
g05n09:221461:221694 [0] NCCL INFO Channel 02/0 : 83[5] -> 84[0] [receive] via NET/IB/0
g05n09:221461:221694 [0] NCCL INFO Channel 00/0 : 84[0] -> 85[1] via P2P/IPC
g05n09:221461:221694 [0] NCCL INFO Channel 02/0 : 84[0] -> 85[1] via P2P/IPC
g05n09:221461:221694 [0] NCCL INFO Channel 01/0 : 84[0] -> 93[3] [send] via NET/IB/2
g05n09:221461:221694 [0] NCCL INFO Channel 03/0 : 84[0] -> 93[3] [send] via NET/IB/2
g05n09:221461:221694 [0] NCCL INFO Connected all rings
g05n09:221461:221694 [0] NCCL INFO Channel 01/0 : 84[0] -> 85[1] via P2P/IPC
g05n09:221461:221694 [0] NCCL INFO Channel 03/0 : 84[0] -> 85[1] via P2P/IPC
g05n09:221461:221694 [0] NCCL INFO Channel 01/0 : 84[0] -> 86[2] via P2P/IPC
g05n09:221461:221694 [0] NCCL INFO Channel 03/0 : 84[0] -> 86[2] via P2P/IPC
g05n09:221461:221694 [0] NCCL INFO Channel 02/0 : 79[1] -> 84[0] [receive] via NET/IB/0
g05n09:221461:221694 [0] NCCL INFO Channel 00/0 : 84[0] -> 90[0] [send] via NET/IB/0
g05n09:221461:221694 [0] NCCL INFO Channel 00/0 : 72[0] -> 84[0] [receive] via NET/IB/0
g05n09:221461:221694 [0] NCCL INFO Channel 00/0 : 84[0] -> 72[0] [send] via NET/IB/0
g05n09:221461:221694 [0] NCCL INFO Channel 00/0 : 90[0] -> 84[0] [receive] via NET/IB/0
g05n09:221461:221694 [0] NCCL INFO Channel 02/0 : 84[0] -> 79[1] [send] via NET/IB/0
g05n09:221461:221694 [0] NCCL INFO Connected all trees
g05n09:221461:221694 [0] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g05n09:221461:221694 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g05n09:221461:221694 [0] NCCL INFO comm 0x11ae83a50 rank 84 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x7b12926388d12ce3 - Init COMPLETE
g05n09:221461:221780 [0] NCCL INFO Using network IB
g05n09:221461:221780 [0] NCCL INFO comm 0x11cb8c900 rank 10 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x7dd05d77214d2e8 - Init START
g05n09:221461:221780 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g05n09:221461:221780 [0] NCCL INFO Trees [0] 9/11/-1->10->8 [1] -1/-1/-1->10->9
g05n09:221461:221780 [0] NCCL INFO P2P Chunksize set to 131072
g05n09:221461:221780 [0] NCCL INFO Channel 00/0 : 9[4] -> 10[0] [receive] via NET/IB/0
g05n09:221461:221780 [0] NCCL INFO Channel 01/0 : 9[4] -> 10[0] [receive] via NET/IB/0
g05n09:221461:221780 [0] NCCL INFO Channel 00/0 : 10[0] -> 11[2] [send] via NET/IB/0
g05n09:221461:221780 [0] NCCL INFO Channel 01/0 : 10[0] -> 11[2] [send] via NET/IB/0
g05n09:221461:221780 [0] NCCL INFO Connected all rings
g05n09:221461:221780 [0] NCCL INFO Channel 00/0 : 8[2] -> 10[0] [receive] via NET/IB/0
g05n09:221461:221780 [0] NCCL INFO Channel 00/0 : 10[0] -> 8[2] [send] via NET/IB/0
g05n09:221461:221780 [0] NCCL INFO Channel 00/0 : 11[2] -> 10[0] [receive] via NET/IB/0
g05n09:221461:221780 [0] NCCL INFO Channel 00/0 : 10[0] -> 9[4] [send] via NET/IB/0
g05n09:221461:221780 [0] NCCL INFO Channel 01/0 : 10[0] -> 9[4] [send] via NET/IB/0
g05n09:221461:221780 [0] NCCL INFO Connected all trees
g05n09:221461:221780 [0] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g05n09:221461:221780 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g05n09:221461:221780 [0] NCCL INFO comm 0x11cb8c900 rank 10 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x7dd05d77214d2e8 - Init COMPLETE
g05n09:221461:221802 [0] NCCL INFO Using network IB
g05n09:221461:221802 [0] NCCL INFO comm 0x11dc12d00 rank 21 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0xaedb5f80914767fd - Init START
g05n09:221461:221802 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g05n09:221461:221802 [0] NCCL INFO Trees [0] 22/23/-1->21->18 [1] 22/-1/-1->21->20
g05n09:221461:221802 [0] NCCL INFO P2P Chunksize set to 131072
g05n09:221461:221802 [0] NCCL INFO Channel 00/0 : 20[2] -> 21[0] [receive] via NET/IB/0
g05n09:221461:221802 [0] NCCL INFO Channel 01/0 : 20[2] -> 21[0] [receive] via NET/IB/0
g05n09:221461:221802 [0] NCCL INFO Channel 00/0 : 21[0] -> 22[4] via P2P/IPC
g05n09:221461:221802 [0] NCCL INFO Channel 01/0 : 21[0] -> 22[4] via P2P/IPC
g05n09:221461:221802 [0] NCCL INFO Connected all rings
g05n09:221461:221802 [0] NCCL INFO Channel 00/0 : 21[0] -> 23[2] [send] via NET/IB/0
g05n09:221461:221802 [0] NCCL INFO Channel 00/0 : 18[0] -> 21[0] [receive] via NET/IB/0
g05n09:221461:221802 [0] NCCL INFO Channel 00/0 : 21[0] -> 18[0] [send] via NET/IB/0
g05n09:221461:221802 [0] NCCL INFO Channel 00/0 : 23[2] -> 21[0] [receive] via NET/IB/0
g05n09:221461:221802 [0] NCCL INFO Channel 01/0 : 21[0] -> 20[2] [send] via NET/IB/0
g05n09:221461:221802 [0] NCCL INFO Connected all trees
g05n09:221461:221802 [0] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g05n09:221461:221802 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g05n09:221461:221802 [0] NCCL INFO comm 0x11dc12d00 rank 21 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0xaedb5f80914767fd - Init COMPLETE
g05n09:221468:221468 [4] NCCL INFO cudaDriverVersion 12020
g05n09:221468:221468 [4] NCCL INFO Bootstrap : Using ib0:10.41.16.21<0>
g05n09:221468:221468 [4] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g05n09:221468:221468 [4] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g05n09:221468:221695 [4] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.16.21<0>
g05n09:221468:221695 [4] NCCL INFO Using network IB
g05n09:221468:221695 [4] NCCL INFO comm 0x17d613c20 rank 88 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x7b12926388d12ce3 - Init START
g05n09:221468:221695 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g05n09:221468:221695 [4] NCCL INFO Trees [0] 89/-1/-1->88->87 [1] 87/94/-1->88->76 [2] 89/-1/-1->88->87 [3] 87/-1/-1->88->81
g05n09:221468:221695 [4] NCCL INFO P2P Chunksize set to 131072
g05n09:221468:221695 [4] NCCL INFO Channel 00/0 : 88[4] -> 89[5] via P2P/IPC
g05n09:221468:221695 [4] NCCL INFO Channel 01/0 : 88[4] -> 89[5] via P2P/IPC
g05n09:221468:221695 [4] NCCL INFO Channel 02/0 : 88[4] -> 89[5] via P2P/IPC
g05n09:221468:221695 [4] NCCL INFO Channel 03/0 : 88[4] -> 89[5] via P2P/IPC
g05n09:221468:221695 [4] NCCL INFO Connected all rings
g05n09:221468:221695 [4] NCCL INFO Channel 01/0 : 88[4] -> 94[4] [send] via NET/IB/3
g05n09:221468:221695 [4] NCCL INFO Channel 03/0 : 81[3] -> 88[4] [receive] via NET/IB/3
g05n09:221468:221695 [4] NCCL INFO Channel 01/0 : 76[4] -> 88[4] [receive] via NET/IB/3
g05n09:221468:221695 [4] NCCL INFO Channel 01/0 : 88[4] -> 76[4] [send] via NET/IB/3
g05n09:221468:221695 [4] NCCL INFO Channel 03/0 : 88[4] -> 81[3] [send] via NET/IB/3
g05n09:221468:221695 [4] NCCL INFO Channel 01/0 : 94[4] -> 88[4] [receive] via NET/IB/3
g05n09:221468:221695 [4] NCCL INFO Channel 00/0 : 88[4] -> 87[3] via P2P/IPC
g05n09:221468:221695 [4] NCCL INFO Channel 01/0 : 88[4] -> 87[3] via P2P/IPC
g05n09:221468:221695 [4] NCCL INFO Channel 02/0 : 88[4] -> 87[3] via P2P/IPC
g05n09:221468:221695 [4] NCCL INFO Channel 03/0 : 88[4] -> 87[3] via P2P/IPC
g05n09:221468:221695 [4] NCCL INFO Connected all trees
g05n09:221468:221695 [4] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g05n09:221468:221695 [4] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g05n09:221468:221695 [4] NCCL INFO comm 0x17d613c20 rank 88 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x7b12926388d12ce3 - Init COMPLETE
g05n09:221468:221784 [4] NCCL INFO Using network IB
g05n09:221468:221784 [4] NCCL INFO comm 0x1804c6b20 rank 11 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x8264b1a1a7aef6de - Init START
g05n09:221468:221784 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g05n09:221468:221784 [4] NCCL INFO Trees [0] -1/-1/-1->11->10 [1] 3/-1/-1->11->-1
g05n09:221468:221784 [4] NCCL INFO P2P Chunksize set to 131072
g05n09:221468:221784 [4] NCCL INFO Channel 00/0 : 10[2] -> 11[4] [receive] via NET/IB/1
g05n09:221468:221784 [4] NCCL INFO Channel 01/0 : 10[2] -> 11[4] [receive] via NET/IB/1
g05n09:221468:221784 [4] NCCL INFO Channel 00/0 : 11[4] -> 0[0] [send] via NET/IB/1
g05n09:221468:221784 [4] NCCL INFO Channel 01/0 : 11[4] -> 0[0] [send] via NET/IB/1
g05n09:221468:221784 [4] NCCL INFO Connected all rings
g05n09:221468:221784 [4] NCCL INFO Channel 01/0 : 11[4] -> 3[0] [send] via NET/IB/1
g05n09:221468:221784 [4] NCCL INFO Channel 01/0 : 3[0] -> 11[4] [receive] via NET/IB/1
g05n09:221468:221784 [4] NCCL INFO Channel 00/0 : 11[4] -> 10[2] [send] via NET/IB/1
g05n09:221468:221784 [4] NCCL INFO Connected all trees
g05n09:221468:221784 [4] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g05n09:221468:221784 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g05n09:221468:221784 [4] NCCL INFO comm 0x1804c6b20 rank 11 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x8264b1a1a7aef6de - Init COMPLETE
g05n09:221468:221799 [g05n10:696646:696646 [2] NCCL INFO cudaDriverVersion 12020
g05n10:696646:696646 [2] NCCL INFO Bootstrap : Using ib0:10.41.16.22<0>
g05n10:696646:696646 [2] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g05n10:696646:696646 [2] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g05n10:696646:696879 [2] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.16.22<0>
g05n10:696646:696879 [2] NCCL INFO Using network IB
g05n10:696646:696879 [2] NCCL INFO comm 0x13e7f42e0 rank 92 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x7b12926388d12ce3 - Init START
g05n10:696646:696879 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g05n10:696646:696879 [2] NCCL INFO Trees [0] 93/-1/-1->92->91 [1] -1/-1/-1->92->90 [2] 93/-1/-1->92->91 [3] -1/-1/-1->92->90
g05n10:696646:696879 [2] NCCL INFO P2P Chunksize set to 131072
g05n10:696646:696879 [2] NCCL INFO Channel 00/0 : 92[2] -> 93[3] via P2P/IPC
g05n10:696646:696879 [2] NCCL INFO Channel 02/0 : 92[2] -> 93[3] via P2P/IPC
g05n10:696646:696879 [2] NCCL INFO Channel 01/0 : 92[2] -> 91[1] via P2P/IPC
g05n10:696646:696879 [2] NCCL INFO Channel 03/0 : 92[2] -> 91[1] via P2P/IPC
g05n10:696646:696879 [2] NCCL INFO Connected all rings
g05n10:696646:696879 [2] NCCL INFO Channel 01/0 : 92[2] -> 90[0] via P2P/IPC
g05n10:696646:696879 [2] NCCL INFO Channel 03/0 : 92[2] -> 90[0] via P2P/IPC
g05n10:696646:696879 [2] NCCL INFO Channel 00/0 : 92[2] -> 91[1] via P2P/IPC
g05n10:696646:696879 [2] NCCL INFO Channel 02/0 : 92[2] -> 91[1] via P2P/IPC
g05n10:696646:696879 [2] NCCL INFO Connected all trees
g05n10:696646:696879 [2] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g05n10:696646:696879 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g05n10:696646:696879 [2] NCCL INFO comm 0x13e7f42e0 rank 92 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x7b12926388d12ce3 - Init COMPLETE
g05n10:696646:696960 [2] NCCL INFO Using network IB
g05n10:696646:696960 [2] NCCL INFO comm 0x14251b000 rank 11 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0x7dd05d77214d2e8 - Init START
g05n10:696646:696960 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g05n10:696646:696960 [2] NCCL INFO Trees [0] -1/-1/-1->11->10 [1] 3/-1/-1->11->-1
g05n10:696646:696960 [2] NCCL INFO P2P Chunksize set to 131072
g05n10:696646:696960 [2] NCCL INFO Channel 00/0 : 10[0] -> 11[2] [receive] via NET/IB/0
g05n10:696646:696960 [2] NCCL INFO Channel 01/0 : 10[0] -> 11[2] [receive] via NET/IB/0
g05n10:696646:696960 [2] NCCL INFO Channel 00/0 : 11[2] -> 0[4] [send] via NET/IB/0
g05n10:696646:696960 [2] NCCL INFO Channel 01/0 : 11[2] -> 0[4] [send] via NET/IB/0
g05n10:696646:696960 [2] NCCL INFO Connected all rings
g05n10:696646:696960 [2] NCCL INFO Channel 01/0 : 11[2] -> 3[4] [send] via NET/IB/0
g05n10:696646:696960 [2] NCCL INFO Channel 01/0 : 3[4] -> 11[2] [receive] via NET/IB/0
g05n10:696646:696960 [2] NCCL INFO Channel 00/0 : 11[2] -> 10[0] [send] via NET/IB/0
g05n10:696646:696960 [2] NCCL INFO Connected all trees
g05n10:696646:696960 [2] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g05n10:696646:696960 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g05n10:696646:696960 [2] NCCL INFO comm 0x14251b000 rank 11 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0x7dd05d77214d2e8 - Init COMPLETE
g05n10:696646:696980 [2] NCCL INFO Using network IB
g05n10:696646:696980 [2] NCCL INFO comm 0x1414de700 rank 23 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0xaedb5f80914767fd - Init START
g05n10:696646:696980 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g05n10:696646:696980 [2] NCCL INFO Trees [0] -1/-1/-1->23->21 [1] 11/-1/-1->23->-1
g05n10:696646:696980 [2] NCCL INFO P2P Chunksize set to 131072
g05n10:696646:696980 [2] NCCL INFO Channel 00/0 : 22[4] -> 23[2] [receive] via NET/IB/0
g05n10:696646:696980 [2] NCCL INFO Channel 01/0 : 22[4] -> 23[2] [re4] NCCL INFO Using network IB
g05n09:221468:221799 [4] NCCL INFO comm 0x1813b2260 rank 22 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xaedb5f80914767fd - Init START
g05n09:221468:221799 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g05n09:221468:221799 [4] NCCL INFO Trees [0] 20/-1/-1->22->21 [1] -1/-1/-1->22->21
g05n09:221468:221799 [4] NCCL INFO P2P Chunksize set to 131072
g05n09:221468:221799 [4] NCCL INFO Channel 00/0 : 22[4] -> 23[2] [send] via NET/IB/1
g05n09:221468:221799 [4] NCCL INFO Channel 01/0 : 22[4] -> 23[2] [send] via NET/IB/1
g05n09:221468:221799 [4] NCCL INFO Connected all rings
g05n09:221468:221799 [4] NCCL INFO Channel 00/0 : 20[2] -> 22[4] [receive] via NET/IB/0
g05n09:221468:221799 [4] NCCL INFO Channel 00/0 : 22[4] -> 20[2] [send] via NET/IB/0
g05n09:221468:221799 [4] NCCL INFO Channel 00/0 : 22[4] -> 21[0] via P2P/IPC
g05n09:221468:221799 [4] NCCL INFO Channel 01/0 : 22[4] -> 21[0] via P2P/IPC
g05n09:221468:221799 [4] NCCL INFO Connected all trees
g05n09:221468:221799 [4] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g05n09:221468:221799 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g05n09:221468:221799 [4] NCCL INFO comm 0x1813b2260 rank 22 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xaedb5f80914767fd - Init COMPLETE
g05n10:696644:696644 [1] NCCL INFO cudaDriverVersion 12020
g05n10:696644:696644 [1] NCCL INFO Bootstrap : Using ib0:10.41.16.22<0>
g05n10:696644:696644 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g05n10:696644:696644 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g05n10:696644:696877 [1] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.16.22<0>
g05n10:696644:696877 [1] NCCL INFO Using network IB
g05n10:696644:696877 [1] NCCL INFO comm 0x14f3041e0 rank 91 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x7b12926388d12ce3 - Init START
g05n10:696644:696877 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g05n10:696644:696877 [1] NCCL INFO Trees [0] 92/-1/-1->91->90 [1] 90/-1/-1->91->95 [2] 92/-1/-1->91->90 [3] 90/-1/-1->91->95
g05n10:696644:696877 [1] NCCL INFO P2P Chunksize set to 131072
g05n10:696644:696877 [1] NCCL INFO Channel 00/0 : 91[1] -> 92[2] via P2P/IPC
g05n10:696644:696877 [1] NCCL INFO Channel 02/0 : 91[1] -> 92[2] via P2P/IPC
g05n10:696644:696877 [1] NCCL INFO Channel 01/0 : 91[1] -> 90[0] via P2P/IPC
g05n10:696644:696877 [1] NCCL INFO Channel 03/0 : 91[1] -> 90[0] via P2P/IPC
g05n10:696644:696877 [1] NCCL INFO Connected all rings
g05n10:696644:696877 [1] NCCL INFO Channel 01/0 : 91[1] -> 95[5] via P2P/IPC
g05n10:696644:696877 [1] NCCL INFO Channel 03/0 : 91[1] -> 95[5] via P2P/IPC
g05n10:696644:696877 [1] NCCL INFO Channel 00/0 : 91[1] -> 90[0] via P2P/IPC
g05n10:696644:696877 [1] NCCL INFO Channel 02/0 : 91[1] -> 90[0] via P2P/IPC
g05n10:696644:696877 [1] NCCL INFO Connected all trees
g05n10:696644:696877 [1] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g05n10:696644:696877 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g05n10:696644:696877 [1] NCCL INFO comm 0x14f3041e0 rank 91 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x7b12926388d12ce3 - Init COMPLETE
g05n10:696644:696964 [1] NCCL INFO Using network IB
g05n10:696644:696964 [1] NCCL INFO comm 0x150fe45f0 rank 11 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x1defb2708b178c98 - Init START
g05n10:696644:696964 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g05n10:696644:696964 [1] NCCL INFO Trees [0] -1/-1/-1->11->10 [1] 3/-1/-1->11->-1
g05n10:696644:696964 [1] NCCL INFO P2P Chunksize set to 131072
g05n10:696644:696964 [1] NCCL INFO Channel 00/0 : 10[5] -> 11[1] [receive] via NET/IB/2
g05n10:696644:696964 [1] NCCL INFO Channel 01/0 : 10[5] -> 11[1] [receive] via NET/IB/2
g05n10:696644:696964 [1] NCCL INFO Channel 00/0 : 11[1] -> 0[3] [send] via NET/IB/2
g05n10:696644:696964 [1] NCCL INFO Channel 01/0 : 11[1] -> 0[3] [send] via NET/IB/2
g05n10:696644:696964 [1] NCCL INFO Connected all rings
g05n10:696644:696964 [1] NCCL INFO Channel 01/0 : 11[1] -> 3[3] [send] via NET/IB/2
g05n10:696644:696964 [1] NCCL INFO Channel 01/0 : 3[3] -> 11[1] [receive] via NET/IB/2
g05n10:696644:696964 [1] NCCL INFO Channel 00/0 : 11[1] -> 10[5] [send] via NET/IB/2
g05n10:696644:696964 [1] NCCL INFO Connected all trees
g05n10:696644:696964 [1] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g05n10:696644:696964 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g05n10:696644:696964 [1] NCCL INFO comm 0x150fe45f0 rank 11 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x1defb2708b178c98 - Init COMPLETE
g05n10:696644:696982 [1] NCCL INFO Using network IB
g05n10:696644:696982 [1] NCCL INFO comm 0x150f18970 rank 22 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0x821cecfd780eaba1 - Init START
g05n10:696644:696982 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g05n10:696644:696982 [1] NCCL INFO Trees [0] 23/-1/-1->22->21 [1] 23/10/-1->22->-1
g05n10:696644:696982 [1] NCCL INFO P2P Chunksize set to 131072
g05n10:696644:696982 [1] NCCL INFO Channel 00/0 : 21[3] -> 22[1] [receive] via NET/IB/2
g05n10:696644:696982 [1] NCCL INFO Channel 01/0 : 21[3] -> 22[1] [receive] via NET/IB/2
g05n10:696644:696982 [1] NCCL INFO Channel 00/0 : 22[1] -> 23[5] via P2P/IPC
g05n10:696644:696982 [1] NCCL INFO Channel 01/0 : 22[1] -> 23[5] via P2P/IPC
g05n10:696644:696982 [1] NCCL INFO Connected all rings
g05n10:696644:696982 [1] NCCL INFO Channel 01/0 : 10[1] -> 22[1] [receive] via NET/IB/2
g05n10:696644:696982 [1] NCCL INFO Channel 01/0 : 22[1] -> 10[1] [send] via NET/IB/2
g05n10:696644:696982 [1] NCCL INFO Channel 00/0 : 22[1] -> 21[3] [send] via NET/IB/2
g05n10:696644:696982 [1] NCCL INFO Connected all trees
g05n10:696644:696982 [1] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g05n10:696644:696982 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g05n10:696644:696982 [1] NCCL INFO comm 0x150f18970 rank 22 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0x821cecfd780eaba1 - Init COMPLETE
ceive] via NET/IB/0
g05n10:696646:696980 [2] NCCL INFO Channel 00/0 : 23[2] -> 0[0] [send] via NET/IB/0
g05n10:696646:696980 [2] NCCL INFO Channel 01/0 : 23[2] -> 0[0] [send] via NET/IB/0
g05n10:696646:696980 [2] NCCL INFO Connected all rings
g05n10:696646:696980 [2] NCCL INFO Channel 00/0 : 21[0] -> 23[2] [receive] via NET/IB/0
g05n10:696646:696980 [2] NCCL INFO Channel 01/0 : 11[2] -> 23[2] [receive] via NET/IB/0
g05n10:696646:696980 [2] NCCL INFO Channel 01/0 : 23[2] -> 11[2] [send] via NET/IB/0
g05n10:696646:696980 [2] NCCL INFO Channel 00/0 : 23[2] -> 21[0] [send] via NET/IB/0
g05n10:696646:696980 [2] NCCL INFO Connected all trees
g05n10:696646:696980 [2] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g05n10:696646:696980 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g05n10:696646:696980 [2] NCCL INFO comm 0x1414de700 rank 23 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0xaedb5f80914767fd - Init COMPLETE
g05n10:696649:696649 [3] NCCL INFO cudaDriverVersion 12020
g05n10:696649:696649 [3] NCCL INFO Bootstrap : Using ib0:10.41.16.22<0>
g05n10:696649:696649 [3] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g05n10:696649:696649 [3] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g05n10:696649:696882 [3] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.16.22<0>
g05n10:696649:696882 [3] NCCL INFO Using network IB
g05n10:696649:696882 [3] NCCL INFO comm 0x1478638a0 rank 93 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x7b12926388d12ce3 - Init START
g05n10:696649:696882 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g05n10:696649:696882 [3] NCCL INFO Trees [0] 94/-1/-1->93->92 [1] 95/-1/-1->93->94 [2] 94/-1/-1->93->92 [3] 95/-1/-1->93->94
g05n10:696649:696882 [3] NCCL INFO P2P Chunksize set to 131072
g05n10:696649:696882 [3] NCCL INFO Channel 00/0 : 93[3] -> 94[4] via P2P/IPC
g05n10:696649:696882 [3] NCCL INFO Channel 01/0 : 93[3] -> 94[4] via P2P/IPC
g05n10:696649:696882 [3] NCCL INFO Channel 02/0 : 93[3] -> 94[4] via P2P/IPC
g05n10:696649:696882 [3] NCCL INFO Channel 03/0 : 93[3] -> 94[4] via P2P/IPC
g05n10:696649:696882 [3] NCCL INFO Channel 01/0 : 84[0] -> 93[3] [receive] via NET/IB/3
g05n10:696649:696882 [3] NCCL INFO Channel 03/0 : 84[0] -> 93[3] [receive] via NET/IB/3
g05n10:696649:696882 [3] NCCL INFO Connected all rings
g05n10:696649:696882 [3] NCCL INFO Channel 01/0 : 93[3] -> 95[5] via P2P/IPC
g05n10:696649:696882 [3] NCCL INFO Channel 03/0 : 93[3] -> 95[5] via P2P/IPC
g05n10:696649:696882 [3] NCCL INFO Channel 00/0 : 93[3] -> 92[2] via P2P/IPC
g05n10:696649:696882 [3] NCCL INFO Channel 02/0 : 93[3] -> 92[2] via P2P/IPC
g05n10:696649:696882 [3] NCCL INFO Connected all trees
g05n10:696649:696882 [3] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g05n10:696649:696882 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g05n10:696649:696882 [3] NCCL INFO comm 0x1478638a0 rank 93 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x7b12926388d12ce3 - Init COMPLETE
g05n10:696649:696962 [3] NCCL INFO Using network IB
g05n10:696649:696962 [3] NCCL INFO comm 0x14a50b860 rank 11 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xa77518dab25ce26 - Init START
g05n10:696649:696962 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g05n10:696649:696962 [3] NCCL INFO Trees [0] -1/-1/-1->11->10 [1] 3/-1/-1->11->-1
g05n10:696649:696962 [3] NCCL INFO P2P Chunksize set to 131072
g05n10:696649:696962 [3] NCCL INFO Channel 00/0 : 10[1] -> 11[3] [receive] via NET/IB/3
g05n10:696649:696962 [3] NCCL INFO Channel 01/0 : 10[1] -> 11[3] [receive] via NET/IB/3
g05n10:696649:696962 [3] NCCL INFO Channel 00/0 : 11[3] -> 0[5] [send] via NET/IB/3
g05n10:696649:696962 [3] NCCL INFO Channel 01/0 : 11[3] -> 0[5] [send] via NET/IB/3
g05n10:696649:696962 [3] NCCL INFO Connected all rings
g05n10:696649:696962 [3] NCCL INFO Channel 01/0 : 11[3] -> 3[5] [send] via NET/IB/3
g05n10:696649:696962 [3] NCCL INFO Channel 01/0 : 3[5] -> 11[3] [receive] via NET/IB/3
g05n10:696649:696962 [3] NCCL INFO Channel 00/0 : 11[3] -> 10[1] [send] via NET/IB/3
g05n10:696649:696962 [3] NCCL INFO Connected all trees
g05n10:696649:696962 [3] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g05n10:696649:696962 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g05n10:696649:696962 [3] NCCL INFO comm 0x14a50b860 rank 11 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xa77518dab25ce26 - Init COMPLETE
g05n10:696649:696981 [3] NCCL INFO Using network IB
g05n10:696649:696981 [3] NCCL INFO comm 0x14a512630 rank 23 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xa49a27aa8b15034e - Init START
g05n10:696649:696981 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g05n10:696649:696981 [3] NCCL INFO Trees [0] -1/-1/-1->23->21 [1] 11/-1/-1->23->-1
g05n10:6966f18n05:1007498:1007498 [1] NCCL INFO cudaDriverVersion 12020
f18n05:1007498:1007498 [1] NCCL INFO Bootstrap : Using ib0:10.41.14.109<0>
f18n05:1007498:1007498 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
f18n05:1007498:1007498 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
f18n05:1007498:1007737 [1] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.14.109<0>
f18n05:1007498:1007737 [1] NCCL INFO Using network IB
f18n05:1007498:1007737 [1] NCCL INFO comm 0x147993a40 rank 55 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x7b12926388d12ce3 - Init START
f18n05:1007498:1007737 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
f18n05:1007498:1007737 [1] NCCL INFO Trees [0] 56/-1/-1->55->54 [1] 54/-1/-1->55->59 [2] 56/60/-1->55->54 [3] 54/-1/-1->55->59
f18n05:1007498:1007737 [1] NCCL INFO P2P Chunksize set to 131072
f18n05:1007498:1007737 [1] NCCL INFO Channel 00/0 : 55[1] -> 56[2] via P2P/IPC
f18n05:1007498:1007737 [1] NCCL INFO Channel 02/0 : 55[1] -> 56[2] via P2P/IPC
f18n05:1007498:1007737 [1] NCCL INFO Channel 01/0 : 55[1] -> 54[0] via P2P/IPC
f18n05:1007498:1007737 [1] NCCL INFO Channel 03/0 : 55[1] -> 54[0] via P2P/IPC
f18n05:1007498:1007737 [1] NCCL INFO Connected all rings
f18n05:1007498:1007737 [1] NCCL INFO Channel 01/0 : 55[1] -> 59[5] via P2P/IPC
f18n05:1007498:1007737 [1] NCCL INFO Channel 03/0 : 55[1] -> 59[5] via P2P/IPC
f18n05:1007498:1007737 [1] NCCL INFO Channel 02/0 : 55[1] -> 60[0] [send] via NET/IB/0
f18n05:1007498:1007737 [1] NCCL INFO Channel 02/0 : 60[0] -> 55[1] [receive] via NET/IB/0
f18n05:1007498:1007737 [1] NCCL INFO Channel 00/0 : 55[1] -> 54[0] via P2P/IPC
f18n05:1007498:1007737 [1] NCCL INFO Channel 02/0 : 55[1] -> 54[0] via P2P/IPC
f18n05:1007498:1007737 [1] NCCL INFO Connected all trees
f18n05:1007498:1007737 [1] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
f18n05:1007498:1007737 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
f18n05:1007498:1007737 [1] NCCL INFO comm 0x147993a40 rank 55 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x7b12926388d12ce3 - Init COMPLETE
f18n05:1007498:1007819 [1] NCCL INFO Using network IB
f18n05:1007498:1007819 [1] NCCL INFO comm 0x14a2d4c40 rank 6 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x54676b30da0675d6 - Init START
f18n05:1007498:1007819 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
f18n05:1007498:1007819 [1] NCCL INFO Trees [0] 5/7/-1->6->4 [1] -1/-1/-1->6->5
f18n05:1007498:1007819 [1] NCCL INFO P2P Chunksize set to 131072
f18n05:1007498:1007819 [1] NCCL INFO Channel 00/0 : 5[5] -> 6[1] [receive] via NET/IB/2
f18n05:1007498:1007819 [1] NCCL INFO Channel 01/0 : 5[5] -> 6[1] [receive] via NET/IB/2
f18n05:1007498:1007819 [1] NCCL INFO Channel 00/0 : 6[1] -> 7[3] [send] via NET/IB/2
f18n05:1007498:1007819 [1] NCCL INFO Channel 01/0 : 6[1] -> 7[3] [send] via NET/IB/2
f18n05:1007498:1007819 [1] NCCL INFO Connected all rings
f18n05:1007498:1007819 [1] NCCL INFO Channel 00/0 : 4[3] -> 6[1] [receive] via NET/IB/2
f18n05:1007498:1007819 [1] NCCL INFO Channel 00/0 : 6[1] -> 4[3] [send] via NET/IB/2
f18n05:1007498:1007819 [1] NCCL INFO Channel 00/0 : 7[3] -> 6[1] [receive] via NET/IB/2
f18n05:1007498:1007819 [1] NCCL INFO Channel 00/0 : 6[1] -> 5[5] [send] via NET/IB/2
f18n05:1007498:1007819 [1] NCCL INFO Channel 01/0 : 6[1] -> 5[5] [send] via NET/IB/2
f18n05:1007498:1007819 [1] NCCL INFO Connected all trees
f18n05:1007498:1007819 [1] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
f18n05:1007498:1007819 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f18n05:1007498:1007819 [1] NCCL INFO comm 0x14a2d4c40 rank 6 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x54676b30da0675d6 - Init COMPLETE
f18n05:1007498:1007836 [1] NCCL INFO Using network IB
f18n05:1007498:1007836 [1] NCCL INFO comm 0x14a2991c0 rank 13 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0x821cecfd780e49:696981 [3] NCCL INFO P2P Chunksize set to 131072
g05n10:696649:696981 [3] NCCL INFO Channel 00/0 : 22[5] -> 23[3] [receive] via NET/IB/3
g05n10:696649:696981 [3] NCCL INFO Channel 01/0 : 22[5] -> 23[3] [receive] via NET/IB/3
g05n10:696649:696981 [3] NCCL INFO Channel 00/0 : 23[3] -> 0[1] [send] via NET/IB/3
g05n10:696649:696981 [3] NCCL INFO Channel 01/0 : 23[3] -> 0[1] [send] via NET/IB/3
g05n10:696649:696981 [3] NCCL INFO Connected all rings
g05n10:696649:696981 [3] NCCL INFO Channel 00/0 : 21[1] -> 23[3] [receive] via NET/IB/3
g05n10:696649:696981 [3] NCCL INFO Channel 01/0 : 11[3] -> 23[3] [receive] via NET/IB/3
g05n10:696649:696981 [3] NCCL INFO Channel 01/0 : 23[3] -> 11[3] [send] via NET/IB/3
g05n10:696649:696981 [3] NCCL INFO Channel 00/0 : 23[3] -> 21[1] [send] via NET/IB/3
g05n10:696649:696981 [3] NCCL INFO Connected all trees
g05n10:696649:696981 [3] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g05n10:696649:696981 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g05n10:696649:696981 [3] NCCL INFO comm 0x14a512630 rank 23 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xa49a27aa8b15034e - Init COMPLETE
aba1 - Init START
f18n05:1007498:1007836 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
f18n05:1007498:1007836 [1] NCCL INFO Trees [0] 14/-1/-1->13->15 [1] 14/12/-1->13->16
f18n05:1007498:1007836 [1] NCCL INFO P2P Chunksize set to 131072
f18n05:1007498:1007836 [1] NCCL INFO Channel 00/0 : 12[3] -> 13[1] [receive] via NET/IB/2
f18n05:1007498:1007836 [1] NCCL INFO Channel 01/0 : 12[3] -> 13[1] [receive] via NET/IB/2
f18n05:1007498:1007836 [1] NCCL INFO Channel 00/0 : 13[1] -> 14[5] via P2P/IPC
f18n05:1007498:1007836 [1] NCCL INFO Channel 01/0 : 13[1] -> 14[5] via P2P/IPC
f18n05:1007498:1007836 [1] NCCL INFO Connected all rings
f18n05:1007498:1007836 [1] NCCL INFO Channel 00/0 : 13[1] -> 15[3] [send] via NET/IB/2
f18n05:1007498:1007836 [1] NCCL INFO Channel 01/0 : 13[1] -> 16[1] [send] via NET/IB/2
f18n05:1007498:1007836 [1] NCCL INFO Channel 01/0 : 16[1] -> 13[1] [receive] via NET/IB/2
f18n05:1007498:1007836 [1] NCCL INFO Channel 00/0 : 15[3] -> 13[1] [receive] via NET/IB/2
f18n05:1007498:1007836 [1] NCCL INFO Channel 01/0 : 13[1] -> 12[3] [send] via NET/IB/2
f18n05:1007498:1007836 [1] NCCL INFO Connected all trees
f18n05:1007498:1007836 [1] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
f18n05:1007498:1007836 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f18n05:1007498:1007836 [1] NCCL INFO comm 0x14a2991c0 rank 13 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0x821cecfd780eaba1 - Init COMPLETE
d09n08:1072934:1072934 [2] NCCL INFO cudaDriverVersion 12020
d09n08:1072934:1072934 [2] NCCL INFO Bootstrap : Using ib0:10.41.8.172<0>
d09n08:1072934:1072934 [2] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
d09n08:1072934:1072934 [2] NCCL INFO NET/Plugin : No plugin found, using internal implementation
d09n08:1072934:1073162 [2] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.8.172<0>
d09n08:1072934:1073162 [2] NCCL INFO Using network IB
d09n08:1072934:1073162 [2] NCCL INFO comm 0x1667243f0 rank 8 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x7b12926388d12ce3 - Init START
d09n08:1072934:1073162 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
d09n08:1072934:1073162 [2] NCCL INFO Trees [0] 9/-1/-1->8->7 [1] -1/-1/-1->8->6 [2] 9/-1/-1->8->7 [3] -1/-1/-1->8->6
d09n08:1072934:1073162 [2] NCCL INFO P2P Chunksize set to 131072
d09n08:1072934:1073162 [2] NCCL INFO Channel 00/0 : 8[2] -> 9[3] via P2P/IPC
d09n08:1072934:1073162 [2] NCCL INFO Channel 02/0 : 8[2] -> 9[3] via P2P/IPC
d09n08:1072934:1073162 [2] NCCL INFO Channel 01/0 : 8[2] -> 7[1] via P2P/IPC
d09n08:1072934:1073162 [2] NCCL INFO Channel 03/0 : 8[2] -> 7[1] via P2P/IPC
d09n08:1072934:1073162 [2] NCCL INFO Connected all rings
d09n08:1072934:1073162 [2] NCCL INFO Channel 01/0 : 8[2] -> 6[0] via P2P/IPC
d09n08:1072934:1073162 [2] NCCL INFO Channel 03/0 : 8[2] -> 6[0] via P2P/IPC
d09n08:1072934:1073162 [2] NCCL INFO Channel 00/0 : 8[2] -> 7[1] via P2P/IPC
d09n08:1072934:1073162 [2] NCCL INFO Channel 02/0 : 8[2] -> 7[1] via P2P/IPC
d09n08:1072934:1073162 [2] NCCL INFO Connected all trees
d09n08:1072934:1073162 [2] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
d09n08:1072934:1073162 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
d09n08:1072934:1073162 [2] NCCL INFO comm 0x1667243f0 rank 8 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x7b12926388d12ce3 - Init COMPLETE
d09n08:1072934:1073251 [2] NCCL INFO Using network IB
d09n08:1072934:1073251 [2] NCCL INFO comm 0x1695c8970 rank 1 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0x8264b1a1a7aef6de - Init START
d09n08:1072934:1073251 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
d09n08:1072934:1073251 [2] NCCL INFO Trees [0] -1/-1/-1->1->2 [1] 2/0/-1->1->3
d09n08:1072934:1073251 [2] NCCL INFO P2P Chunksize set to 131072
d09n08:1072934:1073251 [2] NCCL INFO Channel 00/0 : 0[0] -> 1[2] [receive] via NET/IB/0
d09n08:1072934:1073251 [2] NCCL INFO Channel 01/0 : 0[0] -> 1[2] [receive] via NET/IB/0
d09n08:1072934:1073251 [2] NCCL INFO Channel 00/0 : 1[2] -> 2[4] [send] via NET/IB/0
d09n08:1072934:1073251 [2] NCCL INFO Channel 01/0 : 1[2] -> 2[4] [send] via NET/IB/0
d09n08:1072934:1073251 [2] NCCL INFO Connected all rings
d09n08:1072934:1073251 [2] NCCL INFO Channel 01/0 : 1[2] -> 3[0] [send] via NET/IB/0
d09n08:1072934:1073251 [2] NCCL INFO Channel 01/0 : 3[0] -> 1[2] [receive] via NET/IB/0
d09n08:1072934:1073251 [2] NCCL INFO Channel 00/0 : 2[4] -> 1[2] [receive] via NET/IB/0
d09n08:1072934:1073251 [2] NCCL INFO Channel 01/0 : 2[4] -> 1[2] [receive] via NET/IB/0
d09n08:1072934:1073251 [2] NCCL INFO Channel 01/0 : 1[2] -> 0[0] [send] via NET/IB/0
d09n08:1072934:1073251 [2] NCCL INFO Connected all trees
d09n08:1072934:1073251 [2] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
d09n08:1072934:1073251 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d09n08:1072934:1073251 [2] NCCL INFO comm 0x1695c8970 rank 1 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0x8264b1a1a7aef6de - Init COMPLETE
d09n08:1072934:1073267 [2] NCCL INFO Using network IB
d09n08:1072934:1073267 [2] NCCL INFO comm 0x1695d2970 rank 2 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0xaedb5f80914767fd - Init START
d09n08:1072934:1073267 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
d09n08:1072934:1073267 [2] NCCL INFO Trees [0] -1/-1/-1->2->4 [1] 3/0/-1->2->5
d09n0f18n06:1026678:1026678 [0] NCCL INFO cudaDriverVersion 12020
f18n06:1026678:1026678 [0] NCCL INFO Bootstrap : Using ib0:10.41.14.110<0>
f18n06:1026678:1026678 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
f18n06:1026678:1026678 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
f18n06:1026678:1026912 [0] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.14.110<0>
f18n06:1026678:1026912 [0] NCCL INFO Using network IB
f18n06:1026678:1026912 [0] NCCL INFO comm 0x1583833c0 rank 60 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x7b12926388d12ce3 - Init START
f18n06:1026678:1026912 [0] NCCL INFO Setting affinity for GPU 0 to 0f
f18n06:1026678:1026912 [0] NCCL INFO Trees [0] 61/66/-1->60->73 [1] 62/-1/-1->60->61 [2] 61/-1/-1->60->55 [3] 62/-1/-1->60->61
f18n06:1026678:1026912 [0] NCCL INFO P2P Chunksize set to 131072
f18n06:1026678:1026912 [0] NCCL INFO Channel 00/0 : 59[5] -> 60[0] [receive] via NET/IB/0
f18n06:1026678:1026912 [0] NCCL INFO Channel 02/0 : 59[5] -> 60[0] [receive] via NET/IB/0
f18n06:1026678:1026912 [0] NCCL INFO Channel 00/0 : 60[0] -> 61[1] via P2P/IPC
f18n06:1026678:1026912 [0] NCCL INFO Channel 02/0 : 60[0] -> 61[1] via P2P/IPC
f18n06:1026678:1026912 [0] NCCL INFO Channel 01/0 : 60[0] -> 69[3] [send] via NET/IB/2
f18n06:1026678:1026912 [0] NCCL INFO Channel 03/0 : 60[0] -> 69[3] [send] via NET/IB/2
f18n06:1026678:1026912 [0] NCCL INFO Connected all rings
f18n06:1026678:1026912 [0] NCCL INFO Channel 01/0 : 60[0] -> 61[1] via P2P/IPC
f18n06:1026678:1026912 [0] NCCL INFO Channel 03/0 : 60[0] -> 61[1] via P2P/IPC
f18n06:1026678:1026912 [0] NCCL INFO Channel 01/0 : 60[0] -> 62[2] via P2P/IPC
f18n06:1026678:1026912 [0] NCCL INFO Channel 03/0 : 60[0] -> 62[2] via P2P/IPC
f18n06:1026678:1026912 [0] NCCL INFO Channel 02/0 : 55[1] -> 60[0] [receive] via NET/IB/0
f18n06:1026678:1026912 [0] NCCL INFO Channel 00/0 : 60[0] -> 66[0] [send] via NET/IB/0
f18n06:1026678:1026912 [0] NCCL INFO Channel 00/0 : 60[0] -> 73[1] [send] via NET/IB/0
f18n06:1026678:1026912 [0] NCCL INFO Channel 00/0 : 73[1] -> 60[0] [receive] via NET/IB/0
f18n06:1026678:1026912 [0] NCCL INFO Channel 00/0 : 66[0] -> 60[0] [receive] via NET/IB/0
f18n06:1026678:1026912 [0] NCCL INFO Channel 02/0 : 60[0] -> 55[1] [send] via NET/IB/0
f18n06:1026678:1026912 [0] NCCL INFO Connected all trees
f18n06:1026678:1026912 [0] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
f18n06:1026678:1026912 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
f18n06:1026678:1026912 [0] NCCL INFO comm 0x1583833c0 rank 60 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x7b12926388d12ce3 - Init COMPLETE
f18n06:1026678:1027000 [0] NCCL INFO Using network IB
f18n06:1026678:1027000 [0] NCCL INFO comm 0x15a097de0 rank 7 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x7dd05d77214d2e8 - Init START
f18n06:1026678:1027000 [0] NCCL INFO Setting affinity for GPU 0 to 0f
f18n06:1026678:1027000 [0] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] 9/5/-1->7->3
f18n06:1026678:1027000 [0] NCCL INFO P2P Chunksize set to 131072
f18n06:1026678:1027000 [0] NCCL INFO Channel 00/0 : 6[4] -> 7[0] [receive] via NET/IB/0
f18n06:1026678:1027000 [0] NCCL INFO Channel 01/0 : 6[4] -> 7[0] [receive] via NET/IB/0
f18n06:1026678:1027000 [0] NCCL INFO Channel 00/0 : 7[0] -> 8[2] [send] via NET/IB/0
f18n06:1026678:1027000 [0] NCCL INFO Channel 01/0 : 7[0] -> 8[2] [send] via NET/IB/0
f18n06:1026678:1027000 [0] NCCL INFO Connected all rings
f18n06:1026678:1027000 [0] NCCL INFO Channel 01/0 : 5[2] -> 7[0] [receive] via NET/IB/0
f18n06:1026678:1027000 [0] NCCL INFO Channel 01/0 : 7[0] -> 9[4] [send] via NET/IB/0
f18n06:1026678:1027000 [0] NCCL INFO Channel 01/0 : 3[4] -> 7[0] [receive] via NET/IB/0
f18n06:1026678:1027000 [0] NCCL INFO Channel 01/0 : 7[0] -> 3[4] [send] via NET/IB/0
f18n06:1026678:1027000 [0] NCCL INFO Channel 01/0 : 9[4] -> 7[0] [receive] via NET/IB/0
f18n06:1026678:1027000 [0] NCCL INFO f18n05:1007500:1007500 [2] NCCL INFO cudaDriverVersion 12020
f18n05:1007500:1007500 [2] NCCL INFO Bootstrap : Using ib0:10.41.14.109<0>
f18n05:1007500:1007500 [2] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
f18n05:1007500:1007500 [2] NCCL INFO NET/Plugin : No plugin found, using internal implementation
f18n05:1007500:1007738 [2] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.14.109<0>
f18n05:1007500:1007738 [2] NCCL INFO Using network IB
f18n05:1007500:1007738 [2] NCCL INFO comm 0x138db41d0 rank 56 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x7b12926388d12ce3 - Init START
f18n05:1007500:1007738 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
f18n05:1007500:1007738 [2] NCCL INFO Trees [0] 57/-1/-1->56->55 [1] -1/-1/-1->56->54 [2] 57/-1/-1->56->55 [3] -1/-1/-1->56->54
f18n05:1007500:1007738 [2] NCCL INFO P2P Chunksize set to 131072
f18n05:1007500:1007738 [2] NCCL INFO Channel 00/0 : 56[2] -> 57[3] via P2P/IPC
f18n05:1007500:1007738 [2] NCCL INFO Channel 02/0 : 56[2] -> 57[3] via P2P/IPC
f18n05:1007500:1007738 [2] NCCL INFO Channel 01/0 : 56[2] -> 55[1] via P2P/IPC
f18n05:1007500:1007738 [2] NCCL INFO Channel 03/0 : 56[2] -> 55[1] via P2P/IPC
f18n05:1007500:1007738 [2] NCCL INFO Connected all rings
f18n05:1007500:1007738 [2] NCCL INFO Channel 01/0 : 56[2] -> 54[0] via P2P/IPC
f18n05:1007500:1007738 [2] NCCL INFO Channel 03/0 : 56[2] -> 54[0] via P2P/IPC
f18n05:1007500:1007738 [2] NCCL INFO Channel 00/0 : 56[2] -> 55[1] via P2P/IPC
f18n05:1007500:1007738 [2] NCCL INFO Channel 02/0 : 56[2] -> 55[1] via P2P/IPC
f18n05:1007500:1007738 [2] NCCL INFO Connected all trees
f18n05:1007500:1007738 [2] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
f18n05:1007500:1007738 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
f18n05:1007500:1007738 [2] NCCL INFO comm 0x138db41d0 rank 56 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x7b12926388d12ce3 - Init COMPLETE
f18n05:1007500:1007820 [2] NCCL INFO Using network IB
f18n05:1007500:1007820 [2] NCCL INFO comm 0x13ba89730 rank 7 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0x8264b1a1a7aef6de - Init START
f18n05:1007500:1007820 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
f18n05:1007500:1007820 [2] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] 9/5/-1->7->3
f18n05:1007500:1007820 [2] NCCL INFO P2P Chunksize set to 131072
f18n05:1007500:1007820 [2] NCCL INFO Channel 00/0 : 6[0] -> 7[2] [receive] via NET/IB/0
f18n05:1007500:1007820 [2] NCCL INFO Channel 01/0 : 6[0] -> 7[2] [receive] via NET/IB/0
f18n05:1007500:1007820 [2] NCCL INFO Channel 00/0 : 7[2] -> 8[4] [send] via NET/IB/0
f18n05:1007500:1007820 [2] NCCL INFO Channel 01/0 : 7[2] -> 8[4] [send] via NET/IB/0
f18n05:1007500:1007820 [2] NCCL INFO Connected all rings
f18n05:1007500:1007820 [2] NCCL INFO Channel 01/0 : 5[4] -> 7[2] [receive] via NET/IB/0
f18n05:1007500:1007820 [2] NCCL INFO Channel 01/0 : 7[2] -> 9[0] [send] via NET/IB/0
f18n05:1007500:1007820 [2] NCCL INFO Channel 01/0 : 3[0] -> 7[2] [receive] via NET/IB/0
f18n05:1007500:1007820 [2] NCCL INFO Channel 01/0 : 7[2] -> 3[0] [send] via NET/IB/0
f18n05:1007500:1007820 [2] NCCL INFO Channel 01/0 : 9[0] -> 7[2] [receive] via NET/IB/0
f18n05:1007500:1007820 [2] NCCL INFO Channel 01/0 : 7[2] -> 5[4] [send] via NET/IB/0
f18n05:1007500:1007820 [2] NCCL INFO Channel 00/0 : 7[2] -> 6[0] [send] via NET/IB/0
f18n05:1007500:1007820 [2] NCCL INFO Connected all trees
f18n05:1007500:1007820 [2] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
f18n05:1007500:1007820 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f18n05:1007500:1007820 [2] NCCL INFO comm 0x13ba89730 rank 7 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0x8264b1a1a7aef6de - Init COMPLETE
f18n05:1007500:1007835 [2] NCCL INFO Using network IB
f18n05:1007500:1007835 [2] NCCL INFO comm 0x13b9e2670 rank 14 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 8:1072934:1073267 [2] NCCL INFO P2P Chunksize set to 131072
d09n08:1072934:1073267 [2] NCCL INFO Channel 00/0 : 1[4] -> 2[2] [receive] via NET/IB/0
d09n08:1072934:1073267 [2] NCCL INFO Channel 01/0 : 1[4] -> 2[2] [receive] via NET/IB/0
d09n08:1072934:1073267 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[0] [send] via NET/IB/0
d09n08:1072934:1073267 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[0] [send] via NET/IB/0
d09n08:1072934:1073267 [2] NCCL INFO Connected all rings
d09n08:1072934:1073267 [2] NCCL INFO Channel 01/0 : 0[0] -> 2[2] [receive] via NET/IB/0
d09n08:1072934:1073267 [2] NCCL INFO Channel 00/0 : 2[2] -> 4[4] [send] via NET/IB/0
d09n08:1072934:1073267 [2] NCCL INFO Channel 01/0 : 2[2] -> 5[2] [send] via NET/IB/0
d09n08:1072934:1073267 [2] NCCL INFO Channel 01/0 : 5[2] -> 2[2] [receive] via NET/IB/0
d09n08:1072934:1073267 [2] NCCL INFO Channel 00/0 : 4[4] -> 2[2] [receive] via NET/IB/0
d09n08:1072934:1073267 [2] NCCL INFO Channel 01/0 : 2[2] -> 0[0] [send] via NET/IB/0
d09n08:1072934:1073267 [2] NCCL INFO Channel 01/0 : 3[0] -> 2[2] [receive] via NET/IB/0
d09n08:1072934:1073267 [2] NCCL INFO Connected all trees
d09n08:1072934:1073267 [2] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
d09n08:1072934:1073267 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d09n08:1072934:1073267 [2] NCCL INFO comm 0x1695d2970 rank 2 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0xaedb5f80914767fd - Init COMPLETE
Channel 01/0 : 7[0] -> 5[2] [send] via NET/IB/0
f18n06:1026678:1027000 [0] NCCL INFO Channel 00/0 : 7[0] -> 6[4] [send] via NET/IB/0
f18n06:1026678:1027000 [0] NCCL INFO Connected all trees
f18n06:1026678:1027000 [0] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
f18n06:1026678:1027000 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f18n06:1026678:1027000 [0] NCCL INFO comm 0x15a097de0 rank 7 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x7dd05d77214d2e8 - Init COMPLETE
f18n06:1026678:1027018 [0] NCCL INFO Using network IB
f18n06:1026678:1027018 [0] NCCL INFO comm 0x15b121720 rank 15 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0xaedb5f80914767fd - Init START
f18n06:1026678:1027018 [0] NCCL INFO Setting affinity for GPU 0 to 0f
f18n06:1026678:1027018 [0] NCCL INFO Trees [0] 16/17/-1->15->19 [1] 16/-1/-1->15->14
f18n06:1026678:1027018 [0] NCCL INFO P2P Chunksize set to 131072
f18n06:1026678:1027018 [0] NCCL INFO Channel 00/0 : 14[2] -> 15[0] [receive] via NET/IB/0
f18n06:1026678:1027018 [0] NCCL INFO Channel 01/0 : 14[2] -> 15[0] [receive] via NET/IB/0
f18n06:1026678:1027018 [0] NCCL INFO Channel 00/0 : 15[0] -> 16[4] via P2P/IPC
f18n06:1026678:1027018 [0] NCCL INFO Channel 01/0 : 15[0] -> 16[4] via P2P/IPC
f18n06:1026678:1027018 [0] NCCL INFO Connected all rings
f18n06:1026678:1027018 [0] NCCL INFO Channel 00/0 : 15[0] -> 17[2] [send] via NET/IB/0
f18n06:1026678:1027018 [0] NCCL INFO Channel 00/0 : 15[0] -> 19[4] [send] via NET/IB/0
f18n06:1026678:1027018 [0] NCCL INFO Channel 00/0 : 19[4] -> 15[0] [receive] via NET/IB/0
f18n06:1026678:1027018 [0] NCCL INFO Channel 00/0 : 17[2] -> 15[0] [receive] via NET/IB/0
f18n06:1026678:1027018 [0] NCCL INFO Channel 01/0 : 15[0] -> 14[2] [send] via NET/IB/0
f18n06:1026678:1027018 [0] NCCL INFO Connected all trees
f18n06:1026678:1027018 [0] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
f18n06:1026678:1027018 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f18n06:1026678:1027018 [0] NCCL INFO comm 0x15b121720 rank 15 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0xaedb5f80914767fd - Init COMPLETE
f18n06:1026685:1026685 [4] NCCL INFO cudaDriverVersion 12020
f18n06:1026685:1026685 [4] NCCL INFO Bootstrap : Using ib0:10.41.14.110<0>
f18n06:1026685:1026685 [4] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
f18n06:1026685:1026685 [4] NCCL INFO NET/Plugin : No plugin found, using internal implementation
f18n06:1026685:1026913 [4] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.14.110<0>
f18n06:1026685:1026913 [4] NCCL INFO Using network IB
f18n06:1026685:1026913 [4] NCCL INFO comm 0x15c644450 rank 64 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x7b12926388d12ce3 - Init START
f18n06:1026685:1026913 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
f18n06:1026685:1026913 [4] NCCL INFO Trees [0] 65/-1/-1->64->63 [1] 63/70/-1->64->75 [2] 65/-1/-1->64->63 [3] 63/-1/-1->64->57
f18n06:1026685:1026913 [4] NCCL INFO P2P Chunksize set to 131072
f18n06:1026685:1026913 [4] NCCL INFO Channel 00/0 : 64[4] -> 65[5] via P2P/IPC
f18n06:1026685:1026913 [4] NCCL INFO Channel 01/0 : 64[4] -> 65[5] via P2P/IPC
f18n06:1026685:1026913 [4] NCCL INFO Channel 02/0 : 64[4] -> 65[5] via P2P/IPC
f18n06:1026685:1026913 [4] NCCL INFO Channel 03/0 : 64[4] -> 65[5] via P2P/IPC
f18n06:1026685:1026913 [4] NCCL INFO Connected all rings
f18n06:1026685:1026913 [4] NCCL INFO Channel 01/0 : 64[4] -> 70[4] [send] via NET/IB/3
f18n06:1026685:1026913 [4] NCCL INFO Channel 03/0 : 57[3] -> 64[4] [receive] via NET/IB/3
f18n06:1026685:1026913 [4] NCCL INFO Channel 01/0 : 64[4] -> 75[3] [send] via NET/IB/3
f18n06:1026685:1026913 [4] NCCL INFO Channel 01/0 : 75[3] -> 64[4] [receive] via NET/IB/3
f18n06:1026685:1026913 [4] NCCL INFO Channel 03/0 : 64[4] -> 57[3] [send] via NET/IB/3
f18n06:1026685:1026913 [4] NCCL INFO Channel 01/0 : 70[4] -> 64[4] [receive] via NET/IB/3
f18n06:1026685:1026913 [4] NCCL INFO Channel 00/0 : 64[4] -> 63[3] via P2P/IPC
f18n06:1026685:1026913 [4] NCCL INFO Channel 01/0 : 64[4] -> 63[3] via P2P/IPC
f18n06:1026685:1026913 [4] NCCL INFO Channel 02/0 : 64[4] -> 63[3] via P2P/IPC
f18n06:1026685:1026913 [4] NCCL INFO Channel 03/0 : 64[4] -> 63[3] via P2P/IPC
f18n06:1026685:1026913 [4] NCCL INFO Connected all trees
f18n06:1026685:1026913 [4] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
f18n06:1026685:1026913 [4] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
f18n06:1026685:1026913 [4] NCCL INFO comm 0x15c644450 rank 64 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x7b12926388d12ce3 - Init COMPLETE
f18n06:1026685:1027004 [4] NCCL INFO Using network IB
f18n06:1026685:1027004 [4] NCCL INFO comm 0x160384030 rank 8 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x8264b1a1a7aef6de - Init START
f18n06:1026685:1027004 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
f18n06:1026685:1027004 [4] NCCL INFO Trees [0] 4/10/-1->8->0 [1] -1/-1/-1->8->9
f18n06:1026685:1027004 [4] NCCL INFO P2P Chunksize set to 131072
f18n06:1026685:1027004 [4] NCCL INFO Channel 00/0 : 7[2] -> 8[4] [receive] via NET/IB/1
f18n06:1026685:1027004 [4] NCCL INFO Channel 01/0 : 7[2] -> 8[4] [receive] via NET/IB/1
f18n06:1026685:1027004 [4] NCCL INFO Channel 00/0 : 8[4] -> 9[0] [send] via NET/IB/1
f18n06:1026685:1027004 [4] NCCL INFO Channel 01/0 : 8[4] -> 9[0] [send] via NET/IB/1
f18n06:1026685:1027004 [4] NCCL INFO Connected all rings
f18n06:1026685:1027004 [4] NCCL INFO Channel 00/0 : 8[4] -> 10[2] [send] via NET/IB/1
f18n06:1026685:1027004 [4] NCCL INFO Channel 00/0 : 4[2] -> 8[4] [receive] via NET/IB/1
f18n06:1026685:1027004 [4] NCCL INFO Channel 00/0 : 8[4] -> 0[0] [send] via NET/IB/1
f18n06:1026685:1027004 [4] NCCL INFO Channel 00/0 : 0[0] -> 8[4] [receive] via NET/IB/1
f18n06:1026685:1027004 [4] NCCL INFO Channel 00/0 : 8[4] -> 4[2] [send] via NET/IB/1
f18n06:1026685:1027004 [4] NCCL INFO Channel 00/0 : 10[2] -> 8[4] [receive] via NET/IB/1
f18n06:1026685:1027004 [4] NCCL INFO Channel 01/0 : 9[0] -> 8[4] [receive] via 0xaedb5f80914767fd - Init START
f18n05:1007500:1007835 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
f18n05:1007500:1007835 [2] NCCL INFO Trees [0] -1/-1/-1->14->16 [1] 15/12/-1->14->17
f18n05:1007500:1007835 [2] NCCL INFO P2P Chunksize set to 131072
f18n05:1007500:1007835 [2] NCCL INFO Channel 00/0 : 13[4] -> 14[2] [receive] via NET/IB/0
f18n05:1007500:1007835 [2] NCCL INFO Channel 01/0 : 13[4] -> 14[2] [receive] via NET/IB/0
f18n05:1007500:1007835 [2] NCCL INFO Channel 00/0 : 14[2] -> 15[0] [send] via NET/IB/0
f18n05:1007500:1007835 [2] NCCL INFO Channel 01/0 : 14[2] -> 15[0] [send] via NET/IB/0
f18n05:1007500:1007835 [2] NCCL INFO Connected all rings
f18n05:1007500:1007835 [2] NCCL INFO Channel 01/0 : 12[0] -> 14[2] [receive] via NET/IB/0
f18n05:1007500:1007835 [2] NCCL INFO Channel 00/0 : 14[2] -> 16[4] [send] via NET/IB/0
f18n05:1007500:1007835 [2] NCCL INFO Channel 01/0 : 14[2] -> 17[2] [send] via NET/IB/0
f18n05:1007500:1007835 [2] NCCL INFO Channel 01/0 : 17[2] -> 14[2] [receive] via NET/IB/0
f18n05:1007500:1007835 [2] NCCL INFO Channel 00/0 : 16[4] -> 14[2] [receive] via NET/IB/0
f18n05:1007500:1007835 [2] NCCL INFO Channel 01/0 : 14[2] -> 12[0] [send] via NET/IB/0
f18n05:1007500:1007835 [2] NCCL INFO Channel 01/0 : 15[0] -> 14[2] [receive] via NET/IB/0
f18n05:1007500:1007835 [2] NCCL INFO Connected all trees
f18n05:1007500:1007835 [2] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
f18n05:1007500:1007835 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f18n05:1007500:1007835 [2] NCCL INFO comm 0x13b9e2670 rank 14 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0xaedb5f80914767fd - Init COMPLETE
f18n05:1007505:1007505 [5] NCCL INFO cudaDriverVersion 12020
f18n05:1007505:1007505 [5] NCCL INFO Bootstrap : Using ib0:10.41.14.109<0>
f18n05:1007505:1007505 [5] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
f18n05:1007505:1007505 [5] NCCL INFO NET/Plugin : No plugin found, using internal implementation
f18n05:1007505:1007734 [5] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.14.109<0>
f18n05:1007505:1007734 [5] NCCL INFO Using network IB
f18n05:1007505:1007734 [5] NCCL INFO comm 0x139133df0 rank 59 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x7b12926388d12ce3 - Init START
f18n05:1007505:1007734 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
f18n05:1007505:1007734 [5] NCCL INFO Trees [0] -1/-1/-1->59->58 [1] 55/-1/-1->59->57 [2] -1/-1/-1->59->58 [3] 55/-1/-1->59->57
f18n05:1007505:1007734 [5] NCCL INFO P2P Chunksize set to 131072
f18n05:1007505:1007734 [5] NCCL INFO Channel 00/0 : 59[5] -> 60[0] [send] via NET/IB/1
f18n05:1007505:1007734 [5] NCCL INFO Channel 02/0 : 59[5] -> 60[0] [send] via NET/IB/1
f18n05:1007505:1007734 [5] NCCL INFO Channel 01/0 : 59[5] -> 56[2] via P2P/IPC
f18n05:1007505:1007734 [5] NCCL INFO Channel 03/0 : 59[5] -> 56[2] via P2P/IPC
f18n05:1007505:1007734 [5] NCCL INFO Connected all rings
f18n05:1007505:1007734 [5] NCCL INFO Channel 01/0 : 59[5] -> 55[1] via P2P/IPC
f18n05:1007505:1007734 [5] NCCL INFO Channel 03/0 : 59[5] -> 55[1] via P2P/IPC
f18n05:1007505:1007734 [5] NCCL INFO Channel 01/0 : 59[5] -> 57[3] via P2P/IPC
f18n05:1007505:1007734 [5] NCCL INFO Channel 03/0 : 59[5] -> 57[3] via P2P/IPC
f18n05:1007505:1007734 [5] NCCL INFO Channel 00/0 : 59[5] -> 58[4] via P2P/IPC
f18n05:1007505:1007734 [5] NCCL INFO Channel 02/0 : 59[5] -> 58[4] via P2P/IPC
f18n05:1007505:1007734 [5] NCCL INFO Connected all trees
f18n05:1007505:1007734 [5] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
f18n05:1007505:1007734 [5] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
f18n05:1007505:1007734 [5] NCCL INFO comm 0x139133df0 rank 59 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x7b12926388d12ce3 - Init COMPLETE
f18n05:1007505:1007817 [5] NCCL INFO Using network IB
f18n05:1007505:1007817 [5] NCCL INFO comm 0x13be61860 rank 7 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x1defb2708b178c98 - Init START
f18n05:1007505:1007817 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
f18n05:1007505:1007817 [5] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] 9/5/-1->7->3
f18n05:1007505:1007817 [5] NCCL INFO P2P Chunksize set to 131072
f18n05:1007505:1007817 [5] NCCL INFO Channel 00/0 : 6[3] -> 7[5] [receive] via NET/IB/3
f18n05:1007505:1007817 [5] NCCL INFO Channel 01/0 : 6[3] -> 7[5] [receive] via NET/IB/3
f18n05:1007505:1007817 [5] NCCL INFO Channel 00/0 : 7[5] -> 8[1] [send] via NET/IB/3
f18n05:1007505:1007817 [5] NCCL INFO Channel 01/0 : 7[5] -> 8[1] [send] via NET/IB/3
f18n05:1007505:1007817 [5] NCCL INFO Connected all rings
f18n05:1007505:1007817 [5] NCCL INFO Channel 01/0 : 5[1] -> 7[5] [receive] via NET/IB/3
f18n05:1007505:1007817 [5] NCCL INFO Channel 01/0 : 7[5] -> 9[3] [send] via NET/IB/3
f18n05:1007505:1007817 [5] NCCL INFO Channel 01/0 : 3[3] -> 7[5] [receive] via NET/IB/3
f18n05:1007505:1007817 [5] NCCL INFO Channel 01/0 : 7[5] -> 3[3] [send] via NET/IB/3
f18n05:1007505:1007817 [5] NCCL INFO Channel 01/0 : 9[3] -> 7[5] [receive] via NET/IB/3
f18n05:1007505:1007817 [5] NCCL INFO Channel 01/0 : 7[5] -> 5[1] [send] via NET/IB/3
f18n05:1007505:1007817 [5] NCCL INFO Channel 00/0 : 7[5] -> 6[3] [send] via NET/IB/3
f18n05:1007505:1007817 [5] NCCL INFO Connected all trees
f18n05:1007505:1007817 [5] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
f18n05:1007505:1007817 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f18n05:1007505:1007817 [5] NCCL INFO comm 0x13be61860 rank 7 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x1defb2708b178c98 - Init COMPLETE
f18n05:1007505:1007838 [5] NCCL INFO Using network IB
f18n05:1007505:1007838 [5] NCCL INFO comm 0x13ced3c70 rank 14 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x821cecfd780eaba1 - Init START
f18n05:1007505:1007838 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
f18n05:1007505:1007838 [5] NCCL INFO Trees [0] -1/-1/-1->14->13 [1] 15/-1/-1->14->13
f18n05:1007505:1007838 [5] NCCL INFO P2P Chunksize set to 131072
f18n05:1007505:1007838 [5] NCCL INFO Channel 00/0 : 14[5] -> 15[3] [send] via NET/IB/3
f18n05:1007505:1007838 [5] NCCL INFO Channel 01/0 : 14[5] -> 15[3] [send] via NET/IB/3
f18n05:1007505:1007838 [5] NCCL INFO Connected all rings
f18n05:1007505:1007838 [5] NCCL INFO Channel 01/0 : 15[3] -> 14[5] [receive] via NET/IB/2
f18n05:1007505:1007838 [5] NCCL INFO Channel 00/0 : 14[5] -> 13[1] via P2P/IPC
f18n05:1007505:1007838 [5] NCCL INFO Channel 01/0 : 14[5] -> 13[1] via P2P/IPC
f18n05:1007505:1007838 [5] NCCL INFO Connected all trees
f18n05:1007505:1007838 [5] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
f18n05:1007505:1007838 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f18n05:1007505:1007838 [5] NCCL INFO comm 0x13ced3c70 rank 14 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x821cecfd780eaba1 - Init COMPLETE
f16n17:1090237:1090237 [2] NCCL INFO cudaDriverVersion 12020
f16n17:1090237:1090237 [2] NCCL INFO Bootstrap : Using ib0:10.41.14.85<0>
f16n17:1090237:1090237 [2] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
f16n17:1090237:1090237 [2] NCCL INFO NET/Plugin : No plugin found, using internal implementation
f16n17:1090237:1090468 [2] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.14.85<0>
f16n17:1090237:1090468 [2] NCCL INFO Using network IB
f16n17:1090237:1090468 [2] NCCL INFO comm 0x16b043890 rank 32 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x7b12926388d12ce3 - Init START
f16n17:1090237:1090468 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
f16n17:1090237:1090468 [2] NCCL INFO Trees [0] 33/-1/-1->32->31 [1] -1/-1/-1->32->30 [2] 33/-1/-1->32->31 [3] -1/-1/-1->32->30
f16n17:1090237:1090468 [2] NCCL INFO P2P Chunksize set to 131072
f16n17:1090237:1090468 [2] NCCL INFO Channel 00/0 : 32[2] -> 33[3] via P2P/IPC
f16n17:1090237:1090468 [2] NCCL INFO Channel 02/0 : 32[2] -> 33[3] via P2P/IPC
f16n17:1090237:1090468 [2] NCCL INFO Channel 01/0 : 32[2] -> 31[1] via P2P/IPC
f16n17:1090237:1090468 [2] NCCL INFO Channel 03/0 : 32[2] -> 31[1] via P2P/IPC
f16n17:1090237:1090468 [2] NCCL INFO Connected all rings
f16n17:1090237:1090468 [2] NCCL INFO Channel 01/0 : 32[2] -> 30[0] via P2P/IPC
f16n17:1090237:1090468 [2] NCCL INFO Channel 03/0 : 32[2] -> 30[0] via P2P/IPC
f16n17:1090237:1090468 [2] NCCL INFO Channel 00/0 : 32[2] -> 31[1] via P2P/IPC
f16n17:1090237:1090468 [2] NCCL INFO Channel 02/0 : 32[2] -> 31[1] via P2P/IPC
f16n17:1090237:1090468 [2] NCCL INFO Connected all trees
f16n17:1090237:1090468 [2] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
f16n17:1090237:1090468 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
f16n17:1090237:1090468 [2] NCCL INFO comm 0x16b043890 rank 32 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x7b12926388d12ce3 - Init COMPLETE
f16n17:1090237:1090563 [2] NCCL INFO Using network IB
f16n17:1090237:1090563 [2] NCCL INFO comm 0x16d9f09e0 rank 4 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0x8264b1a1a7aef6de - Init START
f16n17:1090237:1090563 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
f16n17:1090237:1090563 [2] NCCL INFO Trees [0] 2/6/-1->4->8 [1] -1/-1/-1->4->5
f16n17:1090237:1090563 [2] NCCL INFO P2P Chunksize set to 131072
f16n17:1090237:1090563 [2] NCCL INFO Channel 00/0 : 3[0] -> 4[2] [receive] via NET/IB/0
f16n17:1090237:1090563 [2] NCCL INFO Channel 01/0 : 3[0] -> 4[2] [receive] via NET/IB/0
f16n17:1090237:1090563 [2] NCCL INFO Channel 00/0 : 4[2] -> 5[4] [send] via NET/IB/0
f16n17:1090237:1090563 [2] NCCL INFO Channel 01/0 : 4[2] -> 5[4] [send] via NET/IB/0
f16n17:1090237:1090563 [2] NCCL INFO Connected all rings
f16n17:1090237:1090563 [2] NCCL INFO Channel 00/0 : 2[4] -> 4[2] [receive] via NET/IB/0
f16n17:1090237:1090563 [2] NCCL INFO Channel 00/0 : 4[2] -> 6[0] [send] via NET/IB/0
f16n17:1090237:1090563 [2] NCCL INFO Channel 00/0 : 4[2] -> 8[4] [send] via NET/IB/0
f16n17:1090237:1090563 [2] NCCL INFO Channel 00/0 : 8[4] -> 4[2] [receive] via NET/IB/0
f16n17:1090237:1090563 [2] NCCL INFO Channel 00/0 : 6[0] -> 4[2] [receive] via NET/IB/0
f16n17:1090237:1090563 [2] NCCL INFO Channel 00/0 : 4[2] -> 2[4] [send] via NET/IB/0
f16n17:1090237:1090563 [2] NCCL INFO Channel 01/0 : 5[4] -> 4[2] [receive] via NET/IB/0
f16n17:1090237:1090563 [2] NCCL INFO Connected all trees
f16n17:1090237:1090563 [2] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
f16n17:1090237:1090563 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f16n17:1090237:1090563 [2] NCCL INFO comm 0x16d9f09e0 rank 4 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0x8264b1a1a7aef6de - Init COMPLETE
f16n17:1090237:1090578 [2] NCCL INFO Using network IB
f16n17:1090237:1090578 [2] NCCL INFO comm 0x16cd269a0 rank 8 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId NET/IB/1
f18n06:1026685:1027004 [4] NCCL INFO Connected all trees
f18n06:1026685:1027004 [4] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
f18n06:1026685:1027004 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f18n06:1026685:1027004 [4] NCCL INFO comm 0x160384030 rank 8 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x8264b1a1a7aef6de - Init COMPLETE
f18n06:1026685:1027021 [4] NCCL INFO Using network IB
f18n06:1026685:1027021 [4] NCCL INFO comm 0x15f29eef0 rank 16 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xaedb5f80914767fd - Init START
f18n06:1026685:1027021 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
f18n06:1026685:1027021 [4] NCCL INFO Trees [0] 14/-1/-1->16->15 [1] -1/-1/-1->16->15
f18n06:1026685:1027021 [4] NCCL INFO P2P Chunksize set to 131072
f18n06:1026685:1027021 [4] NCCL INFO Channel 00/0 : 16[4] -> 17[2] [send] via NET/IB/1
f18n06:1026685:1027021 [4] NCCL INFO Channel 01/0 : 16[4] -> 17[2] [send] via NET/IB/1
f18n06:1026685:1027021 [4] NCCL INFO Connected all rings
f18n06:1026685:1027021 [4] NCCL INFO Channel 00/0 : 14[2] -> 16[4] [receive] via NET/IB/0
f18n06:1026685:1027021 [4] NCCL INFO Channel 00/0 : 16[4] -> 14[2] [send] via NET/IB/0
f18n06:1026685:1027021 [4] NCCL INFO Channel 00/0 : 16[4] -> 15[0] via P2P/IPC
f18n06:1026685:1027021 [4] NCCL INFO Channel 01/0 : 16[4] -> 15[0] via P2P/IPC
f18n06:1026685:1027021 [4] NCCL INFO Connected all trees
f18n06:1026685:1027021 [4] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
f18n06:1026685:1027021 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f18n06:1026685:1027021 [4] NCCL INFO comm 0x15f29eef0 rank 16 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xaedb5f80914767fd - Init COMPLETE
0xaedb5f80914767fd - Init START
f16n17:1090237:1090578 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
f16n17:1090237:1090578 [2] NCCL INFO Trees [0] -1/-1/-1->8->10 [1] 9/6/-1->8->5
f16n17:1090237:1090578 [2] NCCL INFO P2P Chunksize set to 131072
f16n17:1090237:1090578 [2] NCCL INFO Channel 00/0 : 7[4] -> 8[2] [receive] via NET/IB/0
f16n17:1090237:1090578 [2] NCCL INFO Channel 01/0 : 7[4] -> 8[2] [receive] via NET/IB/0
f16n17:1090237:1090578 [2] NCCL INFO Channel 00/0 : 8[2] -> 9[0] [send] via NET/IB/0
f16n17:1090237:1090578 [2] NCCL INFO Channel 01/0 : 8[2] -> 9[0] [send] via NET/IB/0
f16n17:1090237:1090578 [2] NCCL INFO Connected all rings
f16n17:1090237:1090578 [2] NCCL INFO Channel 01/0 : 6[0] -> 8[2] [receive] via NET/IB/0
f16n17:1090237:1090578 [2] NCCL INFO Channel 00/0 : 8[2] -> 10[4] [send] via NET/IB/0
f16n17:1090237:1090578 [2] NCCL INFO Channel 01/0 : 5[2] -> 8[2] [receive] via NET/IB/0
f16n17:1090237:1090578 [2] NCCL INFO Channel 01/0 : 8[2] -> 5[2] [send] via NET/IB/0
f16n17:1090237:1090578 [2] NCCL INFO Channel 00/0 : 10[4] -> 8[2] [receive] via NET/IB/0
f16n17:1090237:1090578 [2] NCCL INFO Channel 01/0 : 8[2] -> 6[0] [send] via NET/IB/0
f16n17:1090237:1090578 [2] NCCL INFO Channel 01/0 : 9[0] -> 8[2] [receive] via NET/IB/0
f16n17:1090237:1090578 [2] NCCL INFO Connected all trees
f16n17:1090237:1090578 [2] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
f16n17:1090237:1090578 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f16n17:1090237:1090578 [2] NCCL INFO comm 0x16cd269a0 rank 8 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0xaedb5f80914767fd - Init COMPLETE
f18n05:1007502:1007502 [3] NCCL INFO cudaDriverVersion 12020
f18n05:1007502:1007502 [3] NCCL INFO Bootstrap : Using ib0:10.41.14.109<0>
f18n05:1007502:1007502 [3] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
f18n05:1007502:1007502 [3] NCCL INFO NET/Plugin : No plugin found, using internal implementation
f18n05:1007502:1007735 [3] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.14.109<0>
f18n05:1007502:1007735 [3] NCCL INFO Using network IB
f18n05:1007502:1007735 [3] NCCL INFO comm 0x14cbc4400 rank 57 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x7b12926388d12ce3 - Init START
f18n05:1007502:1007735 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
f18n05:1007502:1007735 [3] NCCL INFO Trees [0] 58/-1/-1->57->56 [1] 59/-1/-1->57->58 [2] 58/-1/-1->57->56 [3] 59/64/-1->57->58
f18n05:1007502:1007735 [3] NCCL INFO P2P Chunksize set to 131072
f18n05:1007502:1007735 [3] NCCL INFO Channel 00/0 : 57[3] -> 58[4] via P2P/IPC
f18n05:1007502:1007735 [3] NCCL INFO Channel 01/0 : 57[3] -> 58[4] via P2P/IPC
f18n05:1007502:1007735 [3] NCCL INFO Channel 02/0 : 57[3] -> 58[4] via P2P/IPC
f18n05:1007502:1007735 [3] NCCL INFO Channel 03/0 : 57[3] -> 58[4] via P2P/IPC
f18n05:1007502:1007735 [3] NCCL INFO Channel 01/0 : 48[0] -> 57[3] [receive] via NET/IB/3
f18n05:1007502:1007735 [3] NCCL INFO Channel 03/0 : 48[0] -> 57[3] [receive] via NET/IB/3
f18n05:1007502:1007735 [3] NCCL INFO Connected all rings
f18n05:1007502:1007735 [3] NCCL INFO Channel 01/0 : 57[3] -> 59[5] via P2P/IPC
f18n05:1007502:1007735 [3] NCCL INFO Channel 03/0 : 57[3] -> 59[5] via P2P/IPC
f18n05:1007502:1007735 [3] NCCL INFO Channel 03/0 : 57[3] -> 64[4] [send] via NET/IB/3
f18n05:1007502:1007735 [3] NCCL INFO Channel 03/0 : 64[4] -> 57[3] [receive] via NET/IB/3
f18n05:1007502:1007735 [3] NCCL INFO Channel 00/0 : 57[3] -> 56[2] via P2P/IPC
f18n05:1007502:1007735 [3] NCCL INFO Channel 02/0 : 57[3] -> 56[2] via P2P/IPC
f18n05:1007502:1007735 [3] NCCL INFO Connected all trees
f18n05:1007502:1007735 [3] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
f18n05:1007502:1007735 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
f18n05:1007502:1007735 [3] NCCL INFO comm 0x14cbc4400 rank 57 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x7b12926388d12ce3 - Init COMPLETE
f18n05:1007502:1007818 [3] NCCL INFO Using network IB
f18n05:1007502:1007818 [3] NCCL INFO comm 0x14f899ac0 rank 7 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x86d434c9d289d59a - Init START
f18n05:1007502:1007818 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
f18n05:1007502:1007818 [3] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] 9/5/-1->7->3
f18n05:1007502:1007818 [3] NCCL INFO P2P Chunksize set to 131072
f18n05:1007502:1007818 [3] NCCL INFO Channel 00/0 : 6[1] -> 7[3] [receive] via NET/IB/3
f18n05:1007502:1007818 [3] NCCL INFO Channel 01/0 : 6[1] -> 7[3] [receive] via NET/IB/3
f18n05:1007502:1007818 [3] NCCL INFO Channel 00/0 : 7[3] -> 8[5] [send] via NET/IB/3
f18n05:1007502:1007818 [3] NCCL INFO Channel 01/0 : 7[3] -> 8[5] [send] via NET/IB/3
f18n05:1007502:1007818 [3] NCCL INFO Connected all rings
f18n05:1007502:1007818 [3] NCCL INFO Channel 01/0 : 5[5] -> 7[3] [receive] via NET/IB/3
f18n05:1007502:1007818 [3] NCCL INFO Channel 01/0 : 7[3] -> 9[1] [send] via NET/IB/3
f18n05:1007502:1007818 [3] NCCL INFO Channel 01/0 : 3[1] -> 7[3] [receive] via NET/IB/3
f18n05:1007502:1007818 [3] NCCL INFO Channel 01/0 : 7[3] -> 3[1] [send] via NET/IB/3
f18n05:1007502:1007818 [3] NCCL INFO Channel 01/0 : 9[1] -> 7[3] [receive] via NET/IB/3
f18n05:1007502:1007818 [3] NCCL INFO Channel 01/0 : 7[3] -> 5[5] [send] via NET/IB/3
f18n05:1007502:1007818 [3] NCCL INFO Channel 00/0 : 7[3] -> 6[1] [send] via NET/IB/3
f18n05:1007502:1007818 [3] NCCL INFO Connected all trees
f18n05:1007502:1007818 [3] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
f18n05:1007502:1007818 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f18n05:1007502:1007818 [3] NCCL INFO comm 0x14f899ac0 rank 7 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x86d434c9d289d59a - Init COMPLETE
f18n05:1007502:1007839 [3] NCCL INFO Using network IB
f18n05:1007502:1007839 [3] NCCL INFO comm 0x1508e9a40 rank 14 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xa49a27aa8b15034e - Init START
f18n05:1007502:1007839 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
f18n05:1007502:1007839 [3] NCCL INFO Trees [0] -1/-1/-1->14->16 [1] 15/12/-1->14->17
f18n05:1007502:1007839 [3] NCCL INFO P2P Chunksize set to 131072
f18n05:1007502:1007839 [3] NCCL INFO Channel 00/0 : 13[5] -> 14[3] [receive] via NET/IB/3
f18n05:1007502:1007839 [3] NCCL INFO Channel 01/0 : 13[5] -> 14[3] [receive] via NET/IB/3
f18n05:1007502:1007839 [3] NCCL INFO Channel 00/0 : 14[3] -> 15[1] [send] via NET/IB/3
f18n05:1007502:1007839 [3] NCCL INFO Channel 01/0 : 14[3] -> 15[1] [send] via NET/IB/3
f18n05:1007502:1007839 [3] NCCL INFO Connected all rings
f18n05:1007502:1007839 [3] NCCL INFO Channel 01/0 : 12[1] -> 14[3] [receive] via NET/IB/3
f18n05:1007502:1007839 [3] NCCL INFO Channel 00/0 : 14[3] -> 16[5] [send] via NET/IB/3
f18n05:1007502:1007839 [3] NCCL INFO Channel 01/0 : 14[3] -> 17[3] [send] via NET/IB/3
f18n05:1007502:1007839 [3] NCCL INFO Channel 01/0 : 17[3] -> 14[3] [receive] via NET/IB/3
f18n05:1007502:1007839 [3] NCCL INFO Channel 00/0 : 16[5] -> 14[3] [receive] via NET/IB/3
f18n05:1007502:1007839 [3] NCCL INFO Channel 01/0 : 14[3] -> 12[1] [send] via NET/IB/3
f18n05:1007502:1007839 [3] NCCL INFO Channel 01/0 : 15[1] -> 14[3] [receive] via NET/IB/3
f18n05:1007502:1007839 [3] NCCL INFO Connected all trees
f18n05:1007502:1007839 [3] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
f18n05:1007502:1007839 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f18n05:1007502:1007839 [3] NCCL INFO comm 0x1508e9a40 rank 14 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xa49a27aa8b15034e - Init COMPLETE
d09n07:1061891:1062287 [4] NCCL INFO Using network IB
d09n07:1061891:1062287 [4] NCCL INFO comm 0x12597cab0 rank 0 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x7dd05d77214d2e8 - Init START
d09n07:1061891:1062287 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
d09n07:1061891:1062287 [4] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7   8   9  10  11
d09n07:1061891:1062287 [4] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7   8   9  10  11
d09n07:1061891:1062287 [4] NCCL INFO Trees [0] 8/-1/-1->0->-1 [1] -1/-1/-1->0->1
d09n07:1061891:1062287 [4] NCCL INFO P2P Chunksize set to 131072
d09n07:1061891:1062287 [4] NCCL INFO Channel 00/0 : 11[2] -> 0[4] [receive] via NET/IB/1
d09n07:1061891:1062287 [4] NCCL INFO Channel 01/0 : 11[2] -> 0[4] [receive] via NET/IB/1
d09n07:1061891:1062287 [4] NCCL INFO Channel 00/0 : 0[4] -> 1[0] [send] via NET/IB/1
d09n07:1061891:1062287 [4] NCCL INFO Channel 01/0 : 0[4] -> 1[0] [send] via NET/IB/1
d09n07:1061891:1062287 [4] NCCL INFO Connected all rings
d09n07:1061891:1062287 [4] NCCL INFO Channel 00/0 : 8[2] -> 0[4] [receive] via NET/IB/1
d09n07:1061891:1062287 [4] NCCL INFO Channel 00/0 : 0[4] -> 8[2] [send] via NET/IB/1
d09n07:1061891:1062287 [4] NCCL INFO Channel 01/0 : 1[0] -> 0[4] [receive] via NET/IB/1
d09n07:1061891:1062287 [4] NCCL INFO Connected all trees
d09n07:1061891:1062287 [4] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
d09n07:1061891:1062287 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d09n07:1061891:1062287 [4] NCCL INFO comm 0x12597cab0 rank 0 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x7dd05d77214d2e8 - Init COMPLETE
d09n07:1061891:1062319 [4] NCCL INFO Using network IB
d09n07:1061891:1062319 [4] NCCL INFO comm 0x1258ed0e0 rank 1 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xaedb5f80914767fd - Init START
d09n07:1061891:1062319 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
d09n07:1061891:1062319 [4] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
d09n07:1061891:1062319 [4] NCCL INFO P2P Chunksize set to 131072
d09n07:1061891:1062319 [4] NCCL INFO Channel 00/0 : 1[4] -> 2[2] [send] via NET/IB/1
d09n07:1061891:1062319 [4] NCCL INFO Channel 01/0 : 1[4] -> 2[2] [send] via NET/IB/1
d09n07:1061891:1062319 [4] NCCL INFO Connected all rings
d09n07:1061891:1062319 [4] NCCL INFO Channel 00/0 : 1[4] -> 0[0] via P2P/IPC
d09n07:1061891:1062319 [4] NCCL INFO Channel 01/0 : 1[4] -> 0[0] via P2P/IPC
d09n07:1061891:1062319 [4] NCCL INFO Connected all trees
d09n07:1061891:1062319 [4] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
d09n07:1061891:1062319 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d09n07:1061891:1062319 [4] NCCL INFO comm 0x1258ed0e0 rank 1 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xaedb5f80914767fd - Init COMPLETE
f16n16:1079199:1079199 [4] NCCL INFO cudaDriverVersion 12020
f16n16:1079199:1079199 [4] NCCL INFO Bootstrap : Using ib0:10.41.14.84<0>
f16n16:1079199:1079199 [4] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
f16n16:1079199:1079199 [4] NCCL INFO NET/Plugin : No plugin found, using internal implementation
f16n16:1079199:1079430 [4] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.14.84<0>
f16n16:1079199:1079430 [4] NCCL INFO Using network IB
f16n16:1079199:1079430 [4] NCCL INFO comm 0x145c43e20 rank 28 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x7b12926388d12ce3 - Init START
f16n16:1079199:1079430 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
f16n16:1079199:1079430 [4] NCCL INFO Trees [0] 29/-1/-1->28->27 [1] 27/40/-1->28->51 [2] 29/-1/-1->28->27 [3] 27/-1/-1->28->34
f16n16:1079199:1079430 [4] NCCL INFO P2P Chunksize set to 131072
f16n16:1079199:1079430 [4] NCCL INFO Channel 00/0 : 28[4] -> 29[5] via P2P/IPC
f16n16:1079199:1079430 [4] NCCL INFO Channel 01/0 : 28[4] -> 29[5] via P2P/IPC
f16n16:1079199:1079430 [4] NCCL INFO Channel 02/0 : 28[4] -> 29[5] via P2P/IPC
f16n16:1079199:1079430 [4] NCCL INFO Channel 03/0 : 28[4] -> 29[5] via P2P/IPC
f16n16:1079199:1079430 [4] NCCL INFO Connected all rings
f16n16:1079199:1079430 [4] NCCL INFO Channel 03/0 : 28[4] -> 34[4] [send] via NET/IB/3
f16n16:1079199:1079430 [4] NCCL INFO Channel 01/0 : 28[4] -> 40[4] [send] via NET/IB/3
f16n16:1079199:1079430 [4] NCCL INFO Channel 01/0 : 28[4] -> 51[3] [send] via NET/IB/3
f16n16:1079199:1079430 [4] NCCL INFO Channel 01/0 : 51[3] -> 28[4] [receive] via NET/IB/3
f16n16:1079199:1079430 [4] NCCL INFO Channel 01/0 : 40[4] -> 28[4] [receive] via NET/IB/3
f16n16:1079199:1079430 [4] NCCL INFO Channel 03/0 : 34[4] -> 28[4] [receive] via NET/IB/3
f16n16:1079199:1079430 [4] NCCL INFO Channel 00/0 : 28[4] -> 27[3] via P2P/IPC
f16n16:1079199:1079430 [4] NCCL INFO Channel 01/0 : 28[4] -> 27[3] via P2P/IPC
f16n16:1079199:1079430 [4] NCCL INFO Channel 02/0 : 28[4] -> 27[3] via P2P/IPC
f16n16:1079199:1079430 [4] NCCL INFO Channel 03/0 : 28[4] -> 27[3] via P2P/IPC
f16n16:1079199:1079430 [4] NCCL INFO Connected all trees
f16n16:1079199:1079430 [4] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
f16n16:1079199:1079430 [4] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
f16n16:1079199:1079430 [4] NCCL INFO comm 0x145c43e20 rank 28 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x7b12926388d12ce3 - Init COMPLETE
f16n16:1079199:1079512 [4] NCCL INFO Using network IB
f16n16:1079199:1079512 [4] NCCL INFO comm 0x1485a83a0 rank 3 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x7dd05d77214d2e8 - Init START
f16n16:1079199:1079512 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
f16n16:1079199:1079512 [4] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] 7/1/-1->3->11
f16n16:1079199:1079512 [4] NCCL INFO P2P Chunksize set to 131072
f16n16:1079199:1079512 [4] NCCL INFO Channel 00/0 : 2[2] -> 3[4] [receive] via NET/IB/1
f16n16:1079199:1079512 [4] NCCL INFO Channel 01/0 : 2[2] -> 3[4] [receive] via NET/IB/1
f16n16:1079199:1079512 [4] NCCL INFO Channel 00/0 : 3[4] -> 4[0] [send] via NET/IB/1
f16n16:1079199:1079512 [4] NCCL INFO Channel 01/0 : 3[4] -> 4[0] [send] via NET/IB/1
f16n16:1079199:1079512 [4] NCCL INFO Connected all rings
f16n16:1079199:1079512 [4] NCCL INFO Channel 01/0 : 1[0] -> 3[4] [receive] via NET/IB/1
f16n16:1079199:1079512 [4] NCCL INFO Channel 01/0 : 11[2] -> 3[4] [receive] via NET/IB/1
f16n16:1079199:1079512 [4] NCCL INFO Channel 01/0 : 3[4] -> 7[0] [send] via NET/IB/1
f16n16:1079199:1079512 [4] NCCL INFO Channel 01/0 : 7[0] -> 3[4] [receive] via NET/IB/1
f16n16:1079199:1079512 [4] NCCL INFO Channel 01/0 : 3[4] -> 11[2] [send] via NET/IB/1
f16n16:1079199:1079512 [4] NCCL INFO Channel 01/0 : 3[4] -> 1[0] [send] via NET/IB/1
f16n16:1079199:1079512 [4] NCCL INFO Channel 00/0 : 3[4] -> 2[2] [send] via NET/IB/1
f16n16:1079199:1079512 [4] NCCL INFO Connected all trees
f16n16:1079199:1079512 [4] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
f16n16:1079199:1079512 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f16n16:1079199:1079512 [4] NCCL INFO comm 0x1485a83a0 rank 3 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x7dd05d77214d2e8 - Init COMPLETE
f16n16:1079199:1079527 [4] NCCL INFO Using network IB
f16n16:1079199:1079527 [4] NCCL INFO comm 0x14996df00 rank 7 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xaedb5f80914767fd - Init START
f16n16:1079199:1079527 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
f16n16:1079199:1079527 [4] NCCL INFO Trees [0] 3/-1/-1->7->6 [1] -1/-1/-1->7->6
f16n16:1079199:1079527 [4] NCCL INFO P2P Chunksize set to 131072
f16n16:1079199:1079527 [4] NCCL INFO Channel 00/0 : 7[4] -> 8[2] [send] via NET/IB/1
f16n16:1079199:1079527 [4] NCCL INFO Channel 01/0 : 7[4] -> 8[2] [send] via NET/IB/1
f16n16:1079199:1079527 [4] NCCL INFO Connected all rings
f16n16:1079199:1079527 [4] NCCL INFO Channel 00/0 : 3[0] -> 7[4] [receive] via NET/IB/0
f16n16:1079199:1079527 [4] NCCL INFO Channel 00/0 : 7[4] -> 3[0] [send] via NET/IB/0
f16n16:1079199:1079527 [4] NCCL INFO Channel 00/0 : 7[4] -> 6[0] via P2P/IPC
f16n16:1079199:1079527 [4] NCCL INFO Channel 01/0 : 7[4] -> 6[0] via P2P/IPC
f16n16:1079199:1079527 [4] NCCL INFO Connected all trees
f16n16:1079199:1079527 [4] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
f16n16:1079199:1079527 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f16n16:1079199:1079527 [4] NCCL INFO comm 0x14996df00 rank 7 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xaedb5f80914767fd - Init COMPLETE
f16n16:1079192:1079192 [0] NCCL INFO cudaDriverVersion 12020
f16n16:1079192:1079192 [0] NCCL INFO Bootstrap : Using ib0:10.41.14.84<0>
f16n16:1079192:1079192 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
f16n16:1079192:1079192 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
f16n16:1079192:1079425 [0] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.14.84<0>
f16n16:1079192:1079425 [0] NCCL INFO Using network IB
f16n16:1079192:1079425 [0] NCCL INFO comm 0x143203fa0 rank 24 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x7b12926388d12ce3 - Init START
f16n16:1079192:1079425 [0] NCCL INFO Setting affinity for GPU 0 to 0f
f16n16:1079192:1079425 [0] NCCL INFO Trees [0] 25/36/-1->24->49 [1] 26/-1/-1->24->25 [2] 25/-1/-1->24->30 [3] 26/-1/-1->24->25
f16n16:1079192:1079425 [0] NCCL INFO P2P Chunksize set to 131072
f16n16:1079192:1079425 [0] NCCL INFO Channel 00/0 : 23[5] -> 24[0] [receive] via NET/IB/0
f16n16:1079192:1079425 [0] NCCL INFO Channel 02/0 : 23[5] -> 24[0] [receive] via NET/IB/0
f16n16:1079192:1079425 [0] NCCL INFO Channel 00/0 : 24[0] -> 25[1] via P2P/IPC
f16n16:1079192:1079425 [0] NCCL INFO Channel 02/0 : 24[0] -> 25[1] via P2P/IPC
f16n16:1079192:1079425 [0] NCCL INFO Channel 01/0 : 24[0] -> 33[3] [send] via NET/IB/2
f16n16:1079192:1079425 [0] NCCL INFO Channel 03/0 : 24[0] -> 33[3] [send] via NET/IB/2
f16n16:1079192:1079425 [0] NCCL INFO Connected all rings
f16n16:1079192:1079425 [0] NCCL INFO Channel 01/0 : 24[0] -> 25[1] via P2P/IPC
f16n16:1079192:1079425 [0] NCCL INFO Channel 03/0 : 24[0] -> 25[1] via P2P/IPC
f16n16:1079192:1079425 [0] NCCL INFO Channel 01/0 : 24[0] -> 26[2] via P2P/IPC
f16n16:1079192:1079425 [0] NCCL INFO Channel 03/0 : 24[0] -> 26[2] via P2P/IPC
f16n16:1079192:1079425 [0] NCCL INFO Channel 02/0 : 24[0] -> 30[0] [send] via NET/IB/0
f16n16:1079192:1079425 [0] NCCL INFO Channel 00/0 : 24[0] -> 36[0] [send] via NET/IB/0
f16n16:1079192:1079425 [0] NCCL INFO Channel 00/0 : 24[0] -> 49[1] [send] via NET/IB/0
f16n16:1079192:1079425 [0] NCCL INFO Channel 00/0 : 49[1] -> 24[0] [receive] via NET/IB/0
f16n16:1079192:1079425 [0] NCCL INFO Channel 00/0 : 36[0] -> 24[0] [receive] via NET/IB/0
f16n16:1079192:1079425 [0] NCCL INFO Channel 02/0 : 30[0] -> 24[0] [receive] via NET/IB/0
f16n16:1079192:1079425 [0] NCCL INFO Connected all trees
f16n16:1079192:1079425 [0] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
f16n16:1079192:1079425 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
f16n16:1079192:1079425 [0] NCCL INFO comm 0x143203fa0 rank 24 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x7b12926388d12ce3 - Init COMPLETE
f16n16:1079192:1079513 [0] NCCL INFO Using network IB
f16n16:1079192:1079513 [0] NCCL INFO comm 0x145e357e0 rank 3 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x8264b1a1a7aef6de - Init START
f16n16:1079192:1079513 [0] NCCL INFO Setting affinity for GPU 0 to 0f
f16n16:1079192:1079513 [0] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] 7/1/-1->3->11
f16n16:1079192:1079513 [0] NCCL INFO P2P Chunksize set to 131072
f16n16:1079192:1079513 [0] NCCL INFO Channel 00/0 : 2[4] -> 3[0] [receive] via NET/IB/0
f16n16:1079192:1079513 [0] NCCL INFO Channel 01/0 : 2[4] -> 3[0] [receive] via NET/IB/0
f16n16:1079192:1079513 [0] NCCL INFO Channel 00/0 : 3[0] -> 4[2] [send] via NET/IB/0
f16n16:1079192:1079513 [0] NCCL INFO Channel 01/0 : 3[0] -> 4[2] [send] via NET/IB/0
f16n16:1079192:1079513 [0] NCCL INFO Connected all rings
f16n16:1079192:1079513 [0] NCCL INFO Channel 01/0 : 1[2] -> 3[0] [receive] via NET/IB/0
f16n16:1079192:1079513 [0] NCCL INFO Channel 01/0 : 11[4] -> 3[0] [receive] via NET/IB/0
f16n16:1079192:1079513 [0] NCCL INFO Channel 01/0 : 3[0] -> 7[2] [send] via NET/IB/0
f16n16:1079192:1079513 [0] NCCL INFO Channel 01/0 : 7[2] -> 3[0] [receive] via NET/IB/0
f16n16:1079192:1079513 [0] NCCL INFO Channel 01/0 : 3[0] -> 11[4] [send] via NET/IB/0
f16n16:1079192:1079513 [0] NCCL INFf16n17:1090239:1090239 [3] NCCL INFO cudaDriverVersion 12020
f16n17:1090239:1090239 [3] NCCL INFO Bootstrap : Using ib0:10.41.14.85<0>
f16n17:1090239:1090239 [3] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
f16n17:1090239:1090239 [3] NCCL INFO NET/Plugin : No plugin found, using internal implementation
f16n17:1090239:1090471 [3] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.14.85<0>
f16n17:1090239:1090471 [3] NCCL INFO Using network IB
f16n17:1090239:1090471 [3] NCCL INFO comm 0x165453d90 rank 33 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x7b12926388d12ce3 - Init START
f16n17:1090239:1090471 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
f16n17:1090239:1090471 [3] NCCL INFO Trees [0] 34/-1/-1->33->32 [1] 35/-1/-1->33->34 [2] 34/-1/-1->33->32 [3] 35/40/-1->33->34
f16n17:1090239:1090471 [3] NCCL INFO P2P Chunksize set to 131072
f16n17:1090239:1090471 [3] NCCL INFO Channel 00/0 : 33[3] -> 34[4] via P2P/IPC
f16n17:1090239:1090471 [3] NCCL INFO Channel 01/0 : 33[3] -> 34[4] via P2P/IPC
f16n17:1090239:1090471 [3] NCCL INFO Channel 02/0 : 33[3] -> 34[4] via P2P/IPC
f16n17:1090239:1090471 [3] NCCL INFO Channel 03/0 : 33[3] -> 34[4] via P2P/IPC
f16n17:1090239:1090471 [3] NCCL INFO Channel 01/0 : 24[0] -> 33[3] [receive] via NET/IB/3
f16n17:1090239:1090471 [3] NCCL INFO Channel 03/0 : 24[0] -> 33[3] [receive] via NET/IB/3
f16n17:1090239:1090471 [3] NCCL INFO Connected all rings
f16n17:1090239:1090471 [3] NCCL INFO Channel 01/0 : 33[3] -> 35[5] via P2P/IPC
f16n17:1090239:1090471 [3] NCCL INFO Channel 03/0 : 33[3] -> 35[5] via P2P/IPC
f16n17:1090239:1090471 [3] NCCL INFO Channel 03/0 : 33[3] -> 40[4] [send] via NET/IB/3
f16n17:1090239:1090471 [3] NCCL INFO Channel 03/0 : 40[4] -> 33[3] [receive] via NET/IB/3
f16n17:1090239:1090471 [3] NCCL INFO Channel 00/0 : 33[3] -> 32[2] via P2P/IPC
f16n17:1090239:1090471 [3] NCCL INFO Channel 02/0 : 33[3] -> 32[2] via P2P/IPC
f16n17:1090239:1090471 [3] NCCL INFO Connected all trees
f16n17:1090239:1090471 [3] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
f16n17:1090239:1090471 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
f16n17:1090239:1090471 [3] NCCL INFO comm 0x165453d90 rank 33 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x7b12926388d12ce3 - Init COMPLETE
f16n17:1090239:1090562 [3] NCCL INFO Using network IB
f16n17:1090239:1090562 [3] NCCL INFO comm 0x168322d40 rank 4 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x86d434c9d289d59a - Init START
f16n17:1090239:1090562 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
f16n17:1090239:1090562 [3] NCCL INFO Trees [0] 2/6/-1->4->8 [1] -1/-1/-1->4->5
f16n17:1090239:1090562 [3] NCCL INFO P2P Chunksize set to 131072
f16n17:1090239:1090562 [3] NCCL INFO Channel 00/0 : 3[1] -> 4[3] [receive] via NET/IB/3
f16n17:1090239:1090562 [3] NCCL INFO Channel 01/0 : 3[1] -> 4[3] [receive] via NET/IB/3
f16n17:1090239:1090562 [3] NCCL INFO Channel 00/0 : 4[3] -> 5[5] [send] via NET/IB/3
f16n17:1090239:1090562 [3] NCCL INFO Channel 01/0 : 4[3] -> 5[5] [send] via NET/IB/3
f16n17:1090239:1090562 [3] NCCL INFO Connected all rings
f16n17:1090239:1090562 [3] NCCL INFO Channel 00/0 : 2[5] -> 4[3] [receive] via NET/IB/3
f16n17:1090239:1090562 [3] NCCL INFO Channel 00/0 : 4[3] -> 6[1] [send] via NET/IB/3
f16n17:1090239:1090562 [3] NCCL INFO Channel 00/0 : 4[3] -> 8[5] [send] via NET/IB/3
f16n17:1090239:1090562 [3] NCCL INFO Channel 00/0 : 8[5] -> 4[3] [receive] via NET/IB/3
f16n17:1090239:1090562 [3] NCCL INFO Channel 00/0 : 6[1] -> 4[3] [receive] via NET/IB/3
f16n17:1090239:1090562 [3] NCCL INFO Channel 00/0 : 4[3] -> 2[5] [send] via NET/IB/3
f16n17:1090239:1090562 [3] NCCL INFO Channel 01/0 : 5[5] -> 4[3] [receive] via NET/IB/3
f16n17:1090239:1090562 [3] NCCL INFO Connected all trees
f16n17:1090239:1090562 [3] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
f16n17:1090239:1090562 [3] NCCL INFO 2 coll g05n08:178239:178239 [2] NCCL INFO cudaDriverVersion 12020
g05n08:178239:178239 [2] NCCL INFO Bootstrap : Using ib0:10.41.16.20<0>
g05n08:178239:178239 [2] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g05n08:178239:178239 [2] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g05n08:178239:178472 [2] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.16.20<0>
g05n08:178239:178472 [2] NCCL INFO Using network IB
g05n08:178239:178472 [2] NCCL INFO comm 0x16dd13740 rank 80 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x7b12926388d12ce3 - Init START
g05n08:178239:178472 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g05n08:178239:178472 [2] NCCL INFO Trees [0] 81/-1/-1->80->79 [1] -1/-1/-1->80->78 [2] 81/-1/-1->80->79 [3] -1/-1/-1->80->78
g05n08:178239:178472 [2] NCCL INFO P2P Chunksize set to 131072
g05n08:178239:178472 [2] NCCL INFO Channel 00/0 : 80[2] -> 81[3] via P2P/IPC
g05n08:178239:178472 [2] NCCL INFO Channel 02/0 : 80[2] -> 81[3] via P2P/IPC
g05n08:178239:178472 [2] NCCL INFO Channel 01/0 : 80[2] -> 79[1] via P2P/IPC
g05n08:178239:178472 [2] NCCL INFO Channel 03/0 : 80[2] -> 79[1] via P2P/IPC
g05n08:178239:178472 [2] NCCL INFO Connected all rings
g05n08:178239:178472 [2] NCCL INFO Channel 01/0 : 80[2] -> 78[0] via P2P/IPC
g05n08:178239:178472 [2] NCCL INFO Channel 03/0 : 80[2] -> 78[0] via P2P/IPC
g05n08:178239:178472 [2] NCCL INFO Channel 00/0 : 80[2] -> 79[1] via P2P/IPC
g05n08:178239:178472 [2] NCCL INFO Channel 02/0 : 80[2] -> 79[1] via P2P/IPC
g05n08:178239:178472 [2] NCCL INFO Connected all trees
g05n08:178239:178472 [2] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g05n08:178239:178472 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g05n08:178239:178472 [2] NCCL INFO comm 0x16dd13740 rank 80 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x7b12926388d12ce3 - Init COMPLETE
g05n08:178239:178562 [2] NCCL INFO Using network IB
g05n08:178239:178562 [2] NCCL INFO comm 0x1709a2e20 rank 10 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0x8264b1a1a7aef6de - Init START
g05n08:178239:178562 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g05n08:178239:178562 [2] NCCL INFO Trees [0] 9/11/-1->10->8 [1] -1/-1/-1->10->9
g05n08:178239:178562 [2] NCCL INFO P2P Chunksize set to 131072
g05n08:178239:178562 [2] NCCL INFO Channel 00/0 : 9[0] -> 10[2] [receive] via NET/IB/0
g05n08:178239:178562 [2] NCCL INFO Channel 01/0 : 9[0] -> 10[2] [receive] via NET/IB/0
g05n08:178239:178562 [2] NCCL INFO Channel 00/0 : 10[2] -> 11[4] [send] via NET/IB/0
g05n08:178239:178562 [2] NCCL INFO Channel 01/0 : 10[2] -> 11[4] [send] via NET/IB/0
g05n08:178239:178562 [2] NCCL INFO Connected all rings
g05n08:178239:178562 [2] NCCL INFO Channel 00/0 : 8[4] -> 10[2] [receive] via NET/IB/0
g05n08:178239:178562 [2] NCCL INFO Channel 00/0 : 10[2] -> 8[4] [send] via NET/IB/0
g05n08:178239:178562 [2] NCCL INFO Channel 00/0 : 11[4] -> 10[2] [receive] via NET/IB/0
g05n08:178239:178562 [2] NCCL INFO Channel 00/0 : 10[2] -> 9[0] [send] via NET/IB/0
g05n08:178239:178562 [2] NCCL INFO Channel 01/0 : 10[2] -> 9[0] [send] via NET/IB/0
g05n08:178239:178562 [2] NCCL INFO Connected all trees
g05n08:178239:178562 [2] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g05n08:178239:178562 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g05n08:178239:178562 [2] NCCL INFO comm 0x1709a2e20 rank 10 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0x8264b1a1a7aef6de - Init COMPLETE
g05n08:178239:178578 [2] NCCL INFO Using network IB
g05n08:178239:178578 [2] NCCL INFO comm 0x17064d0d0 rank 20 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0xaedb5f80914767fd - Init START
g05n08:178239:178578 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g05n08:178239:178578 [2] NCCL INFO Trees [0] -1/-1/-1->20->22 [1] 21/18/-1->20->17
g05n08:178239:178578 [2] NCCL INFO P2P Chunksizechannels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f16n17:1090239:1090562 [3] NCCL INFO comm 0x168322d40 rank 4 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x86d434c9d289d59a - Init COMPLETE
f16n17:1090239:1090577 [3] NCCL INFO Using network IB
f16n17:1090239:1090577 [3] NCCL INFO comm 0x168088520 rank 8 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xa49a27aa8b15034e - Init START
f16n17:1090239:1090577 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
f16n17:1090239:1090577 [3] NCCL INFO Trees [0] -1/-1/-1->8->10 [1] 9/6/-1->8->5
f16n17:1090239:1090577 [3] NCCL INFO P2P Chunksize set to 131072
f16n17:1090239:1090577 [3] NCCL INFO Channel 00/0 : 7[5] -> 8[3] [receive] via NET/IB/3
f16n17:1090239:1090577 [3] NCCL INFO Channel 01/0 : 7[5] -> 8[3] [receive] via NET/IB/3
f16n17:1090239:1090577 [3] NCCL INFO Channel 00/0 : 8[3] -> 9[1] [send] via NET/IB/3
f16n17:1090239:1090577 [3] NCCL INFO Channel 01/0 : 8[3] -> 9[1] [send] via NET/IB/3
f16n17:1090239:1090577 [3] NCCL INFO Connected all rings
f16n17:1090239:1090577 [3] NCCL INFO Channel 01/0 : 6[1] -> 8[3] [receive] via NET/IB/3
f16n17:1090239:1090577 [3] NCCL INFO Channel 00/0 : 8[3] -> 10[5] [send] via NET/IB/3
f16n17:1090239:1090577 [3] NCCL INFO Channel 01/0 : 5[3] -> 8[3] [receive] via NET/IB/3
f16n17:1090239:1090577 [3] NCCL INFO Channel 01/0 : 8[3] -> 5[3] [send] via NET/IB/3
f16n17:1090239:1090577 [3] NCCL INFO Channel 00/0 : 10[5] -> 8[3] [receive] via NET/IB/3
f16n17:1090239:1090577 [3] NCCL INFO Channel 01/0 : 8[3] -> 6[1] [send] via NET/IB/3
f16n17:1090239:1090577 [3] NCCL INFO Channel 01/0 : 9[1] -> 8[3] [receive] via NET/IB/3
f16n17:1090239:1090577 [3] NCCL INFO Connected all trees
f16n17:1090239:1090577 [3] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
f16n17:1090239:1090577 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f16n17:1090239:1090577 [3] NCCL INFO comm 0x168088520 rank 8 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xa49a27aa8b15034e - Init COMPLETE
 set to 131072
g05n08:178239:178578 [2] NCCL INFO Channel 00/0 : 19[4] -> 20[2] [receive] via NET/IB/0
g05n08:178239:178578 [2] NCCL INFO Channel 01/0 : 19[4] -> 20[2] [receive] via NET/IB/0
g05n08:178239:178578 [2] NCCL INFO Channel 00/0 : 20[2] -> 21[0] [send] via NET/IB/0
g05n08:178239:178578 [2] NCCL INFO Channel 01/0 : 20[2] -> 21[0] [send] via NET/IB/0
g05n08:178239:178578 [2] NCCL INFO Connected all rings
g05n08:178239:178578 [2] NCCL INFO Channel 01/0 : 18[0] -> 20[2] [receive] via NET/IB/0
g05n08:178239:178578 [2] NCCL INFO Channel 00/0 : 20[2] -> 22[4] [send] via NET/IB/0
g05n08:178239:178578 [2] NCCL INFO Channel 01/0 : 17[2] -> 20[2] [receive] via NET/IB/0
g05n08:178239:178578 [2] NCCL INFO Channel 01/0 : 20[2] -> 17[2] [send] via NET/IB/0
g05n08:178239:178578 [2] NCCL INFO Channel 00/0 : 22[4] -> 20[2] [receive] via NET/IB/0
g05n08:178239:178578 [2] NCCL INFO Channel 01/0 : 20[2] -> 18[0] [send] via NET/IB/0
g05n08:178239:178578 [2] NCCL INFO Channel 01/0 : 21[0] -> 20[2] [receive] via NET/IB/0
g05n08:178239:178578 [2] NCCL INFO Connected all trees
g05n08:178239:178578 [2] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g05n08:178239:178578 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g05n08:178239:178578 [2] NCCL INFO comm 0x17064d0d0 rank 20 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0xaedb5f80914767fd - Init COMPLETE
f16n17:1090235:1090235 [1] NCCL INFO cudaDriverVersion 12020
f16n17:1090235:1090235 [1] NCCL INFO Bootstrap : Using ib0:10.41.14.85<0>
f16n17:1090235:1090235 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
f16n17:1090235:1090235 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
f16n17:1090235:1090467 [1] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.14.85<0>
f16n17:1090235:1090467 [1] NCCL INFO Using network IB
f16n17:1090235:1090467 [1] NCCL INFO comm 0x13dc63c90 rank 31 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x7b12926388d12ce3 - Init START
f16n17:1090235:1090467 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
f16n17:1090235:1090467 [1] NCCL INFO Trees [0] 32/-1/-1->31->30 [1] 30/-1/-1->31->35 [2] 32/36/-1->31->30 [3] 30/-1/-1->31->35
f16n17:1090235:1090467 [1] NCCL INFO P2P Chunksize set to 131072
f16n17:1090235:1090467 [1] NCCL INFO Channel 00/0 : 31[1] -> 32[2] via P2P/IPC
f16n17:1090235:1090467 [1] NCCL INFO Channel 02/0 : 31[1] -> 32[2] via P2P/IPC
f16n17:1090235:1090467 [1] NCCL INFO Channel 01/0 : 31[1] -> 30[0] via P2P/IPC
f16n17:1090235:1090467 [1] NCCL INFO Channel 03/0 : 31[1] -> 30[0] via P2P/IPC
f16n17:1090235:1090467 [1] NCCL INFO Connected all rings
f16n17:1090235:1090467 [1] NCCL INFO Channel 01/0 : 31[1] -> 35[5] via P2P/IPC
f16n17:1090235:1090467 [1] NCCL INFO Channel 03/0 : 31[1] -> 35[5] via P2P/IPC
f16n17:1090235:1090467 [1] NCCL INFO Channel 02/0 : 31[1] -> 36[0] [send] via NET/IB/0
f16n17:1090235:1090467 [1] NCCL INFO Channel 02/0 : 36[0] -> 31[1] [receive] via NET/IB/0
f16n17:1090235:1090467 [1] NCCL INFO Channel 00/0 : 31[1] -> 30[0] via P2P/IPC
f16n17:1090235:1090467 [1] NCCL INFO Channel 02/0 : 31[1] -> 30[0] via P2P/IPC
f16n17:1090235:1090467 [1] NCCL INFO Connected all trees
f16n17:1090235:1090467 [1] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
f16n17:1090235:1090467 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
f16n17:1090235:1090467 [1] NCCL INFO comm 0x13dc63c90 rank 31 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x7b12926388d12ce3 - Init COMPLETE
f16n17:1090235:1090560 [1] NCCL INFO Using network IB
f16n17:1090235:1090560 [1] NCCL INFO comm 0x13fb11070 rank 3 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x54676b30da0675d6 - Init START
f16n17:1090235:1090560 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
f16n17:1090235:1090560 [1] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] 7/1/-1->3->11
f16n17:1090235:1090560 [1] NCCL INFO P2P Chunksize set to 131072
f16n17:1090235:1090560 [1] NCCL INFO Channel 00/0 : 2[5] -> 3[1] [receive] via NET/IB/2
f16n17:1090235:1090560 [1] NCCL INFO Channel 01/0 : 2[5] -> 3[1] [receive] via NET/IB/2
f16n17:1090235:1090560 [1] NCCL INFO Channel 00/0 : 3[1] -> 4[3] [send] via NET/IB/2
f16n17:1090235:1090560 [1] NCCL INFO Channel 01/0 : 3[1] -> 4[3] [send] via NET/IB/2
f16n17:1090235:1090560 [1] NCCL INFO Connected all rings
f16n17:1090235:1090560 [1] NCCL INFO Channel 01/0 : 1[3] -> 3[1] [receive] via NET/IB/2
f16n17:1090235:1090560 [1] NCCL INFO Channel 01/0 : 11[5] -> 3[1] [receive] via NET/IB/2
f16n17:1090235:1090560 [1] NCCL INFO Channel 01/0 : 3[1] -> 7[3] [send] via NET/IB/2
f16n17:1090235:1090560 [1] NCCL INFO Channel 01/0 : 7[3] -> 3[1] [receive] via NET/IB/2
f16n17:1090235:1090560 [1] NCCL INFO Channel 01/0 : 3[1] -> 11[5] [send] via NET/IB/2
f16n17:1090235:1090560 [1] NCCL INFO Channel 01/0 : 3[1] -> 1[3] [send] via NET/IB/2
f16n17:1090235:1090560 [1] NCCL INFO Channel 00/0 : 3[1] -> 2[5] [send] via NET/IB/2
f16n17:1090235:1090560 [1] NCCL INFO Connected all trees
f16n17:1090235:1090560 [1] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
f16n17:1090235:1090560 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f16n17:1090235:1090560 [1] NCCL INFO comm 0x13fb11070 rank 3 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x54676b30da0675d6 - Init COMPLETE
f16n17:1090235:1090579 [1] NCCL INFO Using network IB
f16n17:1090235:1090579 [1] NCCL INFO comm 0x14058fa00 rank 7 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0x821cecfd780eaba1 - Init START
f16n17:1090235:1090579 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
f16n17:1090235:1090579 [1] NCCL INFO Trees [0] 8/-1/-1->7->9 [1] 8/6/-1->7->5
f16n17:1090235:1090579 [1] NCCL INFO P2P Chunksize set to 131072
f16n17:1090235:1090579 [1] NCCL INFO Channel 00/0 : 6[3] -> 7[1] [receive] via NET/IB/2
f16n17:1090235:1090579 [1] NCCL INFO Channel 01/0 : 6[3] -> 7[1] [receive] via NET/IB/2
f16n17:1090235:1090579 [1] NCCL INFO Channel 00/0 : 7[1] -> 8[5] via P2P/IPC
f16n17:1090235:1090579 [1] NCCL INFO Channel 01/0 : 7[1] -> 8[5] via P2P/IPC
f16n17:1090235:1090579 [1] NCCL INFO Connected all rings
f16n17:1090235:1090579 [1] NCCL INFO Channel 01/0 : 5[5] -> 7[1] [receive] via NET/IB/2
f16n17:1090235:1090579 [1] NCCL INFO Channel 00/0 : 7[1] -> 9[3] [send] via NET/IB/2
f16n17:1090235:1090579 [1] NCCL INFO Channel 00/0 : 9[3] -> 7[1] [receive] via NET/IB/2
f16n17:1090235:1090579 [1] NCCL INFO Channel 01/0 : 7[1] -> 5[5] [send] via NET/IB/2
f16n17:1090235:1090579 [1] NCCL INFO Channel 01/0 : 7[1] -> 6[3] [send] via NET/IB/2
f16n17:1090235:1090579 [1] NCCL INFO Connected all trees
f16n17:1090235:1090579 [1] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
f16n17:1090235:1090579 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f16n17:1090235:1090579 [1] NCCL INFO comm 0x14058fa00 rank 7 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0x821cecfd780eaba1 - Init COMPLETE
g05n08:178237:178237 [1] NCCL INFO cudaDriverVersion 12020
g05n08:178237:178237 [1] NCCL INFO Bootstrap : Using ib0:10.41.16.20<0>
g05n08:178237:178237 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g05n08:178237:178237 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g05n08:178237:178470 [1] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.16.20<0>
g05n08:178237:178470 [1] NCCL INFO Using network IB
g05n08:178237:178470 [1] NCCL INFO comm 0x113b13bd0 rank 79 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x7b12926388d12ce3 - Init START
g05n08:178237:178470 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g05n08:178237:178470 [1] NCCL INFO Trees [0] 80/-1/-1->79->78 [1] 78/-1/-1->79->83 [2] 80/84/-1->79->78 [3] 78/-1/-1->79->83
g05n08:178237:178470 [1] NCCL INFO P2P Chunksize set to 131072
g05n08:178237:178470 [1] NCCL INFO Channel 00/0 : 79[1] -> 80[2] via P2P/IPC
g05n08:178237:178470 [1] NCCL INFO Channel 02/0 : 79[1] -> 80[2] via P2P/IPC
g05n08:178237:178470 [1] NCCL INFO Channel 01/0 : 79[1] -> 78[0] via P2P/IPC
g05n08:178237:178470 [1] NCCL INFO Channel 03/0 : 79[1] -> 78[0] via P2P/IPC
g05n08:178237:178470 [1] NCCL INFO Connected all rings
g05n08:178237:178470 [1] NCCL INFO Channel 01/0 : 79[1] -> 83[5] via P2P/IPC
g05n08:178237:178470 [1] NCCL INFO Channel 03/0 : 79[1] -> 83[5] via P2P/IPC
g05n08:178237:178470 [1] NCCL INFO Channel 02/0 : 79[1] -> 84[0] [send] via NET/IB/0
g05n08:178237:178470 [1] NCCL INFO Channel 02/0 : 84[0] -> 79[1] [receive] via NET/IB/0
g05n08:178237:178470 [1] NCCL INFO Channel 00/0 : 79[1] -> 78[0] via P2P/IPC
g05n08:178237:178470 [1] NCCL INFO Channel 02/0 : 79[1] -> 78[0] via P2P/IPC
g05n08:178237:178470 [1] NCCL INFO Connected all trees
g05n08:178237:178470 [1] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g05n08:178237:178470 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g05n08:178237:178470 [1] NCCL INFO comm 0x113b13bd0 rank 79 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x7b12926388d12ce3 - Init COMPLETE
g05n08:178237:178557 [1] NCCL INFO Using network IB
g05n08:178237:178557 [1] NCCL INFO comm 0x116782d40 rank 9 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x54676b30da0675d6 - Init START
g05n08:178237:178557 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g05n08:178237:178557 [1] NCCL INFO Trees [0] -1/-1/-1->9->10 [1] 10/8/-1->9->7
g05n08:178237:178557 [1] NCCL INFO P2P Chunksize set to 131072
g05n08:178237:178557 [1] NCCL INFO Channel 00/0 : 8[5] -> 9[1] [receive] via NET/IB/2
g05n08:178237:178557 [1] NCCL INFO Channel 01/0 : 8[5] -> 9[1] [receive] via NET/IB/2
g05n08:178237:178557 [1] NCCL INFO Channel 00/0 : 9[1] -> 10[3] [send] via NET/IB/2
g05n08:178237:178557 [1] NCCL INFO Channel 01/0 : 9[1] -> 10[3] [send] via NET/IB/2
g05n08:178237:178557 [1] NCCL INFO Connected all rings
g05n08:178237:178557 [1] NCCL INFO Channel 01/0 : 7[3] -> 9[1] [receive] via NET/IB/2
g05n08:178237:178557 [1] NCCL INFO Channel 01/0 : 9[1] -> 7[3] [send] via NET/IB/2
g05n08:178237:178557 [1] NCCL INFO Channel 00/0 : 10[3] -> 9[1] [receive] via NET/IB/2
g05n08:178237:178557 [1] NCCL INFO Channel 01/0 : 10[3] -> 9[1] [receive] via NET/IB/2
g05n08:178237:178557 [1] NCCL INFO Channel 01/0 : 9[1] -> 8[5] [send] via NET/IB/2
g05n08:178237:178557 [1] NCCL INFO Connected all trees
g05n08:178237:178557 [1] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g05n08:178237:178557 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g05n08:178237:178557 [1] NCCL INFO comm 0x116782d40 rank 9 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x54676b30da0675d6 - Init COMPLETE
g05n08:178237:178579 [1] NCCL INFO Using network IB
g05n08:178237:178579 [1] NCCL INFO comm 0x116457d30 rank 19 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0x821cecfd780eaba1 - Init START
g05n08:178237:178579 [1] NCCL INFO Setting affinity for GPU 1 to f0g05n10:696651:696651 [5] NCCL INFO cudaDriverVersion 12020
g05n10:696651:696651 [5] NCCL INFO Bootstrap : Using ib0:10.41.16.22<0>
g05n10:696651:696651 [5] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g05n10:696651:696651 [5] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g05n10:696651:696881 [5] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.16.22<0>
g05n10:696651:696881 [5] NCCL INFO Using network IB
g05n10:696651:696881 [5] NCCL INFO comm 0x16851d4f0 rank 95 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x7b12926388d12ce3 - Init START
g05n10:696651:696881 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g05n10:696651:696881 [5] NCCL INFO Trees [0] -1/-1/-1->95->94 [1] 91/-1/-1->95->93 [2] -1/-1/-1->95->94 [3] 91/-1/-1->95->93
g05n10:696651:696881 [5] NCCL INFO P2P Chunksize set to 131072
g05n10:696651:696881 [5] NCCL INFO Channel 00/0 : 95[5] -> 0[0] [send] via NET/IB/1
g05n10:696651:696881 [5] NCCL INFO Channel 02/0 : 95[5] -> 0[0] [send] via NET/IB/1
g05n10:696651:696881 [5] NCCL INFO Channel 01/0 : 95[5] -> 92[2] via P2P/IPC
g05n10:696651:696881 [5] NCCL INFO Channel 03/0 : 95[5] -> 92[2] via P2P/IPC
g05n10:696651:696881 [5] NCCL INFO Connected all rings
g05n10:696651:696881 [5] NCCL INFO Channel 01/0 : 95[5] -> 91[1] via P2P/IPC
g05n10:696651:696881 [5] NCCL INFO Channel 03/0 : 95[5] -> 91[1] via P2P/IPC
g05n10:696651:696881 [5] NCCL INFO Channel 01/0 : 95[5] -> 93[3] via P2P/IPC
g05n10:696651:696881 [5] NCCL INFO Channel 03/0 : 95[5] -> 93[3] via P2P/IPC
g05n10:696651:696881 [5] NCCL INFO Channel 00/0 : 95[5] -> 94[4] via P2P/IPC
g05n10:696651:696881 [5] NCCL INFO Channel 02/0 : 95[5] -> 94[4] via P2P/IPC
g05n10:696651:696881 [5] NCCL INFO Connected all trees
g05n10:696651:696881 [5] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g05n10:696651:696881 [5] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g05n10:696651:696881 [5] NCCL INFO comm 0x16851d4f0 rank 95 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x7b12926388d12ce3 - Init COMPLETE
g05n10:696651:696959 [5] NCCL INFO Using network IB
g05n10:696651:696959 [5] NCCL INFO comm 0x16b2e9120 rank 11 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x54676b30da0675d6 - Init START
g05n10:696651:696959 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g05n10:696651:696959 [5] NCCL INFO Trees [0] -1/-1/-1->11->10 [1] 3/-1/-1->11->-1
g05n10:696651:696959 [5] NCCL INFO P2P Chunksize set to 131072
g05n10:696651:696959 [5] NCCL INFO Channel 00/0 : 10[3] -> 11[5] [receive] via NET/IB/3
g05n10:696651:696959 [5] NCCL INFO Channel 01/0 : 10[3] -> 11[5] [receive] via NET/IB/3
g05n10:696651:696959 [5] NCCL INFO Channel 00/0 : 11[5] -> 0[1] [send] via NET/IB/3
g05n10:696651:696959 [5] NCCL INFO Channel 01/0 : 11[5] -> 0[1] [send] via NET/IB/3
g05n10:696651:696959 [5] NCCL INFO Connected all rings
g05n10:696651:696959 [5] NCCL INFO Channel 01/0 : 11[5] -> 3[1] [send] via NET/IB/3
g05n10:696651:696959 [5] NCCL INFO Channel 01/0 : 3[1] -> 11[5] [receive] via NET/IB/3
g05n10:696651:696959 [5] NCCL INFO Channel 00/0 : 11[5] -> 10[3] [send] via NET/IB/3
g05n10:696651:696959 [5] NCCL INFO Connected all trees
g05n10:696651:696959 [5] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g05n10:696651:696959 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g05n10:696651:696959 [5] NCCL INFO comm 0x16b2e9120 rank 11 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x54676b30da0675d6 - Init COMPLETE
g05n10:696651:696983 [5] NCCL INFO Using network IB
g05n10:696651:696983 [5] NCCL INFO comm 0x16a3535a0 rank 23 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x821cecfd780eaba1 - Init START
g05n10:696651:696983 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g05n10:696651:696983 [5] NCCL INFO Trees [0] -1/-1/-1->23->22 [1] -1/-1/-1->23->22
g05n10:696651:696983 [5] NCCL INFO P2P Chunksize set to 131072
g05n10:696651:696983 [5] NCCL INFO Channel 00/0 : 23[5] -> 0[3] [send] via NET/IB/3
g05n10:696651:696983 [5] NCCL INFO Channel 01/0 : 23[5] -> 0[3] [send] via NET/IB/3
g05n10:696651:696983 [5] NCCL INFO Connected all rings
g05n10:696651:696983 [5] NCCL INFO Channel 00/0 : 23[5] -> 22[1] via P2P/IPC
g05n10:696651:696983 [5] NCCL INFO Channel 01/0 : 23[5] -> 22[1] via P2P/IPC
g05n10:696651:696983 [5] NCCL INFO Connected all trees
g05n10:696651:696983 [5] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g05n10:696651:696983 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g05n10:696651:696983 [5] NCCL INFO comm 0x16a3535a0 rank 23 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x821cecfd780eaba1 - Init COMPLETE
000000
g05n08:178237:178579 [1] NCCL INFO Trees [0] 20/-1/-1->19->21 [1] 20/18/-1->19->17
g05n08:178237:178579 [1] NCCL INFO P2P Chunksize set to 131072
g05n08:178237:178579 [1] NCCL INFO Channel 00/0 : 18[3] -> 19[1] [receive] via NET/IB/2
g05n08:178237:178579 [1] NCCL INFO Channel 01/0 : 18[3] -> 19[1] [receive] via NET/IB/2
g05n08:178237:178579 [1] NCCL INFO Channel 00/0 : 19[1] -> 20[5] via P2P/IPC
g05n08:178237:178579 [1] NCCL INFO Channel 01/0 : 19[1] -> 20[5] via P2P/IPC
g05n08:178237:178579 [1] NCCL INFO Connected all rings
g05n08:178237:178579 [1] NCCL INFO Channel 01/0 : 17[5] -> 19[1] [receive] via NET/IB/2
g05n08:178237:178579 [1] NCCL INFO Channel 00/0 : 19[1] -> 21[3] [send] via NET/IB/2
g05n08:178237:178579 [1] NCCL INFO Channel 00/0 : 21[3] -> 19[1] [receive] via NET/IB/2
g05n08:178237:178579 [1] NCCL INFO Channel 01/0 : 19[1] -> 17[5] [send] via NET/IB/2
g05n08:178237:178579 [1] NCCL INFO Channel 01/0 : 19[1] -> 18[3] [send] via NET/IB/2
g05n08:178237:178579 [1] NCCL INFO Connected all trees
g05n08:178237:178579 [1] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g05n08:178237:178579 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g05n08:178237:178579 [1] NCCL INFO comm 0x116457d30 rank 19 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0x821cecfd780eaba1 - Init COMPLETE
d11n17:1141833:1141833 [1] NCCL INFO cudaDriverVersion 12020
d11n17:1141833:1141833 [1] NCCL INFO Bootstrap : Using ib0:10.41.8.217<0>
d11n17:1141833:1141833 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
d11n17:1141833:1141833 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
d11n17:1141833:1142062 [1] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.8.217<0>
d11n17:1141833:1142062 [1] NCCL INFO Using network IB
d11n17:1141833:1142062 [1] NCCL INFO comm 0x142313e50 rank 19 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x7b12926388d12ce3 - Init START
d11n17:1141833:1142062 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
d11n17:1141833:1142062 [1] NCCL INFO Trees [0] 20/-1/-1->19->18 [1] 18/-1/-1->19->23 [2] 20/30/-1->19->18 [3] 18/-1/-1->19->23
d11n17:1141833:1142062 [1] NCCL INFO P2P Chunksize set to 131072
d11n17:1141833:1142062 [1] NCCL INFO Channel 00/0 : 19[1] -> 20[2] via P2P/IPC
d11n17:1141833:1142062 [1] NCCL INFO Channel 02/0 : 19[1] -> 20[2] via P2P/IPC
d11n17:1141833:1142062 [1] NCCL INFO Channel 01/0 : 19[1] -> 18[0] via P2P/IPC
d11n17:1141833:1142062 [1] NCCL INFO Channel 03/0 : 19[1] -> 18[0] via P2P/IPC
d11n17:1141833:1142062 [1] NCCL INFO Connected all rings
d11n17:1141833:1142062 [1] NCCL INFO Channel 01/0 : 19[1] -> 23[5] via P2P/IPC
d11n17:1141833:1142062 [1] NCCL INFO Channel 03/0 : 19[1] -> 23[5] via P2P/IPC
d11n17:1141833:1142062 [1] NCCL INFO Channel 02/0 : 19[1] -> 30[0] [send] via NET/IB/0
d11n17:1141833:1142062 [1] NCCL INFO Channel 02/0 : 30[0] -> 19[1] [receive] via NET/IB/0
d11n17:1141833:1142062 [1] NCCL INFO Channel 00/0 : 19[1] -> 18[0] via P2P/IPC
d11n17:1141833:1142062 [1] NCCL INFO Channel 02/0 : 19[1] -> 18[0] via P2P/IPC
d11n17:1141833:1142062 [1] NCCL INFO Connected all trees
d11n17:1141833:1142062 [1] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
d11n17:1141833:1142062 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
d11n17:1141833:1142062 [1] NCCL INFO comm 0x142313e50 rank 19 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x7b12926388d12ce3 - Init COMPLETE
d11n17:1141833:1142151 [1] NCCL INFO Using network IB
d11n17:1141833:1142151 [1] NCCL INFO comm 0x144126e60 rank 2 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x1defb2708b178c98 - Init START
d11n17:1141833:1142151 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
d11n17:1141833:1142151 [1] NCCL INFO Trees [0] 1/3/-1->2->4 [1] -1/-1/-1->2->1
d11n17:1141833:1142151 [1] NCCL INFO P2P Chunksize set to 131072
d11n17:1141833:1142151 [1] NCCL INFO Channel 00/0 : 1[5] -> 2[1] [receive] via NET/IB/2
d11n17:1141833:1142151 [1] NCCL INFO Channel 01/0 : 1[5] -> 2[1] [receive] via NET/IB/2
d11n17:1141833:1142151 [1] NCCL INFO Channel 00/0 : 2[1] -> 3[3] [send] via NET/IB/2
d11n17:1141833:1142151 [1] NCCL INFO Channel 01/0 : 2[1] -> 3[3] [send] via NET/IB/2
d11n17:1141833:1142151 [1] NCCL INFO Connected all rings
d11n17:1141833:1142151 [1] NCCL INFO Channel 00/0 : 2[1] -> 4[5] [send] via NET/IB/2
d11n17:1141833:1142151 [1] NCCL INFO Channel 00/0 : 4[5] -> 2[1] [receive] via NET/IB/2
d11n17:1141833:1142151 [1] NCCL INFO Channel 00/0 : 3[3] -> 2[1] [receive] via NET/IB/2
d11n17:1141833:1142151 [1] NCCL INFO Channel 00/0 : 2[1] -> 1[5] [send] via NET/IB/2
d11n17:1141833:1142151 [1] NCCL INFO Channel 01/0 : 2[1] -> 1[5] [send] via NET/IB/2
d11n17:1141833:1142151 [1] NCCL INFO Connected all trees
d11n17:1141833:1142151 [1] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
d11n17:1141833:1142151 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d11n17:1141833:1142151 [1] NCCL INFO comm 0x144126e60 rank 2 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x1defb2708b178c98 - Init COMPLETE
d11n17:1141833:1142170 [1] NCCL INFO Using network IB
d11n17:1141833:1142170 [1] NCCL INFO comm 0x1451c7f20 rank 4 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0x821cecfd780eaba1 - Init START
d11n17:1141833:1142170 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
d11n17:1141833:1142170 [1] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/1/-1->4->10
d11n17:1141833:1142170 [1] NCCL INFO P2P Chunksize set to 131072
d11n17:1141833:1142170 [1] NCCL INFO Channel 00/0 : 3[3] -> 4[1] [receive] via NET/IB/2
d11n17:1141833:1142170 [1] NCCL INFO Channel 01/0 : 3[3] -> 4[1] [receive] via NET/IB/2
d11n17:1141833:1142170 [1] NCCL INFO Channel 00/0 : 4[1] -> 5[5] via P2P/IPC
d11n17:1141833:1142170 [1] NCCL INFO Channel 01/0 : 4[1] -> 5[5] via P2P/IPC
d11n17:1141833:1142170 [1] NCCL INFO Connected all rings
d11n17:1141833:1142170 [1] NCCL INFO Channel 01/0 : 1[1] -> 4[1] [receive] via NET/IB/2
d11n17:1141833:1142170 [1] NCCL INFO Channel 01/0 : 4[1] -> 10[1] [send] via NET/IB/2
d11n17:1141833:1142170 [1] NCCL INFO Channel 01/0 : 10[1] -> 4[1] [receive] via NET/IB/2
d11n17:1141833:1142170 [1] NCCL INFO Channel 01/0 : 4[1] -> 1[1] [send] via NET/IB/2
d11n17:1141833:1142170 [1] NCCL INFO Channel 00/0 : 4[1] -> 3[3] [send] via NET/IB/2
d11n17:1141833:1142170 [1] NCCL INFO Connected all trees
d11n17:1141833:1142170 [1] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
d11n17:1141833:1142170 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d11n17:1141833:1142170 [1] NCCL INFO comm 0x1451c7f20 rank 4 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0x821cecfd780eaba1 - Init COMPLETE
O Channel 01/0 : 3[0] -> 1[2] [send] via NET/IB/0
f16n16:1079192:1079513 [0] NCCL INFO Channel 00/0 : 3[0] -> 2[4] [send] via NET/IB/0
f16n16:1079192:1079513 [0] NCCL INFO Connected all trees
f16n16:1079192:1079513 [0] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
f16n16:1079192:1079513 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f16n16:1079192:1079513 [0] NCCL INFO comm 0x145e357e0 rank 3 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x8264b1a1a7aef6de - Init COMPLETE
f16n16:1079192:1079530 [0] NCCL INFO Using network IB
f16n16:1079192:1079530 [0] NCCL INFO comm 0x145fa20e0 rank 6 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0xaedb5f80914767fd - Init START
f16n16:1079192:1079530 [0] NCCL INFO Setting affinity for GPU 0 to 0f
f16n16:1079192:1079530 [0] NCCL INFO Trees [0] 7/9/-1->6->13 [1] 7/-1/-1->6->8
f16n16:1079192:1079530 [0] NCCL INFO P2P Chunksize set to 131072
f16n16:1079192:1079530 [0] NCCL INFO Channel 00/0 : 5[2] -> 6[0] [receive] via NET/IB/0
f16n16:1079192:1079530 [0] NCCL INFO Channel 01/0 : 5[2] -> 6[0] [receive] via NET/IB/0
f16n16:1079192:1079530 [0] NCCL INFO Channel 00/0 : 6[0] -> 7[4] via P2P/IPC
f16n16:1079192:1079530 [0] NCCL INFO Channel 01/0 : 6[0] -> 7[4] via P2P/IPC
f16n16:1079192:1079530 [0] NCCL INFO Connected all rings
f16n16:1079192:1079530 [0] NCCL INFO Channel 01/0 : 6[0] -> 8[2] [send] via NET/IB/0
f16n16:1079192:1079530 [0] NCCL INFO Channel 00/0 : 6[0] -> 9[0] [send] via NET/IB/0
f16n16:1079192:1079530 [0] NCCL INFO Channel 00/0 : 6[0] -> 13[4] [send] via NET/IB/0
f16n16:1079192:1079530 [0] NCCL INFO Channel 00/0 : 13[4] -> 6[0] [receive] via NET/IB/0
f16n16:1079192:1079530 [0] NCCL INFO Channel 00/0 : 9[0] -> 6[0] [receive] via NET/IB/0
f16n16:1079192:1079530 [0] NCCL INFO Channel 01/0 : 8[2] -> 6[0] [receive] via NET/IB/0
f16n16:1079192:1079530 [0] NCCL INFO Connected all trees
f16n16:1079192:1079530 [0] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
f16n16:1079192:1079530 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f16n16:1079192:1079530 [0] NCCL INFO comm 0x145fa20e0 rank 6 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0xaedb5f80914767fd - Init COMPLETE
f17n02:1075530:1075530 [0] NCCL INFO cudaDriverVersion 12020
f17n02:1075530:1075530 [0] NCCL INFO Bootstrap : Using ib0:10.41.14.88<0>
f17n02:1075530:1075530 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
f17n02:1075530:1075530 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
f17n02:1075530:1075768 [0] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.14.88<0>
f17n02:1075530:1075768 [0] NCCL INFO Using network IB
f17n02:1075530:1075768 [0] NCCL INFO comm 0x142b037c0 rank 48 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x7b12926388d12ce3 - Init START
f17n02:1075530:1075768 [0] NCCL INFO Setting affinity for GPU 0 to 0f
f17n02:1075530:1075768 [0] NCCL INFO Trees [0] 49/72/-1->48->0 [1] 50/-1/-1->48->49 [2] 49/-1/-1->48->54 [3] 50/-1/-1->48->49
f17n02:1075530:1075768 [0] NCCL INFO P2P Chunksize set to 131072
f17n02:1075530:1075768 [0] NCCL INFO Channel 00/0 : 47[5] -> 48[0] [receive] via NET/IB/0
f17n02:1075530:1075768 [0] NCCL INFO Channel 02/0 : 47[5] -> 48[0] [receive] via NET/IB/0
f17n02:1075530:1075768 [0] NCCL INFO Channel 00/0 : 48[0] -> 49[1] via P2P/IPC
f17n02:1075530:1075768 [0] NCCL INFO Channel 02/0 : 48[0] -> 49[1] via P2P/IPC
f17n02:1075530:1075768 [0] NCCL INFO Channel 01/0 : 48[0] -> 57[3] [send] via NET/IB/2
f17n02:1075530:1075768 [0] NCCL INFO Channel 03/0 : 48[0] -> 57[3] [send] via NET/IB/2
f17n02:1075530:1075768 [0] NCCL INFO Connected all rings
f17n02:1075530:1075768 [0] NCCL INFO Channel 01/0 : 48[0] -> 49[1] via P2P/IPC
f17n02:1075530:1075768 [0] NCCL INFO Channel 03/0 : 48[0] -> 49[1] via P2P/IPC
f17n02:1075530:1075768 [0] NCCL INFO Channel 01/0 : 48[0] -> 50[2] via P2P/IPC
f17n02:1075530:1075768 [0] NCCL INFO Channel 03/0 : 48[0] -> 50[2] via P2P/IPC
f17n02:1075530:1075768 [0] NCCL INFO Channel 02/0 : 48[0] -> 54[0] [send] via NET/IB/0
f17n02:1075530:1075768 [0] NCCL INFO Channel 00/0 : 48[0] -> 72[0] [send] via NET/IB/0
f17n02:1075530:1075768 [0] NCCL INFO Channel 00/0 : 0[0] -> 48[0] [receive] via NET/IB/0
f17n02:1075530:1075768 [0] NCCL INFO Channel 00/0 : 48[0] -> 0[0] [send] via NET/IB/0
f17n02:1075530:1075768 [0] NCCL INFO Channel 00/0 : 72[0] -> 48[0] [receive] via NET/IB/0
f17n02:1075530:1075768 [0] NCCL INFO Channel 02/0 : 54[0] -> 48[0] [receive] via NET/IB/0
f17n02:1075530:1075768 [0] NCCL INFO Connected all trees
f17n02:1075530:1075768 [0] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
f17n02:1075530:1075768 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
f17n02:1075530:1075768 [0] NCCL INFO comm 0x142b037c0 rank 48 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x7b12926388d12ce3 - Init COMPLETE
f17n02:1075530:1075884 [0] NCCL INFO Using network IB
f17n02:1075530:1075884 [0] NCCL INFO comm 0x1447df0e0 rank 6 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x8264b1a1a7aef6de - Init START
f17n02:1075530:1075884 [0] NCCL INFO Setting affinity for GPU 0 to 0f
f17n02:1075530:1075884 [0] NCCL INFO Trees [0] 5/7/-1->6->4 [1] -1/-1/-1->6->5
f17n02:1075530:1075884 [0] NCCL INFO P2P Chunksize set to 131072
f17n02:1075530:1075884 [0] NCCL INFO Channel 00/0 : 5[4] -> 6[0] [receive] via NET/IB/0
f17n02:1075530:1075884 [0] NCCL INFO Channel 01/0 : 5[4] -> 6[0] [receive] via NET/IB/0
f17n02:1075530:1075884 [0] NCCL INFO Channel 00/0 : 6[0] -> 7[2] [send] via NET/IB/0
f17n02:1075530:1075884 [0] NCCL INFO Channel 01/0 : 6[0] -> 7[2] [send] via NET/IB/0
f17n02:1075530:1075884 [0] NCCL INFO Connected all rings
f17n02:1075530:1075884 [0] NCCL INFO Channel 00/0 : 4[2] -> 6[0] [receive] via NET/IB/0
f17n02:1075530:1075884 [0] NCCL INFO Channel 00/0 : 6[0] -> 4[2] [send] via NET/IB/0
f17n02:1075530:1075884 [0] NCCL INFO Channel 00/0 : 7[2] -> 6[0] [receive] via NET/IB/0
f17n02:1075530:1075884 [0] NCCL INFO Channel 00/0 : 6[0] -> 5[4] [send] via NET/IB/0
f17n02:1075530:1075884 [0] NCCL INFO Channel 01/0 : 6[0] -> 5[4] [send] via NET/IB/0
f17n02:1075530:1075884 [0] NCCL INFO Connected all trees
f17n02:1075530:1075884 [0] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
f17n02:1075530:1075884 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f17n02:1075530:1075884 [0] NCCL INFO comm 0x1447df0e0 rank 6 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x8264b1a1a7aef6de - Init COMPLETE
f17n02:1075530:1075899 [0] NCCL INFO Using network IB
f17n02:1075530:1075899 [0] NCCL INFO comm 0x14545f6f0 rank 12 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0xaedb5f80914767fd - Init START
f17n02:1075530:1075899 [0] NCCL INFO Setting affinity for GPU 0 to 0f
f17n02:1075530:1075899 [0] NCCL INFO Trees [0] 13/18/-1->12->0 [1] 13/-1/-1->12->14
f17n02:1075530:1075899 [0] NCCL INFO P2P Chunksize set to 131072
f17n02:1075530:1075899 [0] NCCL INFO Channel 00/0 : 11[2] -> 12[0] [receive] via NET/IB/0
f17n02:1075530:1075899 [0] NCCL INFO Channel 01/0 : 11[2] -> 12[0] [receive] via NET/IB/0
f17n02:1075530:1075899 [0] NCCL INFO Channel 00/0 : 12[0] -> 13[4] via P2P/IPC
f17n02:1075530:1075899 [0] NCCL INFO Channel 01/0 : 12[0] -> 13[4] via P2P/IPC
f17n02:1075530:1075899 [0] NCCL INFO Connected all rings
f17n02:1075530:1075899 [0] NCCL INFO Channel 01/0 : 12[0] -> 14[2] [send] via NET/IB/0
f17n02:1075530:1075899 [0] NCCL INFO Channel 00/0 : 12[0] -> 18[0] [send] via NET/IB/0
f17n02:1075530:1075899 [0] NCCL INFO Channel 00/0 : 0[0] -> 12[0] [receive] via NET/IB/0
f17n02:1075530:1075899 [0] NCCL INFO Channel 00/0 : 12[0] -> 0[0] [send] via NET/IB/0
f17n02:1075530:1075899 [0] NCCL INFO Channel 00/0 : 18[0] -> 12[0] [receive] via NET/IB/0
f17n02:1075530:1075899 [0] NCCL INFO Channel 01/0 : 14[2] -> 12[0] [receive] via NET/IB/0
f17n02:1075530:1075899 [0] NCCL INFO Connected all trees
f17n02:1075530:1075899 [0] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
f17n02:1075530:1075899 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f17n02:1075530:1075899 [0] NCCL INFO comm 0x14545f6f0 rank 12 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0xaedb5f80914767fd - Init COMPLETE
f17n02:1075537:1075537 [4] NCCL INFO cudaDriverVersion 12020
f17n02:1075537:1075537 [4] NCCL INFO Bootstrap : Using ib0:10.41.14.88<0>
f17n02:1075537:1075537 [4] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
f17n02:1075537:1075537 [4] NCCL INFO NET/Plugin : No plugin found, using internal implementation
f17n02:1075537:1075770 [4] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.14.88<0>
f17n02:1075537:1075770 [4] NCCL INFO Using network IB
f17n02:1075537:1075770 [4] NCCL INFO comm 0x15da13b80 rank 52 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x7b12926388d12ce3 - Init START
f17n02:1075537:1075770 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
f17n02:1075537:1075770 [4] NCCL INFO Trees [0] 53/-1/-1->52->51 [1] 51/76/-1->52->4 [2] 53/-1/-1->52->51 [3] 51/-1/-1->52->58
f17n02:1075537:1075770 [4] NCCL INFO P2P Chunksize set to 131072
f17n02:1075537:1075770 [4] NCCL INFO Channel 00/0 : 52[4] -> 53[5] via P2P/IPC
f17n02:1075537:1075770 [4] NCCL INFO Channel 01/0 : 52[4] -> 53[5] via P2P/IPC
f17n02:1075537:1075770 [4] NCCL INFO Channel 02/0 : 52[4] -> 53[5] via P2P/IPC
f17n02:1075537:1075770 [4] NCCL INFO Channel 03/0 : 52[4] -> 53[5] via P2P/IPC
f17n02:1075537:1075770 [4] NCCL INFO Connected all rings
f17n02:1075537:1075770 [4] NCCL INFO Channel 03/0 : 52[4] -> 58[4] [send] via NET/IB/3
f17n02:1075537:1075770 [4] NCCL INFO Channel 01/0 : 52[4] -> 76[4] [send] via NET/IB/3
f17n02:1075537:1075770 [4] NCCL INFO Channel 01/0 : 4[4] -> 52[4] [receive] via NET/IB/3
f17n02:1075537:1075770 [4] NCCL INFO Channel 01/0 : 52[4] -> 4[4] [send] via NET/IB/3
f17n02:1075537:1075770 [4] NCCL INFO Channel 01/0 : 76[4] -> 52[4] [receive] via NET/IB/3
f17n02:1075537:1075770 [4] NCCL INFO Channel 03/0 : 58[4] -> 52[4] [receive] via NET/IB/3
f17n02:1075537:1075770 [4] NCCL INFO Channel 00/0 : 52[4] -> 51[3] via P2P/IPC
f17n02:1075537:1075770 [4] NCCL INFO Channel 01/0 : 52[4] -> 51[3] via P2P/IPC
f17n02:1075537:1075770 [4] NCCL INFO Channel 02/0 : 52[4] -> 51[3] via P2P/IPC
f17n02:1075537:1075770 [4] NCCL INFO Channel 03/0 : 52[4] -> 51[3] via P2P/IPC
f17n02:1075537:1075770 [4] NCCL INFO Connected all trees
f17n02:1075537:1075770 [4] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
f17n02:1075537:1075770 [4] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
f17n02:1075537:1075770 [4] NCCL INFO comm 0x15da13b80 rank 52 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x7b12926388d12ce3 - Init COMPLETE
f17n02:1075537:1075880 [4] NCCL INFO Using network IB
f17n02:1075537:1075880 [4] NCCL INFO comm 0x1606c8da0 rank 6 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x7dd05d77214d2e8 - Init START
f17n02:1075537:1075880 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
f17n02:1075537:1075880 [4] NCCL INFO Trees [0] 5/7/-1->6->4 [1] -1/-1/-1->6->5
f17n02:1075537:1075880 [4] NCCL INFO P2P Chunksize set to 131072
f17n02:1075537:1075880 [4] NCCL INFO Channel 00/0 : 5[2] -> 6[4] [receive] via NET/IB/1
f17n02:1075537:1075880 [4] NCCL INFO Channel 01/0 : 5[2] -> 6[4] [receive] via NET/IB/1
f17n02:1075537:1075880 [4] NCCL INFO Channel 00/0 : 6[4] -> 7[0] [send] via NET/IB/1
f17n02:1075537:1075880 [4] NCCL INFO Channel 01/0 : 6[4] -> 7[0] [send] via NET/IB/1
f17n02:1075537:1075880 [4] NCCL INFO Connected all rings
f17n02:1075537:1075880 [4] NCCL INFO Channel 00/0 : 4[0] -> 6[4] [receive] via NET/IB/1
f17n02:1075537:1075880 [4] NCCL INFO Channel 00/0 : 6[4] -> 4[0] [send] via NET/IB/1
f17n02:1075537:1075880 [4] NCCL INFO Channel 00/0 : 7[0] -> 6[4] [receive] via NET/IB/1
f17n02:1075537:1075880 [4] NCCL INFO Channel 00/0 : 6[4] -> 5[2] [send] via NET/IB/1
f17n02:1075537:1075880 [4] NCCL INFO Channel 01/0 : 6[4] -> 5[2] [send] via NET/IB/1
f17n02:1075537:1075880 [4] NCCL INFO Connected all trees
f17n02:1075537:1075880 [4] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
f17n02:1075537:1075880 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f17n02:1075537:1075880 [4] NCCL INFO comm 0x1606c8da0 rank 6 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x7dd05d77214d2e8 - Init COMPLETE
f17n02:1075537:1075898 [4] NCCL INFO Using network IB
f17n02:1075537:1075898 [4] NCCL INFO comm 0x1616f7b30 rank 13 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xaedb5f80914767fd - Init START
f17n02:1075537:1075898 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
f17n02:1075537:1075898 [4] NCCL INFO Trees [0] 6/-1/-1->13->12 [1] -1/-1/-1->13->12
f17n02:1075537:1075898 [4] NCCL INFO P2P Chunksize set to 131072
f17n02:1075537:1075898 [4] NCCL INFO Channel 00/0 : 13[4] -> 14[2] [send] via NET/IB/1
f17n02:1075537:1075898 [4] NCCL INFO Channel 01/0 : 13[4] -> 14[2] [send] via NET/IB/1
f17n02:1075537:1075898 [4] NCCL INFO Connected all rings
f17n02:1075537:1075898 [4] NCCL INFO Channel 00/0 : 6[0] -> 13[4] [receive] via NET/IB/0
f17n02:1075537:1075898 [4] NCCL INFO Channel 00/0 : 13[4] -> 6[0] [send] via NET/IB/0
f17n02:1075537:1075898 [4] NCCL INFO Channel 00/0 : 13[4] -> 12[0] via P2P/IPC
f17n02:1075537:1075898 [4] NCCL INFO Channel 01/0 : 13[4] -> 12[0] via P2P/IPC
f17n02:1075537:1075898 [4] NCCL INFO Connected all trees
f17n02:1075537:1075898 [4] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
f17n02:1075537:1075898 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f17n02:1075537:1075898 [4] NCCL INFO comm 0x1616f7b30 rank 13 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xaedb5f80914767fd - Init COMPLETE
d11n17:1141837:1141837 [5] NCCL INFO cudaDriverVersion 12020
d11n17:1141837:1141837 [5] NCCL INFO Bootstrap : Using ib0:10.41.8.217<0>
d11n17:1141837:1141837 [5] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
d11n17:1141837:1141837 [5] NCCL INFO NET/Plugin : No plugin found, using internal implementation
d11n17:1141837:1142066 [5] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.8.217<0>
d11n17:1141837:1142066 [5] NCCL INFO Using network IB
d11n17:1141837:1142066 [5] NCCL INFO comm 0x13f0435b0 rank 23 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x7b12926388d12ce3 - Init START
d11n17:1141837:1142066 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
d11n17:1141837:1142066 [5] NCCL INFO Trees [0] -1/-1/-1->23->22 [1] 19/-1/-1->23->21 [2] -1/-1/-1->23->22 [3] 19/-1/-1->23->21
d11n17:1141837:1142066 [5] NCCL INFO P2P Chunksize set to 131072
d11n17:1141837:1142066 [5] NCCL INFO Channel 00/0 : 23[5] -> 24[0] [send] via NET/IB/1
d11n17:1141837:1142066 [5] NCCL INFO Channel 02/0 : 23[5] -> 24[0] [send] via NET/IB/1
d11n17:1141837:1142066 [5] NCCL INFO Channel 01/0 : 23[5] -> 20[2] via P2P/IPC
d11n17:1141837:1142066 [5] NCCL INFO Channel 03/0 : 23[5] -> 20[2] via P2P/IPC
d11n17:1141837:1142066 [5] NCCL INFO Connected all rings
d11n17:1141837:1142066 [5] NCCL INFO Channel 01/0 : 23[5] -> 19[1] via P2P/IPC
d11n17:1141837:1142066 [5] NCCL INFO Channel 03/0 : 23[5] -> 19[1] via P2P/IPC
d11n17:1141837:1142066 [5] NCCL INFO Channel 01/0 : 23[5] -> 21[3] via P2P/IPC
d11n17:1141837:1142066 [5] NCCL INFO Channel 03/0 : 23[5] -> 21[3] via P2P/IPC
d11n17:1141837:1142066 [5] NCCL INFO Channel 00/0 : 23[5] -> 22[4] via P2P/IPC
d11n17:1141837:1142066 [5] NCCL INFO Channel 02/0 : 23[5] -> 22[4] via P2P/IPC
d11n17:1141837:1142066 [5] NCCL INFO Connected all trees
d11n17:1141837:1142066 [5] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
d11n17:1141837:1142066 [5] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
d11n17:1141837:1142066 [5] NCCL INFO comm 0x13f0435b0 rank 23 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x7b12926388d12ce3 - Init COMPLETE
d11n17:1141837:1142148 [5] NCCL INFO Using network IB
d11n17:1141837:1142148 [5] NCCL INFO comm 0x141cdde00 rank 2 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x54676b30da0675d6 - Init START
d11n17:1141837:1142148 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
d11n17:1141837:1142148 [5] NCCL INFO Trees [0] 1/3/-1->2->4 [1] -1/-1/-1->2->1
d11n17:1141837:1142148 [5] NCCL INFO P2P Chunksize set to 131072
d11n17:1141837:1142148 [5] NCCL INFO Channel 00/0 : 1[3] -> 2[5] [receive] via NET/IB/3
d11n17:1141837:1142148 [5] NCCL INFO Channel 01/0 : 1[3] -> 2[5] [receive] via NET/IB/3
d11n17:1141837:1142148 [5] NCCL INFO Channel 00/0 : 2[5] -> 3[1] [send] via NET/IB/3
d11n17:1141837:1142148 [5] NCCL INFO Channel 01/0 : 2[5] -> 3[1] [send] via NET/IB/3
d11n17:1141837:1142148 [5] NCCL INFO Connected all rings
d11n17:1141837:1142148 [5] NCCL INFO Channel 00/0 : 2[5] -> 4[3] [send] via NET/IB/3
d11n17:1141837:1142148 [5] NCCL INFO Channel 00/0 : 4[3] -> 2[5] [receive] via NET/IB/3
d11n17:1141837:1142148 [5] NCCL INFO Channel 00/0 : 3[1] -> 2[5] [receive] via NET/IB/3
d11n17:1141837:1142148 [5] NCCL INFO Channel 00/0 : 2[5] -> 1[3] [send] via NET/IB/3
d11n17:1141837:1142148 [5] NCCL INFO Channel 01/0 : 2[5] -> 1[3] [send] via NET/IB/3
d11n17:1141837:1142148 [5] NCCL INFO Connected all trees
d11n17:1141837:1142148 [5] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
d11n17:1141837:1142148 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d11n17:1141837:1142148 [5] NCCL INFO comm 0x141cdde00 rank 2 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x54676b30da0675d6 - Init COMPLETE
d11n17:1141837:1142168 [5] NCCL INFO Using network IB
d11n17:1141837:1142168 [5] NCCL INFO comm 0x141c8ff00 rank d11n16:1129249:1129249 [4] NCCL INFO cudaDriverVersion 12020
d11n16:1129249:1129249 [4] NCCL INFO Bootstrap : Using ib0:10.41.8.216<0>
d11n16:1129249:1129249 [4] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
d11n16:1129249:1129249 [4] NCCL INFO NET/Plugin : No plugin found, using internal implementation
d11n16:1129249:1129480 [4] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.8.216<0>
d11n16:1129249:1129480 [4] NCCL INFO Using network IB
d11n16:1129249:1129480 [4] NCCL INFO comm 0x127893780 rank 16 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x7b12926388d12ce3 - Init START
d11n16:1129249:1129480 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
d11n16:1129249:1129480 [4] NCCL INFO Trees [0] 17/-1/-1->16->15 [1] 15/22/-1->16->27 [2] 17/-1/-1->16->15 [3] 15/-1/-1->16->9
d11n16:1129249:1129480 [4] NCCL INFO P2P Chunksize set to 131072
d11n16:1129249:1129480 [4] NCCL INFO Channel 00/0 : 16[4] -> 17[5] via P2P/IPC
d11n16:1129249:1129480 [4] NCCL INFO Channel 01/0 : 16[4] -> 17[5] via P2P/IPC
d11n16:1129249:1129480 [4] NCCL INFO Channel 02/0 : 16[4] -> 17[5] via P2P/IPC
d11n16:1129249:1129480 [4] NCCL INFO Channel 03/0 : 16[4] -> 17[5] via P2P/IPC
d11n16:1129249:1129480 [4] NCCL INFO Connected all rings
d11n16:1129249:1129480 [4] NCCL INFO Channel 01/0 : 16[4] -> 22[4] [send] via NET/IB/3
d11n16:1129249:1129480 [4] NCCL INFO Channel 03/0 : 9[3] -> 16[4] [receive] via NET/IB/3
d11n16:1129249:1129480 [4] NCCL INFO Channel 01/0 : 16[4] -> 27[3] [send] via NET/IB/3
d11n16:1129249:1129480 [4] NCCL INFO Channel 01/0 : 27[3] -> 16[4] [receive] via NET/IB/3
d11n16:1129249:1129480 [4] NCCL INFO Channel 03/0 : 16[4] -> 9[3] [send] via NET/IB/3
d11n16:1129249:1129480 [4] NCCL INFO Channel 01/0 : 22[4] -> 16[4] [receive] via NET/IB/3
d11n16:1129249:1129480 [4] NCCL INFO Channel 00/0 : 16[4] -> 15[3] via P2P/IPC
d11n16:1129249:1129480 [4] NCCL INFO Channel 01/0 : 16[4] -> 15[3] via P2P/IPC
d11n16:1129249:1129480 [4] NCCL INFO Channel 02/0 : 16[4] -> 15[3] via P2P/IPC
d11n16:1129249:1129480 [4] NCCL INFO Channel 03/0 : 16[4] -> 15[3] via P2P/IPC
d11n16:1129249:1129480 [4] NCCL INFO Connected all trees
d11n16:1129249:1129480 [4] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
d11n16:1129249:1129480 [4] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
d11n16:1129249:1129480 [4] NCCL INFO comm 0x127893780 rank 16 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x7b12926388d12ce3 - Init COMPLETE
d11n16:1129249:1129562 [4] NCCL INFO Using network IB
d11n16:1129249:1129562 [4] NCCL INFO comm 0x12a5fd360 rank 2 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x8264b1a1a7aef6de - Init START
d11n16:1129249:1129562 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
d11n16:1129249:1129562 [4] NCCL INFO Trees [0] 1/3/-1->2->4 [1] -1/-1/-1->2->1
d11n16:1129249:1129562 [4] NCCL INFO P2P Chunksize set to 131072
d11n16:1129249:1129562 [4] NCCL INFO Channel 00/0 : 1[2] -> 2[4] [receive] via NET/IB/1
d11n16:1129249:1129562 [4] NCCL INFO Channel 01/0 : 1[2] -> 2[4] [receive] via NET/IB/1
d11n16:1129249:1129562 [4] NCCL INFO Channel 00/0 : 2[4] -> 3[0] [send] via NET/IB/1
d11n16:1129249:1129562 [4] NCCL INFO Channel 01/0 : 2[4] -> 3[0] [send] via NET/IB/1
d11n16:1129249:1129562 [4] NCCL INFO Connected all rings
d11n16:1129249:1129562 [4] NCCL INFO Channel 00/0 : 2[4] -> 4[2] [send] via NET/IB/1
d11n16:1129249:1129562 [4] NCCL INFO Channel 00/0 : 4[2] -> 2[4] [receive] via NET/IB/1
d11n16:1129249:1129562 [4] NCCL INFO Channel 00/0 : 3[0] -> 2[4] [receive] via NET/IB/1
d11n16:1129249:1129562 [4] NCCL INFO Channel 00/0 : 2[4] -> 1[2] [send] via NET/IB/1
d11n16:1129249:1129562 [4] NCCL INFO Channel 01/0 : 2[4] -> 1[2] [send] via NET/IB/1
d11n16:1129249:1129562 [4] NCCL INFO Connected all trees
d11n16:1129249:1129562 [4] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
d11n16:1129249:1129562 [4] NCCL INF5 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x821cecfd780eaba1 - Init START
d11n17:1141837:1142168 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
d11n17:1141837:1142168 [5] NCCL INFO Trees [0] -1/-1/-1->5->4 [1] 7/-1/-1->5->4
d11n17:1141837:1142168 [5] NCCL INFO P2P Chunksize set to 131072
d11n17:1141837:1142168 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[3] [send] via NET/IB/3
d11n17:1141837:1142168 [5] NCCL INFO Channel 01/0 : 5[5] -> 6[3] [send] via NET/IB/3
d11n17:1141837:1142168 [5] NCCL INFO Connected all rings
d11n17:1141837:1142168 [5] NCCL INFO Channel 01/0 : 5[5] -> 7[1] [send] via NET/IB/2
d11n17:1141837:1142168 [5] NCCL INFO Channel 01/0 : 7[1] -> 5[5] [receive] via NET/IB/2
d11n17:1141837:1142168 [5] NCCL INFO Channel 00/0 : 5[5] -> 4[1] via P2P/IPC
d11n17:1141837:1142168 [5] NCCL INFO Channel 01/0 : 5[5] -> 4[1] via P2P/IPC
d11n17:1141837:1142168 [5] NCCL INFO Connected all trees
d11n17:1141837:1142168 [5] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
d11n17:1141837:1142168 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d11n17:1141837:1142168 [5] NCCL INFO comm 0x141c8ff00 rank 5 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x821cecfd780eaba1 - Init COMPLETE
O 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d11n16:1129249:1129562 [4] NCCL INFO comm 0x12a5fd360 rank 2 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x8264b1a1a7aef6de - Init COMPLETE
d11n16:1129249:1129582 [4] NCCL INFO Using network IB
d11n16:1129249:1129582 [4] NCCL INFO comm 0x12a6a8d60 rank 4 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xaedb5f80914767fd - Init START
d11n16:1129249:1129582 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
d11n16:1129249:1129582 [4] NCCL INFO Trees [0] 2/-1/-1->4->3 [1] -1/-1/-1->4->3
d11n16:1129249:1129582 [4] NCCL INFO P2P Chunksize set to 131072
d11n16:1129249:1129582 [4] NCCL INFO Channel 00/0 : 4[4] -> 5[2] [send] via NET/IB/1
d11n16:1129249:1129582 [4] NCCL INFO Channel 01/0 : 4[4] -> 5[2] [send] via NET/IB/1
d11n16:1129249:1129582 [4] NCCL INFO Connected all rings
d11n16:1129249:1129582 [4] NCCL INFO Channel 00/0 : 2[2] -> 4[4] [receive] via NET/IB/0
d11n16:1129249:1129582 [4] NCCL INFO Channel 00/0 : 4[4] -> 2[2] [send] via NET/IB/0
d11n16:1129249:1129582 [4] NCCL INFO Channel 00/0 : 4[4] -> 3[0] via P2P/IPC
d11n16:1129249:1129582 [4] NCCL INFO Channel 01/0 : 4[4] -> 3[0] via P2P/IPC
d11n16:1129249:1129582 [4] NCCL INFO Connected all trees
d11n16:1129249:1129582 [4] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
d11n16:1129249:1129582 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d11n16:1129249:1129582 [4] NCCL INFO comm 0x12a6a8d60 rank 4 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xaedb5f80914767fd - Init COMPLETE
f16n18:1091137:1091137 [4] NCCL INFO cudaDriverVersion 12020
f16n18:1091137:1091137 [4] NCCL INFO Bootstrap : Using ib0:10.41.14.86<0>
f16n18:1091137:1091137 [4] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
f16n18:1091137:1091137 [4] NCCL INFO NET/Plugin : No plugin found, using internal implementation
f16n18:1091137:1091366 [4] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.14.86<0>
f16n18:1091137:1091366 [4] NCCL INFO Using network IB
f16n18:1091137:1091366 [4] NCCL INFO comm 0x15cc54220 rank 40 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x7b12926388d12ce3 - Init START
f16n18:1091137:1091366 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
f16n18:1091137:1091366 [4] NCCL INFO Trees [0] 41/-1/-1->40->39 [1] 39/46/-1->40->28 [2] 41/-1/-1->40->39 [3] 39/-1/-1->40->33
f16n18:1091137:1091366 [4] NCCL INFO P2P Chunksize set to 131072
f16n18:1091137:1091366 [4] NCCL INFO Channel 00/0 : 40[4] -> 41[5] via P2P/IPC
f16n18:1091137:1091366 [4] NCCL INFO Channel 01/0 : 40[4] -> 41[5] via P2P/IPC
f16n18:1091137:1091366 [4] NCCL INFO Channel 02/0 : 40[4] -> 41[5] via P2P/IPC
f16n18:1091137:1091366 [4] NCCL INFO Channel 03/0 : 40[4] -> 41[5] via P2P/IPC
f16n18:1091137:1091366 [4] NCCL INFO Connected all rings
f16n18:1091137:1091366 [4] NCCL INFO Channel 01/0 : 40[4] -> 46[4] [send] via NET/IB/3
f16n18:1091137:1091366 [4] NCCL INFO Channel 03/0 : 33[3] -> 40[4] [receive] via NET/IB/3
f16n18:1091137:1091366 [4] NCCL INFO Channel 01/0 : 28[4] -> 40[4] [receive] via NET/IB/3
f16n18:1091137:1091366 [4] NCCL INFO Channel 01/0 : 40[4] -> 28[4] [send] via NET/IB/3
f16n18:1091137:1091366 [4] NCCL INFO Channel 03/0 : 40[4] -> 33[3] [send] via NET/IB/3
f16n18:1091137:1091366 [4] NCCL INFO Channel 01/0 : 46[4] -> 40[4] [receive] via NET/IB/3
f16n18:1091137:1091366 [4] NCCL INFO Channel 00/0 : 40[4] -> 39[3] via P2P/IPC
f16n18:1091137:1091366 [4] NCCL INFO Channel 01/0 : 40[4] -> 39[3] via P2P/IPC
f16n18:1091137:1091366 [4] NCCL INFO Channel 02/0 : 40[4] -> 39[3] via P2P/IPC
f16n18:1091137:1091366 [4] NCCL INFO Channel 03/0 : 40[4] -> 39[3] via P2P/IPC
f16n18:1091137:1091366 [4] NCCL INFO Connected all trees
f16n18:1091137:1091366 [4] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
f16n18:1091137:1091366 [4] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
f16n18:1091137:1091366 [4] NCCL INFO comm 0x15cc54220 rank 40 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x7b12926388d12ce3 - Init COMPLETE
f16n18:1091137:1091467 [4] NCCL INFO Using network IB
f16n18:1091137:1091467 [4] NCCL INFO comm 0x15f8c47a0 rank 5 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x8264b1a1a7aef6de - Init START
f16n18:1091137:1091467 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
f16n18:1091137:1091467 [4] NCCL INFO Trees [0] -1/-1/-1->5->6 [1] 6/4/-1->5->7
f16n18:1091137:1091467 [4] NCCL INFO P2P Chunksize set to 131072
f16n18:1091137:1091467 [4] NCCL INFO Channel 00/0 : 4[2] -> 5[4] [receive] via NET/IB/1
f16n18:1091137:1091467 [4] NCCL INFO Channel 01/0 : 4[2] -> 5[4] [receive] via NET/IB/1
f16n18:1091137:1091467 [4] NCCL INFO Channel 00/0 : 5[4] -> 6[0] [send] via NET/IB/1
f16n18:1091137:1091467 [4] NCCL INFO Channel 01/0 : 5[4] -> 6[0] [send] via NET/IB/1
f16n18:1091137:1091467 [4] NCCL INFO Connected all rings
f16n18:1091137:1091467 [4] NCCL INFO Channel 01/0 : 5[4] -> 7[2] [send] via NET/IB/1
f16n18:1091137:1091467 [4] NCCL INFO Channel 01/0 : 7[2] -> 5[4] [receive] via NET/IB/1
f16n18:1091137:1091467 [4] NCCL INFO Channel 00/0 : 6[0] -> 5[4] [receive] via NET/IB/1
f16n18:1091137:1091467 [4] NCCL INFO Channel 01/0 : 6[0] -> 5[4] [receive] via NET/IB/1
f16n18:1091137:1091467 [4] NCCL INFO Channel 01/0 : 5[4] -> 4[2] [send] via NET/IB/1
f16n18:1091137:1091467 [4] NCCL INFO Connected all trees
f16n18:1091137:1091467 [4] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
f16n18:1091137:1091467 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f16n18:1091137:1091467 [4] NCCL INFO comm 0x15f8c47a0 rank 5 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x8264b1a1a7aef6de - Init COMPLETE
f16n18:1091137:1091481 [4] NCCL INFO Using network IB
f16n18:1091137:1091481 [4] NCCL INFO comm 0x15f8c0690 rank 10 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xaedb5f80914767fd - Init START
f16n18:1091137:1091481 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
f16n18:1091137:1091481 [4] NCCL INFO Trees [0] 8/-1/-1->10->9 [1] -1/-1/-1->10->9
f16n18:1091137:1091481 [4] NCCL INFO P2P Chunksize set to 131072
f16n18:1091137:1091481 [4] NCCL INFO Channel 00/0 : 10[4] -> 11[2] [send] via NET/IB/1
f16n18:1091137:1091481 [4] NCCL INFO Channel 01/0 : 10[4] -> 11[2] [send] via NET/IB/1
f16n18:1091137:1091481 [4] NCCL INFO Connected all rings
f16n18:1091137:1091481 [4] NCCL INFO Channel 00/0 : 8[2] -> 10[4] [receive] via NET/IB/0
f16n18:1091137:1091481 [4] NCCL INFO Channel 00/0 : 10[4] -> 8[2] [send] via NET/IB/0
f16n18:1091137:1091481 [4] NCCL INFO Channel 00/0 : 10[4] -> 9[0] via P2P/IPC
f16n18:1091137:1091481 [4] NCCL INFO Channel 01/0 : 10[4] -> 9[0] via P2P/IPC
f16n18:1091137:1091481 [4] NCCL INFO Connected all trees
f16n18:1091137:1091481 [4] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
f16n18:1091137:1091481 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f16n18:1091137:1091481 [4] NCCL INFO comm 0x15f8c0690 rank 10 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xaedb5f80914767fd - Init COMPLETE
d11n16:1129245:1129245 [0] NCCL INFO cudaDriverVersion 12020
d11n16:1129245:1129245 [0] NCCL INFO Bootstrap : Using ib0:10.41.8.216<0>
d11n16:1129245:1129245 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
d11n16:1129245:1129245 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
d11n16:1129245:1129476 [0] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.8.216<0>
d11n16:1129245:1129476 [0] NCCL INFO Using network IB
d11n16:1129245:1129476 [0] NCCL INFO comm 0x136523890 rank 12 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x7b12926388d12ce3 - Init START
d11n16:1129245:1129476 [0] NCCL INFO Setting affinity for GPU 0 to 0f
d11n16:1129245:1129476 [0] NCCL INFO Trees [0] 13/18/-1->12->25 [1] 14/-1/-1->12->13 [2] 13/-1/-1->12->7 [3] 14/-1/-1->12->13
d11n16:1129245:1129476 [0] NCCL INFO P2P Chunksize set to 131072
d11n16:1129245:1129476 [0] NCCL INFO Channel 00/0 : 11[5] -> 12[0] [receive] via NET/IB/0
d11n16:1129245:1129476 [0] NCCL INFO Channel 02/0 : 11[5] -> 12[0] [receive] via NET/IB/0
d11n16:1129245:1129476 [0] NCCL INFO Channel 00/0 : 12[0] -> 13[1] via P2P/IPC
d11n16:1129245:1129476 [0] NCCL INFO Channel 02/0 : 12[0] -> 13[1] via P2P/IPC
d11n16:1129245:1129476 [0] NCCL INFO Channel 01/0 : 12[0] -> 21[3] [send] via NET/IB/2
d11n16:1129245:1129476 [0] NCCL INFO Channel 03/0 : 12[0] -> 21[3] [send] via NET/IB/2
d11n16:1129245:1129476 [0] NCCL INFO Connected all rings
d11n16:1129245:1129476 [0] NCCL INFO Channel 01/0 : 12[0] -> 13[1] via P2P/IPC
d11n16:1129245:1129476 [0] NCCL INFO Channel 03/0 : 12[0] -> 13[1] via P2P/IPC
d11n16:1129245:1129476 [0] NCCL INFO Channel 01/0 : 12[0] -> 14[2] via P2P/IPC
d11n16:1129245:1129476 [0] NCCL INFO Channel 03/0 : 12[0] -> 14[2] via P2P/IPC
d11n16:1129245:1129476 [0] NCCL INFO Channel 02/0 : 7[1] -> 12[0] [receive] via NET/IB/0
d11n16:1129245:1129476 [0] NCCL INFO Channel 00/0 : 12[0] -> 18[0] [send] via NET/IB/0
d11n16:1129245:1129476 [0] NCCL INFO Channel 00/0 : 12[0] -> 25[1] [send] via NET/IB/0
d11n16:1129245:1129476 [0] NCCL INFO Channel 00/0 : 25[1] -> 12[0] [receive] via NET/IB/0
d11n16:1129245:1129476 [0] NCCL INFO Channel 00/0 : 18[0] -> 12[0] [receive] via NET/IB/0
d11n16:1129245:1129476 [0] NCCL INFO Channel 02/0 : 12[0] -> 7[1] [send] via NET/IB/0
d11n16:1129245:1129476 [0] NCCL INFO Connected all trees
d11n16:1129245:1129476 [0] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
d11n16:1129245:1129476 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
d11n16:1129245:1129476 [0] NCCL INFO comm 0x136523890 rank 12 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x7b12926388d12ce3 - Init COMPLETE
d11n16:1129245:1129558 [0] NCCL INFO Using network IB
d11n16:1129245:1129558 [0] NCCL INFO comm 0x13a25c2e0 rank 1 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x7dd05d77214d2e8 - Init START
d11n16:1129245:1129558 [0] NCCL INFO Setting affinity for GPU 0 to 0f
d11n16:1129245:1129558 [0] NCCL INFO Trees [0] -1/-1/-1->1->2 [1] 2/0/-1->1->3
d11n16:1129245:1129558 [0] NCCL INFO P2P Chunksize set to 131072
d11n16:1129245:1129558 [0] NCCL INFO Channel 00/0 : 0[4] -> 1[0] [receive] via NET/IB/0
d11n16:1129245:1129558 [0] NCCL INFO Channel 01/0 : 0[4] -> 1[0] [receive] via NET/IB/0
d11n16:1129245:1129558 [0] NCCL INFO Channel 00/0 : 1[0] -> 2[2] [send] via NET/IB/0
d11n16:1129245:1129558 [0] NCCL INFO Channel 01/0 : 1[0] -> 2[2] [send] via NET/IB/0
d11n16:1129245:1129558 [0] NCCL INFO Connected all rings
d11n16:1129245:1129558 [0] NCCL INFO Channel 01/0 : 1[0] -> 3[4] [send] via NET/IB/0
d11n16:1129245:1129558 [0] NCCL INFO Channel 01/0 : 3[4] -> 1[0] [receive] via NET/IB/0
d11n16:1129245:1129558 [0] NCCL INFO Channel 00/0 : 2[2] -> 1[0] [receive] via NET/IB/0
d11n16:1129245:1129558 [0] NCCL INFO Channel 01/0 : 2[2] -> 1[0] [receive] via NET/IB/0
d11n16:1129245:1129558 [0] NCCL INFO Channel 01/0 : 1[0] -> 0[4] [send] via NET/IB/0
d11n16:1129245:1129558 [0] NCCL INFO Connected all trees
d11n16:1129245:1129558 [0] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
d11n16:1129245:1129558 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d11n16:1129245:1129558 [0] NCCL INFO comm 0x13a25c2e0 rank 1 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x7dd05d77214d2e8 - Init COMPLETE
d11n16:1129245:1129577 [0] NCCL INFO Using network IB
d11n16:1129245:1129577 [0] NCCL INFO comm 0x13996f040 rank 3 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0xaedb5f80914767fd - Init START
d11n16:1129245:1129577 [0] NCCL INFO Setting affinity for GPU 0 to 0f
d11n16:1129245:1129577 [0] NCCL INFO Trees [0] 4/5/-1->3->7 [1] 4/-1/-1->3->2
d11n16:1129245:1129577 [0] NCCL INFO P2P Chunksize set to 131072
d11n16:1129245:1129577 [0] NCCL INFO Channel 00/0 : 2[2] -> 3[0] [receive] via NET/IB/0
d11n16:1129245:1129577 [0] NCCL INFO Channel 01/0 : 2[2] -> 3[0] [receive] via NET/IB/0
d11n16:1129245:1129577 [0] NCCL INFO Channel 00/0 : 3[0] -> 4[4] via P2P/IPC
d11n16:1129245:1129577 [0] NCCL INFO Channel 01/0 : 3[0] -> 4[4] via P2P/IPC
d11n16:1129245:1129577 [0] NCCL INFO Connected all rings
d11n16:1129245:1129577 [0] NCCL INFO Channel 00/0 : 3[0] -> 5[2] [send] via NET/IB/0
d11n16:1129245:1129577 [0] NCCL INFO Channel 00/0 : 3[0] -> 7[4] [send] via NET/IB/0
d11n16:1129245:1129577 [0] NCCL INFO Channel 00/0 : 7[4] -> 3[0] [receive] via NET/IB/0
d11n16:1129245:1129577 [0] NCCL INFO Channel 00/0 : 5[2] -> 3[0] [receive] via NET/IB/0
d11n16:1129245:1129577 [0] NCCL INFO Channel 01/0 : 3[0] -> 2[2] [send] via NET/IB/0
d11n16:1129245:1129577 [0] NCCL INFO Connected all trees
d11n16:1129245:1129577 [0] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
d11n16:1129245:1129577 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d11n16:1129245:1129577 [0] NCCL INFO comm 0x13996f040 rank 3 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0xaedb5f80914767fd - Init COMPLETE
d11n17:1141835:1141835 [3] NCCL INFO cudaDriverVersion 12020
d11n17:1141835:1141835 [3] NCCL INFO Bootstrap : Using ib0:10.41.8.217<0>
d11n17:1141835:1141835 [3] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
d11n17:1141835:1141835 [3] NCCL INFO NET/Plugin : No plugin found, using internal implementation
d11n17:1141835:1142067 [3] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.8.217<0>
d11n17:1141835:1142067 [3] NCCL INFO Using network IB
d11n17:1141835:1142067 [3] NCCL INFO comm 0x1556c3fd0 rank 21 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x7b12926388d12ce3 - Init START
d11n17:1141835:1142067 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
d11n17:1141835:1142067 [3] NCCL INFO Trees [0] 22/-1/-1->21->20 [1] 23/-1/-1->21->22 [2] 22/-1/-1->21->20 [3] 23/34/-1->21->22
d11n17:1141835:1142067 [3] NCCL INFO P2P Chunksize set to 131072
d11n17:1141835:1142067 [3] NCCL INFO Channel 00/0 : 21[3] -> 22[4] via P2P/IPC
d11n17:1141835:1142067 [3] NCCL INFO Channel 01/0 : 21[3] -> 22[4] via P2P/IPC
d11n17:1141835:1142067 [3] NCCL INFO Channel 02/0 : 21[3] -> 22[4] via P2P/IPC
d11n17:1141835:1142067 [3] NCCL INFO Channel 03/0 : 21[3] -> 22[4] via P2P/IPC
d11n17:1141835:1142067 [3] NCCL INFO Channel 01/0 : 12[0] -> 21[3] [receive] via NET/IB/3
d11n17:1141835:1142067 [3] NCCL INFO Channel 03/0 : 12[0] -> 21[3] [receive] via NET/IB/3
d11n17:1141835:1142067 [3] NCCL INFO Connected all rings
d11n17:1141835:1142067 [3] NCCL INFO Channel 01/0 : 21[3] -> 23[5] via P2P/IPC
d11n17:1141835:1142067 [3] NCCL INFO Channel 03/0 : 21[3] -> 23[5] via P2P/IPC
d11n17:1141835:1142067 [3] NCCL INFO Channel 03/0 : 21[3] -> 34[4] [send] via NET/IB/3
d11n17:1141835:1142067 [3] NCCL INFO Channel 03/0 : 34[4] -> 21[3] [receive] via NET/IB/3
d11n17:1141835:1142067 [3] NCCL INFO Channel 00/0 : 21[3] -> 20[2] via P2P/IPC
d11n17:1141835:1142067 [3] NCCL INFO Channel 02/0 : 21[3] -> 20[2] via P2P/IPC
d11n17:1141835:1142067 [3] NCCL INFO Connected all trees
d11n17:1141835:1142067 [3] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
d11n17:1141835:1142067 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
d11n17:1141835:1142067 [3] NCCL INFO comm 0x1556c3fd0 rank 21 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x7b12926388d12ce3 - Init COMPLETE
d11n17:1141835:1142152 [3] NCCL INFO Using network IB
d11n17:1141835:1142152 [3] NCCL INFO comm 0x157f99700 rank 2 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xa77518dab25ce26 - Init START
d11n17:1141835:1142152 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
d11n17:1141835:1142152 [3] NCCL INFO Trees [0] 1/3/-1->2->4 [1] -1/-1/-1->2->1
d11n17:1141835:1142152 [3] NCCL INFO P2P Chunksize set to 131072
d11n17:1141835:1142152 [3] NCCL INFO Channel 00/0 : 1[1] -> 2[3] [receive] via NET/IB/3
d11n17:1141835:1142152 [3] NCCL INFO Channel 01/0 : 1[1] -> 2[3] [receive] via NET/IB/3
d11n17:1141835:1142152 [3] NCCL INFO Channel 00/0 : 2[3] -> 3[5] [send] via NET/IB/3
d11n17:1141835:1142152 [3] NCCL INFO Channel 01/0 : 2[3] -> 3[5] [send] via NET/IB/3
d11n17:1141835:1142152 [3] NCCL INFO Connected all rings
d11n17:1141835:1142152 [3] NCCL INFO Channel 00/0 : 2[3] -> 4[1] [send] via NET/IB/3
d11n17:1141835:1142152 [3] NCCL INFO Channel 00/0 : 4[1] -> 2[3] [receive] via NET/IB/3
d11n17:1141835:1142152 [3] NCCL INFO Channel 00/0 : 3[5] -> 2[3] [receive] via NET/IB/3
d11n17:1141835:1142152 [3] NCCL INFO Channel 00/0 : 2[3] -> 1[1] [send] via NET/IB/3
d11n17:1141835:1142152 [3] NCCL INFO Channel 01/0 : 2[3] -> 1[1] [send] via NET/IB/3
d11n17:1141835:1142152 [3] NCCL INFO Connected all trees
d11n17:1141835:1142152 [3] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
d11n17:1141835:1142152 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d11n17:1141835:1142152 [3] NCCL INFO comm 0x157f99700 rank 2 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xa77518dab25ce26 - Init COMPLETE
d11n17:1141835:1142169 [3] NCCL INFO Using network IB
d11n17:1141835:1142169 [3] NCCL INFO comm 0x15849a2b0 rank 5 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xa49a27aa8b15034e - Init START
d11n17:1141835:1142169 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
d11n17:1141835:1142169 [3] NCCL INFO Trees [0] -1/-1/-1->5->3 [1] 8/2/-1->5->11
d11n17:1141835:1142169 [3] NCCL INFO P2P Chunksize set to 131072
d11n17:1141835:1142169 [3] NCCL INFO Channel 00/0 : 4[5] -> 5[3] [receive] via NET/IB/3
d11n17:1141835:1142169 [3] NCCL INFO Channel 01/0 : 4[5] -> 5[3] [receive] via NET/IB/3
d11n17:1141835:1142169 [3] NCCL INFO Channel 00/0 : 5[3] -> 6[1] [send] via NET/IB/3
d11n17:1141835:1142169 [3] NCCL INFO Channel 01/0 : 5[3] -> 6[1] [send] via NET/IB/3
d11n17:1141835:1142169 [3] NCCL INFO Connected all rings
d11n17:1141835:1142169 [3] NCCL INFO Channel 00/0 : 3[1] -> 5[3] [receive] via NET/IB/3
d11n17:1141835:1142169 [3] NCCL INFO Channel 01/0 : 2[3] -> 5[3] [receive] via NET/IB/3
d11n17:1141835:1142169 [3] NCCL INFO Channel 01/0 : 5[3] -> 8[3] [send] via NET/IB/3
d11n17:1141835:1142169 [3] NCCL INFO Channel 01/0 : 5[3] -> 11[3] [send] via NET/IB/3
d11n17:1141835:1142169 [3] NCCL INFO Channel 01/0 : 11[3] -> 5[3] [receive] via NET/IB/3
d11n17:1141835:1142169 [3] NCCL INFO Channel 01/0 : 8[3] -> 5[3] [receive] via NET/IB/3
d11n17:1141835:1142169 [3] NCCL INFO Channel 01/0 : 5[3] -> 2[3] [send] via NET/IB/3
d11n17:1141835:1142169 [3] NCCL INFO Channel 00/0 : 5[3] -> 3[1] [send] via NET/IB/3
d11n17:1141835:1142169 [3] NCCL INFO Connected all trees
d11n17:1141835:1142169 [3] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
d11n17:1141835:1142169 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d11n17:1141835:1142169 [3] NCCL INFO comm 0x15849a2b0 rank 5 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xa49a27aa8b15034e - Init COMPLETE
f17n01:969565:969565 [5] NCCL INFO cudaDriverVersion 12020
f17n01:969565:969565 [5] NCCL INFO Bootstrap : Using ib0:10.41.14.87<0>
f17n01:969565:969565 [5] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
f17n01:969565:969565 [5] NCCL INFO NET/Plugin : No plugin found, using internal implementation
f17n01:969565:969792 [5] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.14.87<0>
f17n01:969565:969792 [5] NCCL INFO Using network IB
f17n01:969565:969792 [5] NCCL INFO comm 0x16ea23f90 rank 47 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x7b12926388d12ce3 - Init START
f17n01:969565:969792 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
f17n01:969565:969792 [5] NCCL INFO Trees [0] -1/-1/-1->47->46 [1] 43/-1/-1->47->45 [2] -1/-1/-1->47->46 [3] 43/-1/-1->47->45
f17n01:969565:969792 [5] NCCL INFO P2P Chunksize set to 131072
f17n01:969565:969792 [5] NCCL INFO Channel 00/0 : 47[5] -> 48[0] [send] via NET/IB/1
f17n01:969565:969792 [5] NCCL INFO Channel 02/0 : 47[5] -> 48[0] [send] via NET/IB/1
f17n01:969565:969792 [5] NCCL INFO Channel 01/0 : 47[5] -> 44[2] via P2P/IPC
f17n01:969565:969792 [5] NCCL INFO Channel 03/0 : 47[5] -> 44[2] via P2P/IPC
f17n01:969565:969792 [5] NCCL INFO Connected all rings
f17n01:969565:969792 [5] NCCL INFO Channel 01/0 : 47[5] -> 43[1] via P2P/IPC
f17n01:969565:969792 [5] NCCL INFO Channel 03/0 : 47[5] -> 43[1] via P2P/IPC
f17n01:969565:969792 [5] NCCL INFO Channel 01/0 : 47[5] -> 45[3] via P2P/IPC
f17n01:969565:969792 [5] NCCL INFO Channel 03/0 : 47[5] -> 45[3] via P2P/IPC
f17n01:969565:969792 [5] NCCL INFO Channel 00/0 : 47[5] -> 46[4] via P2P/IPC
f17n01:969565:969792 [5] NCCL INFO Channel 02/0 : 47[5] -> 46[4] via P2P/IPC
f17n01:969565:969792 [5] NCCL INFO Connected all trees
f17n01:969565:969792 [5] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
f17n01:969565:969792 [5] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
f17n01:969565:969792 [5] NCCL INFO comm 0x16ea23f90 rank 47 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x7b12926388d12ce3 - Init COMPLETE
f17n01:969565:969877 [5] NCCL INFO Using network IB
f17n01:969565:969877 [5] NCCL INFO comm 0x1718aabf0 rank 5 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x54676b30da0675d6 - Init START
f17n01:969565:969877 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
f17n01:969565:969877 [5] NCCL INFO Trees [0] -1/-1/-1->5->6 [1] 6/4/-1->5->7
f17n01:969565:969877 [5] NCCL INFO P2P Chunksize set to 131072
f17n01:969565:969877 [5] NCCL INFO Channel 00/0 : 4[3] -> 5[5] [receive] via NET/IB/3
f17n01:969565:969877 [5] NCCL INFO Channel 01/0 : 4[3] -> 5[5] [receive] via NET/IB/3
f17n01:969565:969877 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[1] [send] via NET/IB/3
f17n01:969565:969877 [5] NCCL INFO Channel 01/0 : 5[5] -> 6[1] [send] via NET/IB/3
f17n01:969565:969877 [5] NCCL INFO Connected all rings
f17n01:969565:969877 [5] NCCL INFO Channel 01/0 : 5[5] -> 7[3] [send] via NET/IB/3
f17n01:969565:969877 [5] NCCL INFO Channel 01/0 : 7[3] -> 5[5] [receive] via NET/IB/3
f17n01:969565:969877 [5] NCCL INFO Channel 00/0 : 6[1] -> 5[5] [receive] via NET/IB/3
f17n01:969565:969877 [5] NCCL INFO Channel 01/0 : 6[1] -> 5[5] [receive] via NET/IB/3
f17n01:969565:969877 [5] NCCL INFO Channel 01/0 : 5[5] -> 4[3] [send] via NET/IB/3
f17n01:969565:969877 [5] NCCL INFO Connected all trees
f17n01:969565:969877 [5] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
f17n01:969565:969877 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f17n01:969565:969877 [5] NCCL INFO comm 0x1718aabf0 rank 5 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x54676b30da0675d6 - Init COMPLETE
f17n01:969565:969899 [5] NCCL INFO Using network IB
f17n01:969565:969899 [5] NCCL INFO comm 0x1717ed4e0 rank 11 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x821cecfd780eaba1 - Init START
f17n01:969565:969899 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
f17n01:969565:969899 [5] NCCL INFO Trees [0] -1/-1/-1->11->10 [1] 16/-1/-1->11->10
f17n01:969565:969899 [5] NCCL INFO P2P Chunksize set to 131072
f17n01:969565:969899 [5] NCCL INFO Channel 00/0 : 11[5] -> 12[3] [send] via NET/IB/3
f17n01:969565:969899 [5] NCCL INFO Channel 01/0 : 11[5] -> 12[3] [send] via NET/IB/3
f17n01:969565:969899 [5] NCCL INFO Connected all rings
f17n01:969565:969899 [5] NCCL INFO Channel 01/0 : 11[5] -> 16[1] [send] via NET/IB/2
f17n01:969565:969899 [5] NCCL INFO Channel 01/0 : 16[1] -> 11[5] [receive] via NET/IB/2
f17n01:969565:969899 [5] NCCL INFO Channel 00/0 : 11[5] -> 10[1] via P2P/IPC
f17n01:969565:969899 [5] NCCL INFO Channel 01/0 : 11[5] -> 10[1] via P2P/IPC
f17n01:969565:969899 [5] NCCL INFO Connected all trees
f17n01:969565:969899 [5] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
f17n01:969565:969899 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f17n01:969565:969899 [5] NCCL INFO comm 0x1717ed4e0 rank 11 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x821cecfd780eaba1 - Init COMPLETE
f17n01:969561:969561 [3] NCCL INFO cudaDriverVersion 12020
f17n01:969561:969561 [3] NCCL INFO Bootstrap : Using ib0:10.41.14.87<0>
f17n01:969561:969561 [3] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
f17n01:969561:969561 [3] NCCL INFO NET/Plugin : No plugin found, using internal implementation
f17n01:969561:969793 [3] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.14.87<0>
f17n01:969561:969793 [3] NCCL INFO Using network IB
f17n01:969561:969793 [3] NCCL INFO comm 0x12f4a3980 rank 45 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x7b12926388d12ce3 - Init START
f17n01:969561:969793 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
f17n01:969561:969793 [3] NCCL INFO Trees [0] 46/-1/-1->45->44 [1] 47/-1/-1->45->46 [2] 46/-1/-1->45->44 [3] 47/70/-1->45->46
f17n01:969561:969793 [3] NCCL INFO P2P Chunksize set to 131072
f17n01:969561:969793 [3] NCCL INFO Channel 00/0 : 45[3] -> 46[4] via P2P/IPC
f17n01:969561:969793 [3] NCCL INFO Channel 01/0 : 45[3] -> 46[4] via P2P/IPC
f17n01:969561:969793 [3] NCCL INFO Channel 02/0 : 45[3] -> 46[4] via P2P/IPC
f17n01:969561:969793 [3] NCCL INFO Channel 03/0 : 45[3] -> 46[4] via P2P/IPC
f17n01:969561:969793 [3] NCCL INFO Channel 01/0 : 36[0] -> 45[3] [receive] via NET/IB/3
f17n01:969561:969793 [3] NCCL INFO Channel 03/0 : 36[0] -> 45[3] [receive] via NET/IB/3
f17n01:969561:969793 [3] NCCL INFO Connected all rings
f17n01:969561:969793 [3] NCCL INFO Channel 01/0 : 45[3] -> 47[5] via P2P/IPC
f17n01:969561:969793 [3] NCCL INFO Channel 03/0 : 45[3] -> 47[5] via P2P/IPC
f17n01:969561:969793 [3] NCCL INFO Channel 03/0 : 45[3] -> 70[4] [send] via NET/IB/3
f17n01:969561:969793 [3] NCCL INFO Channel 03/0 : 70[4] -> 45[3] [receive] via NET/IB/3
f17n01:969561:969793 [3] NCCL INFO Channel 00/0 : 45[3] -> 44[2] via P2P/IPC
f17n01:969561:969793 [3] NCCL INFO Channel 02/0 : 45[3] -> 44[2] via P2P/IPC
f17n01:969561:969793 [3] NCCL INFO Connected all trees
f17n01:969561:969793 [3] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
f17n01:969561:969793 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
f17n01:969561:969793 [3] NCCL INFO comm 0x12f4a3980 rank 45 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x7b12926388d12ce3 - Init COMPLETE
f17n01:969561:969881 [3] NCCL INFO Using network IB
f17n01:969561:969881 [3] NCCL INFO comm 0x13288c260 rank 5 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xa77518dab25ce26 - Init START
f17n01:969561:969881 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
f17n01:969561:969881 [3] NCCL INFO Trees [0] -1/-1/-1->5->6 [1] 6/4/-1->5->7
f17n01:969561:969881 [3] NCCL INFO P2P Chunksize set to 131072
f17n01:969561:969881 [3] NCCL INFO Channel 00/0 : 4[1] -> 5[3] [receive] via NET/IB/3
f17n01:969561:969881 [3] NCCL INFO Channel 01/0 : 4[1] -> 5[3] [receive] via NET/IB/3
f17n01:969561:969881 [3] NCCL INFO Channel 00/0 : 5[3] -> 6[5] [send] via NET/IB/3
f17n01:969561:969881 [3] NCCL INFO Channel 01/0 : 5[3] -> 6[5] [send] via NET/IB/3
f17n01:969561:969881 [3] NCCL INFO Connected all rings
f17n01:969561:969881 [3] NCCL INFO Channel 01/0 : 5[3] -> 7[1] [send] via NET/IB/3
f17n01:969561:969881 [3] NCCL INFO Channel 01/0 : 7[1] -> 5[3] [receive] via NET/IB/3
f17n01:969561:969881 [3] NCCL INFO Channel 00/0 : 6[5] -> 5[3] [receive] via NET/IB/3
f17n01:969561:969881 [3] NCCL INFO Channel 01/0 : 6[5] -> 5[3] [receive] via NET/IB/3
f17n01:969561:969881 [3] NCCL INFO Channel 01/0 : 5[3] -> 4[1] [send] via NET/IB/3
f17n01:969561:969881 [3] NCCL INFO Connected all trees
f17n01:969561:969881 [3] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
f17n01:969561:969881 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f17n01:969561:969881 [3] NCCL INFO comm 0x13288c260 rank 5 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xa77518dab25ce26 - Init COMPLETE
f17n01:969561:969897 [3] NCCL INFO Using network IB
f17n01:969561:969897 [3] NCCL INFO comm 0x1311b56b0 rank 11 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xa49a27aa8b15034e - Init START
f17n01:969561:969897 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
f17n01:969561:969897 [3] NCCL INFO Trees [0] -1/-1/-1->11->9 [1] 17/5/-1->11->23
f17n01:969561:969897 [3] NCCL INFO P2P Chunksize set to 131072
f17n01:969561:969897 [3] NCCL INFO Channel 00/0 : 10[5] -> 11[3] [receive] via NET/IB/3
f17n01:969561:969897 [3] NCCL INFO Channel 01/0 : 10[5] -> 11[3] [receive] via NET/IB/3
f17n01:969561:969897 [3] NCCL INFO Channel 00/0 : 11[3] -> 12[1] [send] via NET/IB/3
f17n01:969561:969897 [3] NCCL INFO Channel 01/0 : 11[3] -> 12[1] [send] via NET/IB/3
f17n01:969561:969897 [3] NCCL INFO Connected all rings
f17n01:969561:969897 [3] NCCL INFO Channel 00/0 : 9[1] -> 11[3] [receive] via NET/IB/3
f17n01:969561:969897 [3] NCCL INFO Channel 01/0 : 5[3] -> 11[3] [receive] via NET/IB/3
f17n01:969561:969897 [3] NCCL INFO Channel 01/0 : 11[3] -> 17[3] [send] via NET/IB/3
f17n01:969561:969897 [3] NCCL INFO Channel 01/0 : 23[3] -> 11[3] [receive] via NET/IB/3
f17n01:969561:969897 [3] NCCL INFO Channel 01/0 : 11[3] -> 23[3] [send] via NET/IB/3
f17n01:969561:969897 [3] NCCL INFO Channel 01/0 : 17[3] -> 11[3] [receive] via NET/IB/3
f17n01:969561:969897 [3] NCCL INFO Channel 01/0 : 11[3] -> 5[3] [send] via NET/IB/3
f17n01:969561:969897 [3] NCCL INFO Channel 00/0 : 11[3] -> 9[1] [send] via NET/IB/3
f17n01:969561:969897 [3] NCCL INFO Connected all trees
f17n01:969561:969897 [3] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
f17n01:969561:969897 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f17n01:969561:969897 [3] NCCL INFO comm 0x1311b56b0 rank 11 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xa49a27aa8b15034e - Init COMPLETE
f17n01:969560:969560 [2] NCCL INFO cudaDriverVersion 12020
f17n01:969560:969560 [2] NCCL INFO Bootstrap : Using ib0:10.41.14.87<0>
f17n01:969560:969560 [2] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
f17n01:969560:969560 [2] NCCL INFO NET/Plugin : No plugin found, using internal implementation
f17n01:969560:969790 [2] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.14.87<0>
f17n01:969560:969790 [2] NCCL INFO Using network IB
f17n01:969560:969790 [2] NCCL INFO comm 0x15e3a3d10 rank 44 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x7b12926388d12ce3 - Init START
f17n01:969560:969790 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
f17n01:969560:969790 [2] NCCL INFO Trees [0] 45/-1/-1->44->43 [1] -1/-1/-1->44->42 [2] 45/-1/-1->44->43 [3] -1/-1/-1->44->42
f17n01:969560:969790 [2] NCCL INFO P2P Chunksize set to 131072
f17n01:969560:969790 [2] NCCL INFO Channel 00/0 : 44[2] -> 45[3] via P2P/IPC
f17n01:969560:969790 [2] NCCL INFO Channel 02/0 : 44[2] -> 45[3] via P2P/IPC
f17n01:969560:969790 [2] NCCL INFO Channel 01/0 : 44[2] -> 43[1] via P2P/IPC
f17n01:969560:969790 [2] NCCL INFO Channel 03/0 : 44[2] -> 43[1] via P2P/IPC
f17n01:969560:969790 [2] NCCL INFO Connected all rings
f17n01:969560:969790 [2] NCCL INFO Channel 01/0 : 44[2] -> 42[0] via P2P/IPC
f17n01:969560:969790 [2] NCCL INFO Channel 03/0 : 44[2] -> 42[0] via P2P/IPC
f17n01:969560:969790 [2] NCCL INFO Channel 00/0 : 44[2] -> 43[1] via P2P/IPC
f17n01:969560:969790 [2] NCCL INFO Channel 02/0 : 44[2] -> 43[1] via P2P/IPC
f17n01:969560:969790 [2] NCCL INFO Connected all trees
f17n01:969560:969790 [2] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
f17n01:969560:969790 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
f17n01:969560:969790 [2] NCCL INFO comm 0x15e3a3d10 rank 44 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x7b12926388d12ce3 - Init COMPLETE
f17n01:969560:969876 [2] NCCL INFO Using network IB
f17n01:969560:969876 [2] NCCL INFO comm 0x1600bfd70 rank 5 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0x7dd05d77214d2e8 - Init START
f17n01:969560:969876 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
f17n01:969560:969876 [2] NCCL INFO Trees [0] -1/-1/-1->5->6 [1] 6/4/-1->5->7
f17n01:969560:969876 [2] NCCL INFO P2P Chunksize set to 131072
f17n01:969560:969876 [2] NCCL INFO Channel 00/0 : 4[0] -> 5[2] [receive] via NET/IB/0
f17n01:969560:969876 [2] NCCL INFO Channel 01/0 : 4[0] -> 5[2] [receive] via NET/IB/0
f17n01:969560:969876 [2] NCCL INFO Channel 00/0 : 5[2] -> 6[4] [send] via NET/IB/0
f17n01:969560:969876 [2] NCCL INFO Channel 01/0 : 5[2] -> 6[4] [send] via NET/IB/0
f17n01:969560:969876 [2] NCCL INFO Connected all rings
f17n01:969560:969876 [2] NCCL INFO Channel 01/0 : 5[2] -> 7[0] [send] via NET/IB/0
f17n01:969560:969876 [2] NCCL INFO Channel 01/0 : 7[0] -> 5[2] [receive] via NET/IB/0
f17n01:969560:969876 [2] NCCL INFO Channel 00/0 : 6[4] -> 5[2] [receive] via NET/IB/0
f17n01:969560:969876 [2] NCCL INFO Channel 01/0 : 6[4] -> 5[2] [receive] via NET/IB/0
f17n01:969560:969876 [2] NCCL INFO Channel 01/0 : 5[2] -> 4[0] [send] via NET/IB/0
f17n01:969560:969876 [2] NCCL INFO Connected all trees
f17n01:969560:969876 [2] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
f17n01:969560:969876 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f17n01:969560:969876 [2] NCCL INFO comm 0x1600bfd70 rank 5 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0x7dd05d77214d2e8 - Init COMPLETE
f17n01:969560:969895 [2] NCCL INFO Using network IB
f17n01:969560:969895 [2] NCCL INFO comm 0x16116bb90 rank 11 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0xaedb5f80914767fd - Init START
f17n01:969560:969895 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
f17n01:969560:969895 [2] NCCL INFO Trees [0] -1/-1/-1->11->9 [1] 17/5/-1->11->23
f17n01:969560:969895 [2] NCCL INFO P2P Chunksize set to 131072
f17n01:969560:969895 [2] NCCL INFO Channel 00/0 : 10[4] -> 11[2] [receive] via NET/IB/0
f17n01:969560:969895 [2] NCCL INFO Channel 01/0 : 10[4] -> 11[2] [receive] via NET/IB/0
f17n01:969560:969895 [2] NCCL INFO Channel 00/0 : 11[2] -> 12[0] [send] via NET/IB/0
f17n01:969560:969895 [2] NCCL INFO Channel 01/0 : 11[2] -> 12[0] [send] via NET/IB/0
f17n01:969560:969895 [2] NCCL INFO Connected all rings
f17n01:969560:969895 [2] NCCL INFO Channel 00/0 : 9[0] -> 11[2] [receive] via NET/IB/0
f17n01:969560:969895 [2] NCCL INFO Channel 01/0 : 5[2] -> 11[2] [receive] via NET/IB/0
f17n01:969560:969895 [2] NCCL INFO Channel 01/0 : 11[2] -> 17[2] [send] via NET/IB/0
f17n01:969560:969895 [2] NCCL INFO Channel 01/0 : 23[2] -> 11[2] [receive] via NET/IB/0
f17n01:969560:969895 [2] NCCL INFO Channel 01/0 : 11[2] -> 23[2] [send] via NET/IB/0
f17n01:969560:969895 [2] NCCL INFO Channel 01/0 : 17[2] -> 11[2] [receive] via NET/IB/0
f17n01:969560:969895 [2] NCCL INFO Channel 01/0 : 11[2] -> 5[2] [send] via NET/IB/0
f17n01:969560:969895 [2] NCCL INFO Channel 00/0 : 11[2] -> 9[0] [send] via NET/IB/0
f17n01:969560:969895 [2] NCCL INFO Connected all trees
f17n01:969560:969895 [2] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
f17n01:969560:969895 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f17n01:969560:969895 [2] NCCL INFO comm 0x16116bb90 rank 11 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0xaedb5f80914767fd - Init COMPLETE
f17n01:969558:969558 [1] NCCL INFO cudaDriverVersion 12020
f17n01:969558:969558 [1] NCCL INFO Bootstrap : Using ib0:10.41.14.87<0>
f17n01:969558:969558 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
f17n01:969558:969558 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
f17n01:969558:969795 [1] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.14.87<0>
f17n01:969558:969795 [1] NCCL INFO Using network IB
f17n01:969558:969795 [1] NCCL INFO comm 0x17a243f60 rank 43 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x7b12926388d12ce3 - Init START
f17n01:969558:969795 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
f17n01:969558:969795 [1] NCCL INFO Trees [0] 44/-1/-1->43->42 [1] 42/-1/-1->43->47 [2] 44/66/-1->43->42 [3] 42/-1/-1->43->47
f17n01:969558:969795 [1] NCCL INFO P2P Chunksize set to 131072
f17n01:969558:969795 [1] NCCL INFO Channel 00/0 : 43[1] -> 44[2] via P2P/IPC
f17n01:969558:969795 [1] NCCL INFO Channel 02/0 : 43[1] -> 44[2] via P2P/IPC
f17n01:969558:969795 [1] NCCL INFO Channel 01/0 : 43[1] -> 42[0] via P2P/IPC
f17n01:969558:969795 [1] NCCL INFO Channel 03/0 : 43[1] -> 42[0] via P2P/IPC
f17n01:969558:969795 [1] NCCL INFO Connected all rings
f17n01:969558:969795 [1] NCCL INFO Channel 01/0 : 43[1] -> 47[5] via P2P/IPC
f17n01:969558:969795 [1] NCCL INFO Channel 03/0 : 43[1] -> 47[5] via P2P/IPC
f17n01:969558:969795 [1] NCCL INFO Channel 02/0 : 43[1] -> 66[0] [send] via NET/IB/0
f17n01:969558:969795 [1] NCCL INFO Channel 02/0 : 66[0] -> 43[1] [receive] via NET/IB/0
f17n01:969558:969795 [1] NCCL INFO Channel 00/0 : 43[1] -> 42[0] via P2P/IPC
f17n01:969558:969795 [1] NCCL INFO Channel 02/0 : 43[1] -> 42[0] via P2P/IPC
f17n01:969558:969795 [1] NCCL INFO Connected all trees
f17n01:969558:969795 [1] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
f17n01:969558:969795 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
f17n01:969558:969795 [1] NCCL INFO comm 0x17a243f60 rank 43 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x7b12926388d12ce3 - Init COMPLETE
f17n01:969558:969880 [1] NCCL INFO Using network IB
f17n01:969558:969880 [1] NCCL INFO comm 0x17dfcc480 rank 5 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x1defb2708b178c98 - Init START
f17n01:969558:969880 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
f17n01:969558:969880 [1] NCCL INFO Trees [0] -1/-1/-1->5->6 [1] 6/4/-1->5->7
f17n01:969558:969880 [1] NCCL INFO P2P Chunksize set to 131072
f17n01:969558:969880 [1] NCCL INFO Channel 00/0 : 4[5] -> 5[1] [receive] via NET/IB/2
f17n01:969558:969880 [1] NCCL INFO Channel 01/0 : 4[5] -> 5[1] [receive] via NET/IB/2
f17n01:969558:969880 [1] NCCL INFO Channel 00/0 : 5[1] -> 6[3] [send] via NET/IB/2
f17n01:969558:969880 [1] NCCL INFO Channel 01/0 : 5[1] -> 6[3] [send] via NET/IB/2
f17n01:969558:969880 [1] NCCL INFO Connected all rings
f17n01:969558:969880 [1] NCCL INFO Channel 01/0 : 5[1] -> 7[5] [send] via NET/IB/2
f17n01:969558:969880 [1] NCCL INFO Channel 01/0 : 7[5] -> 5[1] [receive] via NET/IB/2
f17n01:969558:969880 [1] NCCL INFO Channel 00/0 : 6[3] -> 5[1] [receive] via NET/IB/2
f17n01:969558:969880 [1] NCCL INFO Channel 01/0 : 6[3] -> 5[1] [receive] via NET/IB/2
f17n01:969558:969880 [1] NCCL INFO Channel 01/0 : 5[1] -> 4[5] [send] via NET/IB/2
f17n01:969558:969880 [1] NCCL INFO Connected all trees
f17n01:969558:969880 [1] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
f17n01:969558:969880 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f17n01:969558:969880 [1] NCCL INFO comm 0x17dfcc480 rank 5 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x1defb2708b178c98 - Init COMPLETE
f17n01:969558:969900 [1] NCCL INFO Using network IB
f17n01:969558:969900 [1] NCCL INFO comm 0x17d124bc0 rank 10 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0x821cecfd780eaba1 - Init START
f17n01:969558:969900 [1] NCCL INFO Setting affinity for GPU 1 to f0000000f16n18:1091130:1091130 [0] NCCL INFO cudaDriverVersion 12020
f16n18:1091130:1091130 [0] NCCL INFO Bootstrap : Using ib0:10.41.14.86<0>
f16n18:1091130:1091130 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
f16n18:1091130:1091130 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
f16n18:1091130:1091362 [0] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.14.86<0>
f16n18:1091130:1091362 [0] NCCL INFO Using network IB
f16n18:1091130:1091362 [0] NCCL INFO comm 0x13f023e40 rank 36 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x7b12926388d12ce3 - Init START
f16n18:1091130:1091362 [0] NCCL INFO Setting affinity for GPU 0 to 0f
f16n18:1091130:1091362 [0] NCCL INFO Trees [0] 37/42/-1->36->24 [1] 38/-1/-1->36->37 [2] 37/-1/-1->36->31 [3] 38/-1/-1->36->37
f16n18:1091130:1091362 [0] NCCL INFO P2P Chunksize set to 131072
f16n18:1091130:1091362 [0] NCCL INFO Channel 00/0 : 35[5] -> 36[0] [receive] via NET/IB/0
f16n18:1091130:1091362 [0] NCCL INFO Channel 02/0 : 35[5] -> 36[0] [receive] via NET/IB/0
f16n18:1091130:1091362 [0] NCCL INFO Channel 00/0 : 36[0] -> 37[1] via P2P/IPC
f16n18:1091130:1091362 [0] NCCL INFO Channel 02/0 : 36[0] -> 37[1] via P2P/IPC
f16n18:1091130:1091362 [0] NCCL INFO Channel 01/0 : 36[0] -> 45[3] [send] via NET/IB/2
f16n18:1091130:1091362 [0] NCCL INFO Channel 03/0 : 36[0] -> 45[3] [send] via NET/IB/2
f16n18:1091130:1091362 [0] NCCL INFO Connected all rings
f16n18:1091130:1091362 [0] NCCL INFO Channel 01/0 : 36[0] -> 37[1] via P2P/IPC
f16n18:1091130:1091362 [0] NCCL INFO Channel 03/0 : 36[0] -> 37[1] via P2P/IPC
f16n18:1091130:1091362 [0] NCCL INFO Channel 01/0 : 36[0] -> 38[2] via P2P/IPC
f16n18:1091130:1091362 [0] NCCL INFO Channel 03/0 : 36[0] -> 38[2] via P2P/IPC
f16n18:1091130:1091362 [0] NCCL INFO Channel 02/0 : 31[1] -> 36[0] [receive] via NET/IB/0
f16n18:1091130:1091362 [0] NCCL INFO Channel 00/0 : 36[0] -> 42[0] [send] via NET/IB/0
f16n18:1091130:1091362 [0] NCCL INFO Channel 00/0 : 24[0] -> 36[0] [receive] via NET/IB/0
f16n18:1091130:1091362 [0] NCCL INFO Channel 00/0 : 36[0] -> 24[0] [send] via NET/IB/0
f16n18:1091130:1091362 [0] NCCL INFO Channel 00/0 : 42[0] -> 36[0] [receive] via NET/IB/0
f16n18:1091130:1091362 [0] NCCL INFO Channel 02/0 : 36[0] -> 31[1] [send] via NET/IB/0
f16n18:1091130:1091362 [0] NCCL INFO Connected all trees
f16n18:1091130:1091362 [0] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
f16n18:1091130:1091362 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
f16n18:1091130:1091362 [0] NCCL INFO comm 0x13f023e40 rank 36 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x7b12926388d12ce3 - Init COMPLETE
f16n18:1091130:1091462 [0] NCCL INFO Using network IB
f16n18:1091130:1091462 [0] NCCL INFO comm 0x141cc0f20 rank 4 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x7dd05d77214d2e8 - Init START
f16n18:1091130:1091462 [0] NCCL INFO Setting affinity for GPU 0 to 0f
f16n18:1091130:1091462 [0] NCCL INFO Trees [0] 2/6/-1->4->8 [1] -1/-1/-1->4->5
f16n18:1091130:1091462 [0] NCCL INFO P2P Chunksize set to 131072
f16n18:1091130:1091462 [0] NCCL INFO Channel 00/0 : 3[4] -> 4[0] [receive] via NET/IB/0
f16n18:1091130:1091462 [0] NCCL INFO Channel 01/0 : 3[4] -> 4[0] [receive] via NET/IB/0
f16n18:1091130:1091462 [0] NCCL INFO Channel 00/0 : 4[0] -> 5[2] [send] via NET/IB/0
f16n18:1091130:1091462 [0] NCCL INFO Channel 01/0 : 4[0] -> 5[2] [send] via NET/IB/0
f16n18:1091130:1091462 [0] NCCL INFO Connected all rings
f16n18:1091130:1091462 [0] NCCL INFO Channel 00/0 : 2[2] -> 4[0] [receive] via NET/IB/0
f16n18:1091130:1091462 [0] NCCL INFO Channel 00/0 : 4[0] -> 6[4] [send] via NET/IB/0
f16n18:1091130:1091462 [0] NCCL INFO Channel 00/0 : 4[0] -> 8[2] [send] via NET/IB/0
f16n18:1091130:1091462 [0] NCCL INFO Channel 00/0 : 8[2] -> 4[0] [receive] via NET/IB/0
f16n18:1091130:1091462 [0] NCCL INFO Channel 00/0 : 6[4] -> 4[0] [receive] via NET/IB/0
f16n18:1091130:1091462 [0] NCCL INFO Channel 00/0 : 4[0] -> 2[2] [send] via NET/IB/0
f16n18:1091130:1091462 [0] NCCL INFO Channel 01/0 : 5[2] -> 4[0] [receive] via NET/IB/0
f16n18:1091130:1091462 [0] NCCL INFO Connected all trees
f16n18:1091130:1091462 [0] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
f16n18:1091130:1091462 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f16n18:1091130:1091462 [0] NCCL INFO comm 0x141cc0f20 rank 4 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x7dd05d77214d2e8 - Init COMPLETE
f16n18:1091130:1091482 [0] NCCL INFO Using network IB
f16n18:1091130:1091482 [0] NCCL INFO comm 0x141ced8b0 rank 9 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0xaedb5f80914767fd - Init START
f16n18:1091130:1091482 [0] NCCL INFO Setting affinity for GPU 0 to 0f
f16n18:1091130:1091482 [0] NCCL INFO Trees [0] 10/11/-1->9->6 [1] 10/-1/-1->9->8
f16n18:1091130:1091482 [0] NCCL INFO P2P Chunksize set to 131072
f16n18:1091130:1091482 [0] NCCL INFO Channel 00/0 : 8[2] -> 9[0] [receive] via NET/IB/0
f16n18:1091130:1091482 [0] NCCL INFO Channel 01/0 : 8[2] -> 9[0] [receive] via NET/IB/0
f16n18:1091130:1091482 [0] NCCL INFO Channel 00/0 : 9[0] -> 10[4] via P2P/IPC
f16n18:1091130:1091482 [0] NCCL INFO Channel 01/0 : 9[0] -> 10[4] via P2P/IPC
f16n18:1091130:1091482 [0] NCCL INFO Connected all rings
f16n18:1091130:1091482 [0] NCCL INFO Channel 00/0 : 9[0] -> 11[2] [send] via NET/IB/0
f16n18:1091130:1091482 [0] NCCL INFO Channel 00/0 : 6[0] -> 9[0] [receive] via NET/IB/0
f16n18:1091130:1091482 [0] NCCL INFO Channel 00/0 : 9[0] -> 6[0] [send] via NET/IB/0
f16n18:1091130:1091482 [0] NCCL INFO Channel 00/0 : 11[2] -> 9[0] [receive] via NET/IB/0
f16n18:1091130:1091482 [0] NCCL INFO Channel 01/0 : 9[0] -> 8[2] [send] via NET/IB/0
f16n18:1091130:1091482 [0] NCCL INFO Connected all trees
f16n18:1091130:1091482 [0] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
f16n18:1091130:1091482 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f16n18:1091130:1091482 [0] NCCL INFO comm 0x141ced8b0 rank 9 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0xaedb5f80914767fd - Init COMPLETE
g04n18:916021:916021 [0] NCCL INFO cudaDriverVersion 12020
g04n18:916021:916021 [0] NCCL INFO Bootstrap : Using ib0:10.41.16.12<0>
g04n18:916021:916021 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g04n18:916021:916021 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g04n18:916021:916259 [0] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.16.12<0>
g04n18:916021:916259 [0] NCCL INFO Using network IB
g04n18:916021:916259 [0] NCCL INFO comm 0x143573c30 rank 72 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x7b12926388d12ce3 - Init START
g04n18:916021:916259 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g04n18:916021:916259 [0] NCCL INFO Trees [0] 73/84/-1->72->48 [1] 74/-1/-1->72->73 [2] 73/-1/-1->72->78 [3] 74/-1/-1->72->73
g04n18:916021:916259 [0] NCCL INFO P2P Chunksize set to 131072
g04n18:916021:916259 [0] NCCL INFO Channel 00/0 : 71[5] -> 72[0] [receive] via NET/IB/0
g04n18:916021:916259 [0] NCCL INFO Channel 02/0 : 71[5] -> 72[0] [receive] via NET/IB/0
g04n18:916021:916259 [0] NCCL INFO Channel 00/0 : 72[0] -> 73[1] via P2P/IPC
g04n18:916021:916259 [0] NCCL INFO Channel 02/0 : 72[0] -> 73[1] via P2P/IPC
g04n18:916021:916259 [0] NCCL INFO Channel 01/0 : 72[0] -> 81[3] [send] via NET/IB/2
g04n18:916021:916259 [0] NCCL INFO Channel 03/0 : 72[0] -> 81[3] [send] via NET/IB/2
g04n18:916021:916259 [0] NCCL INFO Connected all rings
g04n18:916021:916259 [0] NCCL INFO Channel 01/0 : 72[0] -> 73[1] via P2P/IPC
g04n18:916021:916259 [0] NCCL INFO Channel 03/0 : 72[0] -> 73[1] via P2P/IPC
g04n18:916021:916259 [0] NCCL INFO Channel 01/0 : 72[0] -> 74[2] via P2P/IPC
g04n18:916021:916259 [0] NCCL INFO Channel 03/0 : 72[0] -> 74[2] via P2P/IPC
g04n18:916021:916259 [0] NCCL INFO Channel 02/0 : 72[0] -> 78[0] [send] via NET/IB/0
g04n18:916021:916259 [0] NCCL INFO Channel 00/0 : 72[0] -> 84[0] [send] via NET/IB/0
g04n18:916021:916259 [0] NCCL INFO Channel 00/0 : 48[0] -> 72[0] [receive] via NET/IB/0
g04n18:916021:916259 [0] NCCL INFO Channel 00/0 : 72[0] -> 48[0] [send] via NET/IB/0
g04n18:916021:916259 [0] NCCL INFO Channel 00/0 : 84[0] -> 72[0] [receive] via NET/IB/0
g04n18:916021:916259 [0] NCCL INFO Channel 02/0 : 78[0] -> 72[0] [receive] via NET/IB/0
g04n18:916021:916259 [0] NCCL INFO Connected all trees
g04n18:916021:916259 [0] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g04n18:916021:916259 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g04n18:916021:916259 [0] NCCL INFO comm 0x143573c30 rank 72 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x7b12926388d12ce3 - Init COMPLETE
g04n18:916021:916345 [0] NCCL INFO Using network IB
g04n18:916021:916345 [0] NCCL INFO comm 0x145ed2de0 rank 9 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x8264b1a1a7aef6de - Init START
g04n18:916021:916345 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g04n18:916021:916345 [0] NCCL INFO Trees [0] -1/-1/-1->9->10 [1] 10/8/-1->9->7
g04n18:916021:916345 [0] NCCL INFO P2P Chunksize set to 131072
g04n18:916021:916345 [0] NCCL INFO Channel 00/0 : 8[4] -> 9[0] [receive] via NET/IB/0
g04n18:916021:916345 [0] NCCL INFO Channel 01/0 : 8[4] -> 9[0] [receive] via NET/IB/0
g04n18:916021:916345 [0] NCCL INFO Channel 00/0 : 9[0] -> 10[2] [send] via NET/IB/0
g04n18:916021:916345 [0] NCCL INFO Channel 01/0 : 9[0] -> 10[2] [send] via NET/IB/0
g04n18:916021:916345 [0] NCCL INFO Connected all rings
g04n18:916021:916345 [0] NCCL INFO Channel 01/0 : 7[2] -> 9[0] [receive] via NET/IB/0
g04n18:916021:916345 [0] NCCL INFO Channel 01/0 : 9[0] -> 7[2] [send] via NET/IB/0
g04n18:916021:916345 [0] NCCL INFO Channel 00/0 : 10[2] -> 9[0] [receive] via NET/IB/0
g04n18:916021:916345 [0] NCCL INFO Channel 01/0 : 10[2] -> 9[0] [receive] via NET/IB/0
g04n18:916021:916345 [0] NCCL INFO Channel 01/0 : 9[0] -> 8[4] [send] via NET/IB/0
g04n18:916021:916345 [0] NCCL INFO Connected all trees
g04n18:916021:916345 [0] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g04n18:916021:916345 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g04n18:916021:916345 [0] NCCL INFO comm 0x145ed2de0 rank 9 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x8264b1a1a7aef6de - Init COMPLETE
g04n18:916021:916360 [0] NCCL INFO Using network IB
g04n18:916021:916360 [0] NCCL INFO comm 0x1461ed630 rank 18 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0xaedb5f80914767fd - Init START
g04n18:916021:916360 [0] NCCL INFO Setting affinity for GPU 0 to 0f
g04n18:916021:916360 [0] NCCL INFO Trees [0] 19/21/-1->18->12 [1] 19/-1/-1->18->20
g04n18:916021:916360 [0] NCCL INFO P2P Chunksize set to 131072
g04n18:916021:916360 [0] NCCL INFO Channel 00/0 : 17[2] -> 18[0] [receive] via NET/IB/0
g04n18:916021:916360 [0] NCCL INFO Channel 01/0 : 17[2] -> 18[0] [receive] via NET/IB/0
g04n18:916021:916360 [0] NCCL INFO Channel 00/0 : 18[0] -> 19[4] via P2P/IPC
g04n18:916021:916360 [0] NCCL INFO Channel 01/0 : 18[0] -> 19[4] via P2P/IPC
g04n18:916021:916360 [0] NCCL INFO Connected all rings
g04n18:916021:916360 [0] NCCL INFO Channel 01/0 : 18[0] -> 20[2] [send] via NET/IB/0
g04n18:916021:916360 [0] NCCL INFO Channel 00/0 : 18[0] -> 21[0] [send] via NET/IB/0
g04n18:916021:916360 [0] NCCL INFO Channel 00/0 : 12[0] -> 18[0] [receive] via NET/IB/0
g04n18:916021:916360 [0] NCCL INFO Channel 00/0 : 18[0] -> 12[0] [send] via NET/IB/0
g04n18:916021:916360 [0] NCCL INFO Channel 00/0 : 21[0] -> 18[0] [receive] via NET/IB/0
g04n18:916021:916360 [0] NCCL INFO Channel 01/0 : 20[2] -> 18[0] [receive] via NET/IB/0
g04n18:916021:916360 [0] NCCL INFO Connected all trees
g04n18:916021:916360 [0] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g04n18:916021:916360 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g04n18:916021:916360 [0] NCCL INFO comm 0x1461ed630 rank 18 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0xaedb5f80914767fd - Init COMPLETE
g05n08:178242:178242 [3] NCCL INFO cudaDriverVersion 12020
g05n08:178242:178242 [3] NCCL INFO Bootstrap : Using ib0:10.41.16.20<0>
g05n08:178242:178242 [3] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g05n08:178242:178242 [3] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g05n08:178242:178474 [3] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.16.20<0>
g05n08:178242:178474 [3] NCCL INFO Using network IB
g05n08:178242:178474 [3] NCCL INFO comm 0x15f4c3eb0 rank 81 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x7b12926388d12ce3 - Init START
g05n08:178242:178474 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g05n08:178242:178474 [3] NCCL INFO Trees [0] 82/-1/-1->81->80 [1] 83/-1/-1->81->82 [2] 82/-1/-1->81->80 [3] 83/88/-1->81->82
g05n08:178242:178474 [3] NCCL INFO P2P Chunksize set to 131072
g05n08:178242:178474 [3] NCCL INFO Channel 00/0 : 81[3] -> 82[4] via P2P/IPC
g05n08:178242:178474 [3] NCCL INFO Channel 01/0 : 81[3] -> 82[4] via P2P/IPC
g05n08:178242:178474 [3] NCCL INFO Channel 02/0 : 81[3] -> 82[4] via P2P/IPC
g05n08:178242:178474 [3] NCCL INFO Channel 03/0 : 81[3] -> 82[4] via P2P/IPC
g05n08:178242:178474 [3] NCCL INFO Channel 01/0 : 72[0] -> 81[3] [receive] via NET/IB/3
g05n08:178242:178474 [3] NCCL INFO Channel 03/0 : 72[0] -> 81[3] [receive] via NET/IB/3
g05n08:178242:178474 [3] NCCL INFO Connected all rings
g05n08:178242:178474 [3] NCCL INFO Channel 01/0 : 81[3] -> 83[5] via P2P/IPC
g05n08:178242:178474 [3] NCCL INFO Channel 03/0 : 81[3] -> 83[5] via P2P/IPC
g05n08:178242:178474 [3] NCCL INFO Channel 03/0 : 81[3] -> 88[4] [send] via NET/IB/3
g05n08:178242:178474 [3] NCCL INFO Channel 03/0 : 88[4] -> 81[3] [receive] via NET/IB/3
g05n08:178242:178474 [3] NCCL INFO Channel 00/0 : 81[3] -> 80[2] via P2P/IPC
g05n08:178242:178474 [3] NCCL INFO Channel 02/0 : 81[3] -> 80[2] via P2P/IPC
g05n08:178242:178474 [3] NCCL INFO Connected all trees
g05n08:178242:178474 [3] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g05n08:178242:178474 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g05n08:178242:178474 [3] NCCL INFO comm 0x15f4c3eb0 rank 81 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x7b12926388d12ce3 - Init COMPLETE
g05n08:178242:178559 [3] NCCL INFO Using network IB
g05n08:178242:178559 [3] NCCL INFO comm 0x162250030 rank 10 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x86d434c9d289d59a - Init START
g05n08:178242:178559 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g05n08:178242:178559 [3] NCCL INFO Trees [0] 9/11/-1->10->8 [1] -1/-1/-1->10->9
g05n08:178242:178559 [3] NCCL INFO P2P Chunksize set to 131072
g05n08:178242:178559 [3] NCCL INFO Channel 00/0 : 9[1] -> 10[3] [receive] via NET/IB/3
g05n08:178242:178559 [3] NCCL INFO Channel 01/0 : 9[1] -> 10[3] [receive] via NET/IB/3
g05n08:178242:178559 [3] NCCL INFO Channel 00/0 : 10[3] -> 11[5] [send] via NET/IB/3
g05n08:178242:178559 [3] NCCL INFO Channel 01/0 : 10[3] -> 11[5] [send] via NET/IB/3
g05n08:178242:178559 [3] NCCL INFO Connected all rings
g05n08:178242:178559 [3] NCCL INFO Channel 00/0 : 8[5] -> 10[3] [receive] via NET/IB/3
g05n08:178242:178559 [3] NCCL INFO Channel 00/0 : 10[3] -> 8[5] [send] via NET/IB/3
g05n08:178242:178559 [3] NCCL INFO Channel 00/0 : 11[5] -> 10[3] [receive] via NET/IB/3
g05n08:178242:178559 [3] NCCL INFO Channel 00/0 : 10[3] -> 9[1] [send] via NET/IB/3
g05n08:178242:178559 [3] NCCL INFO Channel 01/0 : 10[3] -> 9[1] [send] via NET/IB/3
g05n08:178242:178559 [3] NCCL INFO Connected all trees
g05n08:178242:178559 [3] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g05n08:178242:178559 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g05n08:178242:178559 [3] NCCL INFO comm 0x162250030 rank 10 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x86d434c9d289d59a - Init COMPLETE
g05n08:178242:178575 [3] NCCL INFO Usd09n08:1072933:1073245 [1] NCCL INFO Using network IB
d09n08:1072933:1073245 [1] NCCL INFO comm 0x15d82db20 rank 0 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x54676b30da0675d6 - Init START
d09n08:1072933:1073245 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
d09n08:1072933:1073245 [1] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7   8   9  10  11
d09n08:1072933:1073245 [1] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7   8   9  10  11
d09n08:1072933:1073245 [1] NCCL INFO Trees [0] 8/-1/-1->0->-1 [1] -1/-1/-1->0->1
d09n08:1072933:1073245 [1] NCCL INFO P2P Chunksize set to 131072
d09n08:1072933:1073245 [1] NCCL INFO Channel 00/0 : 11[5] -> 0[1] [receive] via NET/IB/2
d09n08:1072933:1073245 [1] NCCL INFO Channel 01/0 : 11[5] -> 0[1] [receive] via NET/IB/2
d09n08:1072933:1073245 [1] NCCL INFO Channel 00/0 : 0[1] -> 1[3] [send] via NET/IB/2
d09n08:1072933:1073245 [1] NCCL INFO Channel 01/0 : 0[1] -> 1[3] [send] via NET/IB/2
d09n08:1072933:1073245 [1] NCCL INFO Connected all rings
d09n08:1072933:1073245 [1] NCCL INFO Channel 00/0 : 8[5] -> 0[1] [receive] via NET/IB/2
d09n08:1072933:1073245 [1] NCCL INFO Channel 00/0 : 0[1] -> 8[5] [send] via NET/IB/2
d09n08:1072933:1073245 [1] NCCL INFO Channel 01/0 : 1[3] -> 0[1] [receive] via NET/IB/2
d09n08:1072933:1073245 [1] NCCL INFO Connected all trees
d09n08:1072933:1073245 [1] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
d09n08:1072933:1073245 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d09n08:1072933:1073245 [1] NCCL INFO comm 0x15d82db20 rank 0 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x54676b30da0675d6 - Init COMPLETE
d09n08:1072933:1073265 [1] NCCL INFO Using network IB
d09n08:1072933:1073265 [1] NCCL INFO comm 0x15c70e000 rank 1 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0x821cecfd780eaba1 - Init START
d09n08:1072933:1073265 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
d09n08:1072933:1073265 [1] NCCL INFO Trees [0] 2/-1/-1->1->3 [1] 2/0/-1->1->4
d09n08:1072933:1073265 [1] NCCL INFO P2P Chunksize set to 131072
d09n08:1072933:1073265 [1] NCCL INFO Channel 00/0 : 0[3] -> 1[1] [receive] via NET/IB/2
d09n08:1072933:1073265 [1] NCCL INFO Channel 01/0 : 0[3] -> 1[1] [receive] via NET/IB/2
d09n08:1072933:1073265 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[5] via P2P/IPC
d09n08:1072933:1073265 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[5] via P2P/IPC
d09n08:1072933:1073265 [1] NCCL INFO Connected all rings
d09n08:1072933:1073265 [1] NCCL INFO Channel 00/0 : 1[1] -> 3[3] [send] via NET/IB/2
d09n08:1072933:1073265 [1] NCCL INFO Channel 01/0 : 1[1] -> 4[1] [send] via NET/IB/2
d09n08:1072933:1073265 [1] NCCL INFO Channel 01/0 : 4[1] -> 1[1] [receive] via NET/IB/2
d09n08:1072933:1073265 [1] NCCL INFO Channel 00/0 : 3[3] -> 1[1] [receive] via NET/IB/2
d09n08:1072933:1073265 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[3] [send] via NET/IB/2
d09n08:1072933:1073265 [1] NCCL INFO Connected all trees
d09n08:1072933:1073265 [1] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
d09n08:1072933:1073265 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d09n08:1072933:1073265 [1] NCCL INFO comm 0x15c70e000 rank 1 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0x821cecfd780eaba1 - Init COMPLETE
ing network IB
g05n08:178242:178575 [3] NCCL INFO comm 0x16221caa0 rank 20 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xa49a27aa8b15034e - Init START
g05n08:178242:178575 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g05n08:178242:178575 [3] NCCL INFO Trees [0] -1/-1/-1->20->22 [1] 21/18/-1->20->17
g05n08:178242:178575 [3] NCCL INFO P2P Chunksize set to 131072
g05n08:178242:178575 [3] NCCL INFO Channel 00/0 : 19[5] -> 20[3] [receive] via NET/IB/3
g05n08:178242:178575 [3] NCCL INFO Channel 01/0 : 19[5] -> 20[3] [receive] via NET/IB/3
g05n08:178242:178575 [3] NCCL INFO Channel 00/0 : 20[3] -> 21[1] [send] via NET/IB/3
g05n08:178242:178575 [3] NCCL INFO Channel 01/0 : 20[3] -> 21[1] [send] via NET/IB/3
g05n08:178242:178575 [3] NCCL INFO Connected all rings
g05n08:178242:178575 [3] NCCL INFO Channel 01/0 : 18[1] -> 20[3] [receive] via NET/IB/3
g05n08:178242:178575 [3] NCCL INFO Channel 00/0 : 20[3] -> 22[5] [send] via NET/IB/3
g05n08:178242:178575 [3] NCCL INFO Channel 01/0 : 17[3] -> 20[3] [receive] via NET/IB/3
g05n08:178242:178575 [3] NCCL INFO Channel 01/0 : 20[3] -> 17[3] [send] via NET/IB/3
g05n08:178242:178575 [3] NCCL INFO Channel 00/0 : 22[5] -> 20[3] [receive] via NET/IB/3
g05n08:178242:178575 [3] NCCL INFO Channel 01/0 : 20[3] -> 18[1] [send] via NET/IB/3
g05n08:178242:178575 [3] NCCL INFO Channel 01/0 : 21[1] -> 20[3] [receive] via NET/IB/3
g05n08:178242:178575 [3] NCCL INFO Connected all trees
g05n08:178242:178575 [3] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g05n08:178242:178575 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g05n08:178242:178575 [3] NCCL INFO comm 0x16221caa0 rank 20 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xa49a27aa8b15034e - Init COMPLETE
f16n17:1090234:1090234 [0] NCCL INFO cudaDriverVersion 12020
f16n17:1090234:1090234 [0] NCCL INFO Bootstrap : Using ib0:10.41.14.85<0>
f16n17:1090234:1090234 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
f16n17:1090234:1090234 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
f16n17:1090234:1090469 [0] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.14.85<0>
f16n17:1090234:1090469 [0] NCCL INFO Using network IB
f16n17:1090234:1090469 [0] NCCL INFO comm 0x151fc3c80 rank 30 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x7b12926388d12ce3 - Init START
f16n17:1090234:1090469 [0] NCCL INFO Setting affinity for GPU 0 to 0f
f16n17:1090234:1090469 [0] NCCL INFO Trees [0] 31/-1/-1->30->37 [1] 32/-1/-1->30->31 [2] 31/24/-1->30->19 [3] 32/-1/-1->30->31
f16n17:1090234:1090469 [0] NCCL INFO P2P Chunksize set to 131072
f16n17:1090234:1090469 [0] NCCL INFO Channel 00/0 : 29[5] -> 30[0] [receive] via NET/IB/0
f16n17:1090234:1090469 [0] NCCL INFO Channel 02/0 : 29[5] -> 30[0] [receive] via NET/IB/0
f16n17:1090234:1090469 [0] NCCL INFO Channel 00/0 : 30[0] -> 31[1] via P2P/IPC
f16n17:1090234:1090469 [0] NCCL INFO Channel 02/0 : 30[0] -> 31[1] via P2P/IPC
f16n17:1090234:1090469 [0] NCCL INFO Channel 01/0 : 30[0] -> 39[3] [send] via NET/IB/2
f16n17:1090234:1090469 [0] NCCL INFO Channel 03/0 : 30[0] -> 39[3] [send] via NET/IB/2
f16n17:1090234:1090469 [0] NCCL INFO Connected all rings
f16n17:1090234:1090469 [0] NCCL INFO Channel 01/0 : 30[0] -> 31[1] via P2P/IPC
f16n17:1090234:1090469 [0] NCCL INFO Channel 03/0 : 30[0] -> 31[1] via P2P/IPC
f16n17:1090234:1090469 [0] NCCL INFO Channel 01/0 : 30[0] -> 32[2] via P2P/IPC
f16n17:1090234:1090469 [0] NCCL INFO Channel 03/0 : 30[0] -> 32[2] via P2P/IPC
f16n17:1090234:1090469 [0] NCCL INFO Channel 02/0 : 24[0] -> 30[0] [receive] via NET/IB/0
f16n17:1090234:1090469 [0] NCCL INFO Channel 00/0 : 30[0] -> 37[1] [send] via NET/IB/0
f16n17:1090234:1090469 [0] NCCL INFO Channel 02/0 : 19[1] -> 30[0] [receive] via NET/IB/0
f16n17:1090234:1090469 [0] NCCL INFO Channel 02/0 : 30[0] -> 19[1] [send] via NET/IB/0
f16n17:1090234:1090469 [0] NCCL INFO Channel 00/0 : 37[1] -> 30[0] [receive] via NET/IB/0
f16n17:1090234:1090469 [0] NCCL INFO Channel 02/0 : 30[0] -> 24[0] [send] via NET/IB/0
f16n17:1090234:1090469 [0] NCCL INFO Connected all trees
f16n17:1090234:1090469 [0] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
f16n17:1090234:1090469 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
f16n17:1090234:1090469 [0] NCCL INFO comm 0x151fc3c80 rank 30 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x7b12926388d12ce3 - Init COMPLETE
f16n17:1090234:1090559 [0] NCCL INFO Using network IB
f16n17:1090234:1090559 [0] NCCL INFO comm 0x154c5d370 rank 3 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x414883fe5ee4d1ab - Init START
f16n17:1090234:1090559 [0] NCCL INFO Setting affinity for GPU 0 to 0f
f16n17:1090234:1090559 [0] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] 7/1/-1->3->11
f16n17:1090234:1090559 [0] NCCL INFO P2P Chunksize set to 131072
f16n17:1090234:1090559 [0] NCCL INFO Channel 00/0 : 2[4] -> 3[0] [receive] via NET/IB/0
f16n17:1090234:1090559 [0] NCCL INFO Channel 01/0 : 2[4] -> 3[0] [receive] via NET/IB/0
f16n17:1090234:1090559 [0] NCCL INFO Channel 00/0 : 3[0] -> 4[2] [send] via NET/IB/0
f16n17:1090234:1090559 [0] NCCL INFO Channel 01/0 : 3[0] -> 4[2] [send] via NET/IB/0
f16n17:1090234:1090559 [0] NCCL INFO Connected all rings
f16n17:1090234:1090559 [0] NCCL INFO Channel 01/0 : 1[2] -> 3[0] [receive] via NET/IB/0
f16n17:1090234:1090559 [0] NCCL INFO Channel 01/0 : 11[4] -> 3[0] [receive] via NET/IB/0
f16n17:1090234:1090559 [0] NCCL INFO Channel 01/0 : 3[0] -> 7[2] [send] via NET/IB/0
f16n17:1090234:1090559 [0] NCCL INFO Channel 01/0 : 7[2] -> 3[0] [receive] via NET/IB/0
f16n17:1090234:1090559 [0] NCCL INFO Channel 01/0 : 3[0] -> 11[4] [send] via NET/IB/0
f16n17:1090234:1090559 [0] NCCL INFO Channel 01/0 : 3[0] -> 1[2] [send] via NET/IB/0
f16n17:1090234:1090559 [0] NCCL INFO Channel 00/0 : 3[0] -> 2[4] [send] via NET/IB/0
f16n17:1090234:1090559 [0] NCCL INFO Connected all trees
f16n17:1090234:1090559 [0] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
f16n17:1090234:1090559 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f16n17:1090234:1090559 [0] NCCL INFO comm 0x154c5d370 rank 3 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x414883fe5ee4d1ab - Init COMPLETE
f16n17:1090234:1090576 [0] NCCL INFO Using network IB
f16n17:1090234:1090576 [0] NCCL INFO comm 0x1548a1a10 rank 7 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0x32e49bb35e6c8ace - Init START
f16n17:1090234:1090576 [0] NCCL INFO Setting affinity for GPU 0 to 0f
f16n17:1090234:1090576 [0] NCCL INFO Trees [0] 8/-1/-1->7->9 [1] 8/6/-1->7->5
f16n17:1090234:1090576 [0] NCCL INFO P2P Chunksize set to 131072
f16n17:1090234:1090576 [0] NCCL INFO Channel 00/0 : 6[2] -> 7[0] [receive] via NET/IB/0
f16n17:1090234:1090576 [0] NCCL INFO Channel 01/0 : 6[2] -> 7[0] [receive] via NET/IB/0
f16n17:1090234:1090576 [0] NCCL INFO Channel 00/0 : 7[0] -> 8[4] via P2P/IPC
f16n17:1090234:1090576 [0] NCCL INFO Channel 01/0 : 7[0] -> 8[4] via P2P/IPC
f16n17:1090234:1090576 [0] NCCL INFO Connected all rings
f16n17:1090234:1090576 [0] NCCL INFO Channel 01/0 : 5[4] -> 7[0] [receive] via NET/IB/0
f16n17:1090234:1090576 [0] NCCL INFO Channel 00/0 : 7[0] -> 9[2] [send] via NET/IB/0
f16n17:1090234:1090576 [0] NCCL INFO Channel 00/0 : 9[2] -> 7[0] [receive] via NET/IB/0
f16n17:1090234:1090576 [0] NCCL INFO Channel 01/0 : 7[0] -> 5[4] [send] via NET/IB/0
f16n17:1090234:1090576 [0] NCCL INFO Channel 01/0 : 7[0] -> 6[2] [send] via NET/IB/0
f16n17:1090234:1090576 [0] NCCL INFO Connected all trees
f16n17:1090234:1090576 [0] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
f16n17:1090234:1090576 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f16n17:1090234:1090576 [0] NCCL INFO comm 0x1548a1a10 rank 7 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0x32e49bb35e6c8ace - Init COMPLETE
g05n08:178244:178244 [5] NCCL INFO cudaDriverVersion 12020
g05n08:178244:178244 [5] NCCL INFO Bootstrap : Using ib0:10.41.16.20<0>
g05n08:178244:178244 [5] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g05n08:178244:178244 [5] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g05n08:178244:178473 [5] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.16.20<0>
g05n08:178244:178473 [5] NCCL INFO Using network IB
g05n08:178244:178473 [5] NCCL INFO comm 0x15b123c50 rank 83 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x7b12926388d12ce3 - Init START
g05n08:178244:178473 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g05n08:178244:178473 [5] NCCL INFO Trees [0] -1/-1/-1->83->82 [1] 79/-1/-1->83->81 [2] -1/-1/-1->83->82 [3] 79/-1/-1->83->81
g05n08:178244:178473 [5] NCCL INFO P2P Chunksize set to 131072
g05n08:178244:178473 [5] NCCL INFO Channel 00/0 : 83[5] -> 84[0] [send] via NET/IB/1
g05n08:178244:178473 [5] NCCL INFO Channel 02/0 : 83[5] -> 84[0] [send] via NET/IB/1
g05n08:178244:178473 [5] NCCL INFO Channel 01/0 : 83[5] -> 80[2] via P2P/IPC
g05n08:178244:178473 [5] NCCL INFO Channel 03/0 : 83[5] -> 80[2] via P2P/IPC
g05n08:178244:178473 [5] NCCL INFO Connected all rings
g05n08:178244:178473 [5] NCCL INFO Channel 01/0 : 83[5] -> 79[1] via P2P/IPC
g05n08:178244:178473 [5] NCCL INFO Channel 03/0 : 83[5] -> 79[1] via P2P/IPC
g05n08:178244:178473 [5] NCCL INFO Channel 01/0 : 83[5] -> 81[3] via P2P/IPC
g05n08:178244:178473 [5] NCCL INFO Channel 03/0 : 83[5] -> 81[3] via P2P/IPC
g05n08:178244:178473 [5] NCCL INFO Channel 00/0 : 83[5] -> 82[4] via P2P/IPC
g05n08:178244:178473 [5] NCCL INFO Channel 02/0 : 83[5] -> 82[4] via P2P/IPC
g05n08:178244:178473 [5] NCCL INFO Connected all trees
g05n08:178244:178473 [5] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g05n08:178244:178473 [5] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g05n08:178244:178473 [5] NCCL INFO comm 0x15b123c50 rank 83 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x7b12926388d12ce3 - Init COMPLETE
g05n08:178244:178561 [5] NCCL INFO Using network IB
g05n08:178244:178561 [5] NCCL INFO comm 0x15dd995e0 rank 10 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x1defb2708b178c98 - Init START
g05n08:178244:178561 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g05n08:178244:178561 [5] NCCL INFO Trees [0] 9/11/-1->10->8 [1] -1/-1/-1->10->9
g05n08:178244:178561 [5] NCCL INFO P2P Chunksize set to 131072
g05n08:178244:178561 [5] NCCL INFO Channel 00/0 : 9[3] -> 10[5] [receive] via NET/IB/3
g05n08:178244:178561 [5] NCCL INFO Channel 01/0 : 9[3] -> 10[5] [receive] via NET/IB/3
g05n08:178244:178561 [5] NCCL INFO Channel 00/0 : 10[5] -> 11[1] [send] via NET/IB/3
g05n08:178244:178561 [5] NCCL INFO Channel 01/0 : 10[5] -> 11[1] [send] via NET/IB/3
g05n08:178244:178561 [5] NCCL INFO Connected all rings
g05n08:178244:178561 [5] NCCL INFO Channel 00/0 : 8[1] -> 10[5] [receive] via NET/IB/3
g05n08:178244:178561 [5] NCCL INFO Channel 00/0 : 10[5] -> 8[1] [send] via NET/IB/3
g05n08:178244:178561 [5] NCCL INFO Channel 00/0 : 11[1] -> 10[5] [receive] via NET/IB/3
g05n08:178244:178561 [5] NCCL INFO Channel 00/0 : 10[5] -> 9[3] [send] via NET/IB/3
g05n08:178244:178561 [5] NCCL INFO Channel 01/0 : 10[5] -> 9[3] [send] via NET/IB/3
g05n08:178244:178561 [5] NCCL INFO Connected all trees
g05n08:178244:178561 [5] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g05n08:178244:178561 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g05n08:178244:178561 [5] NCCL INFO comm 0x15dd995e0 rank 10 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x1defb2708b178c98 - Init COMPLETE
g05n08:178244:178577 [5] NCCL INFO Using network IB
g05n08:178244:178577 [5] NCCL INFO comm 0x15ee4e6a0 rank 20 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x821cecfd780eaba1 - f16n17:1090242:1090242 [5] NCCL INFO cudaDriverVersion 12020
f16n17:1090242:1090242 [5] NCCL INFO Bootstrap : Using ib0:10.41.14.85<0>
f16n17:1090242:1090242 [5] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
f16n17:1090242:1090242 [5] NCCL INFO NET/Plugin : No plugin found, using internal implementation
f16n17:1090242:1090472 [5] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.14.85<0>
f16n17:1090242:1090472 [5] NCCL INFO Using network IB
f16n17:1090242:1090472 [5] NCCL INFO comm 0x128d341d0 rank 35 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x7b12926388d12ce3 - Init START
f16n17:1090242:1090472 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
f16n17:1090242:1090472 [5] NCCL INFO Trees [0] -1/-1/-1->35->34 [1] 31/-1/-1->35->33 [2] -1/-1/-1->35->34 [3] 31/-1/-1->35->33
f16n17:1090242:1090472 [5] NCCL INFO P2P Chunksize set to 131072
f16n17:1090242:1090472 [5] NCCL INFO Channel 00/0 : 35[5] -> 36[0] [send] via NET/IB/1
f16n17:1090242:1090472 [5] NCCL INFO Channel 02/0 : 35[5] -> 36[0] [send] via NET/IB/1
f16n17:1090242:1090472 [5] NCCL INFO Channel 01/0 : 35[5] -> 32[2] via P2P/IPC
f16n17:1090242:1090472 [5] NCCL INFO Channel 03/0 : 35[5] -> 32[2] via P2P/IPC
f16n17:1090242:1090472 [5] NCCL INFO Connected all rings
f16n17:1090242:1090472 [5] NCCL INFO Channel 01/0 : 35[5] -> 31[1] via P2P/IPC
f16n17:1090242:1090472 [5] NCCL INFO Channel 03/0 : 35[5] -> 31[1] via P2P/IPC
f16n17:1090242:1090472 [5] NCCL INFO Channel 01/0 : 35[5] -> 33[3] via P2P/IPC
f16n17:1090242:1090472 [5] NCCL INFO Channel 03/0 : 35[5] -> 33[3] via P2P/IPC
f16n17:1090242:1090472 [5] NCCL INFO Channel 00/0 : 35[5] -> 34[4] via P2P/IPC
f16n17:1090242:1090472 [5] NCCL INFO Channel 02/0 : 35[5] -> 34[4] via P2P/IPC
f16n17:1090242:1090472 [5] NCCL INFO Connected all trees
f16n17:1090242:1090472 [5] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
f16n17:1090242:1090472 [5] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
f16n17:1090242:1090472 [5] NCCL INFO comm 0x128d341d0 rank 35 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x7b12926388d12ce3 - Init COMPLETE
f16n17:1090242:1090561 [5] NCCL INFO Using network IB
f16n17:1090242:1090561 [5] NCCL INFO comm 0x12c191760 rank 4 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x1defb2708b178c98 - Init START
f16n17:1090242:1090561 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
f16n17:1090242:1090561 [5] NCCL INFO Trees [0] 2/6/-1->4->8 [1] -1/-1/-1->4->5
f16n17:1090242:1090561 [5] NCCL INFO P2P Chunksize set to 131072
f16n17:1090242:1090561 [5] NCCL INFO Channel 00/0 : 3[3] -> 4[5] [receive] via NET/IB/3
f16n17:1090242:1090561 [5] NCCL INFO Channel 01/0 : 3[3] -> 4[5] [receive] via NET/IB/3
f16n17:1090242:1090561 [5] NCCL INFO Channel 00/0 : 4[5] -> 5[1] [send] via NET/IB/3
f16n17:1090242:1090561 [5] NCCL INFO Channel 01/0 : 4[5] -> 5[1] [send] via NET/IB/3
f16n17:1090242:1090561 [5] NCCL INFO Connected all rings
f16n17:1090242:1090561 [5] NCCL INFO Channel 00/0 : 2[1] -> 4[5] [receive] via NET/IB/3
f16n17:1090242:1090561 [5] NCCL INFO Channel 00/0 : 4[5] -> 6[3] [send] via NET/IB/3
f16n17:1090242:1090561 [5] NCCL INFO Channel 00/0 : 4[5] -> 8[1] [send] via NET/IB/3
f16n17:1090242:1090561 [5] NCCL INFO Channel 00/0 : 8[1] -> 4[5] [receive] via NET/IB/3
f16n17:1090242:1090561 [5] NCCL INFO Channel 00/0 : 6[3] -> 4[5] [receive] via NET/IB/3
f16n17:1090242:1090561 [5] NCCL INFO Channel 00/0 : 4[5] -> 2[1] [send] via NET/IB/3
f16n17:1090242:1090561 [5] NCCL INFO Channel 01/0 : 5[1] -> 4[5] [receive] via NET/IB/3
f16n17:1090242:1090561 [5] NCCL INFO Connected all trees
f16n17:1090242:1090561 [5] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
f16n17:1090242:1090561 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f16n17:1090242:1090561 [5] NCCL INFO comm 0x12c191760 rank 4 nranks 12 cudaDev 5 nvmInit START
g05n08:178244:178577 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g05n08:178244:178577 [5] NCCL INFO Trees [0] -1/-1/-1->20->19 [1] 21/-1/-1->20->19
g05n08:178244:178577 [5] NCCL INFO P2P Chunksize set to 131072
g05n08:178244:178577 [5] NCCL INFO Channel 00/0 : 20[5] -> 21[3] [send] via NET/IB/3
g05n08:178244:178577 [5] NCCL INFO Channel 01/0 : 20[5] -> 21[3] [send] via NET/IB/3
g05n08:178244:178577 [5] NCCL INFO Connected all rings
g05n08:178244:178577 [5] NCCL INFO Channel 01/0 : 21[3] -> 20[5] [receive] via NET/IB/2
g05n08:178244:178577 [5] NCCL INFO Channel 00/0 : 20[5] -> 19[1] via P2P/IPC
g05n08:178244:178577 [5] NCCL INFO Channel 01/0 : 20[5] -> 19[1] via P2P/IPC
g05n08:178244:178577 [5] NCCL INFO Connected all trees
g05n08:178244:178577 [5] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g05n08:178244:178577 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g05n08:178244:178577 [5] NCCL INFO comm 0x15ee4e6a0 rank 20 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x821cecfd780eaba1 - Init COMPLETE
lDev 5 busId 3505000 commId 0x1defb2708b178c98 - Init COMPLETE
f16n17:1090242:1090581 [5] NCCL INFO Using network IB
f16n17:1090242:1090581 [5] NCCL INFO comm 0x12acdc190 rank 8 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x821cecfd780eaba1 - Init START
f16n17:1090242:1090581 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
f16n17:1090242:1090581 [5] NCCL INFO Trees [0] -1/-1/-1->8->7 [1] 9/-1/-1->8->7
f16n17:1090242:1090581 [5] NCCL INFO P2P Chunksize set to 131072
f16n17:1090242:1090581 [5] NCCL INFO Channel 00/0 : 8[5] -> 9[3] [send] via NET/IB/3
f16n17:1090242:1090581 [5] NCCL INFO Channel 01/0 : 8[5] -> 9[3] [send] via NET/IB/3
f16n17:1090242:1090581 [5] NCCL INFO Connected all rings
f16n17:1090242:1090581 [5] NCCL INFO Channel 01/0 : 9[3] -> 8[5] [receive] via NET/IB/2
f16n17:1090242:1090581 [5] NCCL INFO Channel 00/0 : 8[5] -> 7[1] via P2P/IPC
f16n17:1090242:1090581 [5] NCCL INFO Channel 01/0 : 8[5] -> 7[1] via P2P/IPC
f16n17:1090242:1090581 [5] NCCL INFO Connected all trees
f16n17:1090242:1090581 [5] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
f16n17:1090242:1090581 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f16n17:1090242:1090581 [5] NCCL INFO comm 0x12acdc190 rank 8 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x821cecfd780eaba1 - Init COMPLETE
d09n08:1072937:1072937 [5] NCCL INFO cudaDriverVersion 12020
d09n08:1072937:1072937 [5] NCCL INFO Bootstrap : Using ib0:10.41.8.172<0>
d09n08:1072937:1072937 [5] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
d09n08:1072937:1072937 [5] NCCL INFO NET/Plugin : No plugin found, using internal implementation
d09n08:1072937:1073166 [5] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.8.172<0>
d09n08:1072937:1073166 [5] NCCL INFO Using network IB
d09n08:1072937:1073166 [5] NCCL INFO comm 0x15a4f43a0 rank 11 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x7b12926388d12ce3 - Init START
d09n08:1072937:1073166 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
d09n08:1072937:1073166 [5] NCCL INFO Trees [0] -1/-1/-1->11->10 [1] 7/-1/-1->11->9 [2] -1/-1/-1->11->10 [3] 7/-1/-1->11->9
d09n08:1072937:1073166 [5] NCCL INFO P2P Chunksize set to 131072
d09n08:1072937:1073166 [5] NCCL INFO Channel 00/0 : 11[5] -> 12[0] [send] via NET/IB/1
d09n08:1072937:1073166 [5] NCCL INFO Channel 02/0 : 11[5] -> 12[0] [send] via NET/IB/1
d09n08:1072937:1073166 [5] NCCL INFO Channel 01/0 : 11[5] -> 8[2] via P2P/IPC
d09n08:1072937:1073166 [5] NCCL INFO Channel 03/0 : 11[5] -> 8[2] via P2P/IPC
d09n08:1072937:1073166 [5] NCCL INFO Connected all rings
d09n08:1072937:1073166 [5] NCCL INFO Channel 01/0 : 11[5] -> 7[1] via P2P/IPC
d09n08:1072937:1073166 [5] NCCL INFO Channel 03/0 : 11[5] -> 7[1] via P2P/IPC
d09n08:1072937:1073166 [5] NCCL INFO Channel 01/0 : 11[5] -> 9[3] via P2P/IPC
d09n08:1072937:1073166 [5] NCCL INFO Channel 03/0 : 11[5] -> 9[3] via P2P/IPC
d09n08:1072937:1073166 [5] NCCL INFO Channel 00/0 : 11[5] -> 10[4] via P2P/IPC
d09n08:1072937:1073166 [5] NCCL INFO Channel 02/0 : 11[5] -> 10[4] via P2P/IPC
d09n08:1072937:1073166 [5] NCCL INFO Connected all trees
d09n08:1072937:1073166 [5] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
d09n08:1072937:1073166 [5] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
d09n08:1072937:1073166 [5] NCCL INFO comm 0x15a4f43a0 rank 11 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x7b12926388d12ce3 - Init COMPLETE
d09n08:1072937:1073249 [5] NCCL INFO Using network IB
d09n08:1072937:1073249 [5] NCCL INFO comm 0x15c6cc6f0 rank 1 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x1defb2708b178c98 - Init START
d09n08:1072937:1073249 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
d09n08:1072937:1073249 [5] NCCL INFO Trees [0] -1/-1/-1->1->2 [1] 2/0/-1->1->3
d09n08:1072937:1073249 [5] NCCL INFO P2P Chunksize set to 131072
d09n08:1072937:1073249 [5] NCCL INFO Channel 00/0 : 0[3] -> 1[5] [receive] via NET/IB/3
d09n08:1072937:1073249 [5] NCCL INFO Channel 01/0 : 0[3] -> 1[5] [receive] via NET/IB/3
d09n08:1072937:1073249 [5] NCCL INFO Channel 00/0 : 1[5] -> 2[1] [send] via NET/IB/3
d09n08:1072937:1073249 [5] NCCL INFO Channel 01/0 : 1[5] -> 2[1] [send] via NET/IB/3
d09n08:1072937:1073249 [5] NCCL INFO Connected all rings
d09n08:1072937:1073249 [5] NCCL INFO Channel 01/0 : 1[5] -> 3[3] [send] via NET/IB/3
d09n08:1072937:1073249 [5] NCCL INFO Channel 01/0 : 3[3] -> 1[5] [receive] via NET/IB/3
d09n08:1072937:1073249 [5] NCCL INFO Channel 00/0 : 2[1] -> 1[5] [receive] via NET/IB/3
d09n08:1072937:1073249 [5] NCCL INFO Channel 01/0 : 2[1] -> 1[5] [receive] via NET/IB/3
d09n08:1072937:1073249 [5] NCCL INFO Channel 01/0 : 1[5] -> 0[3] [send] via NET/IB/3
d09n08:1072937:1073249 [5] NCCL INFO Connected all trees
d09n08:1072937:1073249 [5] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
d09n08:1072937:1073249 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d09n08:1072937:1073249 [5] NCCL INFO comm 0x15c6cc6f0 rank 1 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x1defb2708b178c98 - Init COMPLETE
d09n08:1072937:1073268 [5] NCCL INFO Using network IB
d09n08:1072937:1073268 [5] NCCL INFO comm 0x15d391de0 rank 2 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x821cecfd780eaba1 - Init START
d09n08:1072937:1073268 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
d09n08:1072937:1073268 [5] NCCL INFO Trees [0] -1/-1/-1->2->1 [1] 3/-1/-1->2->1
d09n08:1072937:1073268 [5] NCCL INFO P2P Chunksize set to 131072
d09n08:1072937:1073268 [5] NCCL INFO Channel 00/0 : 2[5] -> 3[3] [send] via NET/IB/3
d09n08:1072937:1073268 [5] NCCL INFO Channel 01/0 : 2[5] -> 3[3] [send] via NET/IB/3
d09n08:1072937:1073268 [5] NCCL INFO Connected all rings
d09n08:1072937:1073268 [5] NCCL INFO Channel 01/0 : 3[3] -> 2[5] [receive] via NET/IB/2
d09n08:1072937:1073268 [5] NCCL INFO Channel 00/0 : 2[5] -> 1[1] via P2P/IPC
d09n08:1072937:1073268 [5] NCCL INFO Channel 01/0 : 2[5] -> 1[1] via P2P/IPC
d09n08:1072937:1073268 [5] NCCL INFO Connected all trees
d09n08:1072937:1073268 [5] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
d09n08:1072937:1073268 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d09n08:1072937:1073268 [5] NCCL INFO comm 0x15d391de0 rank 2 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x821cecfd780eaba1 - Init COMPLETE
d09n08:1072935:1072935 [3] NCCL INFO cudaDriverVersion 12020
d09n08:1072935:1072935 [3] NCCL INFO Bootstrap : Using ib0:10.41.8.172<0>
d09n08:1072935:1072935 [3] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
d09n08:1072935:1072935 [3] NCCL INFO NET/Plugin : No plugin found, using internal implementation
d09n08:1072935:1073167 [3] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.8.172<0>
d09n08:1072935:1073167 [3] NCCL INFO Using network IB
d09n08:1072935:1073167 [3] NCCL INFO comm 0x143443ce0 rank 9 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x7b12926388d12ce3 - Init START
d09n08:1072935:1073167 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
d09n08:1072935:1073167 [3] NCCL INFO Trees [0] 10/-1/-1->9->8 [1] 11/-1/-1->9->10 [2] 10/-1/-1->9->8 [3] 11/16/-1->9->10
d09n08:1072935:1073167 [3] NCCL INFO P2P Chunksize set to 131072
d09n08:1072935:1073167 [3] NCCL INFO Channel 00/0 : 9[3] -> 10[4] via P2P/IPC
d09n08:1072935:1073167 [3] NCCL INFO Channel 01/0 : 9[3] -> 10[4] via P2P/IPC
d09n08:1072935:1073167 [3] NCCL INFO Channel 02/0 : 9[3] -> 10[4] via P2P/IPC
d09n08:1072935:1073167 [3] NCCL INFO Channel 03/0 : 9[3] -> 10[4] via P2P/IPC
d09n08:1072935:1073167 [3] NCCL INFO Channel 01/0 : 0[0] -> 9[3] [receive] via NET/IB/3
d09n08:1072935:1073167 [3] NCCL INFO Channel 03/0 : 0[0] -> 9[3] [receive] via NET/IB/3
d09n08:1072935:1073167 [3] NCCL INFO Connected all rings
d09n08:1072935:1073167 [3] NCCL INFO Channel 01/0 : 9[3] -> 11[5] via P2P/IPC
d09n08:1072935:1073167 [3] NCCL INFO Channel 03/0 : 9[3] -> 11[5] via P2P/IPC
d09n08:1072935:1073167 [3] NCCL INFO Channel 03/0 : 9[3] -> 16[4] [send] via NET/IB/3
d09n08:1072935:1073167 [3] NCCL INFO Channel 03/0 : 16[4] -> 9[3] [receive] via NET/IB/3
d09n08:1072935:1073167 [3] NCCL INFO Channel 00/0 : 9[3] -> 8[2] via P2P/IPC
d09n08:1072935:1073167 [3] NCCL INFO Channel 02/0 : 9[3] -> 8[2] via P2P/IPC
d09n08:1072935:1073167 [3] NCCL INFO Connected all trees
d09n08:1072935:1073167 [3] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
d09n08:1072935:1073167 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
d09n08:1072935:1073167 [3] NCCL INFO comm 0x143443ce0 rank 9 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x7b12926388d12ce3 - Init COMPLETE
d09n08:1072935:1073250 [3] NCCL INFO Using network IB
d09n08:1072935:1073250 [3] NCCL INFO comm 0x14501f6f0 rank 1 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x86d434c9d289d59a - Init START
d09n08:1072935:1073250 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
d09n08:1072935:1073250 [3] NCCL INFO Trees [0] -1/-1/-1->1->2 [1] 2/0/-1->1->3
d09n08:1072935:1073250 [3] NCCL INFO P2P Chunksize set to 131072
d09n08:1072935:1073250 [3] NCCL INFO Channel 00/0 : 0[1] -> 1[3] [receive] via NET/IB/3
d09n08:1072935:1073250 [3] NCCL INFO Channel 01/0 : 0[1] -> 1[3] [receive] via NET/IB/3
d09n08:1072935:1073250 [3] NCCL INFO Channel 00/0 : 1[3] -> 2[5] [send] via NET/IB/3
d09n08:1072935:1073250 [3] NCCL INFO Channel 01/0 : 1[3] -> 2[5] [send] via NET/IB/3
d09n08:1072935:1073250 [3] NCCL INFO Connected all rings
d09n08:1072935:1073250 [3] NCCL INFO Channel 01/0 : 1[3] -> 3[1] [send] via NET/IB/3
d09n08:1072935:1073250 [3] NCCL INFO Channel 01/0 : 3[1] -> 1[3] [receive] via NET/IB/3
d09n08:1072935:1073250 [3] NCCL INFO Channel 00/0 : 2[5] -> 1[3] [receive] via NET/IB/3
d09n08:1072935:1073250 [3] NCCL INFO Channel 01/0 : 2[5] -> 1[3] [receive] via NET/IB/3
d09n08:1072935:1073250 [3] NCCL INFO Channel 01/0 : 1[3] -> 0[1] [send] via NET/IB/3
d09n08:1072935:1073250 [3] NCCL INFO Connected all trees
d09n08:1072935:1073250 [3] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
d09n08:1072935:1073250 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d09n08:1072935:1073250 [3] NCCL INFO comm 0x14501f6f0 rank 1 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x86d434c9d289d59ag04n18:916028:916028 [4] NCCL INFO cudaDriverVersion 12020
g04n18:916028:916028 [4] NCCL INFO Bootstrap : Using ib0:10.41.16.12<0>
g04n18:916028:916028 [4] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g04n18:916028:916028 [4] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g04n18:916028:916256 [4] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.16.12<0>
g04n18:916028:916256 [4] NCCL INFO Using network IB
g04n18:916028:916256 [4] NCCL INFO comm 0x147df3740 rank 76 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x7b12926388d12ce3 - Init START
g04n18:916028:916256 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g04n18:916028:916256 [4] NCCL INFO Trees [0] 77/-1/-1->76->75 [1] 75/88/-1->76->52 [2] 77/-1/-1->76->75 [3] 75/-1/-1->76->82
g04n18:916028:916256 [4] NCCL INFO P2P Chunksize set to 131072
g04n18:916028:916256 [4] NCCL INFO Channel 00/0 : 76[4] -> 77[5] via P2P/IPC
g04n18:916028:916256 [4] NCCL INFO Channel 01/0 : 76[4] -> 77[5] via P2P/IPC
g04n18:916028:916256 [4] NCCL INFO Channel 02/0 : 76[4] -> 77[5] via P2P/IPC
g04n18:916028:916256 [4] NCCL INFO Channel 03/0 : 76[4] -> 77[5] via P2P/IPC
g04n18:916028:916256 [4] NCCL INFO Connected all rings
g04n18:916028:916256 [4] NCCL INFO Channel 03/0 : 76[4] -> 82[4] [send] via NET/IB/3
g04n18:916028:916256 [4] NCCL INFO Channel 01/0 : 76[4] -> 88[4] [send] via NET/IB/3
g04n18:916028:916256 [4] NCCL INFO Channel 01/0 : 52[4] -> 76[4] [receive] via NET/IB/3
g04n18:916028:916256 [4] NCCL INFO Channel 01/0 : 76[4] -> 52[4] [send] via NET/IB/3
g04n18:916028:916256 [4] NCCL INFO Channel 01/0 : 88[4] -> 76[4] [receive] via NET/IB/3
g04n18:916028:916256 [4] NCCL INFO Channel 03/0 : 82[4] -> 76[4] [receive] via NET/IB/3
g04n18:916028:916256 [4] NCCL INFO Channel 00/0 : 76[4] -> 75[3] via P2P/IPC
g04n18:916028:916256 [4] NCCL INFO Channel 01/0 : 76[4] -> 75[3] via P2P/IPC
g04n18:916028:916256 [4] NCCL INFO Channel 02/0 : 76[4] -> 75[3] via P2P/IPC
g04n18:916028:916256 [4] NCCL INFO Channel 03/0 : 76[4] -> 75[3] via P2P/IPC
g04n18:916028:916256 [4] NCCL INFO Connected all trees
g04n18:916028:916256 [4] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g04n18:916028:916256 [4] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g04n18:916028:916256 [4] NCCL INFO comm 0x147df3740 rank 76 nranks 96 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x7b12926388d12ce3 - Init COMPLETE
g04n18:916028:916342 [4] NCCL INFO Using network IB
g04n18:916028:916342 [4] NCCL INFO comm 0x14bb83460 rank 9 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x7dd05d77214d2e8 - Init START
g04n18:916028:916342 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g04n18:916028:916342 [4] NCCL INFO Trees [0] -1/-1/-1->9->10 [1] 10/8/-1->9->7
g04n18:916028:916342 [4] NCCL INFO P2P Chunksize set to 131072
g04n18:916028:916342 [4] NCCL INFO Channel 00/0 : 8[2] -> 9[4] [receive] via NET/IB/1
g04n18:916028:916342 [4] NCCL INFO Channel 01/0 : 8[2] -> 9[4] [receive] via NET/IB/1
g04n18:916028:916342 [4] NCCL INFO Channel 00/0 : 9[4] -> 10[0] [send] via NET/IB/1
g04n18:916028:916342 [4] NCCL INFO Channel 01/0 : 9[4] -> 10[0] [send] via NET/IB/1
g04n18:916028:916342 [4] NCCL INFO Connected all rings
g04n18:916028:916342 [4] NCCL INFO Channel 01/0 : 7[0] -> 9[4] [receive] via NET/IB/1
g04n18:916028:916342 [4] NCCL INFO Channel 01/0 : 9[4] -> 7[0] [send] via NET/IB/1
g04n18:916028:916342 [4] NCCL INFO Channel 00/0 : 10[0] -> 9[4] [receive] via NET/IB/1
g04n18:916028:916342 [4] NCCL INFO Channel 01/0 : 10[0] -> 9[4] [receive] via NET/IB/1
g04n18:916028:916342 [4] NCCL INFO Channel 01/0 : 9[4] -> 8[2] [send] via NET/IB/1
g04n18:916028:916342 [4] NCCL INFO Connected all trees
g04n18:916028:916342 [4] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g04n18:916028:916342 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g04n18: - Init COMPLETE
d09n08:1072935:1073269 [3] NCCL INFO Using network IB
d09n08:1072935:1073269 [3] NCCL INFO comm 0x145cfe9d0 rank 2 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xa49a27aa8b15034e - Init START
d09n08:1072935:1073269 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
d09n08:1072935:1073269 [3] NCCL INFO Trees [0] -1/-1/-1->2->4 [1] 3/0/-1->2->5
d09n08:1072935:1073269 [3] NCCL INFO P2P Chunksize set to 131072
d09n08:1072935:1073269 [3] NCCL INFO Channel 00/0 : 1[5] -> 2[3] [receive] via NET/IB/3
d09n08:1072935:1073269 [3] NCCL INFO Channel 01/0 : 1[5] -> 2[3] [receive] via NET/IB/3
d09n08:1072935:1073269 [3] NCCL INFO Channel 00/0 : 2[3] -> 3[1] [send] via NET/IB/3
d09n08:1072935:1073269 [3] NCCL INFO Channel 01/0 : 2[3] -> 3[1] [send] via NET/IB/3
d09n08:1072935:1073269 [3] NCCL INFO Connected all rings
d09n08:1072935:1073269 [3] NCCL INFO Channel 01/0 : 0[1] -> 2[3] [receive] via NET/IB/3
d09n08:1072935:1073269 [3] NCCL INFO Channel 00/0 : 2[3] -> 4[5] [send] via NET/IB/3
d09n08:1072935:1073269 [3] NCCL INFO Channel 01/0 : 2[3] -> 5[3] [send] via NET/IB/3
d09n08:1072935:1073269 [3] NCCL INFO Channel 01/0 : 5[3] -> 2[3] [receive] via NET/IB/3
d09n08:1072935:1073269 [3] NCCL INFO Channel 00/0 : 4[5] -> 2[3] [receive] via NET/IB/3
d09n08:1072935:1073269 [3] NCCL INFO Channel 01/0 : 2[3] -> 0[1] [send] via NET/IB/3
d09n08:1072935:1073269 [3] NCCL INFO Channel 01/0 : 3[1] -> 2[3] [receive] via NET/IB/3
d09n08:1072935:1073269 [3] NCCL INFO Connected all trees
d09n08:1072935:1073269 [3] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
d09n08:1072935:1073269 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d09n08:1072935:1073269 [3] NCCL INFO comm 0x145cfe9d0 rank 2 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xa49a27aa8b15034e - Init COMPLETE
916028:916342 [4] NCCL INFO comm 0x14bb83460 rank 9 nranks 12 cudaDev 4 nvmlDev 4 busId 3504000 commId 0x7dd05d77214d2e8 - Init COMPLETE
g04n18:916028:916361 [4] NCCL INFO Using network IB
g04n18:916028:916361 [4] NCCL INFO comm 0x149c64870 rank 19 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xaedb5f80914767fd - Init START
g04n18:916028:916361 [4] NCCL INFO Setting affinity for GPU 4 to f00000,00000000,00000000,00000000
g04n18:916028:916361 [4] NCCL INFO Trees [0] 15/-1/-1->19->18 [1] -1/-1/-1->19->18
g04n18:916028:916361 [4] NCCL INFO P2P Chunksize set to 131072
g04n18:916028:916361 [4] NCCL INFO Channel 00/0 : 19[4] -> 20[2] [send] via NET/IB/1
g04n18:916028:916361 [4] NCCL INFO Channel 01/0 : 19[4] -> 20[2] [send] via NET/IB/1
g04n18:916028:916361 [4] NCCL INFO Connected all rings
g04n18:916028:916361 [4] NCCL INFO Channel 00/0 : 15[0] -> 19[4] [receive] via NET/IB/0
g04n18:916028:916361 [4] NCCL INFO Channel 00/0 : 19[4] -> 15[0] [send] via NET/IB/0
g04n18:916028:916361 [4] NCCL INFO Channel 00/0 : 19[4] -> 18[0] via P2P/IPC
g04n18:916028:916361 [4] NCCL INFO Channel 01/0 : 19[4] -> 18[0] via P2P/IPC
g04n18:916028:916361 [4] NCCL INFO Connected all trees
g04n18:916028:916361 [4] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g04n18:916028:916361 [4] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g04n18:916028:916361 [4] NCCL INFO comm 0x149c64870 rank 19 nranks 24 cudaDev 4 nvmlDev 4 busId 3504000 commId 0xaedb5f80914767fd - Init COMPLETE
d11n17:1141834:1141834 [2] NCCL INFO cudaDriverVersion 12020
d11n17:1141834:1141834 [2] NCCL INFO Bootstrap : Using ib0:10.41.8.217<0>
d11n17:1141834:1141834 [2] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
d11n17:1141834:1141834 [2] NCCL INFO NET/Plugin : No plugin found, using internal implementation
d11n17:1141834:1142064 [2] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.8.217<0>
d11n17:1141834:1142064 [2] NCCL INFO Using network IB
d11n17:1141834:1142064 [2] NCCL INFO comm 0x149443840 rank 20 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x7b12926388d12ce3 - Init START
d11n17:1141834:1142064 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
d11n17:1141834:1142064 [2] NCCL INFO Trees [0] 21/-1/-1->20->19 [1] -1/-1/-1->20->18 [2] 21/-1/-1->20->19 [3] -1/-1/-1->20->18
d11n17:1141834:1142064 [2] NCCL INFO P2P Chunksize set to 131072
d11n17:1141834:1142064 [2] NCCL INFO Channel 00/0 : 20[2] -> 21[3] via P2P/IPC
d11n17:1141834:1142064 [2] NCCL INFO Channel 02/0 : 20[2] -> 21[3] via P2P/IPC
d11n17:1141834:1142064 [2] NCCL INFO Channel 01/0 : 20[2] -> 19[1] via P2P/IPC
d11n17:1141834:1142064 [2] NCCL INFO Channel 03/0 : 20[2] -> 19[1] via P2P/IPC
d11n17:1141834:1142064 [2] NCCL INFO Connected all rings
d11n17:1141834:1142064 [2] NCCL INFO Channel 01/0 : 20[2] -> 18[0] via P2P/IPC
d11n17:1141834:1142064 [2] NCCL INFO Channel 03/0 : 20[2] -> 18[0] via P2P/IPC
d11n17:1141834:1142064 [2] NCCL INFO Channel 00/0 : 20[2] -> 19[1] via P2P/IPC
d11n17:1141834:1142064 [2] NCCL INFO Channel 02/0 : 20[2] -> 19[1] via P2P/IPC
d11n17:1141834:1142064 [2] NCCL INFO Connected all trees
d11n17:1141834:1142064 [2] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
d11n17:1141834:1142064 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
d11n17:1141834:1142064 [2] NCCL INFO comm 0x149443840 rank 20 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x7b12926388d12ce3 - Init COMPLETE
d11n17:1141834:1142150 [2] NCCL INFO Using network IB
d11n17:1141834:1142150 [2] NCCL INFO comm 0x14bda1440 rank 2 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0x7dd05d77214d2e8 - Init START
d11n17:1141834:1142150 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
d11n17:1141834:1142150 [2] NCCL INFO Trees [0] 1/3/-1->2->4 [1] -1/-1/-1->2->1
d11n17:1141834:1142150 [2] NCCL INFO P2P Chunksize set to 131072
d11n17:1141834:1142150 [2] NCCL INFO Channel 00/0 : 1[0] -> 2[2] [receive] via NET/IB/0
d11n17:1141834:1142150 [2] NCCL INFO Channel 01/0 : 1[0] -> 2[2] [receive] via NET/IB/0
d11n17:1141834:1142150 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[4] [send] via NET/IB/0
d11n17:1141834:1142150 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[4] [send] via NET/IB/0
d11n17:1141834:1142150 [2] NCCL INFO Connected all rings
d11n17:1141834:1142150 [2] NCCL INFO Channel 00/0 : 2[2] -> 4[0] [send] via NET/IB/0
d11n17:1141834:1142150 [2] NCCL INFO Channel 00/0 : 4[0] -> 2[2] [receive] via NET/IB/0
d11n17:1141834:1142150 [2] NCCL INFO Channel 00/0 : 3[4] -> 2[2] [receive] via NET/IB/0
d11n17:1141834:1142150 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[0] [send] via NET/IB/0
d11n17:1141834:1142150 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[0] [send] via NET/IB/0
d11n17:1141834:1142150 [2] NCCL INFO Connected all trees
d11n17:1141834:1142150 [2] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
d11n17:1141834:1142150 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d11n17:1141834:1142150 [2] NCCL INFO comm 0x14bda1440 rank 2 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0x7dd05d77214d2e8 - Init COMPLETE
d11n17:1141834:1142167 [2] NCCL INFO Using network IB
d11n17:1141834:1142167 [2] NCCL INFO comm 0x14c0b11c0 rank 5 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0xaedb5f80914767fd - Init START
d11n17:1141834:1142167 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
d11n17:1141834:1142167 [2] NCCL INFO Trees [0] -1/-1/-1->5->3 [1] 8/2/-1->5->11
d11n17:1141834:1142167 [2] NCCL INFO P2P Chunksize set to 131072
d11n17:1141834:1142167 [2] NCCL INFO Channel 00/0 : 4[4] -> 5[2] [receive] via NET/IB/0
d11n17:1141834:1142167 [2] NCCL INFO Channel 01/0 : 4[4] -> 5[2] [receive] via NET/IB/0
d11n17:1141834:1142167 [2] NCCL INFO Channel 00/0 : 5[2] -> 6[0] [send] via NET/IB/0
d11n17:1141834:1142167 [2] NCCL INFO Channel 01/0 : 5[2] -> 6[0] [send] via NET/IB/0
d11n17:1141834:1142167 [2] NCCL INFO Connected all rings
d11n17:1141834:1142167 [2] NCCL INFO Channel 00/0 : 3[0] -> 5[2] [receive] via NET/IB/0
d11n17:1141834:1142167 [2] NCCL INFO Channel 01/0 : 2[2] -> 5[2] [receive] via NET/IB/0
d11n17:1141834:1142167 [2] NCCL INFO Channel 01/0 : 5[2] -> 8[2] [send] via NET/IB/0
d11n17:1141834:1142167 [2] NCCL INFO Channel 01/0 : 5[2] -> 11[2] [send] via NET/IB/0
d11n17:1141834:1142167 [2] NCCL INFO Channel 01/0 : 11[2] -> 5[2] [receive] via NET/IB/0
d11n17:1141834:1142167 [2] NCCL INFO Channel 01/0 : 8[2] -> 5[2] [receive] via NET/IB/0
d11n17:1141834:1142167 [2] NCCL INFO Channel 01/0 : 5[2] -> 2[2] [send] via NET/IB/0
d11n17:1141834:1142167 [2] NCCL INFO Channel 00/0 : 5[2] -> 3[0] [send] via NET/IB/0
d11n17:1141834:1142167 [2] NCCL INFO Connected all trees
d11n17:1141834:1142167 [2] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
d11n17:1141834:1142167 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d11n17:1141834:1142167 [2] NCCL INFO comm 0x14c0b11c0 rank 5 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0xaedb5f80914767fd - Init COMPLETE
g04n17:914196:914196 [2] NCCL INFO cudaDriverVersion 12020
g04n17:914196:914196 [2] NCCL INFO Bootstrap : Using ib0:10.41.16.11<0>
g04n17:914196:914196 [2] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g04n17:914196:914196 [2] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g04n17:914196:914425 [2] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.16.11<0>
g04n17:914196:914425 [2] NCCL INFO Using network IB
g04n17:914196:914425 [2] NCCL INFO comm 0x179083c50 rank 68 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x7b12926388d12ce3 - Init START
g04n17:914196:914425 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g04n17:914196:914425 [2] NCCL INFO Trees [0] 69/-1/-1->68->67 [1] -1/-1/-1->68->66 [2] 69/-1/-1->68->67 [3] -1/-1/-1->68->66
g04n17:914196:914425 [2] NCCL INFO P2P Chunksize set to 131072
g04n17:914196:914425 [2] NCCL INFO Channel 00/0 : 68[2] -> 69[3] via P2P/IPC
g04n17:914196:914425 [2] NCCL INFO Channel 02/0 : 68[2] -> 69[3] via P2P/IPC
g04n17:914196:914425 [2] NCCL INFO Channel 01/0 : 68[2] -> 67[1] via P2P/IPC
g04n17:914196:914425 [2] NCCL INFO Channel 03/0 : 68[2] -> 67[1] via P2P/IPC
g04n17:914196:914425 [2] NCCL INFO Connected all rings
g04n17:914196:914425 [2] NCCL INFO Channel 01/0 : 68[2] -> 66[0] via P2P/IPC
g04n17:914196:914425 [2] NCCL INFO Channel 03/0 : 68[2] -> 66[0] via P2P/IPC
g04n17:914196:914425 [2] NCCL INFO Channel 00/0 : 68[2] -> 67[1] via P2P/IPC
g04n17:914196:914425 [2] NCCL INFO Channel 02/0 : 68[2] -> 67[1] via P2P/IPC
g04n17:914196:914425 [2] NCCL INFO Connected all trees
g04n17:914196:914425 [2] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g04n17:914196:914425 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g04n17:914196:914425 [2] NCCL INFO comm 0x179083c50 rank 68 nranks 96 cudaDev 2 nvmlDev 2 busId 406000 commId 0x7b12926388d12ce3 - Init COMPLETE
g04n17:914196:914516 [2] NCCL INFO Using network IB
g04n17:914196:914516 [2] NCCL INFO comm 0x17bdc03c0 rank 8 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0x7dd05d77214d2e8 - Init START
g04n17:914196:914516 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g04n17:914196:914516 [2] NCCL INFO Trees [0] 4/10/-1->8->0 [1] -1/-1/-1->8->9
g04n17:914196:914516 [2] NCCL INFO P2P Chunksize set to 131072
g04n17:914196:914516 [2] NCCL INFO Channel 00/0 : 7[0] -> 8[2] [receive] via NET/IB/0
g04n17:914196:914516 [2] NCCL INFO Channel 01/0 : 7[0] -> 8[2] [receive] via NET/IB/0
g04n17:914196:914516 [2] NCCL INFO Channel 00/0 : 8[2] -> 9[4] [send] via NET/IB/0
g04n17:914196:914516 [2] NCCL INFO Channel 01/0 : 8[2] -> 9[4] [send] via NET/IB/0
g04n17:914196:914516 [2] NCCL INFO Connected all rings
g04n17:914196:914516 [2] NCCL INFO Channel 00/0 : 8[2] -> 10[0] [send] via NET/IB/0
g04n17:914196:914516 [2] NCCL INFO Channel 00/0 : 4[0] -> 8[2] [receive] via NET/IB/0
g04n17:914196:914516 [2] NCCL INFO Channel 00/0 : 8[2] -> 0[4] [send] via NET/IB/0
g04n17:914196:914516 [2] NCCL INFO Channel 00/0 : 0[4] -> 8[2] [receive] via NET/IB/0
g04n17:914196:914516 [2] NCCL INFO Channel 00/0 : 8[2] -> 4[0] [send] via NET/IB/0
g04n17:914196:914516 [2] NCCL INFO Channel 00/0 : 10[0] -> 8[2] [receive] via NET/IB/0
g04n17:914196:914516 [2] NCCL INFO Channel 01/0 : 9[4] -> 8[2] [receive] via NET/IB/0
g04n17:914196:914516 [2] NCCL INFO Connected all trees
g04n17:914196:914516 [2] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g04n17:914196:914516 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g04n17:914196:914516 [2] NCCL INFO comm 0x17bdc03c0 rank 8 nranks 12 cudaDev 2 nvmlDev 2 busId 406000 commId 0x7dd05d77214d2e8 - Init COMPLETE
g04n17:914196:914533 [2] NCCL INFO Using network IB
g04n17:914196:914533 [2] NCCL INFO comm 0x17ae45800 rank 17 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0xaedb5f80914767fd - Init START
g04n17:914196:914533 [2] NCCL INFO Setting affinity for GPU 2 to 0f000000,00000000
g04n17:914196:914533 [2] NCCL INFO Trees [0] -1/-1/-1->17->15 [1] 20/14/-1->17->11
g04n17:914196:914533 [2] NCCL INFO P2P Chunksize set to 131072
g04n17:914196:914533 [2] NCCL INFO Channel 00/0 : 16[4] -> 17[2] [receive] via NET/IB/0
g04n17:914196:914533 [2] NCCL INFO Channel 01/0 : 16[4] -> 17[2] [receive] via NET/IB/0
g04n17:914196:914533 [2] NCCL INFO Channel 00/0 : 17[2] -> 18[0] [send] via NET/IB/0
g04n17:914196:914533 [2] NCCL INFO Channel 01/0 : 17[2] -> 18[0] [send] via NET/IB/0
g04n17:914196:914533 [2] NCCL INFO Connected all rings
g04n17:914196:914533 [2] NCCL INFO Channel 00/0 : 15[0] -> 17[2] [receive] via NET/IB/0
g04n17:914196:914533 [2] NCCL INFO Channel 01/0 : 14[2] -> 17[2] [receive] via NET/IB/0
g04n17:914196:914533 [2] NCCL INFO Channel 01/0 : 17[2] -> 20[2] [send] via NET/IB/0
g04n17:914196:914533 [2] NCCL INFO Channel 01/0 : 11[2] -> 17[2] [receive] via NET/IB/0
g04n17:914196:914533 [2] NCCL INFO Channel 01/0 : 17[2] -> 11[2] [send] via NET/IB/0
g04n17:914196:914533 [2] NCCL INFO Channel 01/0 : 20[2] -> 17[2] [receive] via NET/IB/0
g04n17:914196:914533 [2] NCCL INFO Channel 01/0 : 17[2] -> 14[2] [send] via NET/IB/0
g04n17:914196:914533 [2] NCCL INFO Channel 00/0 : 17[2] -> 15[0] [send] via NET/IB/0
g04n17:914196:914533 [2] NCCL INFO Connected all trees
g04n17:914196:914533 [2] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g04n17:914196:914533 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g04n17:914196:914533 [2] NCCL INFO comm 0x17ae45800 rank 17 nranks 24 cudaDev 2 nvmlDev 2 busId 406000 commId 0xaedb5f80914767fd - Init COMPLETE
g04n17:914201:914201 [5] NCCL INFO cudaDriverVersion 12020
g04n17:914201:914201 [5] NCCL INFO Bootstrap : Using ib0:10.41.16.11<0>
g04n17:914201:914201 [5] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g04n17:914201:914201 [5] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g04n17:914201:914426 [5] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.16.11<0>
g04n17:914201:914426 [5] NCCL INFO Using network IB
g04n17:914201:914426 [5] NCCL INFO comm 0x1484f4310 rank 71 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x7b12926388d12ce3 - Init START
g04n17:914201:914426 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g04n17:914201:914426 [5] NCCL INFO Trees [0] -1/-1/-1->71->70 [1] 67/-1/-1->71->69 [2] -1/-1/-1->71->70 [3] 67/-1/-1->71->69
g04n17:914201:914426 [5] NCCL INFO P2P Chunksize set to 131072
g04n17:914201:914426 [5] NCCL INFO Channel 00/0 : 71[5] -> 72[0] [send] via NET/IB/1
g04n17:914201:914426 [5] NCCL INFO Channel 02/0 : 71[5] -> 72[0] [send] via NET/IB/1
g04n17:914201:914426 [5] NCCL INFO Channel 01/0 : 71[5] -> 68[2] via P2P/IPC
g04n17:914201:914426 [5] NCCL INFO Channel 03/0 : 71[5] -> 68[2] via P2P/IPC
g04n17:914201:914426 [5] NCCL INFO Connected all rings
g04n17:914201:914426 [5] NCCL INFO Channel 01/0 : 71[5] -> 67[1] via P2P/IPC
g04n17:914201:914426 [5] NCCL INFO Channel 03/0 : 71[5] -> 67[1] via P2P/IPC
g04n17:914201:914426 [5] NCCL INFO Channel 01/0 : 71[5] -> 69[3] via P2P/IPC
g04n17:914201:914426 [5] NCCL INFO Channel 03/0 : 71[5] -> 69[3] via P2P/IPC
g04n17:914201:914426 [5] NCCL INFO Channel 00/0 : 71[5] -> 70[4] via P2P/IPC
g04n17:914201:914426 [5] NCCL INFO Channel 02/0 : 71[5] -> 70[4] via P2P/IPC
g04n17:914201:914426 [5] NCCL INFO Connected all trees
g04n17:914201:914426 [5] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g04n17:914201:914426 [5] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g04n17:914201:914426 [5] NCCL INFO comm 0x1484f4310 rank 71 nranks 96 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x7b12926388d12ce3 - Init COMPLETE
g04n17:914201:914515 [5] NCCL INFO Using network IB
g04n17:914201:914515 [5] NCCL INFO comm 0x14b1fbe40 rank 8 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x54676b30da0675d6 - Init START
g04n17:914201:914515 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g04n17:914201:914515 [5] NCCL INFO Trees [0] 4/10/-1->8->0 [1] -1/-1/-1->8->9
g04n17:914201:914515 [5] NCCL INFO P2P Chunksize set to 131072
g04n17:914201:914515 [5] NCCL INFO Channel 00/0 : 7[3] -> 8[5] [receive] via NET/IB/3
g04n17:914201:914515 [5] NCCL INFO Channel 01/0 : 7[3] -> 8[5] [receive] via NET/IB/3
g04n17:914201:914515 [5] NCCL INFO Channel 00/0 : 8[5] -> 9[1] [send] via NET/IB/3
g04n17:914201:914515 [5] NCCL INFO Channel 01/0 : 8[5] -> 9[1] [send] via NET/IB/3
g04n17:914201:914515 [5] NCCL INFO Connected all rings
g04n17:914201:914515 [5] NCCL INFO Channel 00/0 : 8[5] -> 10[3] [send] via NET/IB/3
g04n17:914201:914515 [5] NCCL INFO Channel 00/0 : 4[3] -> 8[5] [receive] via NET/IB/3
g04n17:914201:914515 [5] NCCL INFO Channel 00/0 : 8[5] -> 0[1] [send] via NET/IB/3
g04n17:914201:914515 [5] NCCL INFO Channel 00/0 : 0[1] -> 8[5] [receive] via NET/IB/3
g04n17:914201:914515 [5] NCCL INFO Channel 00/0 : 8[5] -> 4[3] [send] via NET/IB/3
g04n17:914201:914515 [5] NCCL INFO Channel 00/0 : 10[3] -> 8[5] [receive] via NET/IB/3
g04n17:914201:914515 [5] NCCL INFO Channel 01/0 : 9[1] -> 8[5] [receive] via NET/IB/3
g04n17:914201:914515 [5] NCCL INFO Connected all trees
g04n17:914201:914515 [5] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g04n17:914201:914515 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g04n17:914201:914515 [5] NCCL INFO comm 0x14b1fbe40 rank 8 nranks 12 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x54676b30da0675d6 - Init COMPLETE
g04n17:914201:914536 [5] NCCL INFO Using network IB
g04n17:914201:914536 [5] NCCL INFO comm 0x14b2317c0 rank 17 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x821cecfd780eaba1 - Init START
g04n17:914201:914536 [5] NCCL INFO Setting affinity for GPU 5 to 0f0000,00000000,00000000,00000000,00000000
g04n17:914201:914536 [5] NCCL INFO Trees [0] -1/-1/-1->17->16 [1] 19/-1/-1->17->16
g04n17:914201:914536 [5] NCCL INFO P2P Chunksize set to 131072
g04n17:914201:914536 [5] NCCL INFO Channel 00/0 : 17[5] -> 18[3] [send] via NET/IB/3
g04n17:914201:914536 [5] NCCL INFO Channel 01/0 : 17[5] -> 18[3] [send] via NET/IB/3
g04n17:914201:914536 [5] NCCL INFO Connected all rings
g04n17:914201:914536 [5] NCCL INFO Channel 01/0 : 17[5] -> 19[1] [send] via NET/IB/2
g04n17:914201:914536 [5] NCCL INFO Channel 01/0 : 19[1] -> 17[5] [receive] via NET/IB/2
g04n17:914201:914536 [5] NCCL INFO Channel 00/0 : 17[5] -> 16[1] via P2P/IPC
g04n17:914201:914536 [5] NCCL INFO Channel 01/0 : 17[5] -> 16[1] via P2P/IPC
g04n17:914201:914536 [5] NCCL INFO Connected all trees
g04n17:914201:914536 [5] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g04n17:914201:914536 [5] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g04n17:914201:914536 [5] NCCL INFO comm 0x14b2317c0 rank 17 nranks 24 cudaDev 5 nvmlDev 5 busId 3505000 commId 0x821cecfd780eaba1 - Init COMPLETE
g04n17:914199:914199 [3] NCCL INFO cudaDriverVersion 12020
g04n17:914199:914199 [3] NCCL INFO Bootstrap : Using ib0:10.41.16.11<0>
g04n17:914199:914199 [3] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g04n17:914199:914199 [3] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g04n17:914199:914427 [3] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.16.11<0>
g04n17:914199:914427 [3] NCCL INFO Using network IB
g04n17:914199:914427 [3] NCCL INFO comm 0x15a4e3d90 rank 69 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x7b12926388d12ce3 - Init START
g04n17:914199:914427 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g04n17:914199:914427 [3] NCCL INFO Trees [0] 70/-1/-1->69->68 [1] 71/-1/-1->69->70 [2] 70/-1/-1->69->68 [3] 71/82/-1->69->70
g04n17:914199:914427 [3] NCCL INFO P2P Chunksize set to 131072
g04n17:914199:914427 [3] NCCL INFO Channel 00/0 : 69[3] -> 70[4] via P2P/IPC
g04n17:914199:914427 [3] NCCL INFO Channel 01/0 : 69[3] -> 70[4] via P2P/IPC
g04n17:914199:914427 [3] NCCL INFO Channel 02/0 : 69[3] -> 70[4] via P2P/IPC
g04n17:914199:914427 [3] NCCL INFO Channel 03/0 : 69[3] -> 70[4] via P2P/IPC
g04n17:914199:914427 [3] NCCL INFO Channel 01/0 : 60[0] -> 69[3] [receive] via NET/IB/3
g04n17:914199:914427 [3] NCCL INFO Channel 03/0 : 60[0] -> 69[3] [receive] via NET/IB/3
g04n17:914199:914427 [3] NCCL INFO Connected all rings
g04n17:914199:914427 [3] NCCL INFO Channel 01/0 : 69[3] -> 71[5] via P2P/IPC
g04n17:914199:914427 [3] NCCL INFO Channel 03/0 : 69[3] -> 71[5] via P2P/IPC
g04n17:914199:914427 [3] NCCL INFO Channel 03/0 : 69[3] -> 82[4] [send] via NET/IB/3
g04n17:914199:914427 [3] NCCL INFO Channel 03/0 : 82[4] -> 69[3] [receive] via NET/IB/3
g04n17:914199:914427 [3] NCCL INFO Channel 00/0 : 69[3] -> 68[2] via P2P/IPC
g04n17:914199:914427 [3] NCCL INFO Channel 02/0 : 69[3] -> 68[2] via P2P/IPC
g04n17:914199:914427 [3] NCCL INFO Connected all trees
g04n17:914199:914427 [3] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g04n17:914199:914427 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g04n17:914199:914427 [3] NCCL INFO comm 0x15a4e3d90 rank 69 nranks 96 cudaDev 3 nvmlDev 3 busId 3503000 commId 0x7b12926388d12ce3 - Init COMPLETE
g04n17:914199:914519 [3] NCCL INFO Using network IB
g04n17:914199:914519 [3] NCCL INFO comm 0x15cd6da60 rank 8 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xa77518dab25ce26 - Init START
g04n17:914199:914519 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g04n17:914199:914519 [3] NCCL INFO Trees [0] 4/10/-1->8->0 [1] -1/-1/-1->8->9
g04n17:914199:914519 [3] NCCL INFO P2P Chunksize set to 131072
g04n17:914199:914519 [3] NCCL INFO Channel 00/0 : 7[1] -> 8[3] [receive] via NET/IB/3
g04n17:914199:914519 [3] NCCL INFO Channel 01/0 : 7[1] -> 8[3] [receive] via NET/IB/3
g04n17:914199:914519 [3] NCCL INFO Channel 00/0 : 8[3] -> 9[5] [send] via NET/IB/3
g04n17:914199:914519 [3] NCCL INFO Channel 01/0 : 8[3] -> 9[5] [send] via NET/IB/3
g04n17:914199:914519 [3] NCCL INFO Connected all rings
g04n17:914199:914519 [3] NCCL INFO Channel 00/0 : 8[3] -> 10[1] [send] via NET/IB/3
g04n17:914199:914519 [3] NCCL INFO Channel 00/0 : 4[1] -> 8[3] [receive] via NET/IB/3
g04n17:914199:914519 [3] NCCL INFO Channel 00/0 : 8[3] -> 0[5] [send] via NET/IB/3
g04n17:914199:914519 [3] NCCL INFO Channel 00/0 : 0[5] -> 8[3] [receive] via NET/IB/3
g04n17:914199:914519 [3] NCCL INFO Channel 00/0 : 8[3] -> 4[1] [send] via NET/IB/3
g04n17:914199:914519 [3] NCCL INFO Channel 00/0 : 10[1] -> 8[3] [receive] via NET/IB/3
g04n17:914199:914519 [3] NCCL INFO Channel 01/0 : 9[5] -> 8[3] [receive] via NET/IB/3
g04n17:914199:914519 [3] NCCL INFO Connected all trees
g04n17:914199:914519 [3] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g04n17:914199:914519 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g04n17:914199:914519 [3] NCCL INFO comm 0x15cd6da60 rank 8 nranks 12 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xa77518dab25ce26 - Init COMPLETE
g04n17:914199:914537 [3] NCCL INFO Using network IB
g04n17:914199:914537 [3] NCCL INFO comm 0x15d2a9390 rank 17 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xa49a27aa8b15034e - Init START
g04n17:914199:914537 [3] NCCL INFO Setting affinity for GPU 3 to 0f000000,00000000,00000000
g04n17:914199:914537 [3] NCCL INFO Trees [0] -1/-1/-1->17->15 [1] 20/14/-1->17->11
g04n17:914199:914537 [3] NCCL INFO P2P Chunksize set to 131072
g04n17:914199:914537 [3] NCCL INFO Channel 00/0 : 16[5] -> 17[3] [receive] via NET/IB/3
g04n17:914199:914537 [3] NCCL INFO Channel 01/0 : 16[5] -> 17[3] [receive] via NET/IB/3
g04n17:914199:914537 [3] NCCL INFO Channel 00/0 : 17[3] -> 18[1] [send] via NET/IB/3
g04n17:914199:914537 [3] NCCL INFO Channel 01/0 : 17[3] -> 18[1] [send] via NET/IB/3
g04n17:914199:914537 [3] NCCL INFO Connected all rings
g04n17:914199:914537 [3] NCCL INFO Channel 00/0 : 15[1] -> 17[3] [receive] via NET/IB/3
g04n17:914199:914537 [3] NCCL INFO Channel 01/0 : 14[3] -> 17[3] [receive] via NET/IB/3
g04n17:914199:914537 [3] NCCL INFO Channel 01/0 : 17[3] -> 20[3] [send] via NET/IB/3
g04n17:914199:914537 [3] NCCL INFO Channel 01/0 : 11[3] -> 17[3] [receive] via NET/IB/3
g04n17:914199:914537 [3] NCCL INFO Channel 01/0 : 17[3] -> 11[3] [send] via NET/IB/3
g04n17:914199:914537 [3] NCCL INFO Channel 01/0 : 20[3] -> 17[3] [receive] via NET/IB/3
g04n17:914199:914537 [3] NCCL INFO Channel 01/0 : 17[3] -> 14[3] [send] via NET/IB/3
g04n17:914199:914537 [3] NCCL INFO Channel 00/0 : 17[3] -> 15[1] [send] via NET/IB/3
g04n17:914199:914537 [3] NCCL INFO Connected all trees
g04n17:914199:914537 [3] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g04n17:914199:914537 [3] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g04n17:914199:914537 [3] NCCL INFO comm 0x15d2a9390 rank 17 nranks 24 cudaDev 3 nvmlDev 3 busId 3503000 commId 0xa49a27aa8b15034e - Init COMPLETE
g04n17:914194:914194 [1] NCCL INFO cudaDriverVersion 12020
g04n17:914194:914194 [1] NCCL INFO Bootstrap : Using ib0:10.41.16.11<0>
g04n17:914194:914194 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
g04n17:914194:914194 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
g04n17:914194:914429 [1] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.16.11<0>
g04n17:914194:914429 [1] NCCL INFO Using network IB
g04n17:914194:914429 [1] NCCL INFO comm 0x137974090 rank 67 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x7b12926388d12ce3 - Init START
g04n17:914194:914429 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g04n17:914194:914429 [1] NCCL INFO Trees [0] 68/-1/-1->67->66 [1] 66/-1/-1->67->71 [2] 68/78/-1->67->66 [3] 66/-1/-1->67->71
g04n17:914194:914429 [1] NCCL INFO P2P Chunksize set to 131072
g04n17:914194:914429 [1] NCCL INFO Channel 00/0 : 67[1] -> 68[2] via P2P/IPC
g04n17:914194:914429 [1] NCCL INFO Channel 02/0 : 67[1] -> 68[2] via P2P/IPC
g04n17:914194:914429 [1] NCCL INFO Channel 01/0 : 67[1] -> 66[0] via P2P/IPC
g04n17:914194:914429 [1] NCCL INFO Channel 03/0 : 67[1] -> 66[0] via P2P/IPC
g04n17:914194:914429 [1] NCCL INFO Connected all rings
g04n17:914194:914429 [1] NCCL INFO Channel 01/0 : 67[1] -> 71[5] via P2P/IPC
g04n17:914194:914429 [1] NCCL INFO Channel 03/0 : 67[1] -> 71[5] via P2P/IPC
g04n17:914194:914429 [1] NCCL INFO Channel 02/0 : 67[1] -> 78[0] [send] via NET/IB/0
g04n17:914194:914429 [1] NCCL INFO Channel 02/0 : 78[0] -> 67[1] [receive] via NET/IB/0
g04n17:914194:914429 [1] NCCL INFO Channel 00/0 : 67[1] -> 66[0] via P2P/IPC
g04n17:914194:914429 [1] NCCL INFO Channel 02/0 : 67[1] -> 66[0] via P2P/IPC
g04n17:914194:914429 [1] NCCL INFO Connected all trees
g04n17:914194:914429 [1] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
g04n17:914194:914429 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
g04n17:914194:914429 [1] NCCL INFO comm 0x137974090 rank 67 nranks 96 cudaDev 1 nvmlDev 1 busId 405000 commId 0x7b12926388d12ce3 - Init COMPLETE
g04n17:914194:914517 [1] NCCL INFO Using network IB
g04n17:914194:914517 [1] NCCL INFO comm 0x13a26cd40 rank 8 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x1defb2708b178c98 - Init START
g04n17:914194:914517 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g04n17:914194:914517 [1] NCCL INFO Trees [0] 4/10/-1->8->0 [1] -1/-1/-1->8->9
g04n17:914194:914517 [1] NCCL INFO P2P Chunksize set to 131072
g04n17:914194:914517 [1] NCCL INFO Channel 00/0 : 7[5] -> 8[1] [receive] via NET/IB/2
g04n17:914194:914517 [1] NCCL INFO Channel 01/0 : 7[5] -> 8[1] [receive] via NET/IB/2
g04n17:914194:914517 [1] NCCL INFO Channel 00/0 : 8[1] -> 9[3] [send] via NET/IB/2
g04n17:914194:914517 [1] NCCL INFO Channel 01/0 : 8[1] -> 9[3] [send] via NET/IB/2
g04n17:914194:914517 [1] NCCL INFO Connected all rings
g04n17:914194:914517 [1] NCCL INFO Channel 00/0 : 8[1] -> 10[5] [send] via NET/IB/2
g04n17:914194:914517 [1] NCCL INFO Channel 00/0 : 4[5] -> 8[1] [receive] via NET/IB/2
g04n17:914194:914517 [1] NCCL INFO Channel 00/0 : 8[1] -> 0[3] [send] via NET/IB/2
g04n17:914194:914517 [1] NCCL INFO Channel 00/0 : 0[3] -> 8[1] [receive] via NET/IB/2
g04n17:914194:914517 [1] NCCL INFO Channel 00/0 : 8[1] -> 4[5] [send] via NET/IB/2
g04n17:914194:914517 [1] NCCL INFO Channel 00/0 : 10[5] -> 8[1] [receive] via NET/IB/2
g04n17:914194:914517 [1] NCCL INFO Channel 01/0 : 9[3] -> 8[1] [receive] via NET/IB/2
g04n17:914194:914517 [1] NCCL INFO Connected all trees
g04n17:914194:914517 [1] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
g04n17:914194:914517 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g04n17:914194:914517 [1] NCCL INFO comm 0x13a26cd40 rank 8 nranks 12 cudaDev 1 nvmlDev 1 busId 405000 commId 0x1defb2708b178c98 - Init COMPLETE
g04n17:914194:914535 [1] NCCL INFO Using network IB
g04n17:914194:914535 [1] NCCL INFO comm 0x13a683900 rank 16 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0x821cecfd780eaba1 - Init START
g04n17:914194:914535 [1] NCCL INFO Setting affinity for GPU 1 to f0000000
g04n17:914194:914535 [1] NCCL INFO Trees [0] 17/-1/-1->16->15 [1] 17/13/-1->16->11
g04n17:914194:914535 [1] NCCL INFO P2P Chunksize set to 131072
g04n17:914194:914535 [1] NCCL INFO Channel 00/0 : 15[3] -> 16[1] [receive] via NET/IB/2
g04n17:914194:914535 [1] NCCL INFO Channel 01/0 : 15[3] -> 16[1] [receive] via NET/IB/2
g04n17:914194:914535 [1] NCCL INFO Channel 00/0 : 16[1] -> 17[5] via P2P/IPC
g04n17:914194:914535 [1] NCCL INFO Channel 01/0 : 16[1] -> 17[5] via P2P/IPC
g04n17:914194:914535 [1] NCCL INFO Connected all rings
g04n17:914194:914535 [1] NCCL INFO Channel 01/0 : 13[1] -> 16[1] [receive] via NET/IB/2
g04n17:914194:914535 [1] NCCL INFO Channel 01/0 : 11[5] -> 16[1] [receive] via NET/IB/2
g04n17:914194:914535 [1] NCCL INFO Channel 01/0 : 16[1] -> 11[5] [send] via NET/IB/2
g04n17:914194:914535 [1] NCCL INFO Channel 01/0 : 16[1] -> 13[1] [send] via NET/IB/2
g04n17:914194:914535 [1] NCCL INFO Channel 00/0 : 16[1] -> 15[3] [send] via NET/IB/2
g04n17:914194:914535 [1] NCCL INFO Connected all trees
g04n17:914194:914535 [1] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
g04n17:914194:914535 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
g04n17:914194:914535 [1] NCCL INFO comm 0x13a683900 rank 16 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0x821cecfd780eaba1 - Init COMPLETE

f17n01:969558:969900 [1] NCCL INFO Trees [0] 11/-1/-1->10->9 [1] 11/4/-1->10->22
f17n01:969558:969900 [1] NCCL INFO P2P Chunksize set to 131072
f17n01:969558:969900 [1] NCCL INFO Channel 00/0 : 9[3] -> 10[1] [receive] via NET/IB/2
f17n01:969558:969900 [1] NCCL INFO Channel 01/0 : 9[3] -> 10[1] [receive] via NET/IB/2
f17n01:969558:969900 [1] NCCL INFO Channel 00/0 : 10[1] -> 11[5] via P2P/IPC
f17n01:969558:969900 [1] NCCL INFO Channel 01/0 : 10[1] -> 11[5] via P2P/IPC
f17n01:969558:969900 [1] NCCL INFO Connected all rings
f17n01:969558:969900 [1] NCCL INFO Channel 01/0 : 4[1] -> 10[1] [receive] via NET/IB/2
f17n01:969558:969900 [1] NCCL INFO Channel 01/0 : 22[1] -> 10[1] [receive] via NET/IB/2
f17n01:969558:969900 [1] NCCL INFO Channel 01/0 : 10[1] -> 22[1] [send] via NET/IB/2
f17n01:969558:969900 [1] NCCL INFO Channel 01/0 : 10[1] -> 4[1] [send] via NET/IB/2
f17n01:969558:969900 [1] NCCL INFO Channel 00/0 : 10[1] -> 9[3] [send] via NET/IB/2
f17n01:969558:969900 [1] NCCL INFO Connected all trees
f17n01:969558:969900 [1] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
f17n01:969558:969900 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
f17n01:969558:969900 [1] NCCL INFO comm 0x17d124bc0 rank 10 nranks 24 cudaDev 1 nvmlDev 1 busId 405000 commId 0x821cecfd780eaba1 - Init COMPLETE
d09n07:1061887:1062185 [0] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_3:1/IB [2]mlx5_0:1/IB [3]mlx5_2:1/IB ; OOB ib0:10.41.8.171<0>
d09n07:1061887:1062185 [0] NCCL INFO Using network IB
d09n07:1061887:1062185 [0] NCCL INFO comm 0x13b184b00 rank 0 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x7b12926388d12ce3 - Init START
d09n07:1061887:1062185 [0] NCCL INFO Setting affinity for GPU 0 to 0f
d09n07:1061887:1062185 [0] NCCL INFO Channel 00/04 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
d09n07:1061887:1062185 [0] NCCL INFO Channel 01/04 :    0   9  10  11   8   7   6  15  16  17  14  13  12  21  22  23  20  19  18  27
d09n07:1061887:1062185 [0] NCCL INFO Channel 02/04 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
d09n07:1061887:1062185 [0] NCCL INFO Channel 03/04 :    0   9  10  11   8   7   6  15  16  17  14  13  12  21  22  23  20  19  18  27
d09n07:1061887:1062185 [0] NCCL INFO Trees [0] 1/48/-1->0->-1 [1] 2/-1/-1->0->1 [2] 1/-1/-1->0->6 [3] 2/-1/-1->0->1
d09n07:1061887:1062185 [0] NCCL INFO P2P Chunksize set to 131072
d09n07:1061887:1062185 [0] NCCL INFO Channel 00/0 : 95[5] -> 0[0] [receive] via NET/IB/0
d09n07:1061887:1062185 [0] NCCL INFO Channel 02/0 : 95[5] -> 0[0] [receive] via NET/IB/0
d09n07:1061887:1062185 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/IPC
d09n07:1061887:1062185 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/IPC
d09n07:1061887:1062185 [0] NCCL INFO Channel 01/0 : 0[0] -> 9[3] [send] via NET/IB/2
d09n07:1061887:1062185 [0] NCCL INFO Channel 03/0 : 0[0] -> 9[3] [send] via NET/IB/2
d09n07:1061887:1062185 [0] NCCL INFO Connected all rings
d09n07:1061887:1062185 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/IPC
d09n07:1061887:1062185 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/IPC
d09n07:1061887:1062185 [0] NCCL INFO Channel 01/0 : 0[0] -> 2[2] via P2P/IPC
d09n07:1061887:1062185 [0] NCCL INFO Channel 03/0 : 0[0] -> 2[2] via P2P/IPC
d09n07:1061887:1062185 [0] NCCL INFO Channel 02/0 : 0[0] -> 6[0] [send] via NET/IB/0
d09n07:1061887:1062185 [0] NCCL INFO Channel 00/0 : 48[0] -> 0[0] [receive] via NET/IB/0
d09n07:1061887:1062185 [0] NCCL INFO Channel 00/0 : 0[0] -> 48[0] [send] via NET/IB/0
d09n07:1061887:1062185 [0] NCCL INFO Channel 02/0 : 6[0] -> 0[0] [receive] via NET/IB/0
d09n07:1061887:1062185 [0] NCCL INFO Connected all trees
d09n07:1061887:1062185 [0] NCCL INFO threadThresholds 8/8/64 | 768/8/64 | 512 | 512
d09n07:1061887:1062185 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
d09n07:1061887:1062185 [0] NCCL INFO comm 0x13b184b00 rank 0 nranks 96 cudaDev 0 nvmlDev 0 busId 404000 commId 0x7b12926388d12ce3 - Init COMPLETE
d09n07:1061887:1062295 [0] NCCL INFO Using network IB
d09n07:1061887:1062295 [0] NCCL INFO comm 0x13debe2c0 rank 0 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x8264b1a1a7aef6de - Init START
d09n07:1061887:1062295 [0] NCCL INFO Setting affinity for GPU 0 to 0f
d09n07:1061887:1062295 [0] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7   8   9  10  11
d09n07:1061887:1062295 [0] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7   8   9  10  11
d09n07:1061887:1062295 [0] NCCL INFO Trees [0] 8/-1/-1->0->-1 [1] -1/-1/-1->0->1
d09n07:1061887:1062295 [0] NCCL INFO P2P Chunksize set to 131072
d09n07:1061887:1062295 [0] NCCL INFO Channel 00/0 : 11[4] -> 0[0] [receive] via NET/IB/0
d09n07:1061887:1062295 [0] NCCL INFO Channel 01/0 : 11[4] -> 0[0] [receive] via NET/IB/0
d09n07:1061887:1062295 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[2] [send] via NET/IB/0
d09n07:1061887:1062295 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[2] [send] via NET/IB/0
d09n07:1061887:1062295 [0] NCCL INFO Connected all rings
d09n07:1061887:1062295 [0] NCCL INFO Channel 00/0 : 8[4] -> 0[0] [receive] via NET/IB/0
d09n07:1061887:1062295 [0] NCCL INFO Channel 00/0 : 0[0] -> 8[4] [send] via NET/IB/0
d09n07:1061887:1062295 [0] NCCL INFO Channel 01/0 : 1[2] -> 0[0] [receive] via NET/IB/0
d09n07:1061887:1062295 [0] NCCL INFO Connected all trees
d09n07:1061887:1062295 [0] NCCL INFO threadThresholds 8/8/64 | 96/8/64 | 512 | 512
d09n07:1061887:1062295 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d09n07:1061887:1062295 [0] NCCL INFO comm 0x13debe2c0 rank 0 nranks 12 cudaDev 0 nvmlDev 0 busId 404000 commId 0x8264b1a1a7aef6de - Init COMPLETE
d09n07:1061887:1062318 [0] NCCL INFO Using network IB
d09n07:1061887:1062318 [0] NCCL INFO comm 0x13e012520 rank 0 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0xaedb5f80914767fd - Init START
d09n07:1061887:1062318 [0] NCCL INFO Setting affinity for GPU 0 to 0f
d09n07:1061887:1062318 [0] NCCL INFO Channel 00/02 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
d09n07:1061887:1062318 [0] NCCL INFO Channel 01/02 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
d09n07:1061887:1062318 [0] NCCL INFO Trees [0] 1/12/-1->0->-1 [1] 1/-1/-1->0->2
d09n07:1061887:1062318 [0] NCCL INFO P2P Chunksize set to 131072
d09n07:1061887:1062318 [0] NCCL INFO Channel 00/0 : 23[2] -> 0[0] [receive] via NET/IB/0
d09n07:1061887:1062318 [0] NCCL INFO Channel 01/0 : 23[2] -> 0[0] [receive] via NET/IB/0
d09n07:1061887:1062318 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[4] via P2P/IPC
d09n07:1061887:1062318 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[4] via P2P/IPC
d09n07:1061887:1062318 [0] NCCL INFO Connected all rings
d09n07:1061887:1062318 [0] NCCL INFO Channel 01/0 : 0[0] -> 2[2] [send] via NET/IB/0
d09n07:1061887:1062318 [0] NCCL INFO Channel 00/0 : 12[0] -> 0[0] [receive] via NET/IB/0
d09n07:1061887:1062318 [0] NCCL INFO Channel 00/0 : 0[0] -> 12[0] [send] via NET/IB/0
d09n07:1061887:1062318 [0] NCCL INFO Channel 01/0 : 2[2] -> 0[0] [receive] via NET/IB/0
d09n07:1061887:1062318 [0] NCCL INFO Connected all trees
d09n07:1061887:1062318 [0] NCCL INFO threadThresholds 8/8/64 | 192/8/64 | 512 | 512
d09n07:1061887:1062318 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
d09n07:1061887:1062318 [0] NCCL INFO comm 0x13e012520 rank 0 nranks 24 cudaDev 0 nvmlDev 0 busId 404000 commId 0xaedb5f80914767fd - Init COMPLETE
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=86, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=14, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=81, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=89, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_inArgs= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=64, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel__pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=11, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=61, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=78, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=35, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=26, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
d11n16:1129247:1129567 [2] NCCL INFO [Service thread] Connection closed by localRank 0
g05n09:221464:221789 [2] NCCL INFO [Service thread] Connection closed by localRank 0
d09n07:1061891:1062296 [4] NCCL INFO [Service thread] Connection closed by localRank 0
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=12, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=94, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=57, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=20, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=87, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=68, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
g05n09:221469:221787 [5] NCCL INFO [Service thread] Connection closed by localRank 0
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=31, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=75, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=45, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
g05n08:178242:178564 [3] NCCL INFO [Service thread] Connection closed by localRank 0
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_inArgs= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=48, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel__pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=50, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=55, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=42, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=22, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=19, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=70, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=51, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
f18n06:1026685:1027007 [4] NCCL INFO [Service thread] Connection closed by localRank 0
d09n08:1072937:1073261 [5] NCCL INFO [Service thread] Connection closed by localRank 0
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=38, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_inArgs= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=10, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
_pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=73, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
d09n08:1072932:1073256 [0] NCCL INFO [Service thread] Connection closed by localRank 0
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=79, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=59, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=28, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=80, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=15, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
f18n06:1026679:1027013 [1] NCCL INFO [Service thread] Connection closed by localRank 0
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_inArgs= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=30, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_insize=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
_pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=93, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel__pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=95, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=62, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
f16n17:1090242:1090571 [5] NCCL INFO [Service thread] Connection closed by localRank 0
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=37, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=85, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=67, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_inArgs= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=43, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel__pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=46, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=29, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
g05n08:178236:178567 [0] NCCL INFO [Service thread] Connection closed by localRank 0
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=33, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=74, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
f16n16:1079195:1079518 [2] NCCL INFO [Service thread] Connection closed by localRank 0
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=69, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=60, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=92, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=56, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=49, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
f18n05:1007502:1007821 [3] NCCL INFO [Service thread] Connection closed by localRank 0
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=18, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_d11n16:1129245:1129563 [0] NCCL INFO [Service thread] Connection closed by localRank 0
size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
d11n17:1141834:1142153 [2] NCCL INFO [Service thread] Connection closed by localRank 0
g04n17:914196:914520 [2] NCCL INFO [Service thread] Connection closed by localRank 0
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=47, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=84, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=83, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=53, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_inArgs= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=13, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel__pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=17, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=32, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
No existing process group found, creating a new group named: ep_size_4
g05n10:696650:696968 [4] NCCL INFO [Service thread] Connection closed by localRank 0
d09n07:1061887:1062297 [0] NCCL INFO [Service thread] Connection closed by localRank 0
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=9, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_inArgs= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=24, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel__pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=25, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=36, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=71, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=41, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=1, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=91, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=88, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=82, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
g05n09:221467:221790 [3] NCCL INFO [Service thread] Connection closed by localRank 0
f17n02:1075533:1075889 [2] NCCL INFO [Service thread] Connection closed by localRank 0
g05n09:221464:221464 [2] NCCL INFO comm 0x14a5d9ea0 rank 10 nranks 12 cudaDev 2 busId 406000 - Abort COMPLETE
g04n18:916027:916356 [3] NCCL INFO [Service thread] Connection closed by localRank 0
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=21, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=16, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=44, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
g05n09:221464:221804 [2] NCCL INFO [Service thread] Connection closed by localRank 0
d11n16:1129247:1129247 [2] NCCL INFO comm 0x14aff6a00 rank 1 nranks 12 cudaDev 2 busId 406000 - Abort COMPLETE
f18n05:1007498:1007825 [1] NCCL INFO [Service thread] Connection closed by localRank 0
f16n17:1090235:1090568 [1] NCCL INFO [Service thread] Connection closed by localRank 0
d09n07:1061889:1062301 [2] NCCL INFO [Service thread] Connection closed by localRank 0
f17n02:1075530:1075886 [0] NCCL INFO [Service thread] Connection closed by localRank 0
f17n01:969561:969887 [3] NCCL INFO [Service thread] Connection closed by localRank 0
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=27, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_d11n17:1141836:1142156 [4] NCCL INFO [Service thread] Connection closed by localRank 0
size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=2, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=8, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
f17n01:969557:969884 [0] NCCL INFO [Service thread] Connection closed by localRank 0
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_inArgs= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=54, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel__pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=58, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_inArgs= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=63, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
_pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=65, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=34, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=90, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
d09n07:1061891:1061891 [4] NCCL INFO comm 0x12597cab0 rank 0 nranks 12 cudaDev 4 busId 3504000 - Abort COMPLETE
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=52, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=72, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=3, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=39, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=66, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=23, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
f17n02:1075534:1075893 [3] NCCL INFO [Service thread] Connection closed by localRank 0
g04n17:914200:914524 [4] NCCL INFO [Service thread] Connection closed by localRank 0
g05n08:178242:178242 [3] NCCL INFO comm 0x162250030 rank 10 nranks 12 cudaDev 3 busId 3503000 - Abort COMPLETE
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_inArgs= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=76, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel__pipeline=True, use_ring_exchange_p2p=False, local_rank=5, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=77, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
g05n09:221469:221469 [5] NCCL INFO comm 0x16e2827e0 rank 11 nranks 12 cudaDev 5 busId 3505000 - Abort COMPLETE
d11n16:1129247:1129583 [2] NCCL INFO [Service thread] Connection closed by localRank 0
g05n09:221469:221810 [5] NCCL INFO [Service thread] Connection closed by localRank 1
f16n18:1091133:1091472 [2] NCCL INFO [Service thread] Connection closed by localRank 0
g04n18:916022:916348 [1] NCCL INFO [Service thread] Connection closed by localRank 0
f18n05:1007505:1007828 [5] NCCL INFO [Service thread] Connection closed by localRank 0
f16n16:1079199:1079514 [4] NCCL INFO [Service thread] Connection closed by localRank 0
g05n08:178239:178563 [2] NCCL INFO [Service thread] Connection closed by localRank 0
d09n07:1061891:1062325 [4] NCCL INFO [Service thread] Connection closed by localRank 1
g05n08:178242:178588 [3] NCCL INFO [Service thread] Connection closed by localRank 0
d11n17:1141833:1142159 [1] NCCL INFO [Service thread] Connection closed by localRank 0
f18n06:1026685:1026685 [4] NCCL INFO comm 0x160384030 rank 8 nranks 12 cudaDev 4 busId 3504000 - Abort COMPLETE
d09n08:1072937:1072937 [5] NCCL INFO comm 0x15c6cc6f0 rank 1 nranks 12 cudaDev 5 busId 3505000 - Abort COMPLETE
d09n08:1072936:1073255 [4] NCCL INFO [Service thread] Connection closed by localRank 0
Args= Namespace(num_layers=32, encoder_num_layers=32, decoder_num_layers=None, num_experts=[4], mlp_type='standard', topk=1, expert_interval=2, hidden_size=4096, ffn_hidden_size=16384, num_attention_heads=32, num_key_value_heads=32, kv_channels=128, max_position_embeddings=2048, use_rotary_position_embeddings=False, rope_theta=10000, rotary_percent=1.0, add_position_embedding=True, make_vocab_size_divisible_by=128, normalization='layernorm', layernorm_epsilon=1e-05, apply_layernorm_1p=False, mem_efficient_ln=True, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=False, onnx_safe=None, bert_binary_head=True, num_experts_switch=None, untie_embeddings_and_output_weights=False, embedding_weights_in_fp32=False, attention_dropout=0.1, hidden_dropout=0.1, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=24, rampup_batch_size=None, recompute_granularity=None, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=1, checkpoint_activations=True, distribute_checkpointed_activations=False, checkpoint_num_layers=1, train_iters=18310546, train_samples=None, train_tokens=300000000000, random_ltd=False, log_interval=10, exit_interval=None, exit_duration_in_mins=30000000, exit_signal_handler=False, tensorboard_dir='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/tensorboard/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true_batch5_2024.03.04-16.42.17', masked_softmax_fusion=True, bias_gelu_fusion=True, bias_dropout_fusion=True, moe_token_dropping=True, moe_train_capacity_factor=1.0, moe_eval_capacity_factor=1.0, moe_min_capacity=4, moe_loss_coeff=0.01, create_moe_param_group=True, use_flash_attn_v1=False, use_flash_attn_v2=False, use_flash_attn_triton=False, add_bias_linear=True, optimizer='adam', dataloader_type='single', ds_inference=False, cpu_optimizer=False, cpu_torch_adam=False, ds_fused_adam=False, no_pipeline_parallel=True, use_tutel=False, inference=False, async_tensor_model_parallel_allreduce=False, no_persist_layer_norm=False, sequence_parallel=False, ds_sequence_parallel_size=1, force_ds_sequence_parallel=False, gradient_accumulation_fusion=True, use_dataset_only=False, seed=1234, data_parallel_random_init=False, init_method_std=0.014, init_method_xavier_uniform=False, lr=0.00012, lr_decay_style='cosine', lr_decay_iters=None, lr_decay_samples=None, lr_decay_tokens=300000000000, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_tokens=375000000, min_lr=1e-06, override_opt_param_scheduler=True, use_checkpoint_opt_param_scheduler=False, save='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', save_interval=10000, no_save_optim=None, no_save_rng=None, load='/gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed/output/checkpoint/gpt-6.7B-lr-1.2e-4-minlr-1.0e-6-bs-24-gpus-96-mp-8-pp-1-ep-4-mlc-0.01-cap-1.0-drop-true', load_tag=None, no_load_optim=None, no_load_rng=None, no_load_lr_state=False, finetune=False, perform_initialization=True, use_checkpoint_args=False, exit_on_missing_checkpoint=False, universal_checkpoint=False, fp16=True, bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=True, attention_softmax_in_fp32=False, accumulate_allreduce_grads_in_fp32=False, fp16_lm_cross_entropy=False, tensor_model_parallel_size=8, enable_expert_tensor_parallelism=False, pipeline_model_parallel_size=1, pipeline_model_parallel_split_rank=None, moe_expert_parallel_size=4, num_layers_per_virtual_pipeline_stage=None, overlap_p2p_comm=False, distributed_backend='nccl', distributed_timeout_minutes=10, DDP_impl='local', use_contiguous_buffers_in_local_ddp=True, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=4, lazy_mpu_init=None, use_cpu_initialization=None, empty_unused_memory_level=0, standalone_embedding_stage=False, use_distributed_optimizer=False, eval_iters=10, eval_interval=100, skip_train=False, aml_data_download_path=None, data_path=['/gpfs/alpine2/stf218/world-shared/sajal/gptdata/gpttext_article_document'], split='98,2,0', train_data_path=None, valid_data_path=None, test_data_path=None, data_cache_path=None, vocab_size=None, vocab_file='gpt2-vocab.json', merge_file='gpt2-merges.txt', vocab_extra_ids=0, seq_length=2048, encoder_seq_length=2048, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, mmap_warmup=False, num_workers=0, tokenizer_type='GPT2BPETokenizer', tokenizer_model=None, data_impl='mmap', reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, train_data_exact_num_epochs=None, return_data_index=False, data_efficiency_curriculum_learning=False, train_idx_path=None, train_desc_path=None, train_doc_idx_path=None, train_sample_idx_path=None, train_shuffle_idx_path=None, repeated_dataloader=False, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, log_params_norm=False, log_num_zeros_in_grad=False, timing_log_level=0, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=1, log_timers_to_tensorboard=True, log_batch_size_to_tensorboard=True, log_learning_rate_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_optimizer_states_to_tensorboard=False, log_memory_to_tensorboard=False, log_world_size_to_tensorboard=False, zero_stage=1.0, zero_reduce_scatter=False, zero_contigious_gradients=False, zero_reduce_bucket_size=0.0, zero_allgather_bucket_size=0.0, remote_device='none', use_pin_memory=False, scattered_embeddings=False, split_transformers=False, memory_centric_tiled_linear=False, tile_factor=1, deepspeed_activation_checkpointing=True, partition_activations=False, contigious_checkpointing=False, checkpoint_in_cpu=False, synchronize_each_layer=False, profile_backward=False, num_layers_teacher=None, num_experts_teacher=[1], hidden_size_teacher=None, num_attention_heads_teacher=None, mos=False, kd=False, kd_alpha_ce=1, kd_beta_ce=1, kd_temp=1.0, reset_iteration=False, load_teacher=None, inference_batch_times_seqlen_threshold=512, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', fp8_e4m3=False, fp8_hybrid=False, fp8_wgrad=True, fp8_margin=0, fp8_interval=1, transformer_impl='local', fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', retro_workdir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_return_doc_ids=False, deepspeed=True, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, ds_pipeline_enabled=False, rank=40, world_size=96, transformer_pipeline_model_parallel_size=1, data_parallel_size=12, virtual_pipeline_model_parallel_size=None, params_dtype=torch.float16, consumed_train_samples=0, consumed_valid_samples=0, consumed_train_tokens=0, variable_seq_lengths=False, curriculum_learning_legacy=False, compression_training=False, use_flash_attn=False, padded_vocab_size=51200, deepspeed_config_dict={'train_batch_size': 24, 'train_micro_batch_size_per_gpu': 1, 'steps_per_print': 10, 'zero_optimization': {'stage': 0}, 'gradient_clipping': 1.0, 'prescale_gradients': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 500, 'hysteresis': 2, 'min_loss_scale': 1, 'initial_scale_power': 11}, 'bf16': {'enabled': False}, 'curriculum_learning': {'enabled': False, 'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 2349624, 'difficulty_step': 8}}, 'wall_clock_breakdown': False})
No existing process group found, creating a new group named: ep_size_4
d11n16:1129248:1129568 [3] NCCL INFO [Service thread] Connection closed by localRank 0
f16n16:1079200:1079520 [5] NCCL INFO [Service thread] Connection closed by localRank 0
g05n09:221462:221793 [1] NCCL INFO [Service thread] Connection closed by localRank 0
g05n10:696649:696970 [3] NCCL INFO [Service thread] Connection closed by localRank 0
f18n06:1026681:1027010 [2] NCCL INFO [Service thread] Connection closed by localRank 0
f18n06:1026685:1027027 [4] NCCL INFO [Service thread] Connection closed by localRank 1
f16n17:1090239:1090565 [3] NCCL INFO [Service thread] Connection closed by localRank 0
g05n08:178237:178566 [1] NCCL INFO [Service thread] Connection closed by localRank 0
f16n17:1090234:1090567 [0] NCCL INFO [Service thread] Connection closed by localRank 0
d09n08:1072933:1073257 [1] NCCL INFO [Service thread] Connection closed by localRank 0
f18n06:1026679:1026679 [1] NCCL INFO comm 0x14bd74a90 rank 7 nranks 12 cudaDev 1 busId 405000 - Abort COMPLETE
d09n08:1072937:1073275 [5] NCCL INFO [Service thread] Connection closed by localRank 1
g04n17:914194:914526 [1] NCCL INFO [Service thread] Connection closed by localRank 0
f17n01:969564:969885 [4] NCCL INFO [Service thread] Connection closed by localRank 0
f16n18:1091131:1091474 [1] NCCL INFO [Service thread] Connection closed by localRank 0
d09n08:1072932:1072932 [0] NCCL INFO comm 0x1522f5a70 rank 0 nranks 12 cudaDev 0 busId 404000 - Abort COMPLETE
d09n08:1072932:1073272 [0] NCCL INFO [Service thread] Connection closed by localRank 0
d09n07:1061892:1062304 [5] NCCL INFO [Service thread] Connection closed by localRank 0
f17n02:1075538:1075891 [5] NCCL INFO [Service thread] Connection closed by localRank 0
f17n02:1075531:1075887 [1] NCCL INFO [Service thread] Connection closed by localRank 0
d11n16:1129246:1129571 [1] NCCL INFO [Service thread] Connection closed by localRank 0
g05n08:178236:178236 [0] NCCL INFO comm 0x1383ad730 rank 9 nranks 12 cudaDev 0 busId 404000 - Abort COMPLETE
f18n06:1026679:1027031 [1] NCCL INFO [Service thread] Connection closed by localRank 0
f16n17:1090242:1090242 [5] NCCL INFO comm 0x12c191760 rank 4 nranks 12 cudaDev 5 busId 3505000 - Abort COMPLETE
g04n18:916024:916351 [2] NCCL INFO [Service thread] Connection closed by localRank 0
g05n09:221461:221785 [0] NCCL INFO [Service thread] Connection closed by localRank 0
g05n09:221468:221786 [4] NCCL INFO [Service thread] Connection closed by localRank 0
g05n10:696646:696965 [2] NCCL INFO [Service thread] Connection closed by localRank 0
f18n05:1007500:1007822 [2] NCCL INFO [Service thread] Connection closed by localRank 0
f18n06:1026678:1027005 [0] NCCL INFO [Service thread] Connection closed by localRank 0
f16n17:1090237:1090564 [2] NCCL INFO [Service thread] Connection closed by localRank 0
f16n16:1079192:1079515 [0] NCCL INFO [Service thread] Connection closed by localRank 0
f16n16:1079195:1079195 [2] NCCL INFO comm 0x147d9c530 rank 3 nranks 12 cudaDev 2 busId 406000 - Abort COMPLETE
g05n08:178244:178571 [5] NCCL INFO [Service thread] Connection closed by localRank 0
f16n17:1090242:1090586 [5] NCCL INFO [Service thread] Connection closed by localRank 1
f17n01:969565:969886 [5] NCCL INFO [Service thread] Connection closed by localRank 0
d09n08:1072935:1073254 [3] NCCL INFO [Service thread] Connection closed by localRank 0
f17n01:969558:969888 [1] NCCL INFO [Service thread] Connection closed by localRank 0
g04n17:914201:914523 [5] NCCL INFO [Service thread] Connection closed by localRank 0
g04n17:914199:914525 [3] NCCL INFO [Service thread] Connection closed by localRank 0
f16n18:1091130:1091468 [0] NCCL INFO [Service thread] Connection closed by localRank 0
d11n17:1141832:1142155 [0] NCCL INFO [Service thread] Connection closed by localRank 0
d11n16:1129250:1129565 [5] NCCL INFO [Service thread] Connection closed by localRank 0
f16n18:1091138:1091470 [5] NCCL INFO [Service thread] Connection closed by localRank 0
g05n08:178243:178565 [4] NCCL INFO [Service thread] Connection closed by localRank 0
g05n08:178236:178581 [0] NCCL INFO [Service thread] Connection closed by localRank 0
d09n07:1061890:1062306 [3] NCCL INFO [Service thread] Connection closed by localRank 0
d09n07:1061888:1062298 [1] NCCL INFO [Service thread] Connection closed by localRank 0
d11n16:1129245:1129245 [0] NCCL INFO comm 0x13a25c2e0 rank 1 nranks 12 cudaDev 0 busId 404000 - Abort COMPLETE
g05n10:696650:696650 [4] NCCL INFO comm 0x153bf5b00 rank 11 nranks 12 cudaDev 4 busId 3504000 - Abort COMPLETE
f16n16:1079193:1079516 [1] NCCL INFO [Service thread] Connection closed by localRank 0
f16n16:1079197:1079522 [3] NCCL INFO [Service thread] Connection closed by localRank 0
f18n05:1007502:1007502 [3] NCCL INFO comm 0x14f899ac0 rank 7 nranks 12 cudaDev 3 busId 3503000 - Abort COMPLETE
f18n06:1026682:1027009 [3] NCCL INFO [Service thread] Connection closed by localRank 0
f18n06:1026686:1027006 [5] NCCL INFO [Service thread] Connection closed by localRank 0
f16n16:1079195:1079532 [2] NCCL INFO [Service thread] Connection closed by localRank 0
d11n17:1141834:1141834 [2] NCCL INFO comm 0x14bda1440 rank 2 nranks 12 cudaDev 2 busId 406000 - Abort COMPLETE
g05n10:696644:696972 [1] NCCL INFO [Service thread] Connection closed by localRank 0
d09n08:1072934:1073253 [2] NCCL INFO [Service thread] Connection closed by localRank 0
f18n05:1007502:1007848 [3] NCCL INFO [Service thread] Connection closed by localRank 0
g05n09:221467:221467 [3] NCCL INFO comm 0x1509e35b0 rank 10 nranks 12 cudaDev 3 busId 3503000 - Abort COMPLETE
f17n02:1075537:1075885 [4] NCCL INFO [Service thread] Connection closed by localRank 0
d11n16:1129249:1129564 [4] NCCL INFO [Service thread] Connection closed by localRank 0
g05n10:696651:696969 [5] NCCL INFO [Service thread] Connection closed by localRank 0
d11n16:1129245:1129588 [0] NCCL INFO [Service thread] Connection closed by localRank 0
d11n17:1141837:1142158 [5] NCCL INFO [Service thread] Connection closed by localRank 0
d11n17:1141835:1142157 [3] NCCL INFO [Service thread] Connection closed by localRank 0
d11n17:1141834:1142179 [2] NCCL INFO [Service thread] Connection closed by localRank 0
f17n01:969560:969882 [2] NCCL INFO [Service thread] Connection closed by localRank 0
g04n18:916021:916349 [0] NCCL INFO [Service thread] Connection closed by localRank 0
g04n18:916028:916347 [4] NCCL INFO [Service thread] Connection closed by localRank 0
f16n17:1090241:1090566 [4] NCCL INFO [Service thread] Connection closed by localRank 0
f18n05:1007497:1007824 [0] NCCL INFO [Service thread] Connection closed by localRank 0
f18n05:1007504:1007823 [4] NCCL INFO [Service thread] Connection closed by localRank 0
g04n17:914196:914196 [2] NCCL INFO comm 0x17bdc03c0 rank 8 nranks 12 cudaDev 2 busId 406000 - Abort COMPLETE
g05n10:696650:696985 [4] NCCL INFO [Service thread] Connection closed by localRank 1
g04n18:916029:916354 [5] NCCL INFO [Service thread] Connection closed by localRank 0
f16n17:1090235:1090235 [1] NCCL INFO comm 0x13fb11070 rank 3 nranks 12 cudaDev 1 busId 405000 - Abort COMPLETE
g04n18:916027:916027 [3] NCCL INFO comm 0x16a4f59a0 rank 9 nranks 12 cudaDev 3 busId 3503000 - Abort COMPLETE
d09n07:1061887:1061887 [0] NCCL INFO comm 0x13debe2c0 rank 0 nranks 12 cudaDev 0 busId 404000 - Abort COMPLETE
f17n01:969561:969561 [3] NCCL INFO comm 0x13288c260 rank 5 nranks 12 cudaDev 3 busId 3503000 - Abort COMPLETE
f17n02:1075533:1075533 [2] NCCL INFO comm 0x138ff89c0 rank 6 nranks 12 cudaDev 2 busId 406000 - Abort COMPLETE
g04n17:914193:914522 [0] NCCL INFO [Service thread] Connection closed by localRank 0
f16n18:1091134:1091473 [3] NCCL INFO [Service thread] Connection closed by localRank 0
g05n10:696643:696966 [0] NCCL INFO [Service thread] Connection closed by localRank 0
g05n09:221467:221806 [3] NCCL INFO [Service thread] Connection closed by localRank 0
f17n02:1075530:1075530 [0] NCCL INFO comm 0x1447df0e0 rank 6 nranks 12 cudaDev 0 busId 404000 - Abort COMPLETE
d09n07:1061889:1061889 [2] NCCL INFO comm 0x131e41f60 rank 0 nranks 12 cudaDev 2 busId 406000 - Abort COMPLETE
f17n02:1075533:1075904 [2] NCCL INFO [Service thread] Connection closed by localRank 0
f18n05:1007498:1007498 [1] NCCL INFO comm 0x14a2d4c40 rank 6 nranks 12 cudaDev 1 busId 405000 - Abort COMPLETE
g04n18:916027:916367 [3] NCCL INFO [Service thread] Connection closed by localRank 0
f18n05:1007498:1007844 [1] NCCL INFO [Service thread] Connection closed by localRank 0
f16n17:1090235:1090585 [1] NCCL INFO [Service thread] Connection closed by localRank 0
d09n07:1061889:1062321 [2] NCCL INFO [Service thread] Connection closed by localRank 0
f17n02:1075530:1075908 [0] NCCL INFO [Service thread] Connection closed by localRank 0
f16n18:1091137:1091469 [4] NCCL INFO [Service thread] Connection closed by localRank 0
g05n09:221464:221464 [2] NCCL INFO comm 0x14a246120 rank 21 nranks 24 cudaDev 2 busId 406000 - Abort COMPLETE
g04n17:914196:914542 [2] NCCL INFO [Service thread] Connection closed by localRank 0
f17n01:969561:969910 [3] NCCL INFO [Service thread] Connection closed by localRank 0
f17n01:969557:969557 [0] NCCL INFO comm 0x144868520 rank 5 nranks 12 cudaDev 0 busId 404000 - Abort COMPLETE
d11n16:1129247:1129247 [2] NCCL INFO comm 0x14b1c6620 rank 3 nranks 24 cudaDev 2 busId 406000 - Abort COMPLETE
d11n17:1141836:1141836 [4] NCCL INFO comm 0x1647c7080 rank 2 nranks 12 cudaDev 4 busId 3504000 - Abort COMPLETE
d09n07:1061891:1061891 [4] NCCL INFO comm 0x1258ed0e0 rank 1 nranks 24 cudaDev 4 busId 3504000 - Abort COMPLETE
d09n07:1061887:1062326 [0] NCCL INFO [Service thread] Connection closed by localRank 0
g04n17:914200:914200 [4] NCCL INFO comm 0x150914dc0 rank 8 nranks 12 cudaDev 4 busId 3504000 - Abort COMPLETE
f17n02:1075534:1075534 [3] NCCL INFO comm 0x152087bb0 rank 6 nranks 12 cudaDev 3 busId 3503000 - Abort COMPLETE
d11n17:1141833:1141833 [1] NCCL INFO comm 0x144126e60 rank 2 nranks 12 cudaDev 1 busId 405000 - Abort COMPLETE
d09n08:1072936:1072936 [4] NCCL INFO comm 0x144d2ae50 rank 1 nranks 12 cudaDev 4 busId 3504000 - Abort COMPLETE
f17n02:1075534:1075906 [3] NCCL INFO [Service thread] Connection closed by localRank 0
g04n17:914200:914538 [4] NCCL INFO [Service thread] Connection closed by localRank 1
g04n18:916022:916022 [1] NCCL INFO comm 0x125f47ff0 rank 9 nranks 12 cudaDev 1 busId 405000 - Abort COMPLETE
d11n16:1129247:1129512 [2] NCCL INFO [Service thread] Connection closed by localRank 2
g05n09:221469:221469 [5] NCCL INFO comm 0x16d5622b0 rank 22 nranks 24 cudaDev 5 busId 3505000 - Abort COMPLETE
g04n18:916022:916370 [1] NCCL INFO [Service thread] Connection closed by localRank 0
g05n08:178242:178242 [3] NCCL INFO comm 0x16221caa0 rank 20 nranks 24 cudaDev 3 busId 3503000 - Abort COMPLETE
f16n18:1091133:1091133 [2] NCCL INFO comm 0x15f7ab310 rank 4 nranks 12 cudaDev 2 busId 406000 - Abort COMPLETE
g05n09:221464:221735 [2] NCCL INFO [Service thread] Connection closed by localRank 2
d09n07:1061891:1062226 [4] NCCL INFO [Service thread] Connection closed by localRank 4
g05n08:178242:178504 [3] NCCL INFO [Service thread] Connection closed by localRank 3
d11n17:1141833:1142176 [1] NCCL INFO [Service thread] Connection closed by localRank 0
d11n17:1141836:1142172 [4] NCCL INFO [Service thread] Connection closed by localRank 1
f17n01:969557:969903 [0] NCCL INFO [Service thread] Connection closed by localRank 0
f18n05:1007505:1007505 [5] NCCL INFO comm 0x13be61860 rank 7 nranks 12 cudaDev 5 busId 3505000 - Abort COMPLETE
f16n16:1079199:1079199 [4] NCCL INFO comm 0x1485a83a0 rank 3 nranks 12 cudaDev 4 busId 3504000 - Abort COMPLETE
g05n08:178239:178239 [2] NCCL INFO comm 0x1709a2e20 rank 10 nranks 12 cudaDev 2 busId 406000 - Abort COMPLETE
d11n16:1129248:1129248 [3] NCCL INFO comm 0x136c42520 rank 1 nranks 12 cudaDev 3 busId 3503000 - Abort COMPLETE
g05n10:696649:696649 [3] NCCL INFO comm 0x14a50b860 rank 11 nranks 12 cudaDev 3 busId 3503000 - Abort COMPLETE
d09n08:1072936:1073271 [4] NCCL INFO [Service thread] Connection closed by localRank 1
f18n06:1026681:1026681 [2] NCCL INFO comm 0x140b8c520 rank 7 nranks 12 cudaDev 2 busId 406000 - Abort COMPLETE
d09n08:1072937:1072937 [5] NCCL INFO comm 0x15d391de0 rank 2 nranks 24 cudaDev 5 busId 3505000 - Abort COMPLETE
f18n06:1026685:1026685 [4] NCCL INFO comm 0x15f29eef0 rank 16 nranks 24 cudaDev 4 busId 3504000 - Abort COMPLETE
f16n17:1090234:1090234 [0] NCCL INFO comm 0x154c5d370 rank 3 nranks 12 cudaDev 0 busId 404000 - Abort COMPLETE
d11n16:1129248:1129585 [3] NCCL INFO [Service thread] Connection closed by localRank 0
g05n09:221462:221462 [1] NCCL INFO comm 0x1259116f0 rank 10 nranks 12 cudaDev 1 busId 405000 - Abort COMPLETE
f16n18:1091131:1091131 [1] NCCL INFO comm 0x146ec7ac0 rank 4 nranks 12 cudaDev 1 busId 405000 - Abort COMPLETE
d09n07:1061892:1061892 [5] NCCL INFO comm 0x13280c630 rank 0 nranks 12 cudaDev 5 busId 3505000 - Abort COMPLETE
g04n17:914194:914194 [1] NCCL INFO comm 0x13a26cd40 rank 8 nranks 12 cudaDev 1 busId 405000 - Abort COMPLETE
g05n09:221469:221733 [5] NCCL INFO [Service thread] Connection closed by localRank 5
g05n09:221462:221811 [1] NCCL INFO [Service thread] Connection closed by localRank 0
f16n18:1091133:1091486 [2] NCCL INFO [Service thread] Connection closed by localRank 0
f17n01:969564:969564 [4] NCCL INFO comm 0x14922b8b0 rank 5 nranks 12 cudaDev 4 busId 3504000 - Abort COMPLETE
f16n16:1079200:1079200 [5] NCCL INFO comm 0x132be7ac0 rank 3 nranks 12 cudaDev 5 busId 3505000 - Abort COMPLETE
f16n17:1090239:1090239 [3] NCCL INFO comm 0x168322d40 rank 4 nranks 12 cudaDev 3 busId 3503000 - Abort COMPLETE
d09n08:1072933:1072933 [1] NCCL INFO comm 0x15d82db20 rank 0 nranks 12 cudaDev 1 busId 405000 - Abort COMPLETE
g05n10:696649:696993 [3] NCCL INFO [Service thread] Connection closed by localRank 0
f18n05:1007505:1007845 [5] NCCL INFO [Service thread] Connection closed by localRank 1
f18n06:1026681:1027024 [2] NCCL INFO [Service thread] Connection closed by localRank 0
f18n06:1026685:1026947 [4] NCCL INFO [Service thread] Connection closed by localRank 4
f16n16:1079199:1079535 [4] NCCL INFO [Service thread] Connection closed by localRank 1
g05n08:178239:178587 [2] NCCL INFO [Service thread] Connection closed by localRank 0
f16n17:1090239:1090589 [3] NCCL INFO [Service thread] Connection closed by localRank 0
f16n17:1090234:1090583 [0] NCCL INFO [Service thread] Connection closed by localRank 0
d09n08:1072933:1073276 [1] NCCL INFO [Service thread] Connection closed by localRank 0
d09n08:1072937:1073197 [5] NCCL INFO [Service thread] Connection closed by localRank 5
g04n17:914194:914541 [1] NCCL INFO [Service thread] Connection closed by localRank 0
g05n08:178237:178237 [1] NCCL INFO comm 0x116782d40 rank 9 nranks 12 cudaDev 1 busId 405000 - Abort COMPLETE
f17n01:969564:969902 [4] NCCL INFO [Service thread] Connection closed by localRank 1
f16n18:1091131:1091492 [1] NCCL INFO [Service thread] Connection closed by localRank 0
f18n06:1026679:1026679 [1] NCCL INFO comm 0x14cbbd660 rank 15 nranks 24 cudaDev 1 busId 405000 - Abort COMPLETE
g04n18:916024:916024 [2] NCCL INFO comm 0x174437430 rank 9 nranks 12 cudaDev 2 busId 406000 - Abort COMPLETE
f17n02:1075531:1075531 [1] NCCL INFO comm 0x16eafcef0 rank 6 nranks 12 cudaDev 1 busId 405000 - Abort COMPLETE
d09n07:1061892:1062328 [5] NCCL INFO [Service thread] Connection closed by localRank 1
d09n08:1072932:1072932 [0] NCCL INFO comm 0x15117fa00 rank 1 nranks 24 cudaDev 0 busId 404000 - Abort COMPLETE
g05n10:696646:696646 [2] NCCL INFO comm 0x14251b000 rank 11 nranks 12 cudaDev 2 busId 406000 - Abort COMPLETE
f18n06:1026678:1026678 [0] NCCL INFO comm 0x15a097de0 rank 7 nranks 12 cudaDev 0 busId 404000 - Abort COMPLETE
g04n17:914199:914199 [3] NCCL INFO comm 0x15cd6da60 rank 8 nranks 12 cudaDev 3 busId 3503000 - Abort COMPLETE
f17n02:1075531:1075911 [1] NCCL INFO [Service thread] Connection closed by localRank 0
f18n05:1007500:1007500 [2] NCCL INFO comm 0x13ba89730 rank 7 nranks 12 cudaDev 2 busId 406000 - Abort COMPLETE
f17n01:969565:969565 [5] NCCL INFO comm 0x1718aabf0 rank 5 nranks 12 cudaDev 5 busId 3505000 - Abort COMPLETE
d11n17:1141832:1141832 [0] NCCL INFO comm 0x16283adc0 rank 2 nranks 12 cudaDev 0 busId 404000 - Abort COMPLETE
f16n16:1079200:1079538 [5] NCCL INFO [Service thread] Connection closed by localRank 1
f17n01:969558:969558 [1] NCCL INFO comm 0x17dfcc480 rank 5 nranks 12 cudaDev 1 busId 405000 - Abort COMPLETE
g05n08:178244:178244 [5] NCCL INFO comm 0x15dd995e0 rank 10 nranks 12 cudaDev 5 busId 3505000 - Abort COMPLETE
g05n09:221461:221461 [0] NCCL INFO comm 0x11cb8c900 rank 10 nranks 12 cudaDev 0 busId 404000 - Abort COMPLETE
d11n16:1129246:1129246 [1] NCCL INFO comm 0x14491e670 rank 1 nranks 12 cudaDev 1 busId 405000 - Abort COMPLETE
f17n02:1075538:1075538 [5] NCCL INFO comm 0x1773e8640 rank 6 nranks 12 cudaDev 5 busId 3505000 - Abort COMPLETE
f18n06:1026679:1026950 [1] NCCL INFO [Service thread] Connection closed by localRank 1
g05n08:178236:178236 [0] NCCL INFO comm 0x1386ed330 rank 19 nranks 24 cudaDev 0 busId 404000 - Abort COMPLETE
f16n17:1090237:1090237 [2] NCCL INFO comm 0x16d9f09e0 rank 4 nranks 12 cudaDev 2 busId 406000 - Abort COMPLETE
d11n16:1129250:1129250 [5] NCCL INFO comm 0x15490d120 rank 2 nranks 12 cudaDev 5 busId 3505000 - Abort COMPLETE
d09n07:1061890:1061890 [3] NCCL INFO comm 0x1779ade60 rank 0 nranks 12 cudaDev 3 busId 3503000 - Abort COMPLETE
g04n18:916024:916365 [2] NCCL INFO [Service thread] Connection closed by localRank 0
f16n17:1090242:1090242 [5] NCCL INFO comm 0x12acdc190 rank 8 nranks 24 cudaDev 5 busId 3505000 - Abort COMPLETE
d09n08:1072935:1072935 [3] NCCL INFO comm 0x14501f6f0 rank 1 nranks 12 cudaDev 3 busId 3503000 - Abort COMPLETE
g05n09:221461:221807 [0] NCCL INFO [Service thread] Connection closed by localRank 0
g05n10:696646:696992 [2] NCCL INFO [Service thread] Connection closed by localRank 0
f18n05:1007500:1007846 [2] NCCL INFO [Service thread] Connection closed by localRank 0
f18n06:1026678:1027028 [0] NCCL INFO [Service thread] Connection closed by localRank 0
f16n17:1090237:1090588 [2] NCCL INFO [Service thread] Connection closed by localRank 0
f16n16:1079193:1079193 [1] NCCL INFO comm 0x14d0db560 rank 3 nranks 12 cudaDev 1 busId 405000 - Abort COMPLETE
g05n08:178237:178586 [1] NCCL INFO [Service thread] Connection closed by localRank 0
g05n09:221468:221468 [4] NCCL INFO comm 0x1804c6b20 rank 11 nranks 12 cudaDev 4 busId 3504000 - Abort COMPLETE
g05n08:178244:178585 [5] NCCL INFO [Service thread] Connection closed by localRank 1
f16n17:1090242:1090505 [5] NCCL INFO [Service thread] Connection closed by localRank 5
f17n01:969565:969906 [5] NCCL INFO [Service thread] Connection closed by localRank 1
d09n08:1072935:1073279 [3] NCCL INFO [Service thread] Connection closed by localRank 0
f17n01:969558:969907 [1] NCCL INFO [Service thread] Connection closed by localRank 0
g04n17:914199:914543 [3] NCCL INFO [Service thread] Connection closed by localRank 0
d11n17:1141832:1142173 [0] NCCL INFO [Service thread] Connection closed by localRank 0
d11n16:1129250:1129589 [5] NCCL INFO [Service thread] Connection closed by localRank 1
f16n18:1091138:1091138 [5] NCCL INFO comm 0x149f40fb0 rank 5 nranks 12 cudaDev 5 busId 3505000 - Abort COMPLETE
g04n17:914201:914201 [5] NCCL INFO comm 0x14b1fbe40 rank 8 nranks 12 cudaDev 5 busId 3505000 - Abort COMPLETE
f16n18:1091130:1091130 [0] NCCL INFO comm 0x141cc0f20 rank 4 nranks 12 cudaDev 0 busId 404000 - Abort COMPLETE
f16n18:1091138:1091493 [5] NCCL INFO [Service thread] Connection closed by localRank 1
f16n16:1079192:1079192 [0] NCCL INFO comm 0x145e357e0 rank 3 nranks 12 cudaDev 0 busId 404000 - Abort COMPLETE
g05n10:696644:696644 [1] NCCL INFO comm 0x150fe45f0 rank 11 nranks 12 cudaDev 1 busId 405000 - Abort COMPLETE
f16n16:1079195:1079195 [2] NCCL INFO comm 0x147a5cd30 rank 6 nranks 24 cudaDev 2 busId 406000 - Abort COMPLETE
d09n08:1072932:1073199 [0] NCCL INFO [Service thread] Connection closed by localRank 0
g05n08:178236:178509 [0] NCCL INFO [Service thread] Connection closed by localRank 0
g05n08:178243:178243 [4] NCCL INFO comm 0x147a1e7a0 rank 10 nranks 12 cudaDev 4 busId 3504000 - Abort COMPLETE
g05n10:696651:696651 [5] NCCL INFO comm 0x16b2e9120 rank 11 nranks 12 cudaDev 5 busId 3505000 - Abort COMPLETE
d09n07:1061890:1062323 [3] NCCL INFO [Service thread] Connection closed by localRank 0
f17n01:969560:969560 [2] NCCL INFO comm 0x1600bfd70 rank 5 nranks 12 cudaDev 2 busId 406000 - Abort COMPLETE
d11n17:1141835:1141835 [3] NCCL INFO comm 0x157f99700 rank 2 nranks 12 cudaDev 3 busId 3503000 - Abort COMPLETE
d09n07:1061888:1061888 [1] NCCL INFO comm 0x12b6e2960 rank 0 nranks 12 cudaDev 1 busId 405000 - Abort COMPLETE
d11n16:1129249:1129249 [4] NCCL INFO comm 0x12a5fd360 rank 2 nranks 12 cudaDev 4 busId 3504000 - Abort COMPLETE
f16n16:1079193:1079539 [1] NCCL INFO [Service thread] Connection closed by localRank 0
d09n08:1072934:1072934 [2] NCCL INFO comm 0x1695c8970 rank 1 nranks 12 cudaDev 2 busId 406000 - Abort COMPLETE
f17n02:1075538:1075910 [5] NCCL INFO [Service thread] Connection closed by localRank 1
d11n16:1129246:1129590 [1] NCCL INFO [Service thread] Connection closed by localRank 0
g05n10:696650:696650 [4] NCCL INFO comm 0x153babd60 rank 23 nranks 24 cudaDev 4 busId 3504000 - Abort COMPLETE
f18n05:1007504:1007504 [4] NCCL INFO comm 0x13476afb0 rank 7 nranks 12 cudaDev 4 busId 3504000 - Abort COMPLETE
f16n16:1079197:1079197 [3] NCCL INFO comm 0x125f98660 rank 3 nranks 12 cudaDev 3 busId 3503000 - Abort COMPLETE
d11n16:1129245:1129245 [0] NCCL INFO comm 0x13996f040 rank 3 nranks 24 cudaDev 0 busId 404000 - Abort COMPLETE
f16n17:1090241:1090241 [4] NCCL INFO comm 0x154fdc740 rank 4 nranks 12 cudaDev 4 busId 3504000 - Abort COMPLETE
f18n05:1007497:1007497 [0] NCCL INFO comm 0x14fa003b0 rank 6 nranks 12 cudaDev 0 busId 404000 - Abort COMPLETE
f16n16:1079195:1079462 [2] NCCL INFO [Service thread] Connection closed by localRank 2
f17n02:1075537:1075537 [4] NCCL INFO comm 0x1606c8da0 rank 6 nranks 12 cudaDev 4 busId 3504000 - Abort COMPLETE
f18n06:1026682:1026682 [3] NCCL INFO comm 0x15bc52420 rank 7 nranks 12 cudaDev 3 busId 3503000 - Abort COMPLETE
g05n10:696643:696643 [0] NCCL INFO comm 0x168625130 rank 11 nranks 12 cudaDev 0 busId 404000 - Abort COMPLETE
f18n06:1026686:1026686 [5] NCCL INFO comm 0x1382caa60 rank 8 nranks 12 cudaDev 5 busId 3505000 - Abort COMPLETE
g04n18:916021:916021 [0] NCCL INFO comm 0x145ed2de0 rank 9 nranks 12 cudaDev 0 busId 404000 - Abort COMPLETE
g04n17:914193:914193 [0] NCCL INFO comm 0x12c6f7a80 rank 8 nranks 12 cudaDev 0 busId 404000 - Abort COMPLETE
g05n09:221468:221808 [4] NCCL INFO [Service thread] Connection closed by localRank 1
g05n10:696644:696989 [1] NCCL INFO [Service thread] Connection closed by localRank 0
f18n05:1007502:1007502 [3] NCCL INFO comm 0x1508e9a40 rank 14 nranks 24 cudaDev 3 busId 3503000 - Abort COMPLETE
d09n08:1072934:1073278 [2] NCCL INFO [Service thread] Connection closed by localRank 0
f18n05:1007502:1007768 [3] NCCL INFO [Service thread] Connection closed by localRank 3
f16n16:1079192:1079536 [0] NCCL INFO [Service thread] Connection closed by localRank 0
f17n02:1075537:1075907 [4] NCCL INFO [Service thread] Connection closed by localRank 1
d11n16:1129249:1129587 [4] NCCL INFO [Service thread] Connection closed by localRank 1
g05n10:696651:696988 [5] NCCL INFO [Service thread] Connection closed by localRank 1
d11n16:1129245:1129513 [0] NCCL INFO [Service thread] Connection closed by localRank 0
d11n17:1141835:1142180 [3] NCCL INFO [Service thread] Connection closed by localRank 0
f17n01:969560:969909 [2] NCCL INFO [Service thread] Connection closed by localRank 0
g04n17:914201:914540 [5] NCCL INFO [Service thread] Connection closed by localRank 1
f16n18:1091130:1091491 [0] NCCL INFO [Service thread] Connection closed by localRank 0
g04n18:916021:916371 [0] NCCL INFO [Service thread] Connection closed by localRank 0
d11n17:1141837:1141837 [5] NCCL INFO comm 0x141cdde00 rank 2 nranks 12 cudaDev 5 busId 3505000 - Abort COMPLETE
f16n18:1091134:1091134 [3] NCCL INFO comm 0x1589f3680 rank 4 nranks 12 cudaDev 3 busId 3503000 - Abort COMPLETE
f16n17:1090241:1090582 [4] NCCL INFO [Service thread] Connection closed by localRank 1
f18n05:1007497:1007841 [0] NCCL INFO [Service thread] Connection closed by localRank 0
f18n05:1007504:1007840 [4] NCCL INFO [Service thread] Connection closed by localRank 1
g04n18:916029:916029 [5] NCCL INFO comm 0x156f108e0 rank 9 nranks 12 cudaDev 5 busId 3505000 - Abort COMPLETE
d11n17:1141834:1141834 [2] NCCL INFO comm 0x14c0b11c0 rank 5 nranks 24 cudaDev 2 busId 406000 - Abort COMPLETE
g05n09:221467:221467 [3] NCCL INFO comm 0x15063e300 rank 21 nranks 24 cudaDev 3 busId 3503000 - Abort COMPLETE
g04n18:916028:916028 [4] NCCL INFO comm 0x14bb83460 rank 9 nranks 12 cudaDev 4 busId 3504000 - Abort COMPLETE
g05n08:178243:178582 [4] NCCL INFO [Service thread] Connection closed by localRank 1
g05n10:696650:696912 [4] NCCL INFO [Service thread] Connection closed by localRank 4
g04n18:916029:916369 [5] NCCL INFO [Service thread] Connection closed by localRank 1
d09n07:1061888:1062327 [1] NCCL INFO [Service thread] Connection closed by localRank 0
g04n17:914196:914196 [2] NCCL INFO comm 0x17ae45800 rank 17 nranks 24 cudaDev 2 busId 406000 - Abort COMPLETE
f16n17:1090235:1090235 [1] NCCL INFO comm 0x14058fa00 rank 7 nranks 24 cudaDev 1 busId 405000 - Abort COMPLETE
g04n17:914193:914539 [0] NCCL INFO [Service thread] Connection closed by localRank 0
f16n16:1079197:1079534 [3] NCCL INFO [Service thread] Connection closed by localRank 0
g04n18:916027:916027 [3] NCCL INFO comm 0x1696c1550 rank 18 nranks 24 cudaDev 3 busId 3503000 - Abort COMPLETE
f16n18:1091134:1091488 [3] NCCL INFO [Service thread] Connection closed by localRank 0
d09n07:1061887:1061887 [0] NCCL INFO comm 0x13e012520 rank 0 nranks 24 cudaDev 0 busId 404000 - Abort COMPLETE
g05n10:696643:696986 [0] NCCL INFO [Service thread] Connection closed by localRank 0
g05n09:221467:221731 [3] NCCL INFO [Service thread] Connection closed by localRank 3
f18n06:1026682:1027026 [3] NCCL INFO [Service thread] Connection closed by localRank 0
f18n06:1026686:1027029 [5] NCCL INFO [Service thread] Connection closed by localRank 1
f17n01:969561:969561 [3] NCCL INFO comm 0x1311b56b0 rank 11 nranks 24 cudaDev 3 busId 3503000 - Abort COMPLETE
f17n02:1075533:1075533 [2] NCCL INFO comm 0x1392fd170 rank 12 nranks 24 cudaDev 2 busId 406000 - Abort COMPLETE
g04n18:916027:916294 [3] NCCL INFO [Service thread] Connection closed by localRank 3
f16n18:1091137:1091137 [4] NCCL INFO comm 0x15f8c47a0 rank 5 nranks 12 cudaDev 4 busId 3504000 - Abort COMPLETE
d09n07:1061889:1061889 [2] NCCL INFO comm 0x131e2b7c0 rank 0 nranks 24 cudaDev 2 busId 406000 - Abort COMPLETE
f17n02:1075530:1075530 [0] NCCL INFO comm 0x14545f6f0 rank 12 nranks 24 cudaDev 0 busId 404000 - Abort COMPLETE
f18n05:1007498:1007498 [1] NCCL INFO comm 0x14a2991c0 rank 13 nranks 24 cudaDev 1 busId 405000 - Abort COMPLETE
f16n17:1090235:1090509 [1] NCCL INFO [Service thread] Connection closed by localRank 1
d09n07:1061889:1062224 [2] NCCL INFO [Service thread] Connection closed by localRank 2
f17n02:1075530:1075806 [0] NCCL INFO [Service thread] Connection closed by localRank 0
f16n18:1091137:1091490 [4] NCCL INFO [Service thread] Connection closed by localRank 1
d11n17:1141837:1142177 [5] NCCL INFO [Service thread] Connection closed by localRank 1
d11n17:1141834:1142101 [2] NCCL INFO [Service thread] Connection closed by localRank 2
f17n01:969561:969830 [3] NCCL INFO [Service thread] Connection closed by localRank 3
g04n17:914196:914465 [2] NCCL INFO [Service thread] Connection closed by localRank 2
g04n18:916028:916368 [4] NCCL INFO [Service thread] Connection closed by localRank 1
f17n01:969557:969557 [0] NCCL INFO comm 0x144c13920 rank 10 nranks 24 cudaDev 0 busId 404000 - Abort COMPLETE
d09n07:1061887:1062222 [0] NCCL INFO [Service thread] Connection closed by localRank 0
g04n17:914200:914200 [4] NCCL INFO comm 0x1506f1440 rank 17 nranks 24 cudaDev 4 busId 3504000 - Abort COMPLETE
g04n17:914200:914461 [4] NCCL INFO [Service thread] Connection closed by localRank 4
d11n17:1141836:1141836 [4] NCCL INFO comm 0x164b8b260 rank 5 nranks 24 cudaDev 4 busId 3504000 - Abort COMPLETE
d09n08:1072936:1072936 [4] NCCL INFO comm 0x1461df130 rank 2 nranks 24 cudaDev 4 busId 3504000 - Abort COMPLETE
f17n02:1075534:1075534 [3] NCCL INFO comm 0x151ee0020 rank 12 nranks 24 cudaDev 3 busId 3503000 - Abort COMPLETE
d11n16:1129247:1129247 [2] NCCL INFO comm 0x148324850 rank 14 nranks 96 cudaDev 2 busId 406000 - Abort COMPLETE
d11n17:1141833:1141833 [1] NCCL INFO comm 0x1451c7f20 rank 4 nranks 24 cudaDev 1 busId 405000 - Abort COMPLETE
f17n02:1075533:1075804 [2] NCCL INFO [Service thread] Connection closed by localRank 2
g04n18:916022:916022 [1] NCCL INFO comm 0x1267895e0 rank 18 nranks 24 cudaDev 1 busId 405000 - Abort COMPLETE
g05n09:221464:221464 [2] NCCL INFO comm 0x147913a90 rank 86 nranks 96 cudaDev 2 busId 406000 - Abort COMPLETE
f18n05:1007498:1007771 [1] NCCL INFO [Service thread] Connection closed by localRank 1
d09n07:1061891:1061891 [4] NCCL INFO comm 0x121c33b00 rank 4 nranks 96 cudaDev 4 busId 3504000 - Abort COMPLETE
g05n08:178242:178242 [3] NCCL INFO comm 0x15f4c3eb0 rank 81 nranks 96 cudaDev 3 busId 3503000 - Abort COMPLETE
d11n17:1141833:1142099 [1] NCCL INFO [Service thread] Connection closed by localRank 1
d11n17:1141836:1142104 [4] NCCL INFO [Service thread] Connection closed by localRank 4
f16n18:1091133:1091133 [2] NCCL INFO comm 0x15e9982b0 rank 9 nranks 24 cudaDev 2 busId 406000 - Abort COMPLETE
f17n01:969557:969831 [0] NCCL INFO [Service thread] Connection closed by localRank 0
f18n05:1007505:1007505 [5] NCCL INFO comm 0x13ced3c70 rank 14 nranks 24 cudaDev 5 busId 3505000 - Abort COMPLETE
f16n16:1079199:1079199 [4] NCCL INFO comm 0x14996df00 rank 7 nranks 24 cudaDev 4 busId 3504000 - Abort COMPLETE
d09n08:1072936:1073196 [4] NCCL INFO [Service thread] Connection closed by localRank 4
g05n08:178239:178239 [2] NCCL INFO comm 0x17064d0d0 rank 20 nranks 24 cudaDev 2 busId 406000 - Abort COMPLETE
d11n16:1129248:1129248 [3] NCCL INFO comm 0x136ceb2f0 rank 3 nranks 24 cudaDev 3 busId 3503000 - Abort COMPLETE
g05n10:696649:696649 [3] NCCL INFO comm 0x14a512630 rank 23 nranks 24 cudaDev 3 busId 3503000 - Abort COMPLETE
f17n02:1075534:1075803 [3] NCCL INFO [Service thread] Connection closed by localRank 3
f18n06:1026681:1026681 [2] NCCL INFO comm 0x140bccaf0 rank 15 nranks 24 cudaDev 2 busId 406000 - Abort COMPLETE
d11n16:1129248:1129511 [3] NCCL INFO [Service thread] Connection closed by localRank 3
f16n17:1090234:1090234 [0] NCCL INFO comm 0x1548a1a10 rank 7 nranks 24 cudaDev 0 busId 404000 - Abort COMPLETE
g05n09:221469:221469 [5] NCCL INFO comm 0x16b654220 rank 89 nranks 96 cudaDev 5 busId 3505000 - Abort COMPLETE
d09n07:1061892:1061892 [5] NCCL INFO comm 0x132808a80 rank 1 nranks 24 cudaDev 5 busId 3505000 - Abort COMPLETE
f16n18:1091133:1091403 [2] NCCL INFO [Service thread] Connection closed by localRank 2
g05n09:221462:221462 [1] NCCL INFO comm 0x1255b8f20 rank 21 nranks 24 cudaDev 1 busId 405000 - Abort COMPLETE
g04n18:916022:916297 [1] NCCL INFO [Service thread] Connection closed by localRank 1
f16n18:1091131:1091131 [1] NCCL INFO comm 0x146d3a8c0 rank 9 nranks 24 cudaDev 1 busId 405000 - Abort COMPLETE
g04n17:914194:914194 [1] NCCL INFO comm 0x13a683900 rank 16 nranks 24 cudaDev 1 busId 405000 - Abort COMPLETE
f17n01:969564:969564 [4] NCCL INFO comm 0x1494a1360 rank 11 nranks 24 cudaDev 4 busId 3504000 - Abort COMPLETE
f16n16:1079200:1079200 [5] NCCL INFO comm 0x132853180 rank 7 nranks 24 cudaDev 5 busId 3505000 - Abort COMPLETE
g05n10:696649:696913 [3] NCCL INFO [Service thread] Connection closed by localRank 3
f18n05:1007505:1007769 [5] NCCL INFO [Service thread] Connection closed by localRank 5
f18n06:1026681:1026946 [2] NCCL INFO [Service thread] Connection closed by localRank 2
f18n06:1026685:1026685 [4] NCCL INFO comm 0x15c644450 rank 64 nranks 96 cudaDev 4 busId 3504000 - Abort COMPLETE
f16n16:1079199:1079465 [4] NCCL INFO [Service thread] Connection closed by localRank 4
g05n08:178239:178507 [2] NCCL INFO [Service thread] Connection closed by localRank 2
f16n17:1090234:1090508 [0] NCCL INFO [Service thread] Connection closed by localRank 0
d09n08:1072937:1072937 [5] NCCL INFO comm 0x15a4f43a0 rank 11 nranks 96 cudaDev 5 busId 3505000 - Abort COMPLETE
g04n17:914194:914466 [1] NCCL INFO [Service thread] Connection closed by localRank 1
f17n01:969564:969829 [4] NCCL INFO [Service thread] Connection closed by localRank 4
d09n08:1072933:1072933 [1] NCCL INFO comm 0x15c70e000 rank 1 nranks 24 cudaDev 1 busId 405000 - Abort COMPLETE
f16n17:1090239:1090239 [3] NCCL INFO comm 0x168088520 rank 8 nranks 24 cudaDev 3 busId 3503000 - Abort COMPLETE
f16n18:1091131:1091402 [1] NCCL INFO [Service thread] Connection closed by localRank 1
g05n08:178237:178237 [1] NCCL INFO comm 0x116457d30 rank 19 nranks 24 cudaDev 1 busId 405000 - Abort COMPLETE
d09n07:1061892:1062225 [5] NCCL INFO [Service thread] Connection closed by localRank 5
g04n18:916024:916024 [2] NCCL INFO comm 0x174445820 rank 18 nranks 24 cudaDev 2 busId 406000 - Abort COMPLETE
f17n02:1075531:1075531 [1] NCCL INFO comm 0x16ec68420 rank 12 nranks 24 cudaDev 1 busId 405000 - Abort COMPLETE
f17n02:1075531:1075805 [1] NCCL INFO [Service thread] Connection closed by localRank 1
f16n16:1079200:1079464 [5] NCCL INFO [Service thread] Connection closed by localRank 5
g05n10:696646:696646 [2] NCCL INFO comm 0x1414de700 rank 23 nranks 24 cudaDev 2 busId 406000 - Abort COMPLETE
f18n06:1026678:1026678 [0] NCCL INFO comm 0x15b121720 rank 15 nranks 24 cudaDev 0 busId 404000 - Abort COMPLETE
f17n01:969565:969565 [5] NCCL INFO comm 0x1717ed4e0 rank 11 nranks 24 cudaDev 5 busId 3505000 - Abort COMPLETE
g05n09:221462:221736 [1] NCCL INFO [Service thread] Connection closed by localRank 1
f18n06:1026679:1026679 [1] NCCL INFO comm 0x149f339d0 rank 61 nranks 96 cudaDev 1 busId 405000 - Abort COMPLETE
g05n08:178244:178244 [5] NCCL INFO comm 0x15ee4e6a0 rank 20 nranks 24 cudaDev 5 busId 3505000 - Abort COMPLETE
g04n17:914199:914199 [3] NCCL INFO comm 0x15d2a9390 rank 17 nranks 24 cudaDev 3 busId 3503000 - Abort COMPLETE
d11n17:1141832:1141832 [0] NCCL INFO comm 0x16279cab0 rank 4 nranks 24 cudaDev 0 busId 404000 - Abort COMPLETE
f18n05:1007500:1007500 [2] NCCL INFO comm 0x13b9e2670 rank 14 nranks 24 cudaDev 2 busId 406000 - Abort COMPLETE
f17n02:1075538:1075538 [5] NCCL INFO comm 0x1776b1040 rank 13 nranks 24 cudaDev 5 busId 3505000 - Abort COMPLETE
f17n01:969558:969558 [1] NCCL INFO comm 0x17d124bc0 rank 10 nranks 24 cudaDev 1 busId 405000 - Abort COMPLETE
g05n09:221461:221461 [0] NCCL INFO comm 0x11dc12d00 rank 21 nranks 24 cudaDev 0 busId 404000 - Abort COMPLETE
d11n16:1129246:1129246 [1] NCCL INFO comm 0x145b57760 rank 3 nranks 24 cudaDev 1 busId 405000 - Abort COMPLETE
g04n18:916024:916296 [2] NCCL INFO [Service thread] Connection closed by localRank 2
d11n16:1129250:1129250 [5] NCCL INFO comm 0x153c184f0 rank 4 nranks 24 cudaDev 5 busId 3505000 - Abort COMPLETE
g05n09:221461:221734 [0] NCCL INFO [Service thread] Connection closed by localRank 0
g05n10:696646:696914 [2] NCCL INFO [Service thread] Connection closed by localRank 2
f18n05:1007500:1007770 [2] NCCL INFO [Service thread] Connection closed by localRank 2
f18n06:1026678:1026951 [0] NCCL INFO [Service thread] Connection closed by localRank 0
f16n17:1090239:1090510 [3] NCCL INFO [Service thread] Connection closed by localRank 3
g05n08:178237:178508 [1] NCCL INFO [Service thread] Connection closed by localRank 1
d09n08:1072933:1073200 [1] NCCL INFO [Service thread] Connection closed by localRank 1
g05n08:178244:178505 [5] NCCL INFO [Service thread] Connection closed by localRank 5
d09n07:1061890:1061890 [3] NCCL INFO comm 0x1777333f0 rank 0 nranks 24 cudaDev 3 busId 3503000 - Abort COMPLETE
f17n01:969565:969828 [5] NCCL INFO [Service thread] Connection closed by localRank 5
f17n01:969558:969832 [1] NCCL INFO [Service thread] Connection closed by localRank 1
g04n17:914199:914463 [3] NCCL INFO [Service thread] Connection closed by localRank 3
f16n17:1090237:1090237 [2] NCCL INFO comm 0x16cd269a0 rank 8 nranks 24 cudaDev 2 busId 406000 - Abort COMPLETE
d11n17:1141832:1142100 [0] NCCL INFO [Service thread] Connection closed by localRank 0
d11n16:1129250:1129509 [5] NCCL INFO [Service thread] Connection closed by localRank 5
g05n09:221468:221468 [4] NCCL INFO comm 0x1813b2260 rank 22 nranks 24 cudaDev 4 busId 3504000 - Abort COMPLETE
d09n08:1072935:1072935 [3] NCCL INFO comm 0x145cfe9d0 rank 2 nranks 24 cudaDev 3 busId 3503000 - Abort COMPLETE
f16n16:1079193:1079193 [1] NCCL INFO comm 0x14becfd60 rank 6 nranks 24 cudaDev 1 busId 405000 - Abort COMPLETE
f16n18:1091138:1091138 [5] NCCL INFO comm 0x14b389c00 rank 10 nranks 24 cudaDev 5 busId 3505000 - Abort COMPLETE
g04n17:914201:914201 [5] NCCL INFO comm 0x14b2317c0 rank 17 nranks 24 cudaDev 5 busId 3505000 - Abort COMPLETE
g05n08:178236:178236 [0] NCCL INFO comm 0x135ac3f30 rank 78 nranks 96 cudaDev 0 busId 404000 - Abort COMPLETE
d09n08:1072932:1072932 [0] NCCL INFO comm 0x14e523960 rank 6 nranks 96 cudaDev 0 busId 404000 - Abort COMPLETE
f16n18:1091130:1091130 [0] NCCL INFO comm 0x141ced8b0 rank 9 nranks 24 cudaDev 0 busId 404000 - Abort COMPLETE
g05n10:696644:696644 [1] NCCL INFO comm 0x150f18970 rank 22 nranks 24 cudaDev 1 busId 405000 - Abort COMPLETE
d09n07:1061890:1062227 [3] NCCL INFO [Service thread] Connection closed by localRank 3
f16n16:1079192:1079192 [0] NCCL INFO comm 0x145fa20e0 rank 6 nranks 24 cudaDev 0 busId 404000 - Abort COMPLETE
g05n10:696651:696651 [5] NCCL INFO comm 0x16a3535a0 rank 23 nranks 24 cudaDev 5 busId 3505000 - Abort COMPLETE
g05n08:178243:178243 [4] NCCL INFO comm 0x146b54830 rank 20 nranks 24 cudaDev 4 busId 3504000 - Abort COMPLETE
f16n16:1079193:1079461 [1] NCCL INFO [Service thread] Connection closed by localRank 1
f17n02:1075538:1075802 [5] NCCL INFO [Service thread] Connection closed by localRank 5
d11n16:1129246:1129514 [1] NCCL INFO [Service thread] Connection closed by localRank 1
f17n01:969560:969560 [2] NCCL INFO comm 0x16116bb90 rank 11 nranks 24 cudaDev 2 busId 406000 - Abort COMPLETE
d11n17:1141835:1141835 [3] NCCL INFO comm 0x15849a2b0 rank 5 nranks 24 cudaDev 3 busId 3503000 - Abort COMPLETE
f18n05:1007504:1007504 [4] NCCL INFO comm 0x1345cb160 rank 14 nranks 24 cudaDev 4 busId 3504000 - Abort COMPLETE
f16n16:1079195:1079195 [2] NCCL INFO comm 0x145123560 rank 26 nranks 96 cudaDev 2 busId 406000 - Abort COMPLETE

------------------------------------------------------------
Sender: LSF System <lsfadmin@batch5>
Subject: Job 3330155: <gpt3-megatron> in cluster <summit> Done

Job <gpt3-megatron> was submitted from host <login3> by user <sajaldash> in cluster <summit> at Mon Mar  4 16:41:51 2024
Job was executed on host(s) <1*batch5>, in queue <debug>, as user <sajaldash> in cluster <summit> at Mon Mar  4 16:41:58 2024
                            <42*d09n07>
                            <42*d09n08>
                            <42*d11n16>
                            <42*d11n17>
                            <42*f16n16>
                            <42*f16n17>
                            <42*f16n18>
                            <42*f17n01>
                            <42*f17n02>
                            <42*f18n05>
                            <42*f18n06>
                            <42*g04n17>
                            <42*g04n18>
                            <42*g05n08>
                            <42*g05n09>
                            <42*g05n10>
</ccs/home/sajaldash> was used as the home directory.
</gpfs/alpine2/stf218/world-shared/sajal/Megatron-DeepSpeed> was used as the working directory.
Started at Mon Mar  4 16:41:58 2024
Terminated at Mon Mar  4 16:43:01 2024
Results reported at Mon Mar  4 16:43:01 2024

The output (if any) is above this job summary.



PS:

Read file <logs/gpt3-megatron.3330155.e> for stderr output of this job.

